repo_name,readme_content
public-apis,"# Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

<br >

<p>
    <a href=""https://apilayer.com"">
        <div>
            <img src="".github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png"" width=""100%"" alt=""APILayer Logo"" />
        </div>
    </a>
  </p>

[APILayer](https://apilayer.com/?utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo) is the fastest way to integrate APIs into any product. There are a lot of APIs available at [APILayer Marketplace](https://apilayer.com/#bestSellers&utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo).

<br >

## APILayer APIs
| API | Description | Call this"
system-design-primer,*[English](README.md) ∙ [日本語](README-ja.md) ∙ [简体中文](README-zh-Hans.md) ∙ [繁體中文](README-zh-TW.md) | [العَرَبِيَّة‎](https://github.com/donnemartin/system-design-primer/issues/170) ∙ [বাংলা](https://github.com/donnemartin/system-design-primer/issues/220) ∙ [Português do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) ∙ [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) ∙ [ελληνικά](https://github.com/donnemartin/system-design-primer/issues/130) ∙ [עברית](https://github.com/donnemartin/system-design-primer/issues/272) ∙ [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) ∙ [한국어](https://github.com/donnemartin/system-design-primer/issues/102) ∙ [فارسی](https://github.com/donnemartin/system-design-primer/issues/110) ∙ [Polski](https://github.com/donnemartin/system-design-primer/issues/68) ∙ [русский язык](https://github.com/donnemartin/system-design-primer/issues/87) ∙ [Español](https://github.com/donnemartin/system-desig
awesome-python,"# Awesome Python [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

An opinionated list of awesome Python frameworks, libraries, software and resources.

Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).

- [Awesome Python](#awesome-python)
    - [Admin Panels](#admin-panels)
    - [Algorithms and Design Patterns](#algorithms-and-design-patterns)
    - [ASGI Servers](#asgi-servers)
    - [Asynchronous Programming](#asynchronous-programming)
    - [Audio](#audio)
    - [Authentication](#authentication)
    - [Build Tools](#build-tools)
    - [Built-in Classes Enhancement](#built-in-classes-enhancement)
    - [Caching](#caching)
    - [ChatOps Tools](#chatops-tools)
    - [CMS](#cms)
    - [Code Analysis](#code-analysis)
    - [Command-line Interface Development](#command-line-interface-development)
    - [Command-line Tools](#command-line-tools)
    - [Computer Visio"
Python,"<div align=""center"">
<!-- Title: -->
  <a href=""https://github.com/TheAlgorithms/"">
    <img src=""https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg"" height=""100"">
  </a>
  <h1><a href=""https://github.com/TheAlgorithms/"">The Algorithms</a> - Python</h1>
<!-- Labels: -->
  <!-- First row: -->
  <a href=""https://gitpod.io/#https://github.com/TheAlgorithms/Python"">
    <img src=""https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&style=flat-square"" height=""20"" alt=""Gitpod Ready-to-Code"">
  </a>
  <a href=""https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md"">
    <img src=""https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square"" height=""20"" alt=""Contributions Welcome"">
  </a>
  <img src=""https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&style=flat-square"" height=""20"">
  <a href=""https://the-algorithms.com/discord"">
 "
AutoGPT,"# AutoGPT: Build, Deploy, and Run AI Agents

[![Discord Follow](https://dcbadge.vercel.app/api/server/autogpt?style=flat)](https://discord.gg/autogpt) &ensp;
[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. 

## Hosting Options 
   - Download to self-host
   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta  

## How to Setup for Self-Hosting
> [!NOTE]
> Setting up and hosting the AutoGPT Platform yourself is a technical process. 
> If you'd rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.

https://github.com/user-attachments/assets/d04273a5-b36a-4a37-818e-f631ce72d603

This tutorial as"
Python-100-Days,"## Python - 100天从新手到大师

> **作者**：骆昊
>
> **说明**：从项目上线到获得8w+星标以来，一直收到反馈说基础部分（前15天的内容）对新手来说是比较困难的，建议有配套视频进行讲解。最近把基础部分的内容重新制作了一个名为[“Python-Core-50-Courses”](<https://github.com/jackfrued/Python-Core-50-Courses>)的项目，用更为简单通俗的方式重写了这部分内容并附带了视频讲解，初学者可以看看这个新的仓库。国内用户如果访问GitHub比较慢的话，可以关注我的**知乎号[Python-Jack](https://www.zhihu.com/people/jackfrued)**，上面的[“从零开始学Python”](<https://zhuanlan.zhihu.com/c_1216656665569013760>)专栏比较适合初学者，其他的专栏如“数据思维和统计思维”、“基于Python的数据分析”等也在持续创作和更新中，欢迎大家关注、点赞和评论。
>
> 想获取学习视频的小伙伴，大家可以扫描下面的二维码进入微信小程序，看看有没有适合自己的内容。大家心心念念的机器学习的内容在小程序中都可以找到，由我和我的同事为大家录制的。
>
> <img src=""res/study_card.png"" style=""zoom:20%;"">
>
> 大家在学习过程中如果遇到一些棘手的问题或者需要相关的学习资源，可以加入下面的QQ交流群，三个群是一样的加入一个即可，请不要重复加群，也不要在群里发布广告和其他色情、低俗或敏感内容。**如果缺乏自律性，有付费学习的需求，可以添加我的微信（jackfrued）私聊，备注好自己的称呼和需求，我会给大家提供一些学习方案和职业规划方面的指导**。
>
> <img src=""res/python_study_qq_group.png"" style=""zoom:30%;"">
>
> 配套的视频在抖音和B站持续更新中，有兴趣的小伙伴可以关注我的抖音或B站账号，最近刚刚起号，还希望大家多多支持，非常感谢您！
>
> <img src=""res/qrcode.JPG"" style=""zoom:20%;"">
>
> 大家一直催更的《机器学习和深度学习》因个人和公"
stable-diffusion-webui,"# Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- T"
transformers,"<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg"">
    <img alt=""Hugging Face Transformers Library"" src="
youtube-dl,"[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI)


youtube-dl - download videos from youtube.com or other video platforms

- [INSTALLATION](#installation)
- [DESCRIPTION](#description)
- [OPTIONS](#options)
- [CONFIGURATION](#configuration)
- [OUTPUT TEMPLATE](#output-template)
- [FORMAT SELECTION](#format-selection)
- [VIDEO SELECTION](#video-selection)
- [FAQ](#faq)
- [DEVELOPER INSTRUCTIONS](#developer-instructions)
- [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)

# INSTALLATION

To install it right away for all UNIX users (Linux, macOS, etc.), type:

    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
    sudo chmod a+rx /usr/local/bin/youtube-dl

If you do not have curl, you can alternatively use a recent wget:

    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
  "
HelloGitHub,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif""/>
  <br>中文 | <a href=""README_en.md"">English</a> | <a href=""README_ja.md"">日本語</a>
  <br>分享 GitHub 上有趣、入门级的开源项目。<br>兴趣是最好的老师，这里能够帮你找到编程的兴趣！
</p>

<p align=""center"">
  <a href=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png""><img src=""https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square"" alt=""WeiXin""></a>
  <a href=""https://github.com/521xueweihan/HelloGitHub/stargazers""><img src=""https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square"" alt=""GitHub stars""></a>
  <a href=""https://github.com/521xueweihan/HelloGitHub/issues""><img src=""https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square"" alt=""GitHub issues""></a>
    <a href=""https://weibo.com/hellogithub""><img src=""https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square"""
thefuck,"# The Fuck [![Version][version-badge]][version-link] [![Build Status][workflow-badge]][workflow-link] [![Coverage][coverage-badge]][coverage-link] [![MIT License][license-badge]](LICENSE.md)

*The Fuck* is a magnificent app, inspired by a [@liamosaur](https://twitter.com/liamosaur/)
[tweet](https://twitter.com/liamosaur/status/506975850596536320),
that corrects errors in previous console commands.


Is *The Fuck* too slow? [Try the experimental instant mode!](#experimental-instant-mode)

[![gif with examples][examples-link]][examples-link]

More examples:

```bash
➜ apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

➜ fuck
sudo apt-get install vim [enter/↑/↓/ctrl+c]
[sudo] password for nvbn:
Reading package lists... Done
...
```

```bash
➜ git push
fatal: The current branch master has no upstream branch.
To push the current branch and set the remote as upstream"
yt-dlp,"<!-- MANPAGE: BEGIN EXCLUDED SECTION -->
<div align=""center"">

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&label=Download&style=for-the-badge)](#installation ""Installation"")
[![PyPi](https://img.shields.io/badge/-PyPi-blue.svg?logo=pypi&labelColor=555555&style=for-the-badge)](https://pypi.org/project/yt-dlp ""PyPi"")
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&labelColor=555555&style=for-the-badge)](Collaborators.md#collaborators ""Donate"")
[![Matrix](https://img.shields.io/matrix/yt-dlp:matrix.org?color=brightgreen&labelColor=555555&label=&logo=element&style=for-the-badge)](https://matrix.to/#/#yt-dlp:matrix.org ""Matrix"")
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&labelColor=555555&label=&logo=discord&style=for-the-badge)](https://discord.gg/H5MNcFW63r ""Discord"")
[![Supported Sites]("
pytorch,"![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)

--------------------------------------------------------------------------------

PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.

Our trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).

<!-- toc -->

- [More About PyTorch](#more-about-pytorch)
  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)
  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)
  - [Python First](#python-first)
  - [Imperative Experiences](#imperative-experiences)
  - [Fast and Lean](#fast-and-lean)
  - [Extensions Without Pain](#ex"
django,"======
Django
======

Django is a high-level Python web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.

All documentation is in the ""``docs``"" directory and online at
https://docs.djangoproject.com/en/stable/. If you're just getting started,
here's how we recommend you read the docs:

* First, read ``docs/intro/install.txt`` for instructions on installing Django.

* Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,
  ``docs/intro/tutorial02.txt``, etc.).

* If you want to set up an actual deployment server, read
  ``docs/howto/deployment/index.txt`` for instructions.

* You'll probably want to read through the topical guides (in ``docs/topics``)
  next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific
  problems, and check out the reference (``docs/ref``) for gory details.

* See ``docs/README`` for instructions on building an HTML version of the docs.

Docs are updated rigorously. If yo"
models,"<div align=""center"">
  <img src=""https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png"">
</div>

[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)
[![tf-models-official PyPI](https://badge.fury.io/py/tf-models-official.svg)](https://badge.fury.io/py/tf-models-official)


# Welcome to the Model Garden for TensorFlow

The TensorFlow Model Garden is a repository with a number of different
implementations of state-of-the-art (SOTA) models and modeling solutions for
TensorFlow users. We aim to demonstrate the best practices for modeling so that
TensorFlow users can take full advantage of TensorFlow for their research and
product development.

To improve the transparency and reproducibility of our models, training logs on
[TensorBoard.dev](https://tensorboard.dev) are also provided for models to the
extent possible though not all models are suitable.

| Directory | Description |
|-----------|-------------|
"
fastapi,"<p align=""center"">
  <a href=""https://fastapi.tiangolo.com""><img src=""https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png"" alt=""FastAPI""></a>
</p>
<p align=""center"">
    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster"" target=""_blank"">
    <img src=""https://github.com/fastapi/fastapi/workflows/Test/badge.svg?event=push&branch=master"" alt=""Test"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg"" alt=""Coverage"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package"" alt=""Package version"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://i"
core,"Home Assistant |Chat Status|
=================================================================================

Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.

Check out `home-assistant.io <https://home-assistant.io>`__ for `a
demo <https://demo.home-assistant.io>`__, `installation instructions <https://home-assistant.io/getting-started/>`__,
`tutorials <https://home-assistant.io/getting-started/automation/>`__ and `documentation <https://home-assistant.io/docs/>`__.

This is a project of the `Open Home Foundation <https://www.openhomefoundation.org/>`__.

|screenshot-states|

Featured integrations
---------------------

|screenshot-integrations|

The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the `section on architecture <https://developers.home-assistant.io/docs/architec"
whisper,"# Whisper

[[Blog]](https://openai.com/blog/whisper)
[[Paper]](https://arxiv.org/abs/2212.04356)
[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)
[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.


## Approach

![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-proc"
funNLP,"<center>
    <img style=""border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"" 
    src=""./data/.logo图片/.img.jpg""width=""180"">
    <br>
    <div style=""color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"">NLP民工的乐园</div>
</center>
<br>

[![](https://img.shields.io/github/stars/fighting41love/funnlp?style=social)](https://github.com/fighting41love/funnlp)
[![](https://img.shields.io/badge/dynamic/json?color=blue&label=%E7%9F%A5%E4%B9%8E%E5%85%B3%E6%B3%A8&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dzhihu%26queryKey%3Dmountain-blue-64)](https://www.zhihu.com/people/mountain-blue-64)
[![](data/.logo图片/.捐赠图片/.Citations-487-red.svg)](https://scholar.google.com/citations?hl=en&user=aqZdfDUAAAAJ)

[![](data/.logo图片/.捐赠图片/.Home-%E4%BA%BA%E7%94%9F%E6%B5%AA%E8%B4%B9%E6%8C%87%E5%8D%97-brightgreen.svg)](http://fighting41love.github.io/archives/)
[![]"
flask,"# Flask

Flask is a lightweight [WSGI][] web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around [Werkzeug][]
and [Jinja][], and has become one of the most popular Python web
application frameworks.

Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.

[WSGI]: https://wsgi.readthedocs.io/
[Werkzeug]: https://werkzeug.palletsprojects.com/
[Jinja]: https://jinja.palletsprojects.com/


## A Simple Example

```python
# save this as app.py
from flask import Flask

app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello, World!""
```

```
$ flask run
  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```


## Donate

The Pallets organization develops and sup"
devops-exercises,"<p align=""center""><img src=""images/devops_exercises.png""/></p>

:information_source: &nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE

:bar_chart: &nbsp;There are currently **2624** exercises and questions

:warning: &nbsp;You can use these for preparing for an interview but most of the questions and exercises don't represent an actual interview. Please read [FAQ page](faq.md) for more details

:stop_sign: &nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it's not about learning all the topics and technologies mentioned in this repository

:pencil: &nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines [here](CONTRIBUTING.md)

****

<!-- ALL-TOPICS-LIST:START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<center>
<table>
  <tr>
    <td align=""center""><a href=""t"
awesome-machine-learning,"# Awesome Machine Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![Track Awesome List](https://www.trackawesomelist.com/badge.svg)](https://www.trackawesomelist.com/josephmisiti/awesome-machine-learning/)

A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by `awesome-php`.

_If you want to contribute to this list (please do), send me a pull request or contact me [@josephmisiti](https://twitter.com/josephmisiti)._
Also, a listed repository should be deprecated if:

* Repository's owner explicitly says that ""this library is not maintained"".
* Not committed for a long time (2~3 years).

Further resources:

* For a list of free machine learning books available for download, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md).

* For a list of professional machine learning events, go [h"
gpt_academic,"> [!IMPORTANT]
> 2024.6.1: 版本3.80加入插件二级菜单功能（详见wiki）  
> 2024.5.1: 加入Doc2x翻译PDF论文的功能，[查看详情](https://github.com/binary-husky/gpt_academic/wiki/Doc2x)  
> 2024.3.11: 全力支持Qwen、GLM、DeepseekCoder等中文大语言模型！ SoVits语音克隆模块，[查看详情](https://www.bilibili.com/video/BV1Rp421S7tF/) 
> 2024.1.17: 安装依赖时，请选择`requirements.txt`中**指定的版本**。 安装命令：`pip install -r requirements.txt`。本项目完全开源免费，您可通过订阅[在线服务](https://github.com/binary-husky/gpt_academic/wiki/online)的方式鼓励本项目的发展。

<br>

<div align=center>
<h1 aligh=""center"">
<img src=""docs/logo.png"" width=""40""> GPT 学术优化 (GPT Academic)
</h1>

[![Github][Github-image]][Github-url]
[![License][License-image]][License-url]
[![Releases][Releases-image]][Releases-url]
[![Installation][Installation-image]][Installation-url]
[![Wiki][Wiki-image]][Wiki-url]
[![PR][PRs-image]][PRs-url]

[Github-image]: https://img.shields.io/badge/github-12100E.svg?style=flat-square
[License-image]: https://img.shields.io/github/license/binary-husky/gpt_academic?label=License&style=flat-square&co"
cpython,"This is Python version 3.14.0 alpha 0
=====================================

.. image:: https://github.com/python/cpython/actions/workflows/build.yml/badge.svg?branch=main&event=push
   :alt: CPython build status on GitHub Actions
   :target: https://github.com/python/cpython/actions

.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main
   :alt: CPython build status on Azure DevOps
   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=main

.. image:: https://img.shields.io/badge/discourse-join_chat-brightgreen.svg
   :alt: Python Discourse chat
   :target: https://discuss.python.org/


Copyright © 2001-2024 Python Software Foundation.  All rights reserved.

See the end of this file for further copyright and license information.

.. contents::

General Information
-------------------

- Website: https://www.python.org
- Source code: https://github.com/python/cpython
- Issue tracker: https://github.c"
ansible,"[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![A"
manim,"<p align=""center"">
    <a href=""https://github.com/3b1b/manim"">
        <img src=""https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png"">
    </a>
</p>

[![pypi version](https://img.shields.io/pypi/v/manimgl?logo=pypi)](https://pypi.org/project/manimgl/)
[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)
[![Manim Subreddit](https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&label=reddit&logo=reddit)](https://www.reddit.com/r/manim/)
[![Manim Discord](https://img.shields.io/discord/581738731934056449.svg?label=discord&logo=discord)](https://discord.com/invite/bYCyhM9Kz2)
[![docs](https://github.com/3b1b/manim/workflows/docs/badge.svg)](https://3b1b.github.io/manim/)

Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.

Note, there are two versions of manim.  This repository began as a personal project by the author of [3Blue1Brown"
d2l-zh,"# 动手学深度学习（Dive into Deep Learning，D2L.ai）

[第二版：zh.D2L.ai](https://zh.d2l.ai)  | [第一版：zh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  安装和使用书中源代码： [第二版](https://zh.d2l.ai/chapter_installation/index.html) [第一版](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)

<h5 align=""center""><i>理解深度学习的最佳方法是学以致用。</i></h5>

<p align=""center"">
  <img width=""200""  src=""static/frontpage/_images/eq.jpg"">
  <img width=""200""  src=""static/frontpage/_images/figure.jpg"">
  <img width=""200""  src=""static/frontpage/_images/code.jpg"">
  <img width=""200""  src=""static/frontpage/_images/notebook.gif"">
</p>

本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。

我们的目标是创建一个为实现以下目标的统一资源：
1. 所有人均可在网上免费获取；
1. 提供足够的技术深度，从而帮助读者实际成为深度学习应用科学家：既理解数学原理，又能够实现并不断改进方法；
1. 包含可运行的代码，为读者展示如何在实际中解决问题。这样不仅直接将数学公式对应成实际代码，而且可以修改代码、观察结果并及时获取经验；
1. 允许我们和整个社区不断快速迭代内容，从而紧跟仍在高速发展的深度学习领域；
1. 由包含有关技术细节问答的论坛作为补充，使大家可以相互答疑并交换经验。

<h5 align=""center"">将本书（中英文版）用作教材或参考书的大学</h5>
<p align=""center"">
  <img width=""400""  src"
keras,"# Keras 3: Deep Learning for Humans

Keras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, and PyTorch.
Effortlessly build and train models for computer vision, natural language processing, audio processing,
timeseries forecasting, recommender systems, etc.

- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras
and the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.
- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),
leverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).
- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.

Join nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.


## Installation

### Install "
PayloadsAllTheThings,"# Payloads All The Things 

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !    
I :heart: pull requests :)

You can also contribute with a :beers: IRL, or using the sponsor button 

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.git"
gpt4free,"![248433934-7886223b-c1d1-4260-82aa-da5741f303bb](https://github.com/xtekky/gpt4free/assets/98614666/ea012c87-76e0-496a-8ac4-e2de090cc6c9)

<a href=""https://trendshift.io/repositories/1692"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/1692"" alt=""xtekky%2Fgpt4free | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

---

Written by [@xtekky](https://github.com/xtekky) & maintained by [@hlohaus](https://github.com/hlohaus)

<div id=""top""></div>

> By using this repository or any code related to it, you agree to the [legal notice](https://github.com/xtekky/gpt4free/blob/main/LEGAL_NOTICE.md). The author is **not responsible for the usage of this repository nor endorses it**, nor is the author responsible for any copies, forks, re-uploads made by other users, or anything else related to GPT4Free. This is the author's only account and repository. To prevent impersonation or irresponsible actions, please comply with the GNU GPL license th"
scikit-learn,".. -*- mode: rst -*-

|Azure| |CirrusCI| |Codecov| |CircleCI| |Nightly wheels| |Black| |PythonVersion| |PyPi| |DOI| |Benchmark|

.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main
   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main

.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield
   :target: https://circleci.com/gh/scikit-learn/scikit-learn

.. |CirrusCI| image:: https://img.shields.io/cirrus/github/scikit-learn/scikit-learn/main?label=Cirrus%20CI
   :target: https://cirrus-ci.com/github/scikit-learn/scikit-learn/main

.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9
   :target: https://codecov.io/gh/scikit-learn/scikit-learn

.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=sche"
sherlock,"<p align=center>
  <br>
  <a href=""https://sherlock-project.github.io/"" target=""_blank""><img src=""images/sherlock-logo.png""/></a>
  <br>
  <span>Hunt down social media accounts by username across <a href=""https://sherlockproject.xyz/sites"">400+ social networks</a></span>
  <br>
</p>

<p align=""center"">
  <a href=""https://sherlockproject.xyz/installation"">Installation</a>
  &nbsp;&nbsp;&nbsp;•&nbsp;&nbsp;&nbsp;
  <a href=""https://sherlockproject.xyz/usage"">Usage</a>
  &nbsp;&nbsp;&nbsp;•&nbsp;&nbsp;&nbsp;
  <a href=""https://sherlockproject.xyz/contribute"">Contributing</a>
</p>

<p align=""center"">
<img width=""70%"" height=""70%"" src=""images/demo.png""/>
</a>
</p>


## Installation


| | Command | Notes |
| - | - | - |
| PyPI | `pipx install sherlock-project` | `pip` may be used in place of `pipx` |
| Docker | `docker pull sherlock/sherlock` | |
| Debian family | `apt install sherlock` | Kali, Parrot, Debian Testing and Sid |
| BlackArch | `pacman -S sherlock` |  |
| Homebrew | `brew install"
screenshot-to-code,"# screenshot-to-code

A simple tool to convert screenshots, mockups and Figma designs into clean, functional code using AI. **Now supporting Claude Sonnet 3.5 and GPT-4O!**

https://github.com/abi/screenshot-to-code/assets/23818/6cebadae-2fe3-4986-ac6a-8fb9db030045

Supported stacks:

- HTML + Tailwind
- HTML + CSS
- React + Tailwind
- Vue + Tailwind
- Bootstrap
- Ionic + Tailwind
- SVG

Supported AI models:

- Claude Sonnet 3.5 - Best model!
- GPT-4O - also recommended!
- GPT-4 Turbo (Apr 2024)
- GPT-4 Vision (Nov 2023)
- Claude 3 Sonnet
- DALL-E 3 for image generation

See the [Examples](#-examples) section below for more demos.

We also just added experimental support for taking a video/screen recording of a website in action and turning that into a functional prototype.

![google in app quick 3](https://github.com/abi/screenshot-to-code/assets/23818/8758ffa4-9483-4b9b-bb66-abd6d1594c33)

[Learn more about video here](https://github.com/abi/screenshot-to-code/wiki/Screen-Recording-t"
llama,"## **Note of deprecation**

Thank you for developing with Llama models. As part of the Llama 3.1 release, we’ve consolidated GitHub repos and added some additional repos as we’ve expanded Llama’s functionality into being an e2e Llama Stack. Please use the following repos going forward:
- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations 
- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations
- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications
-"
localstack,"<p align=""center"">
:zap: We are thrilled to announce the release of <a href=""https://blog.localstack.cloud/2024-08-29-localstack-release-v-3-7-0/"">LocalStack 3.7</a> :zap:
</p>

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/localstack/localstack/master/docs/localstack-readme-banner.svg"" alt=""LocalStack - A fully functional local cloud stack"">
</p>

<p align=""center"">
  <a href=""https://circleci.com/gh/localstack/localstack""><img alt=""CircleCI"" src=""https://img.shields.io/circleci/build/gh/localstack/localstack/master?logo=circleci""></a>
  <a href=""https://coveralls.io/github/localstack/localstack?branch=master""><img alt=""Coverage Status"" src=""https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master""></a>
  <a href=""https://pypi.org/project/localstack/""><img alt=""PyPI Version"" src=""https://img.shields.io/pypi/v/localstack?color=blue""></a>
  <a href=""https://hub.docker.com/r/localstack/localstack""><img alt=""Docker Pulls"" src=""https://img.shields."
annotated_deep_learning_paper_implementations,"[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)
[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/labmlai)

# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)

This is a collection of simple PyTorch implementations of
neural networks and related algorithms.
These implementations are documented with explanations,

[The website](https://nn.labml.ai/index.html)
renders these as side-by-side formatted notes.
We believe these would help you understand these algorithms better.

![Screenshot](https://nn.labml.ai/dqn-light.png)

We are actively maintaining this repo and adding new 
implementations almost weekly.
[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.

## Paper Implementations

#### ✨ [Transformers](https://nn.labml.ai/transformers/index.html)

* [Multi-"
private-gpt,"# 🔒 PrivateGPT 📑

[![Tests](https://github.com/zylon-ai/private-gpt/actions/workflows/tests.yml/badge.svg)](https://github.com/zylon-ai/private-gpt/actions/workflows/tests.yml?query=branch%3Amain)
[![Website](https://img.shields.io/website?up_message=check%20it&down_message=down&url=https%3A%2F%2Fdocs.privategpt.dev%2F&label=Documentation)](https://docs.privategpt.dev/)
[![Discord](https://img.shields.io/discord/1164200432894234644?logo=discord&label=PrivateGPT)](https://discord.gg/bK6mRVpErU)
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/ZylonPrivateGPT)](https://twitter.com/ZylonPrivateGPT)

![Gradio UI](/fern/docs/assets/ui.png?raw=true)

PrivateGPT is a production-ready AI project that allows you to ask questions about your documents using the power
of Large Language Models (LLMs), even in scenarios without an Internet connection. 100% private, no data leaves your
execution environment at any point.

>[!TIP]
> If you are looking for an **enterprise-ready, fu"
face_recognition,"# Face Recognition

_You can also read a translated version of this file [in Chinese 简体中文版](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md) or [in Korean 한국어](https://github.com/ageitgey/face_recognition/blob/master/README_Korean.md) or [in Japanese 日本語](https://github.com/m-i-k-i/face_recognition/blob/master/README_Japanese.md)._

Recognize and manipulate faces from Python or from the command line with
the world's simplest face recognition library.

Built using [dlib](http://dlib.net/)'s state-of-the-art face recognition
built with deep learning. The model has an accuracy of 99.38% on the
[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.

This also provides a simple `face_recognition` command line tool that lets
you do face recognition on a folder of images from the command line!


[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)
[![Build Status](https://github.com"
scrapy,".. image:: https://scrapy.org/img/scrapylogo.png
   :target: https://scrapy.org/
   
======
Scrapy
======

.. image:: https://img.shields.io/pypi/v/Scrapy.svg
   :target: https://pypi.python.org/pypi/Scrapy
   :alt: PyPI Version

.. image:: https://img.shields.io/pypi/pyversions/Scrapy.svg
   :target: https://pypi.python.org/pypi/Scrapy
   :alt: Supported Python Versions

.. image:: https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg
   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu
   :alt: Ubuntu

.. .. image:: https://github.com/scrapy/scrapy/workflows/macOS/badge.svg
   .. :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS
   .. :alt: macOS


.. image:: https://github.com/scrapy/scrapy/workflows/Windows/badge.svg
   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows
   :alt: Windows

.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg
   :target: https://pypi.python.org/pypi/Scrapy
   :al"
open-interpreter,"<h1 align=""center"">● Open Interpreter</h1>

<p align=""center"">
    <a href=""https://discord.gg/Hvz9Axh84z"">
        <img alt=""Discord"" src=""https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white""/></a>
    <a href=""docs/README_JA.md""><img src=""https://img.shields.io/badge/ドキュメント-日本語-white.svg"" alt=""JA doc""/></a>
    <a href=""docs/README_ZH.md""><img src=""https://img.shields.io/badge/文档-中文版-white.svg"" alt=""ZH doc""/></a>
    <a href=""docs/README_ES.md""> <img src=""https://img.shields.io/badge/Español-white.svg"" alt=""ES doc""/></a>
    <a href=""docs/README_UK.md""><img src=""https://img.shields.io/badge/Українська-white.svg"" alt=""UK doc""/></a>
    <a href=""docs/README_IN.md""><img src=""https://img.shields.io/badge/Hindi-white.svg"" alt=""IN doc""/></a>
    <a href=""LICENSE""><img src=""https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat"" alt=""License""/></a>
    <br>
    <br><a href=""https://0ggfznkwh4j.typeform.com/to/G21i9lJ2"">Get e"
Real-Time-Voice-Cloning,"# Real-Time Voice Cloning
This repository is an implementation of [Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master's thesis](https://matheo.uliege.be/handle/2268.2/6801).

SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.

**Video demonstration** (click the picture):

[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)



### Papers implemented  
| URL | Designation | Title | Implementation source |
| --- | ----------- | ----- | --------------------- |
|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To"
gpt-engineer,"# gpt-engineer

[![GitHub Repo stars](https://img.shields.io/github/stars/gpt-engineer-org/gpt-engineer?style=social)](https://github.com/gpt-engineer-org/gpt-engineer)
[![Discord Follow](https://dcbadge.vercel.app/api/server/8tcDQ89Ej2?style=flat)](https://discord.gg/8tcDQ89Ej2)
[![License](https://img.shields.io/github/license/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/LICENSE)
[![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/issues)
![GitHub Release](https://img.shields.io/github/v/release/gpt-engineer-org/gpt-engineer)
[![Twitter Follow](https://img.shields.io/twitter/follow/antonosika?style=social)](https://twitter.com/antonosika)

gpt-engineer lets you:
- Specify software in natural language
- Sit back and watch as an AI writes and executes the code
- Ask the AI to implement improvements

## Getting Started

### Install gpt-engine"
requests,"# Requests

**Requests** is a simple, yet elegant, HTTP library.

```python
>>> import requests
>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{""authenticated"": true, ...'
>>> r.json()
{'authenticated': True, ...}
```

Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your `PUT` & `POST` data — but nowadays, just use the `json` method!

Requests is one of the most downloaded Python packages today, pulling in around `30M downloads / week`— according to GitHub, Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D) by `1,000,000+` repositories. You may certainly put your trust in this code.

[![Downloads](https://static.pepy.tech/badge/requests/month)](https://pepy."
faceswap,"# deepfakes_faceswap
<p align=""center"">
  <a href=""https://faceswap.dev""><img src=""https://i.imgur.com/zHvjHnb.png""></img></a>
<br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.
</p>
<p align=""center"">
<img src = ""https://i.imgur.com/nWHFLDf.jpg""></img>
</p>

<p align=""center"">
<a href=""https://www.patreon.com/bePatron?u=23238350""><img src=""https://c5.patreon.com/external/logo/become_a_patron_button.png""></img></a>
&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://discord.gg/FC54sYg""><img src=""https://i.imgur.com/gIpztkv.png""></img></a></p>

<p align=""center"">
  <a href=""https://www.dailymotion.com/video/x810mot""><img src=""https://user-images.githubusercontent.com/36920800/178301720-b69841bb-a1ca-4c20-91db-a2a10f5692ca.png""></img></a>
<br />Emma Stone/Scarlett Johansson FaceSwap using the Phaze-A model
</p>

<p align=""center"">
  <a href=""https://www.youtube.com/watch?v=r1jng79a5xc""><img src=""https://img.youtube.com/vi/r1jng79a5xc/0.jpg""></im"
ComfyUI,"<div align=""center"">

# ComfyUI
**The most powerful and modular diffusion model GUI and backend.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Matrix][matrix-shield]][matrix-url]
<br>
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&logo=matrix&logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
<!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 -->
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue"
you-get,"# You-Get

[![Build Status](https://github.com/soimort/you-get/workflows/develop/badge.svg)](https://github.com/soimort/you-get/actions)
[![PyPI version](https://img.shields.io/pypi/v/you-get.svg)](https://pypi.python.org/pypi/you-get/)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/soimort/you-get?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

**NOTICE (30 May 2022): Support for Python 3.5, 3.6 and 3.7 will eventually be dropped. ([see details here](https://github.com/soimort/you-get/wiki/TLS-1.3-post-handshake-authentication-(PHA)))**

**NOTICE (8 Mar 2019): Read [this](https://github.com/soimort/you-get/blob/develop/CONTRIBUTING.md) if you are looking for the conventional ""Issues"" tab.**

---

[You-Get](https://you-get.org/) is a tiny command-line utility to download media contents (videos, audios, images) from the Web, in case there is no other handy way to do it.

Here's how you use `you-get` to download a video from [YouTube]("
yolov5,"<div align=""center"">
  <p>
    <a href=""https://ultralytics.com/events/yolovision"" target=""_blank"">
      <img width=""100%"" src=""https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png""></a>
  </p>

[中文](https://docs.ultralytics.com/zh) | [한국어](https://docs.ultralytics.com/ko) | [日本語](https://docs.ultralytics.com/ja) | [Русский](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [Français](https://docs.ultralytics.com/fr) | [Español](https://docs.ultralytics.com/es) | [Português](https://docs.ultralytics.com/pt) | [Türkçe](https://docs.ultralytics.com/tr) | [Tiếng Việt](https://docs.ultralytics.com/vi) | [العربية](https://docs.ultralytics.com/ar)

<div>
    <a href=""https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml""><img src=""https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg"" alt=""YOLOv5 CI""></a>
    <a href=""https://zenodo.org/badge/latestdoi/264818686""><img src=""https://zenodo."
hackingtool,"### All in One Hacking tool For Hackers🥇
![](https://img.shields.io/github/license/Z4nzu/hackingtool)
![](https://img.shields.io/github/issues/Z4nzu/hackingtool)
![](https://img.shields.io/github/issues-closed/Z4nzu/hackingtool)
![](https://img.shields.io/badge/Python-3-blue)
![](https://img.shields.io/github/forks/Z4nzu/hackingtool)
![](https://img.shields.io/github/stars/Z4nzu/hackingtool)
![](https://img.shields.io/github/last-commit/Z4nzu/hackingtool)
[![HitCount](http://hits.dwyl.com/Z4nzu/hackingtool.svg)](http://hits.dwyl.com/Z4nzu/hackingtool)
![](https://img.shields.io/badge/platform-Linux%20%7C%20KaliLinux%20%7C%20ParrotOs-blue)

#### Install Kali Linux in WIndows10 Without VirtualBox [YOUTUBE](https://youtu.be/BsFhpIDcd9I) or use Docker

## Update Available V1.2.0 🚀 
- [✔] Installation Bug Fixed
- [x] Added New Tools 
    - [x] Reverse Engineering
    - [x] RAT Tools
    - [x] Web Crawling 
    - [x] Payload Injector
- [x] Multitor Tools update
- [X] Added Tool in wifijammin"
openpilot,"<div align=""center"" style=""text-align: center;"">

<h1>openpilot</h1>

<p>
  <b>openpilot is an operating system for robotics.</b>
  <br>
  Currently, it upgrades the driver assistance system in 275+ supported cars.
</p>

<h3>
  <a href=""https://docs.comma.ai"">Docs</a>
  <span> · </span>
  <a href=""https://docs.comma.ai/contributing/roadmap/"">Roadmap</a>
  <span> · </span>
  <a href=""https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md"">Contribute</a>
  <span> · </span>
  <a href=""https://discord.comma.ai"">Community</a>
  <span> · </span>
  <a href=""https://comma.ai/shop"">Try it on a comma 3X</a>
</h3>

Quick start: `bash <(curl -fsSL openpilot.comma.ai)`

![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)
[![codecov](https://codecov.io/gh/commaai/openpilot/branch/master/graph/badge.svg)](https://codecov.io/gh/commaai/openpilot)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Fol"
grok-1,"# Grok-1

This repository contains JAX example code for loading and running the Grok-1 open-weights model.

Make sure to download the checkpoint and place the `ckpt-0` directory in `checkpoints` - see [Downloading the weights](#downloading-the-weights)

Then, run

```shell
pip install -r requirements.txt
python run.py
```

to test the code.

The script loads the checkpoint and samples from the model on a test input.

Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code.
The implementation of the MoE layer in this repository is not efficient. The implementation was chosen to avoid the need for custom kernels to validate the correctness of the model.

# Model Specifications

Grok-1 is currently designed with the following specifications:

- **Parameters:** 314B
- **Architecture:** Mixture of 8 Experts (MoE)
- **Experts Utilization:** 2 experts used per token
- **Layers:** 64
- **Attention Heads:** 48 fo"
rich,"[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rich/13.2.0)](https://pypi.org/project/rich/) [![PyPI version](https://badge.fury.io/py/rich.svg)](https://badge.fury.io/py/rich)

[![Downloads](https://pepy.tech/badge/rich/month)](https://pepy.tech/project/rich)
[![codecov](https://img.shields.io/codecov/c/github/Textualize/rich?label=codecov&logo=codecov)](https://codecov.io/gh/Textualize/rich)
[![Rich blog](https://img.shields.io/badge/blog-rich%20news-yellowgreen)](https://www.willmcgugan.com/tag/rich/)
[![Twitter Follow](https://img.shields.io/twitter/follow/willmcgugan.svg?style=social)](https://twitter.com/willmcgugan)

![Logo](https://github.com/textualize/rich/raw/master/imgs/logo.svg)

[English readme](https://github.com/textualize/rich/blob/master/README.md)
 • [简体中文 readme](https://github.com/textualize/rich/blob/master/README.cn.md)
 • [正體中文 readme](https://github.com/textualize/rich/blob/master/README.zh-tw.md)
 • [Lengua española readme](https://github"
professional-programming,"<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
## Table of Contents

- [Professional Programming - about this list](#professional-programming---about-this-list)
  - [Principles](#principles)
  - [Contributing to this list](#contributing-to-this-list)
  - [Must-read books](#must-read-books)
  - [Must-read articles](#must-read-articles)
  - [Other general material and list of resources](#other-general-material-and-list-of-resources)
    - [Other lists](#other-lists)
    - [Books](#books)
    - [Articles](#articles)
    - [Axioms](#axioms)
    - [Courses](#courses)
  - [Topics](#topics)
    - [Algorithm and data structures](#algorithm-and-data-structures)
    - [API design & development](#api-design--development)
    - [Attitude, habits, mindset](#attitude-habits-mindset)
      - [Procrastination](#procrastination)
    - [Authentication/authorization](#authenticationauthorization)
    - [A"
big-list-of-naughty-strings,"# Big List of Naughty Strings
The Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer [walks into a bar](http://www.sempf.net/post/On-Testing1).

## Why Test Naughty Strings?

Even multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a [zero-width space](https://en.wikipedia.org/wiki/Zero-width_space) (U+200B) on Twitter:

![](http://i.imgur.com/HyDg2eV.gif)

Although this is not a malicious error, and typical users aren't Tweeting weird unicode, an ""internal server error"" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.

## Usage

`blns.txt` "
MetaGPT,"
# MetaGPT: The Multi-Agent Framework

<p align=""center"">
<a href=""""><img src=""docs/resources/MetaGPT-new-log.png"" alt=""MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks."" width=""150px""></a>
</p>

<p align=""center"">
<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>
</p>

<p align=""center"">
<a href=""docs/README_CN.md""><img src=""https://img.shields.io/badge/文档-中文版-blue.svg"" alt=""CN doc""></a>
<a href=""README.md""><img src=""https://img.shields.io/badge/document-English-blue.svg"" alt=""EN doc""></a>
<a href=""docs/README_JA.md""><img src=""https://img.shields.io/badge/ドキュメント-日本語-blue.svg"" alt=""JA doc""></a>
<a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-blue.svg"" alt=""License: MIT""></a>
<a href=""docs/ROADMAP.md""><img src=""https://img.shields.io/badge/ROADMAP-路线图-blue"" alt=""roadmap""></a>
<a href=""https://discord.gg/DYn29wFk9z""><img src=""https://dcbadge.vercel.app/ap"
pandas,"<picture align=""center"">
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://pandas.pydata.org/static/img/pandas_white.svg"">
  <img alt=""Pandas Logo"" src=""https://pandas.pydata.org/static/img/pandas.svg"">
</picture>

-----------------

# pandas: powerful Python data analysis toolkit

| | |
| --- | --- |
| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |
| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads"
PaddleOCR,"[English](README_en.md) | 简体中文

<p align=""center"">
 <img src=""https://github.com/PaddlePaddle/PaddleOCR/releases/download/v2.8.0/PaddleOCR_logo.png"" align=""middle"" width = ""600""/>
<p align=""center"">
<p align=""center"">
    <a href=""https://discord.gg/z9xaRVjdbD""><img src=""https://img.shields.io/badge/Chat-on%20discord-7289da.svg?sanitize=true"" alt=""Chat""></a>
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleOCR/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleOCR?color=ffa""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""https://pypi.org/project/PaddleOCR/""><img src=""https://img.shields.io/pypi/dm/PaddleOCR?color=9cf""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleOCR/stargazers""><img src=""https://img.shields"
30-Days-Of-Python,"# 🐍 30 Days Of Python

|# Day | Topics                                                    |
|------|:---------------------------------------------------------:|
| 01  |  [Introduction](./readme.md)|
| 02  |  [Variables, Built-in Functions](./02_Day_Variables_builtin_functions/02_variables_builtin_functions.md)|
| 03  |  [Operators](./03_Day_Operators/03_operators.md)|
| 04  |  [Strings](./04_Day_Strings/04_strings.md)|
| 05  |  [Lists](./05_Day_Lists/05_lists.md)|
| 06  |  [Tuples](./06_Day_Tuples/06_tuples.md)|
| 07  |  [Sets](./07_Day_Sets/07_sets.md)|
| 08  |  [Dictionaries](./08_Day_Dictionaries/08_dictionaries.md)|
| 09  |  [Conditionals](./09_Day_Conditionals/09_conditionals.md)|
| 10  |  [Loops](./10_Day_Loops/10_loops.md)|
| 11  |  [Functions](./11_Day_Functions/11_functions.md)|
| 12  |  [Modules](./12_Day_Modules/12_modules.md)|
| 13  |  [List Comprehension](./13_Day_List_comprehension/13_list_comprehension.md)|
| 14  |  [Higher Order Functions](./14_Day_Higher_order_function"
ChatGLM-6B,"# ChatGLM-6B

<p align=""center"">
   🌐 <a href=""https://chatglm.cn/blog"" target=""_blank"">Blog</a> • 🤗 <a href=""https://huggingface.co/THUDM/chatglm-6b"" target=""_blank"">HF Repo</a> • 🐦 <a href=""https://twitter.com/thukeg"" target=""_blank"">Twitter</a> • 📄<a href=""https://arxiv.org/pdf/2406.12793"" target=""_blank""> Report </a> <br>
</p>
<p align=""center"">
    👋 加入我们的  <a href=""https://discord.gg/fK2dz4bg"" target=""_blank"">Discord</a> 和 <a href=""resources/WECHAT.md"" target=""_blank"">WeChat</a>
</p>
<p align=""center"">
📍在 <a href=""https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9"">智谱AI开放平台</a> 体验和使用更大规模的 GLM 商业模型。
</p>

*Read this in [English](README_en.md).*

## GLM-4 开源模型和API

我们已经发布最新的 **GLM-4** 大语言对话模型，该模型在多个指标上有了新的突破，您可以在以下两个渠道体验我们的最新模型。

+ [GLM-4 开源模型](https://github.com/THUDM/GLM-4) 我们已经开源了 GLM-4-9B 系列模型，在各项指标的ce是上有明显提升，欢迎尝试。
+ [智谱清言](https://chatglm.cn/main/detail?fr=ecology_x) 体验最新版 GLM-4，包括 **GLMs，All tools**等功能。
+ [API平台](https://open.bigmodel.cn/?utm_campaign=ope"
Fooocus,"<div align=center>
<img src=""https://github.com/lllyasviel/Fooocus/assets/19834515/483fb86d-c9a2-4c20-997c-46dafc124f25"">
</div>

# Fooocus

[>>> Click Here to Install Fooocus <<<](#download)

Fooocus is an image generating software (based on [Gradio](https://www.gradio.app/) <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>).

Fooocus presents a rethinking of image generator designs. The software is offline, open source, and free, while at the same time, similar to many online image generators like Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images. Fooocus has also simplified the installation: between pressing ""download"" and generating the first image, the number of needed mouse clicks is strictly limited to less than 3. Minimal GPU memory requirement is 4GB (Nvidia).

**Recently many fake websites exist on Google when you search “fooocus”. Do not trust those – here i"
python-patterns,"python-patterns
===============

A collection of design patterns and idioms in Python.

Remember that each pattern has its own trade-offs. And you need to pay attention more to why you're choosing a certain pattern than to how to implement it.

Current Patterns
----------------

__Creational Patterns__:

| Pattern | Description |
|:-------:| ----------- |
| [abstract_factory](patterns/creational/abstract_factory.py) | use a generic function with specific factories |
| [borg](patterns/creational/borg.py) | a singleton with shared-state among instances |
| [builder](patterns/creational/builder.py) | instead of using multiple constructors, builder object receives parameters and returns constructed objects |
| [factory](patterns/creational/factory.py) | delegate a specialized function/method to create instances |
| [lazy_evaluation](patterns/creational/lazy_evaluation.py) | lazily-evaluated property pattern in Python |
| [pool](patterns/creational/pool.py) | preinstantiate and maintain a g"
text-generation-webui,"# Text generation web UI

A Gradio web UI for Large Language Models.

Its goal is to become the [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) of text generation.

|![Image1](https://github.com/oobabooga/screenshots/raw/main/print_instruct.png) | ![Image2](https://github.com/oobabooga/screenshots/raw/main/print_chat.png) |
|:---:|:---:|
|![Image1](https://github.com/oobabooga/screenshots/raw/main/print_default.png) | ![Image2](https://github.com/oobabooga/screenshots/raw/main/print_parameters.png) |

## Features

* Multiple backends for text generation in a single UI and API, including [Transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/ggerganov/llama.cpp) (through [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)), [ExLlamaV2](https://github.com/turboderp/exllamav2), [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). [AutoAW"
ailearning,"<p align=""center"">
    <a href=""https://www.apachecn.org"">
        <img width=""200"" src=""docs/img/logo.jpg"">
    </a>
    <br >
    <a href=""https://www.apachecn.org/""><img src=""https://img.shields.io/badge/%3E-HOME-green.svg""></a>
    <a href=""https://home.apachecn.org/about/""><img src=""https://img.shields.io/badge/%3E-ABOUT-green.svg""></a>
    <a href=""mailto:apache@163.com""><img src=""https://img.shields.io/badge/%3E-Email-green.svg""></a>
</p>

<h1 align=""center""><a href=""https://github.com/apachecn/AiLearning"">AI learning</a></h1>

> 协议：[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)
> 
> 一种新技术一旦开始流行，你要么坐上压路机，要么成为铺路石。——Stewart Brand

* [在线阅读](https://ailearning.apachecn.org)
* [在线阅读（v1）](https://alv1.apachecn.org/)
* [QuantLearning](https://qlearn.apachecn.org/#/)
* [ApacheCN 中文翻译组 713436582](https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt)
* [Apa"
ColossalAI,"# Colossal-AI
<div id=""top"" align=""center"">

   [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)

   Colossal-AI: Making large AI models cheaper, faster, and more accessible

   <h3> <a href=""https://arxiv.org/abs/2110.14883""> Paper </a> |
   <a href=""https://www.colossalai.org/""> Documentation </a> |
   <a href=""https://github.com/hpcaitech/ColossalAI/tree/main/examples""> Examples </a> |
   <a href=""https://github.com/hpcaitech/ColossalAI/discussions""> Forum </a> |
   <a href=""https://cloud.luchentech.com/"">GPU Cloud Playground </a> |
   <a href=""https://hpc-ai.com/blog""> Blog </a></h3>

   [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)
   [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/actions/wor"
black,"[![Black Logo](https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png)](https://black.readthedocs.io/en/stable/)

<h2 align=""center"">The Uncompromising Code Formatter</h2>

<p align=""center"">
<a href=""https://github.com/psf/black/actions""><img alt=""Actions Status"" src=""https://github.com/psf/black/workflows/Test/badge.svg""></a>
<a href=""https://black.readthedocs.io/en/stable/?badge=stable""><img alt=""Documentation Status"" src=""https://readthedocs.org/projects/black/badge/?version=stable""></a>
<a href=""https://coveralls.io/github/psf/black?branch=main""><img alt=""Coverage Status"" src=""https://coveralls.io/repos/github/psf/black/badge.svg?branch=main""></a>
<a href=""https://github.com/psf/black/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://black.readthedocs.io/en/stable/_static/license.svg""></a>
<a href=""https://pypi.org/project/black/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/black""></a>
<a href=""https://pepy.tech/project/black""><img alt=""Downloa"
sentry,"<p align=""center"">
  <p align=""center"">
    <a href=""https://sentry.io/?utm_source=github&utm_medium=logo"" target=""_blank"">
      <img src=""https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png"" alt=""Sentry"" width=""280"" height=""84"" />
    </a>
  </p>
  <p align=""center"">
    Users and logs provide clues. Sentry provides answers.
  </p>
</p>

# What's Sentry?

Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.

<p align=""center"">
  <img src=""https://github.com/getsentry/sentry/raw/master/.github/screenshots/projects.png"" width=""270"" />
  <img src=""https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png"" width=""270"" />
  <img src=""https://github.com/getsentry/sentry/raw/master/.github/screenshots/transaction-summary.png"" width=""270"" />
  <img src=""https://github.com/getsentry/sentry/raw/master/.gith"
stablediffusion,"# Stable Diffusion Version 2
![t2i](assets/stable-samples/txt2img/768/merged-0006.png)
![t2i](assets/stable-samples/txt2img/768/merged-0002.png)
![t2i](assets/stable-samples/txt2img/768/merged-0005.png)

This repository contains [Stable Diffusion](https://github.com/CompVis/stable-diffusion) models trained from scratch and will be continuously updated with
new checkpoints. The following list provides an overview of all currently available models. More coming soon.

## News


**March 24, 2023**

*Stable UnCLIP 2.1*

- New stable diffusion finetune (_Stable unCLIP 2.1_, [Hugging Face](https://huggingface.co/stabilityai/)) at 768x768 resolution,  based on SD2.1-768. This model allows for image variations and mixing operations as described in [*Hierarchical Text-Conditional Image Generation with CLIP Latents*](https://arxiv.org/abs/2204.06125), and, thanks to its modularity, can be combined with other models such as [KARLO](https://github.com/kakaobrain/karlo). Comes in two variants: [*Sta"
cheat.sh,"

![cheat.sh logo](http://cheat.sh/files/big-logo-v2-fixed.png)

Unified access to the best community driven cheat sheets repositories of the world.

Let's imagine for a moment that there is such a thing as an ideal cheat sheet.
What should it look like?
What features should it have?

* **Concise** — It should only contain the things you need, and nothing else.
* **Fast** — It should be possible to use it instantly.
* **Comprehensive** — It should contain answers for every possible question.
* **Universal** — It should be available everywhere, anytime, without any preparations.
* **Unobtrusive** — It should not distract you from your main task.
* **Tutoring** — It should help you to learn the subject.
* **Inconspicuous** — It should be possible to use it completely unnoticed.

Such a thing exists! It's easy to [install](#installation) and there's even [auto-complete](#tab-completion).


## Features

**cheat.sh**

* Has a simple curl/browser/editor interface.
* Covers 56 programming lan"
Deep-Learning-Papers-Reading-Roadmap,"# Deep Learning Papers Reading Roadmap

>If you are a newcomer to the Deep Learning area, the first question you may have is ""Which paper should I start reading from?""

>Here is a reading roadmap of Deep Learning papers!

The roadmap is constructed in accordance with the following four guidelines:

- From outline to detail
- From old to state-of-the-art
- from generic to specific areas
- focus on state-of-the-art

You will find many papers that are quite new but really worth reading.

I would continue adding papers to this roadmap.


---------------------------------------

# 1 Deep Learning History and Basics

## 1.0 Book

**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. ""**Deep learning**."" An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:

## 1.1 Survey

**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. ""**Deep learning**."" Nature"
bert,"# BERT

**\*\*\*\*\* New March 11th, 2020: Smaller BERT Models \*\*\*\*\***

This is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).

We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.

Our goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.

You "
odoo,"[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&colorA=8F8F8F)](https://www.odoo.com/documentation/17.0)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo
----

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an <a href=""https://www.odoo.com/page/crm"">Open Source CRM</a>,
<a href=""https://www.odoo.com/app/website"">Website Builder</a>,
<a href=""https://www.odoo.com/app/ecommerce"">eCommerce</a>,
<a href=""https://www.odoo.com/app/inventory"">Warehouse Management</a>,
<a href=""https://www.odoo.com/app/project"">Project Management</a>,
<a href=""https://www.odoo.com/app/accounting"">Billing &amp; Accounting</a>,
<a href=""htt"
Open-Assistant,"<h1 align=""center"">
    <span>Open-Assistant</span>
  <img width=""auto"" height=""50px"" src=""https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png""/>
</h1>

<blockquote>
<p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href=""https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed"">blog post</a> for more information. The final published oasst2 dataset can be found on HuggingFace at <a href=""https://huggingface.co/datasets/OpenAssistant/oasst2"">OpenAssistant/oasst2</a></p>
</blockquote>

<div align=""center"">

<a href=""https://github.com/LAION-AI/Open-Assistant/stargazers"">![GitHub Repo stars](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)</a>
<a href=""https://laion-ai.github.io/Open-Assistant/"">![Docs](https://img.shields.io/badge/docs-laion--ai.github.io%2FOpen--Assistant%2F-green)</a>
<a href=""https://github.com/L"
diagrams,"![diagrams logo](assets/img/diagrams.png)

# Diagrams

[![license](https://img.shields.io/badge/license-MIT-blue.svg)](/LICENSE)
[![pypi version](https://badge.fury.io/py/diagrams.svg)](https://badge.fury.io/py/diagrams)
![python version](https://img.shields.io/badge/python-%3E%3D%203.6-blue?logo=python)
![Run tests](https://github.com/mingrammer/diagrams/workflows/Run%20tests/badge.svg?branch=master)
[![todos](https://badgen.net/https/api.tickgit.com/badgen/github.com/mingrammer/diagrams?label=todos)](https://www.tickgit.com/browse?repo=github.com/mingrammer/diagrams)
![contributors](https://img.shields.io/github/contributors/mingrammer/diagrams)

<a href=""https://www.buymeacoffee.com/mingrammer"" target=""_blank""><img src=""https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png"" alt=""Buy Me A Coffee"" style=""height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;"" ></a>

**Diagram as Code**.

Diagrams lets you draw th"
Deep-Live-Cam,"
![demo-gif](demo.gif)
![demo-gif](avgpcperformancedemo.gif)

## Deep Live Cam

Real-time face swap and video deepfake with a single click and only a single image.

## Disclaimer

This software is intended as a productive contribution to the AI-generated media industry. It aims to assist artists with tasks like animating custom characters or using them as models for clothing, etc.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to law and ethics. We may shut down the project or add watermarks if legally required.

Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user ac"
interview_internal_reference,"
## 2023年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总。持续更新中。

* [阿里篇](#1)
* [华为篇](#2)
* [百度篇](#3)
* [腾讯篇](#4)
* [美团篇](#5)
* [头条篇](#6)
* [滴滴篇](#7)
* [京东篇](#8)
* [MySQL篇](#9)
* [Redis篇](#10)
* [MongoDB篇](#11)
* [Zookeeper篇](#12)
* [Nginx篇](#13)
* [算法篇](#14)
* [内存篇](#15)
* [cpu篇](#16)
* [磁盘篇](#17)
* [网络通信篇](#18)
* [安全篇](#19)
* [并发篇](#20)

<h3 id=""1"">阿里篇</h3> 

---

##### [1.1.1 如何实现一个高效的单向链表逆序输出？](01.阿里篇/1.1.1%20%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E7%9A%84%E5%8D%95%E5%90%91%E9%93%BE%E8%A1%A8%E9%80%86%E5%BA%8F%E8%BE%93%E5%87%BA%EF%BC%9F.md)

##### [1.1.2 已知sqrt(2)约等于1.414，要求不用数学库，求sqrt(2)精确到小数点后10位](01.阿里篇/1.1.2%20%E5%B7%B2%E7%9F%A5sqrt%282%29%E7%BA%A6%E7%AD%89%E4%BA%8E1.414%EF%BC%8C%E8%A6%81%E6%B1%82%E4%B8%8D%E7%94%A8%E6%95%B0%E5%AD%A6%E5%BA%93%EF%BC%8C%E6%B1%82sqrt%282%29%E7%B2%BE%E7%A1%AE%E5%88%B0%E5%B0%8F%E6%95%B0%E7%82%B9%E5%90%8E10%E4%BD%8D.md)

##### [1.1.3 给定一个二叉搜索树(BST)，找到树中第 K 小的节点](01.阿里篇/1.1.3%20%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%"
FastChat,"# FastChat
| [**Demo**](https://lmarena.ai/) | [**Discord**](https://discord.gg/HSWAKCrnFx) | [**X**](https://x.com/lmsysorg) |

FastChat is an open platform for training, serving, and evaluating large language model based chatbots.
- FastChat powers Chatbot Arena ([lmarena.ai](https://lmarena.ai)), serving over 10 million chat requests for 70+ LLMs.
- Chatbot Arena has collected over 1.5M human votes from side-by-side LLM battles to compile an online [LLM Elo leaderboard](https://lmarena.ai/?leaderboard).

FastChat's core features include:
- The training and evaluation code for state-of-the-art models (e.g., Vicuna, MT-Bench).
- A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.

## News
- [2024/03] 🔥 We released Chatbot Arena technical [report](https://arxiv.org/abs/2403.04132).
- [2023/09] We released **LMSYS-Chat-1M**, a large-scale real-world LLM conversation dataset. Read the [report](https://arxiv.org/abs/2309.11998).
- [2023/08] We released"
airflow,"<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 ""License""); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
-->

<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->
# Apache Airflow

[![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)
[![G"
nanoGPT,"
# nanoGPT

![nanoGPT](assets/nanogpt.jpg)

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.

![repro124m](assets/gpt2_124M_loss.png)

Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).

## install

```
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

Dependencies:

- [pytorch](ht"
python-cheatsheet,"Comprehensive Python Cheatsheet
===============================
<sup>[Download text file](https://raw.githubusercontent.com/gto76/python-cheatsheet/main/README.md), [Buy PDF](https://transactions.sendowl.com/products/78175486/4422834F/view), [Fork me on GitHub](https://github.com/gto76/python-cheatsheet) or [Check out FAQ](https://github.com/gto76/python-cheatsheet/wiki/Frequently-Asked-Questions).
</sup>

![Monty Python](web/image_888.jpeg)


Contents
--------
**&nbsp;&nbsp;&nbsp;** **1. Collections:** **&nbsp;** **[`List`](#list)**__,__ **[`Dictionary`](#dictionary)**__,__ **[`Set`](#set)**__,__ **[`Tuple`](#tuple)**__,__ **[`Range`](#range)**__,__ **[`Enumerate`](#enumerate)**__,__ **[`Iterator`](#iterator)**__,__ **[`Generator`](#generator)**__.__  
**&nbsp;&nbsp;&nbsp;** **2. Types:** **&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**  **[`Type`](#type)**__,__ **[`String`](#string)**__,__ **[`Regular_Exp`](#regex)**__,__ **[`Format`](#format)**__,__ **[`Numbers`](#nu"
quivr,"# Quivr - Your Second Brain, Empowered by Generative AI

<div align=""center"">
    <img src=""./logo.png"" alt=""Quivr-logo"" width=""31%""  style=""border-radius: 50%; padding-bottom: 20px""/>
</div>

[![Discord Follow](https://dcbadge.vercel.app/api/server/HUpRgp2HG8?style=flat)](https://discord.gg/HUpRgp2HG8)
[![GitHub Repo stars](https://img.shields.io/github/stars/quivrhq/quivr?style=social)](https://github.com/quivrhq/quivr)
[![Twitter Follow](https://img.shields.io/twitter/follow/StanGirard?style=social)](https://twitter.com/_StanGirard)

Quivr, your second brain, utilizes the power of GenerativeAI to be your personal assistant ! Think of it as Obsidian, but turbocharged with AI capabilities.

[Roadmap here](https://docs.quivr.app/docs/roadmap)

## Key Features 🎯

- **Fast and Efficient**: Designed with speed and efficiency at its core. Quivr ensures rapid access to your data.
- **Secure**: Your data, your control. Always.
- **OS Compatible**: Ubuntu 20 or newer.
- **File Compatibility**"
mitmproxy,"# mitmproxy

[![Continuous Integration Status](https://github.com/mitmproxy/mitmproxy/workflows/CI/badge.svg?branch=main)](https://github.com/mitmproxy/mitmproxy/actions?query=branch%3Amain)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/a38b0325dfb944839c0c8da354f70b1b)](https://app.codacy.com/gh/mitmproxy/mitmproxy/dashboard)
[![autofix.ci: enabled](https://shields.mitmproxy.org/badge/autofix.ci-yes-success?logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSIjZmZmIiB2aWV3Qm94PSIwIDAgMTI4IDEyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCB0cmFuc2Zvcm09InNjYWxlKDAuMDYxLC0wLjA2MSkgdHJhbnNsYXRlKC0yNTAsLTE3NTApIiBkPSJNMTMyNSAtMzQwcS0xMTUgMCAtMTY0LjUgMzIuNXQtNDkuNSAxMTQuNXEwIDMyIDUgNzAuNXQxMC41IDcyLjV0NS41IDU0djIyMHEtMzQgLTkgLTY5LjUgLTE0dC03MS41IC01cS0xMzYgMCAtMjUxLjUgNjJ0LTE5MSAxNjl0LTkyLjUgMjQxcS05MCAxMjAgLTkwIDI2NnEwIDEwOCA0OC41IDIwMC41dDEzMiAxNTUuNXQxODguNSA4MXExNSA5OSAxMDAuNSAxODAuNXQyMTcgMTMwLjV0MjgyLjUgNDlxMTM2IDAgMjU2LjUgLTQ2IHQyMDkgLTEyNy41dDEyOC41IC0xODkuNXExNDk"
wtfpython,"<p align=""center""><img src=""/images/logo.png#gh-light-mode-only"" alt=""""><img src=""/images/logo-dark.png#gh-dark-mode-only"" alt=""""></p>
<h1 align=""center"">What the f*ck Python! 😱</h1>
<p align=""center"">Exploring and understanding Python through surprising snippets.</p>


Translations: [Chinese 中文](https://github.com/leisurelicht/wtfpython-cn) | [Vietnamese Tiếng Việt](https://github.com/vuduclyunitn/wtfptyhon-vi) | [Spanish Español](https://web.archive.org/web/20220511161045/https://github.com/JoseDeFreitas/wtfpython-es) | [Korean 한국어](https://github.com/buttercrab/wtfpython-ko) | [Russian Русский](https://github.com/satwikkansal/wtfpython/tree/master/translations/ru-russian) | [German Deutsch](https://github.com/BenSt099/wtfpython) | [Add translation](https://github.com/satwikkansal/wtfpython/issues/new?title=Add%20translation%20for%20[LANGUAGE]&body=Expected%20time%20to%20finish:%20[X]%20weeks.%20I%27ll%20start%20working%20on%20it%20from%20[Y].)

Other modes: [Interactive Website](htt"
llama_index,"# 🗂️ LlamaIndex 🦙

[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)
[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)
[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)
[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNSIgaGVpZ2h0PSI0IiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxwYXRoIGQ9Ik00LjQzIDEuODgyYTEuNDQgMS40NCAwIDAgMS0uMDk4LjQyNmMtLjA1LjEyMy0uMTE1LjIzLS4xOTIuMzIyLS4wNzUuMDktLjE2LjE2NS0uMjU1LjIyNmExLjM1MyAxLjM1MyAwIDAgMS0uNTk1LjIxMmMtLjA5OS4wMTItLjE5Mi4wMTQtLjI3OS4wMDZsLTEuNTkzLS4xNHYtLjQwNmgxLjY1OGMuMDkuMDAxLjE3LS4xNjkuMjQ2LS4xOTFhLjYwMy42MDMgMCAwIDAgLjItLjEwNi41MjkuNTI5IDAgMCAwIC4xMzgtLjE3LjY1NC42NTQgMCAwIDAgLjA2NS0uMjRsLjAyOC0uMzJhLjkzLjkzIDAgMCAwLS4wMzYtLjI0OS41NjcuNTY3IDAgMCAwLS4xMDMt"
DragGAN,"<p align=""center"">

  <h1 align=""center"">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>
  <p align=""center"">
    <a href=""https://xingangpan.github.io/""><strong>Xingang Pan</strong></a>
    ·
    <a href=""https://ayushtewari.com/""><strong>Ayush Tewari</strong></a>
    ·
    <a href=""https://people.mpi-inf.mpg.de/~tleimkue/""><strong>Thomas Leimkühler</strong></a>
    ·
    <a href=""https://lingjie0206.github.io/""><strong>Lingjie Liu</strong></a>
    ·
    <a href=""https://www.meka.page/""><strong>Abhimitra Meka</strong></a>
    ·
    <a href=""http://www.mpi-inf.mpg.de/~theobalt/""><strong>Christian Theobalt</strong></a>
  </p>
  <h2 align=""center"">SIGGRAPH 2023 Conference Proceedings</h2>
  <div align=""center"">
    <img src=""DragGAN.gif"", width=""600"">
  </div>

  <p align=""center"">
  <br>
    <a href=""https://pytorch.org/get-started/locally/""><img alt=""PyTorch"" src=""https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white""></a>
  "
GFPGAN,"<p align=""center"">
  <img src=""assets/gfpgan_logo.png"" height=130>
</p>

## <div align=""center""><b><a href=""README.md"">English</a> | <a href=""README_CN.md"">简体中文</a></b></div>

<div align=""center"">
<!-- <a href=""https://twitter.com/_Xintao_"" style=""text-decoration:none;"">
    <img src=""https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png"" width=""4%"" alt="""" />
</a> -->

[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)
[![PyPI](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)
[![Open issue](https://img.shields.io/github/issues/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)
[![Closed issue](https://img.shields.io/github/issues-closed/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)
[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/TencentARC/GFPGAN/blob/master/LIC"
MockingBird,"> 🚧 While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. Join me at https://discord.gg/wrAGwSH5 .
>
![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)


[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)

> English | [中文](README-CN.md)| [中文Linux](README-LINUX-CN.md)

## Features
🌍 **Chinese** supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.

🤩 **PyTorch** worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060

🌍 **Windows + Linux** run in both Windows OS and linux OS (even in M1 MACOS)

🤩 **Easy & Awesome** effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder

🌍 **Webserver Ready** to serve your result with remote calling

### [DEMO"
DeepSpeed,"[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)
[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)
[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)
[![Twitter](https://img.shields.io/twitter/follow/MSFTDeepSpeed)](https://twitter.com/intent/follow?screen_name=MSFTDeepSpeed)
[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40MSFTDeepSpeedJP-blue)](https://twitter.com/MSFTDeepSpeedJP)
[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)


<div align=""center"">
 <img src=""docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only"" width=""400px"">
 <img src=""docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-"
streamlit,"<br>

<img src=""https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png"" alt=""Streamlit logo"" style=""margin-top:50px""></img>

# Welcome to Streamlit 👋

**A faster way to build and share data apps.**

## What is Streamlit?

Streamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.

### Why choose Streamlit?

- **Simple and Pythonic:** Write beautiful, easy-to-read code.
- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.
- **Live editing:** See your app update instantly as you edit your script.
- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.

## Installation

Open a terminal and run:

```bash
$ pip install streamlit
$ "
gym,"[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Important Notice

### The team that has been maintaining Gym since 2021 has moved all future development to [Gymnasium](https://github.com/Farama-Foundation/Gymnasium), a drop in replacement for Gym (import gymnasium as gym), and Gym will not be receiving any future updates. Please switch over to Gymnasium as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out [this blog post](https://farama.org/Announcing-The-Farama-Foundation).

## Gym

Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with t"
TaskMatrix,"# TaskMatrix

**TaskMatrix** connects ChatGPT and a series of Visual Foundation Models to enable **sending** and **receiving** images during chatting.

See our paper: [<font size=5>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</font>](https://arxiv.org/abs/2303.04671)

<a src=""https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"" href=""https://huggingface.co/spaces/microsoft/visual_chatgpt"">
    <img src=""https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"" alt=""Open in Spaces"">
</a>

<a src=""https://colab.research.google.com/assets/colab-badge.svg"" href=""https://colab.research.google.com/drive/1P3jJqKEWEaeNcZg8fODbbWeQ3gxOHk2-?usp=sharing"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open in Colab"">
</a>

## Updates:
- Now TaskMatrix supports [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) and [segment-anything](https://github.com/facebookresearch/segment-anything)! Thanks **@jordd"
12306,"### 12306 购票小助手
#### python版本
  - [ ] 2.7.10 - 2.7.15
  - [x] 3.6 - 3.7.4
  - [ ] 2.7.9

#### 已有功能
  - [x] 自动打码
  - [x] 自动登录
  - [x] 准点预售和捡漏
  - [x] 智能候补
  - [x] 邮件通知
  - [x] server酱通知

#### 依赖库
  - 验证码目前可以本地识别，需要下载模型，放于项目根目录，全部代码来源于此项目 [传送门](https://github.com/zhaipro/easy12306)，表示感谢
    ```
      1. 模型下载链接:https://pan.baidu.com/s/1rS155VjweWVWIJogakechA  密码:bmlm
         群里面也可以下载
      2. git仓库下载：https://github.com/testerSunshine/12306model.git
    ```
  - 自托管云打码服务器搭建：[12306_code_server](https://github.com/YinAoXiong/12306_code_server)
    - 如果大家有空闲的服务器，可搭建之后在这个 [issues](https://github.com/testerSunshine/12306/issues/446) 里面填入自己的服务器(请注意服务器安全！)
  - 项目依赖 [requirements.txt](requirements.txt)
  - 安装方法x:
      - root用户(避免多python环境产生问题): `pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`
      - 非root用户（避免安装和运行时使用了不同环境）: `pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`
      - 许多windows的用户装不了tensorflow的话，可以适当降低版本或者升高版本都是可以的
        "
TTS,"
## 🐸Coqui.ai News
- 📣 ⓍTTSv2 is here with 16 languages and better performance across the board.
- 📣 ⓍTTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).
- 📣 ⓍTTS can now stream with <200ms latency.
- 📣 ⓍTTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)
- 📣 [🐶Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)
- 📣 You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with 🐸TTS.
- 📣 🐸TTS now supports 🐢Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)

<div align=""center"">
<img src=""https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124"
HanLP,"<h2 align=""center"">HanLP: Han Language Processing</h2>

<div align=""center"">
    <a href=""https://github.com/hankcs/HanLP/actions"">
       <img alt=""Unit Tests"" src=""https://github.com/hankcs/hanlp/actions/workflows/unit-tests.yml/badge.svg?branch=master"">
    </a>
    <a href=""https://pypi.org/project/hanlp/"">
        <img alt=""PyPI Version"" src=""https://img.shields.io/pypi/v/hanlp?color=blue"">
    </a>
    <a href=""https://pypi.org/project/hanlp/"">
        <img alt=""Python Versions"" src=""https://img.shields.io/pypi/pyversions/hanlp?colorB=blue"">
    </a>
    <a href=""https://pepy.tech/project/hanlp"">
        <img alt=""Downloads"" src=""https://static.pepy.tech/badge/hanlp"">
    </a>
    <a href=""https://mybinder.org/v2/gh/hankcs/HanLP/doc-zh?filepath=plugins%2Fhanlp_demo%2Fhanlp_demo%2Fzh%2Ftutorial.ipynb"">
        <img alt=""在线运行"" src=""https://mybinder.org/badge_logo.svg"">
    </a>
</div>
<h4 align=""center"">
    <a href=""https://github.com/hankcs/HanLP/tree/master"">English</a> |
    <a"
shadowsocks,"Removed according to regulations.
"
cli,"<h2 align=""center"">
    <a href=""https://httpie.io"" target=""blank_"">
        <img height=""100"" alt=""HTTPie"" src=""https://raw.githubusercontent.com/httpie/cli/master/docs/httpie-logo.svg"" />
    </a>
    <br>
    HTTPie CLI: human-friendly HTTP client for the API era
</h2>

<div align=""center"">

[![HTTPie for Desktop](https://img.shields.io/static/v1?label=HTTPie&message=Desktop&color=4B78E6)](https://httpie.io/product)
[![](https://img.shields.io/static/v1?label=HTTPie&message=Web%20%26%20Mobile&color=73DC8C)](https://httpie.io/app)
[![](https://img.shields.io/static/v1?label=HTTPie&message=CLI&color=FA9BFA)](https://httpie.io/cli)
[![Twitter](https://img.shields.io/twitter/follow/httpie?style=flat&color=%234B78E6&logoColor=%234B78E6)](https://twitter.com/httpie)
[![Chat](https://img.shields.io/discord/725351238698270761?style=flat&label=Chat%20on%20Discord&color=%23FA9BFA)](https://httpie.io/discord)

</div>


<div align=""center"">

[![Docs](https://img.shields.io/badge/stable%20docs-h"
WeChatMsg,"<a href=""https://hellogithub.com/repository/93df3704446343068e67fc174a34be47"" target=""_blank""><img src=""https://api.hellogithub.com/v1/widgets/recommend.svg?rid=93df3704446343068e67fc174a34be47&claim_uid=AzZ0bVgHmTOEej5"" alt=""Featured｜HelloGitHub"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a>
<h1 align=""center"">我的数据我做主</h1>
<div align=""center"">
    <a href=""https://github.com/LC044/WeChatMsg/stargazers"">
        <img src=""https://img.shields.io/github/stars/LC044/WeChatMsg.svg"" />
    </a>
    <a href=""https://memotrace.cn/"" target=""_blank"">
        <img alt=""GitHub forks"" src=""https://img.shields.io/github/forks/LC044/WeChatMsg?color=eb6ea5"">
    </a>
    <a href=""https://memotrace.cn/"" target=""_blank"">
        <img src=""https://img.shields.io/badge/WeChat-留痕-blue.svg"">
    </a>
    <a target=""_blank"" href=""https://memotrace.cn/"">
        <img alt=""Hits"" src=""https://hits.b3log.org/LC044/memotrace.svg"">
    </a>
    <a href=""https://memotrace.cn/"" target=""_blank"">"
ray,".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png

.. image:: https://readthedocs.org/projects/ray/badge/?version=master
    :target: http://docs.ray.io/en/master/?badge=master

.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue
    :target: https://forms.gle/9TSdDYUgxYs8SA9e8

.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue
    :target: https://discuss.ray.io/

.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter
    :target: https://twitter.com/raydistributed

.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3q"
jieba,"jieba
========
“结巴”中文分词：做最好的 Python 中文分词组件

""Jieba"" (Chinese for ""to stutter"") Chinese text segmentation: built to be the best Python Chinese word segmentation module.

- _Scroll down for English documentation._


特点
========
* 支持四种分词模式：
    * 精确模式，试图将句子最精确地切开，适合文本分析；
    * 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
    * 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
    * paddle模式，利用PaddlePaddle深度学习框架，训练序列标注（双向GRU）网络模型实现分词。同时支持词性标注。paddle模式使用需安装paddlepaddle-tiny，`pip install paddlepaddle-tiny==1.6.1`。目前paddle模式支持jieba v0.40及以上版本。jieba v0.40以下版本，请升级jieba，`pip install jieba --upgrade` 。[PaddlePaddle官网](https://www.paddlepaddle.org.cn/)
* 支持繁体分词
* 支持自定义词典
* MIT 授权协议

安装说明
=======

代码对 Python 2/3 均兼容

* 全自动安装：`easy_install jieba` 或者 `pip install jieba` / `pip3 install jieba`
* 半自动安装：先下载 http://pypi.python.org/pypi/jieba/ ，解压后运行 `python setup.py install`
* 手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录
* 通过 `import jieba` 来引用
* 如果需要使用paddle模式下的分词和词性标注功能，请先安装paddlepaddle-tiny，`pip install paddlepaddl"
GPT-SoVITS,"<div align=""center"">


<h1>GPT-SoVITS-WebUI</h1>
A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.<br><br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/RVC-Boss/GPT-SoVITS)

<a href=""https://trendshift.io/repositories/7033"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/7033"" alt=""RVC-Boss%2FGPT-SoVITS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

<!-- img src=""https://counter.seku.su/cmoe?name=gptsovits&theme=r34"" /><br> -->

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb)
[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/🤗%20-online%20demo-yel"
XX-Net,":rocket: XX-Net (翻墙VPN)
=========
这是一个稳健可靠的翻墙系统，已经连续运行 9 年！  
我们不去研究墙有什么缺陷，因为所有的缺陷都会被慢慢的补上。  
我们的策略是化身为普通流量，完全无法区分，最终隐身在茫茫的网络连接中。。。

:electric_plug: 功能特性
=========
* 支持多平台： Android/iOS/Windows/Mac/Linux   
* 采用独特的混淆算法，让您的流量在网络中无法被识别  
* 开源绿色软件，无需安装，可以支持多台设备同时连接
* 模拟Chrome浏览器行为，完全无法识别，稳定翻墙
* 内置 ChatGPT，每个套餐赠送 ChatGPT-3.5 一百万token 


<br>

### 官网下载: [https://xx-net.com](https://xx-net.com)
### Telegram: [https://t.me/xxnetshare](https://t.me/xxnetshare)
### Twitter: [https://twitter.com/XXNetDev](https://twitter.com/XXNetDev)
###
###### [中文帮助文档](https://github.com/XX-net/XX-Net/wiki/%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3) &nbsp; &nbsp; &nbsp;[English Document](https://github.com/XX-net/XX-Net/wiki/English-Home-Page) &nbsp; &nbsp; &nbsp;[فارسی صفحه اصلی](https://github.com/XX-net/XX-Net/wiki/Persian-home-page) 

<br>


### 最新公告：
 2024-03-06
* 最新版5.9.10, 更新黑名单列表。
* 5.9.0 升级GAE服务端到python3
* 5.8.8 改进iOS下连接性能
* 5.7.0 为X-Tunnel增加新通道
* 5.6.0 重构代码，减少系统资源消耗
* 5.1.0，内置ChatGPT
* 原来是4.x.x 老版本的，需要重新下载新"
ccxt,"# CCXT – CryptoCurrency eXchange Trading Library

[![Build Status](https://img.shields.io/travis/com/ccxt/ccxt)](https://travis-ci.com/ccxt/ccxt) [![npm](https://img.shields.io/npm/v/ccxt.svg)](https://npmjs.com/package/ccxt) [![PyPI](https://img.shields.io/pypi/v/ccxt.svg)](https://pypi.python.org/pypi/ccxt) [![NPM Downloads](https://img.shields.io/npm/dy/ccxt.svg)](https://www.npmjs.com/package/ccxt) [![Discord](https://img.shields.io/discord/690203284119617602?logo=discord&logoColor=white)](https://discord.gg/ccxt) [![Supported Exchanges](https://img.shields.io/badge/exchanges-108-blue.svg)](https://github.com/ccxt/ccxt/wiki/Exchange-Markets) [![Twitter Follow](https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&label=CCXT)](https://twitter.com/ccxt_official)

A JavaScript / Python / PHP / C# library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs.

### [Install](#install) · [Usage](#usage) · ["
gradio,"<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/1)getting_started/1)quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->

<div align=""center"">

[<img src=""readme_files/gradio.svg"" alt=""gradio"" width=400>](https://gradio.app)<br>

[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)
[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml)
 [![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)
[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)
![Python version](https://img.shields.io/badge/python-3.8+-important)
[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)

[Website](ht"
OpenHands,"<a name=""readme-top""></a>

<div align=""center"">
  <img src=""./docs/static/img/logo.png"" alt=""Logo"" width=""200"">
  <h1 align=""center"">OpenHands: Code Less, Make More</h1>
</div>


<div align=""center"">
  <a href=""https://github.com/All-Hands-AI/OpenHands/graphs/contributors""><img src=""https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&color=blue"" alt=""Contributors""></a>
  <a href=""https://github.com/All-Hands-AI/OpenHands/stargazers""><img src=""https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&color=blue"" alt=""Stargazers""></a>
  <a href=""https://codecov.io/github/All-Hands-AI/OpenHands?branch=main""><img alt=""CodeCov"" src=""https://img.shields.io/codecov/c/github/All-Hands-AI/OpenHands?style=for-the-badge&color=blue""></a>
  <a href=""https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE""><img src=""https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&color=blue"" alt=""MIT License""></a>
  <br/>
  <"
sqlmap,"# sqlmap ![](https://i.imgur.com/fe85aVR.png)

[![.github/workflows/tests.yml](https://github.com/sqlmapproject/sqlmap/actions/workflows/tests.yml/badge.svg)](https://github.com/sqlmapproject/sqlmap/actions/workflows/tests.yml) [![Python 2.6|2.7|3.x](https://img.shields.io/badge/python-2.6|2.7|3.x-yellow.svg)](https://www.python.org/) [![License](https://img.shields.io/badge/license-GPLv2-red.svg)](https://raw.githubusercontent.com/sqlmapproject/sqlmap/master/LICENSE) [![Twitter](https://img.shields.io/badge/twitter-@sqlmap-blue.svg)](https://twitter.com/sqlmap)

sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester, and a broad range of switches including database fingerprinting, over data fetching from the database, accessing the underlying file system, and executing commands on t"
pytorch-image-models,"# PyTorch Image Models
- [What's New](#whats-new)
- [Introduction](#introduction)
- [Models](#models)
- [Features](#features)
- [Results](#results)
- [Getting Started (Documentation)](#getting-started-documentation)
- [Train, Validation, Inference Scripts](#train-validation-inference-scripts)
- [Awesome PyTorch Resources](#awesome-pytorch-resources)
- [Licenses](#licenses)
- [Citing](#citing)

## What's New

### Aug 21, 2024
* Updated SBB ViT models trained on ImageNet-12k and fine-tuned on ImageNet-1k, challenging quite a number of much larger, slower models

| model | top1 | top5 | param_count | img_size |
| -------------------------------------------------- | ------ | ------ | ----------- | -------- |
| [vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k) | 87.438 | 98.256 | 64.11 | 384 |
| [vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patc"
LLaMA-Factory,"![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/hiyouga/LLaMA-Factory)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-91-green)](#projects-using-llama-factory)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)
[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Open in Colab](https://colab.research.googl"
certbot,".. This file contains a series of comments that are used to include sections of this README in other files. Do not modify these comments unless you know what you are doing. tag:intro-begin

|build-status|

.. |build-status| image:: https://img.shields.io/azure-devops/build/certbot/ba534f81-a483-4b9b-9b4e-a60bec8fee72/5/master
   :target: https://dev.azure.com/certbot/certbot/_build?definitionId=5
   :alt: Azure Pipelines CI status
 
.. image:: https://raw.githubusercontent.com/EFForg/design/master/logos/eff-certbot-lockup.png
  :width: 200
  :alt: EFF Certbot Logo

Certbot is part of EFF’s effort to encrypt the entire Internet. Secure communication over the Web relies on HTTPS, which requires the use of a digital certificate that lets browsers verify the identity of web servers (e.g., is that really google.com?). Web servers obtain their certificates from trusted third parties called certificate authorities (CAs). Certbot is an easy-to-use client that fetches a certificate from Let’s E"
poetry,"# Poetry: Python packaging and dependency management made easy

[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/)
[![Stable Version](https://img.shields.io/pypi/v/poetry?label=stable)][PyPI Releases]
[![Pre-release Version](https://img.shields.io/github/v/release/python-poetry/poetry?label=pre-release&include_prereleases&sort=semver)][PyPI Releases]
[![Python Versions](https://img.shields.io/pypi/pyversions/poetry)][PyPI]
[![Download Stats](https://img.shields.io/pypi/dm/poetry)](https://pypistats.org/packages/poetry)
[![Discord](https://img.shields.io/discord/487711540787675139?logo=discord)][Discord]

Poetry helps you declare, manage and install dependencies of Python projects,
ensuring you have the right stack everywhere.

![Poetry Install](https://raw.githubusercontent.com/python-poetry/poetry/master/assets/install.gif)

Poetry replaces `setup.py`, `requirements.txt`, `setup.cfg`, `MANIFEST.in` and `Pipfile` with a "
Python,"# My Python Eggs 🐍 😄

<hr>

I do not consider myself as a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me: craig@geekcomputers.co.uk.

<hr>

This repository contains a collection of Python scripts that are designed to reduce human workload and serve as educational examples for beginners to get started with Python. The code documentation is aligned correctly for viewing in [Notepad++](https://notepad-plus-plus.org/) :spiral_notepad:

Feel free to explore the scripts and use them for your learning and automation needs!

## List of Scripts:

1. [batch_file_rename.py](https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py) - Batch rename a group of files in a specified directory, changing their extensions.
2. [create_dir_if_not_there.py](htt"
ChatTTS,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
A generative speech model for daily dialogue.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/🤗%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoC"
OpenBB,"<br />
<img src=""https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only"" alt=""OpenBB Terminal logo"" width=""600"">
<img src=""https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only"" alt=""OpenBB Terminal logo"" width=""600"">
<br />
<br />

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://twitter.com/openbb_finance)
![Discord Shield](https://discordapp.com/api/guilds/831165782750789672/widget.png?style=shield)
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)
<a href=""https://codespaces.new/OpenBB-finance/OpenBBTerminal"">
  <img src=""https://github.com/codespaces/badge.svg"" height=""20"" />"
fairseq,"<p align=""center"">
  <img src=""docs/fairseq_logo.png"" width=""150"">
  <br />
  <br />
  <a href=""https://opensource.fb.com/support-ukraine""><img alt=""Support Ukraine"" src=""https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB"" /></a>
  <a href=""https://github.com/pytorch/fairseq/blob/main/LICENSE""><img alt=""MIT License"" src=""https://img.shields.io/badge/license-MIT-blue.svg"" /></a>
  <a href=""https://github.com/pytorch/fairseq/releases""><img alt=""Latest Release"" src=""https://img.shields.io/github/release/pytorch/fairseq.svg"" /></a>
  <a href=""https://github.com/pytorch/fairseq/actions?query=workflow:build""><img alt=""Build Status"" src=""https://github.com/pytorch/fairseq/workflows/build/badge.svg"" /></a>
  <a href=""https://fairseq.readthedocs.io/en/latest/?badge=latest""><img alt=""Documentation Status"" src=""https://readthedocs.org/projects/fairseq/badge/?version=latest"" /></a>
  <a href=""https://app.circleci.com/pipelines/github/facebookresearch/fairseq/""><img al"
chatgpt-on-wechat,"# 简介

> chatgpt-on-wechat（简称CoW）项目是基于大模型的智能对话机器人，支持微信公众号、企业微信应用、飞书、钉钉接入，可选择GPT3.5/GPT4.0/Claude/Gemini/LinkAI/ChatGLM/KIMI/文心一言/讯飞星火/通义千问/LinkAI，能处理文本、语音和图片，通过插件访问操作系统和互联网等外部资源，支持基于自有知识库定制企业AI应用。

最新版本支持的功能如下：

-  ✅   **多端部署：** 有多种部署方式可选择且功能完备，目前已支持微信公众号、企业微信应用、飞书、钉钉等部署方式
-  ✅   **基础对话：** 私聊及群聊的消息智能回复，支持多轮会话上下文记忆，支持 GPT-3.5, GPT-4o-mini, GPT-4o,  GPT-4, Claude-3.5, Gemini, 文心一言, 讯飞星火, 通义千问，ChatGLM-4，Kimi(月之暗面), MiniMax
-  ✅   **语音能力：** 可识别语音消息，通过文字或语音回复，支持 azure, baidu, google, openai(whisper/tts) 等多种语音模型
-  ✅   **图像能力：** 支持图片生成、图片识别、图生图（如照片修复），可选择 Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, vision模型
-  ✅   **丰富插件：** 支持个性化插件扩展，已实现多角色切换、文字冒险、敏感词过滤、聊天记录总结、文档总结和对话、联网搜索等插件
-  ✅   **知识库：** 通过上传知识库文件自定义专属机器人，可作为数字分身、智能客服、私域助手使用，基于 [LinkAI](https://link-ai.tech) 实现

## 声明

1. 本项目遵循 [MIT开源协议](/LICENSE)，仅用于技术研究和学习，使用本项目时需遵守所在地法律法规、相关政策以及企业章程，禁止用于任何违法或侵犯他人权益的行为
2. 境内使用该项目时，请使用国内厂商的大模型服务，并进行必要的内容安全审核及过滤
3. 本项目主要接入协同办公平台，推荐使用公众号、企微自建应用、钉钉、飞书等接入通道，其他通道为历史产物已不维护
4. 任何个人、团队和企业，无论以何种"
detectron2,"<img src="".github/Detectron2-Logo-Horz.svg"" width=""300"" >

<a href=""https://opensource.facebook.com/support-ukraine"">
  <img src=""https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB"" alt=""Support Ukraine - Help Provide Humanitarian Aid to Ukraine."" />
</a>

Detectron2 is Facebook AI Research's next generation library
that provides state-of-the-art detection and segmentation algorithms.
It is the successor of
[Detectron](https://github.com/facebookresearch/Detectron/)
and [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/).
It supports a number of computer vision research projects and production applications in Facebook.

<div align=""center"">
  <img src=""https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png""/>
</div>
<br>

## Learn More about Detectron2

Explain Like I’m 5: Detectron2            |  Using Machine Learning with Detectron2
:-------------------------:|:----------------------"
jax,"<div align=""center"">
<img src=""https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png"" alt=""logo""></img>
</div>

# Transformable numerical computing at scale

![Continuous integration](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg)
![PyPI version](https://img.shields.io/pypi/v/jax)

[**Quickstart**](#quickstart-colab-in-the-cloud)
| [**Transformations**](#transformations)
| [**Install guide**](#installation)
| [**Neural net libraries**](#neural-network-libraries)
| [**Change logs**](https://jax.readthedocs.io/en/latest/changelog.html)
| [**Reference docs**](https://jax.readthedocs.io/en/latest/)


## What is JAX?

JAX is a Python library for accelerator-oriented array computation and program transformation,
designed for high-performance numerical computing and large-scale machine learning.

With its updated version of [Autograd](https://github.com/hips/autograd),
JAX can automatically differentiate native
Python and NumPy functions. It can"
gpt-pilot,"<div align=""center"">

# 🧑‍✈️ GPT PILOT 🧑‍✈️

</div>

---

<div align=""center"">

[![Discord Follow](https://dcbadge.vercel.app/api/server/HaqXugmxr9?style=flat)](https://discord.gg/HaqXugmxr9)
[![GitHub Repo stars](https://img.shields.io/github/stars/Pythagora-io/gpt-pilot?style=social)](https://github.com/Pythagora-io/gpt-pilot)
[![Twitter Follow](https://img.shields.io/twitter/follow/HiPythagora?style=social)](https://twitter.com/HiPythagora)

</div>

---

<div align=""center"">
<a href=""https://www.ycombinator.com/"" target=""_blank""><img src=""https://s3.amazonaws.com/assets.pythagora.ai/yc/PNG/Black.png"" alt=""Pythagora-io%2Fgpt-pilot | Trendshift"" style=""width: 250px; height: 93px;""/></a>
</div>
<br>
<div align=""center"">
<a href=""https://trendshift.io/repositories/466"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/466"" alt=""Pythagora-io%2Fgpt-pilot | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>
</div>

<br>
<br>

<div align=""cent"
pytorch-tutorial,"<p align=""center""><img width=""40%"" src=""logo/pytorch_logo_2018.svg"" /></p>

--------------------------------------------------------------------------------

This repository provides tutorial code for deep learning researchers to learn [PyTorch](https://github.com/pytorch/pytorch). In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish [Official Pytorch Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).


<br/>

## Table of Contents

#### 1. Basics
* [PyTorch Basics](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/pytorch_basics/main.py)
* [Linear Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/linear_regression/main.py#L22-L23)
* [Logistic Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/logistic_regression/main.py#L33-L34)
* [Feedforward Neural Network](https://gi"
ControlNet,"# News: A nightly version of ControlNet 1.1 is released!

[ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly) is released. Those new models will be merged to this repo after we make sure that everything is good.

# Below is ControlNet 1.0

Official implementation of [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet is a neural network structure to control diffusion models by adding extra conditions.

![img](github_page/he.png)

It copys the weights of neural network blocks into a ""locked"" copy and a ""trainable"" copy. 

The ""trainable"" one learns your condition. The ""locked"" one preserves your model. 

Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models.

The ""zero convolution"" is 1×1 convolution with both weight and bias initialized as zeros. 

Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.

No "
linux-insides,"linux-insides
===============

A book-in-progress about the linux kernel and its insides.

**The goal is simple** - to share my modest knowledge about the insides of the linux kernel and help people who are interested in linux kernel insides, and other low-level subject matter. Feel free to go through the book [Start here](https://github.com/0xAX/linux-insides/blob/master/SUMMARY.md)

**Questions/Suggestions**: Feel free about any questions or suggestions by pinging me at twitter [@0xAX](https://twitter.com/0xAX), adding an [issue](https://github.com/0xAX/linux-insides/issues/new) or just drop me an [email](mailto:anotherworldofworld@gmail.com).

Generating eBooks and PDFs - [documentation](https://github.com/GitbookIO/gitbook/blob/master/docs/ebook.md)

# Mailing List

We have a Google Group mailing list for learning the kernel source code. Here are some instructions about how to use it.

#### Join

Send an email with any subject/content to `kernelhacking+subscribe@googlegroups.com`. "
spaCy,"<a href=""https://explosion.ai""><img src=""https://explosion.ai/assets/img/logo.svg"" width=""125"" height=""125"" align=""right"" /></a>

# spaCy: Industrial-strength NLP

spaCy is a library for **advanced Natural Language Processing** in Python and
Cython. It's built on the very latest research, and was designed from day one to
be used in real products.

spaCy comes with [pretrained pipelines](https://spacy.io/models) and currently
supports tokenization and training for **70+ languages**. It features
state-of-the-art speed and **neural network models** for tagging, parsing,
**named entity recognition**, **text classification** and more, multi-task
learning with pretrained **transformers** like BERT, as well as a
production-ready [**training system**](https://spacy.io/usage/training) and easy
model packaging, deployment and workflow management. spaCy is commercial
open-source software, released under the
[MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).

💫 **Version 3.7 ou"
langflow,"<!-- markdownlint-disable MD030 -->

# [![Langflow](./docs/static/img/hero.png)](https://www.langflow.org)

<p align=""center"" style=""font-size: 12px;"">
    Langflow is a low-code app builder for RAG and multi-agent AI applications. It’s Python-based and agnostic to any model, API, or database.
</p>

<p align=""center"" style=""font-size: 12px;"">
    <a href=""https://docs.langflow.org"" style=""text-decoration: underline;"">Docs</a> -
    <a href=""https://astra.datastax.com/signup?type=langflow"" style=""text-decoration: underline;"">Free Cloud Service</a> -
    <a href=""https://docs.langflow.org/getting-started-installation"" style=""text-decoration: underline;"">Self Managed</a>
    
</p>

<div align=""center"">
  <a href=""./README.md""><img alt=""README in English"" src=""https://img.shields.io/badge/English-d9d9d9""></a>
  <a href=""./README.PT.md""><img alt=""README in Portuguese"" src=""https://img.shields.io/badge/Portuguese-d9d9d9""></a>
  <a href=""./README.ES.md""><img alt=""README in Spanish"" src=""https"
stanford_alpaca,"
<p align=""center"" width=""100%"">
<img src=""assets/logo.png"" alt=""Stanford-Alpaca"" style=""width: 50%; min-width: 300px; display: block; margin: auto;"">
</p>

# Stanford Alpaca: An Instruction-following LLaMA Model

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)
[![Weight Diff License](https://img.shields.io/badge/Weight%20Diff%20License-CC%20By%20NC%204.0-yellow)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/WEIGHT_DIFF_LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

This is the repo for the Stanford Alpaca project, which aims t"
interactive-coding-challenges,"<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/interactive-coding-challenges/master/images/cover_challenge.gif"">
</p>

interactive-coding-challenges
============

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/donnemartin/interactive-coding-challenges/master)

**120+ continually updated, interactive, and test-driven coding challenges**, with [Anki flashcards](#anki-flashcards-coding-and-design).

Challenges focus on **algorithms** and **data structures** found in **coding interviews**.

Each challenge has one or more reference solutions that are:

* Fully functional
* Unit tested
* Easy-to-understand

Challenges will soon provide on-demand [incremental hints](https://github.com/donnemartin/interactive-coding-challenges/issues/22) to help you arrive at the optimal solution.

Notebooks also detail:

* Constraints
* Test cases
* Algorithms
* Big-O time and space complexities

Also included are **unit tested reference impleme"
mmdetection,"<div align=""center"">
  <img src=""resources/mmdet-logo.png"" width=""600""/>
  <div>&nbsp;</div>
  <div align=""center"">
    <b><font size=""5"">OpenMMLab website</font></b>
    <sup>
      <a href=""https://openmmlab.com"">
        <i><font size=""4"">HOT</font></i>
      </a>
    </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <b><font size=""5"">OpenMMLab platform</font></b>
    <sup>
      <a href=""https://platform.openmmlab.com"">
        <i><font size=""4"">TRY IT OUT</font></i>
      </a>
    </sup>
  </div>
  <div>&nbsp;</div>

[![PyPI](https://img.shields.io/pypi/v/mmdet)](https://pypi.org/project/mmdet)
[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection.readthedocs.io/en/latest/)
[![badge](https://github.com/open-mmlab/mmdetection/workflows/build/badge.svg)](https://github.com/open-mmlab/mmdetection/actions)
[![codecov](https://codecov.io/gh/open-mmlab/mmdetection/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmdetection)
[![license](https://img.shields.io/"
ultralytics,"<div align=""center"">
  <p>
    <a href=""https://www.ultralytics.com/events/yolovision"" target=""_blank"">
      <img width=""100%"" src=""https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"" alt=""YOLO Vision banner""></a>
  </p>

[中文](https://docs.ultralytics.com/zh) | [한국어](https://docs.ultralytics.com/ko) | [日本語](https://docs.ultralytics.com/ja) | [Русский](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [Français](https://docs.ultralytics.com/fr) | [Español](https://docs.ultralytics.com/es) | [Português](https://docs.ultralytics.com/pt) | [Türkçe](https://docs.ultralytics.com/tr) | [Tiếng Việt](https://docs.ultralytics.com/vi) | [العربية](https://docs.ultralytics.com/ar) <br>

<div>
    <a href=""https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml""><img src=""https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg"" alt=""Ultralytics CI""></a>
    <a href=""https://zenodo.org/badge/latestdoi/2"
OpenVoice,"<div align=""center"">
  <div>&nbsp;</div>
  <img src=""resources/openvoicelogo.jpg"" width=""400""/> 

[Paper](https://arxiv.org/abs/2312.01479) |
[Website](https://research.myshell.ai/open-voice) 

</div>

## Introduction

### OpenVoice V1

As we detailed in our [paper](https://arxiv.org/abs/2312.01479) and [website](https://research.myshell.ai/open-voice), the advantages of OpenVoice are three-fold:

**1. Accurate Tone Color Cloning.**
OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.

**2. Flexible Voice Style Control.**
OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation. 

**3. Zero-shot Cross-lingual Voice Cloning.**
Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.

### OpenVoice V2

In April 2024, we relea"
tqdm,"|Logo|

tqdm
====

|Py-Versions| |Versions| |Conda-Forge-Status| |Docker| |Snapcraft|

|Build-Status| |Coverage-Status| |Branch-Coverage-Status| |Codacy-Grade| |Libraries-Rank| |PyPI-Downloads|

|LICENCE| |OpenHub-Status| |binder-demo| |awesome-python|

``tqdm`` derives from the Arabic word *taqaddum* (تقدّم) which can mean ""progress,""
and is an abbreviation for ""I love you so much"" in Spanish (*te quiero demasiado*).

Instantly make your loops show a smart progress meter - just wrap any
iterable with ``tqdm(iterable)``, and you're done!

.. code:: python

    from tqdm import tqdm
    for i in tqdm(range(10000)):
        ...

``76%|████████████████████████        | 7568/10000 [00:33<00:10, 229.00it/s]``

``trange(N)`` can be also used as a convenient shortcut for
``tqdm(range(N))``.

|Screenshot|
    |Video| |Slides| |Merch|

It can also be executed as a module with pipes:

.. code:: sh

    $ seq 9999999 | tqdm --bytes | wc -l
    75.2MB [00:00, 217MB/s]
    9999999

    $ tar -zcf -"
freqtrade,"# ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/workflows/Freqtrade%20CI/badge.svg)](https://github.com/freqtrade/freqtrade/actions/)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)
[![Maintainability](https://api.codeclimate.com/v1/badges/5737e6d668200b7518ff/maintainability)](https://codeclimate.com/github/freqtrade/freqtrade/maintainability)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plottin"
django-rest-framework,"# [Django REST framework][docs]

[![build-status-image]][build-status]
[![coverage-status-image]][codecov]
[![pypi-version]][pypi]

**Awesome web-browsable Web APIs.**

Full documentation for the project is available at [https://www.django-rest-framework.org/][docs].

---

# Funding

REST framework is a *collaboratively funded project*. If you use
REST framework commercially we strongly encourage you to invest in its
continued development by [signing up for a paid plan][funding].

The initial aim is to provide a single full-time position on REST framework.
*Every single sign-up makes a significant impact towards making that possible.*

[![][sentry-img]][sentry-url]
[![][stream-img]][stream-url]
[![][spacinov-img]][spacinov-url]
[![][retool-img]][retool-url]
[![][bitio-img]][bitio-url]
[![][posthog-img]][posthog-url]
[![][cryptapi-img]][cryptapi-url]
[![][fezto-img]][fezto-url]
[![][svix-img]][svix-url]
[![][zuplo-img]][zuplo-url]

Many thanks to all our [wonderful sponsors][sponsors], "
roop,"## This project has been discontinued

Yes, it still works, you can still use this software. It just won't recieve any updates now.

> I do not have the interest or time to oversee the development of this software. I thank all the amazing people who contributed to this project and made what it is in it's final form.

# Roop

> Take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training.

[![Build Status](https://img.shields.io/github/actions/workflow/status/s0md3v/roop/ci.yml.svg?branch=main)](https://github.com/s0md3v/roop/actions?query=workflow:ci)

<img src=""https://i.ibb.co/4RdPYwQ/Untitled.jpg""/>

## Installation

Be aware, the installation needs technical skills and is not for beginners. Please do not open platform and installation related issues on GitHub.

[Basic](https://github.com/s0md3v/roop/wiki/1.-Installation) - It is more likely to work on your computer, but will be quite slow

[Acceleration](ht"
pytorch-lightning,"<div align=""center"">

<img alt=""Lightning"" src=""https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png"" width=""800px"" style=""max-width: 100%;"">

<br/>
<br/>

**The deep learning framework to pretrain, finetune and deploy AI models.**

**NEW- Deploying models? Check out [LitServe](https://github.com/Lightning-AI/litserve), the PyTorch Lightning for model serving**

______________________________________________________________________

<p align=""center"">
    <a href=""#quick-start"" style=""margin: 0 10px;"">Quick start</a> •
  <a href=""#examples"">Examples</a> •
  <a href=""#why-pytorch-lightning"">PyTorch Lightning</a> •
  <a href=""#lightning-fabric-expert-control"">Fabric</a> •
  <a href=""https://lightning.ai/"">Lightning AI</a> •   
  <a href=""#community"">Community</a> •
  <a href=""https://pytorch-lightning.readthedocs.io/en/stable/"">Docs</a>
</p>

<!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL -->

[![PyPI - Python Version](https://im"
ChatGPT,"# ChatGPT <img src=""https://github.com/acheong08/ChatGPT/blob/main/logo.png?raw=true"" width=""15%""></img>

English - [中文](./README_zh.md) - [Spanish](./README_sp.md) - [日本語](./README_ja.md) - [한국어](./README_ko.md)

[![PyPi](https://img.shields.io/pypi/v/revChatGPT.svg)](https://pypi.python.org/pypi/revChatGPT)
[![Support_Platform](https://img.shields.io/pypi/pyversions/revChatGPT)](https://pypi.python.org/pypi/revChatGPT)
[![Downloads](https://static.pepy.tech/badge/revchatgpt)](https://pypi.python.org/pypi/revChatGPT)

Reverse Engineered ChatGPT API by OpenAI. Extensible for chatbots etc.

[![](https://github.com/acheong08/ChatGPT/blob/main/docs/view.gif?raw=true)](https://pypi.python.org/pypi/revChatGPT)

# Installation

```
python -m pip install --upgrade revChatGPT
```

### Suport Python Version

- Minimum - Python3.9
- Recommend - Python3.11+

<details>

  <summary>

# V1 Standard ChatGPT

V1 uses a cloudflare bypass proxy to make life convenient for everyone. The proxy is open sou"
Real-ESRGAN,"<p align=""center"">
  <img src=""assets/realesrgan_logo.png"" height=120>
</p>

## <div align=""center""><b><a href=""README.md"">English</a> | <a href=""README_CN.md"">简体中文</a></b></div>

<div align=""center"">

👀[**Demos**](#-demos-videos) **|** 🚩[**Updates**](#-updates) **|** ⚡[**Usage**](#-quick-inference) **|** 🏰[**Model Zoo**](docs/model_zoo.md) **|** 🔧[Install](#-dependencies-and-installation)  **|** 💻[Train](docs/Training.md) **|** ❓[FAQ](docs/FAQ.md) **|** 🎨[Contribution](docs/CONTRIBUTING.md)

[![download](https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg)](https://github.com/xinntao/Real-ESRGAN/releases)
[![PyPI](https://img.shields.io/pypi/v/realesrgan)](https://pypi.org/project/realesrgan/)
[![Open issue](https://img.shields.io/github/issues/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)
[![Closed issue](https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)
[![LICENSE](https://img.shi"
CheatSheetSeries,"# Welcome to the OWASP Cheat Sheet Series

[![OWASP Flagship](https://img.shields.io/badge/owasp-flagship%20project-48A646.svg)](https://www.owasp.org/index.php/OWASP_Project_Inventory#tab=Flagship_Projects)
[![Creative Commons License](https://img.shields.io/github/license/OWASP/CheatSheetSeries)](https://creativecommons.org/licenses/by-sa/4.0/ ""CC BY-SA 4.0"")

Welcome to the official repository for the Open Web Application Security Project® (OWASP) Cheat Sheet Series project. The project focuses on providing good security practices for builders in order to secure their applications.

In order to read the cheat sheets and **reference** them, use the project [official website](https://cheatsheetseries.owasp.org). The project details can be viewed on the [OWASP main website](https://owasp.org/www-project-cheat-sheets/) without the cheat sheets.

:triangular_flag_on_post: Markdown files are the working sources and aren't intended to be referenced in any external documentation, books or w"
numpy,"<h1 align=""center"">
<img src=""https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg"" width=""300"">
</h1><br>


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)


NumPy is "
vllm,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png"">
    <img alt=""vLLM"" src=""https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png"" width=55%>
  </picture>
</p>

<h3 align=""center"">
Easy, fast, and cheap LLM serving for everyone
</h3>

<p align=""center"">
| <a href=""https://docs.vllm.ai""><b>Documentation</b></a> | <a href=""https://vllm.ai""><b>Blog</b></a> | <a href=""https://arxiv.org/abs/2309.06180""><b>Paper</b></a> | <a href=""https://discord.gg/jz7wjKhh6g""><b>Discord</b></a> | <a href=""https://x.com/vllm_project""><b>Twitter/X</b></a> |

</p>


---

**vLLM, AMD, Anyscale Meet & Greet at [Ray Summit 2024](http://raysummit.anyscale.com) (Monday, Sept 30th, 5-7pm PT) at Marriott Marquis San Francisco**

We are excited to announce our special vLLM event in collaboration with AMD and Anyscale.
Join"
data-science-ipython-notebooks,"<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif"">
</p>

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png"">
  <br/>
</p>

# data-science-ipython-notebooks

## Index

* [deep-learning](#deep-learning)
    * [tensorflow](#tensor-flow-tutorials)
    * [theano](#theano-tutorials)
    * [keras](#keras-tutorials)
    * [caffe](#deep-learning-misc)
* [scikit-learn](#scikit-learn)
* [statistical-inference-scipy](#statistical-inference-scipy)
* [pandas](#pandas)
* [matplotlib](#matplotlib)
* [numpy](#numpy)
* [python-data](#python-data)
* [kaggle-and-business-analyses](#kaggle-and-business-analyses)
* [spark](#spark)
* [mapreduce-python](#mapreduce-python)
* [amazon web services](#aws)
* [command lines](#commands)
* [misc](#misc)
* [notebook-installation](#notebook-installation)
* [credits](#credits)
* [con"
python-fire,"# Python Fire [![PyPI](https://img.shields.io/pypi/pyversions/fire.svg?style=plastic)](https://github.com/google/python-fire)

_Python Fire is a library for automatically generating command line interfaces
(CLIs) from absolutely any Python object._

-   Python Fire is a simple way to create a CLI in Python.
    [[1]](docs/benefits.md#simple-cli)
-   Python Fire is a helpful tool for developing and debugging Python code.
    [[2]](docs/benefits.md#debugging)
-   Python Fire helps with exploring existing code or turning other people's
    code into a CLI. [[3]](docs/benefits.md#exploring)
-   Python Fire makes transitioning between Bash and Python easier.
    [[4]](docs/benefits.md#bash)
-   Python Fire makes using a Python REPL easier by setting up the REPL with the
    modules and variables you'll need already imported and created.
    [[5]](docs/benefits.md#repl)

## Installation

To install Python Fire with pip, run: `pip install fire`

To install Python Fire with conda, run: `conda "
hosts,"**Take Note!**

With the exception of issues and PRs regarding changes to
`hosts/data/StevenBlack/hosts`, all other issues regarding the content of the
produced hosts files should be made with the appropriate data source that
contributed the content in question. The contact information for all of the data
sources can be found in the `hosts/data/` directory.

---

![Logo](https://raw.githubusercontent.com/StevenBlack/hosts/master/.github/logo.png)

[![latest release](https://img.shields.io/github/release/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/releases)
[![license](https://img.shields.io/github/license/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/blob/master/license.txt)
[![repo size](https://img.shields.io/github/repo-size/StevenBlack/hosts)](https://github.com/StevenBlack/hosts)
[![contributors](https://img.shields.io/github/contributors/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/graphs/contributors)
[![Build Status](https://img.shields.io/gi"
glances,"===============================
Glances - An eye on your system
===============================

|  |pypi| |test| |contributors| |quality|
|  |starts| |docker| |pypistat|
|  |sponsors| |twitter|

.. |pypi| image:: https://img.shields.io/pypi/v/glances.svg
    :target: https://pypi.python.org/pypi/Glances

.. |starts| image:: https://img.shields.io/github/stars/nicolargo/glances.svg
    :target: https://github.com/nicolargo/glances/
    :alt: Github stars

.. |docker| image:: https://img.shields.io/docker/pulls/nicolargo/glances
    :target: https://hub.docker.com/r/nicolargo/glances/
    :alt: Docker pull

.. |pypistat| image:: https://pepy.tech/badge/glances/month
    :target: https://pepy.tech/project/glances
    :alt: Pypi downloads

.. |test| image:: https://github.com/nicolargo/glances/actions/workflows/ci.yml/badge.svg?branch=develop
    :target: https://github.com/nicolargo/glances/actions
    :alt: Linux tests (GitHub Actions)

.. |contributors| image:: https://img.shields.io/g"
tinygrad,"<div align=""center"">

<picture>
  <source media=""(prefers-color-scheme: light)"" srcset=""/docs/logo_tiny_light.svg"">
  <img alt=""tiny corp logo"" src=""/docs/logo_tiny_dark.svg"" width=""50%"" height=""50%"">
</picture>

tinygrad: For something between [PyTorch](https://github.com/pytorch/pytorch) and [karpathy/micrograd](https://github.com/karpathy/micrograd). Maintained by [tiny corp](https://tinygrad.org).

<h3>

[Homepage](https://github.com/tinygrad/tinygrad) | [Documentation](https://docs.tinygrad.org/) | [Discord](https://discord.gg/ZjZadyC7PK)

</h3>

[![GitHub Repo stars](https://img.shields.io/github/stars/tinygrad/tinygrad)](https://github.com/tinygrad/tinygrad/stargazers)
[![Unit Tests](https://github.com/tinygrad/tinygrad/actions/workflows/test.yml/badge.svg)](https://github.com/tinygrad/tinygrad/actions/workflows/test.yml)
[![Discord](https://img.shields.io/discord/1068976834382925865)](https://discord.gg/ZjZadyC7PK)

</div>

---

This may not be the best deep learning framework,"
mindsdb,"<a name=""readme-top""></a>

<div align=""center"">
	<a href=""https://pypi.org/project/MindsDB/"" target=""_blank""><img src=""https://badge.fury.io/py/MindsDB.svg"" alt=""MindsDB Release""></a>
	<a href=""https://www.python.org/downloads/"" target=""_blank""><img src=""https://img.shields.io/badge/python-3.8.x%7C%203.9.x%7C%203.10.x%7C%203.11.x-brightgreen.svg"" alt=""Python supported""></a>
	<a href=""https://ossrank.com/p/630""><img src=""https://shields.io/endpoint?url=https://ossrank.com/shield/630""></a>
	<img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dm/Mindsdb"">
	<a href=""https://hub.docker.com/u/mindsdb"" target=""_blank""><img src=""https://img.shields.io/docker/pulls/mindsdb/mindsdb"" alt=""Docker pulls""></a>

  <br />
  <br />

  <a href=""https://github.com/mindsdb/mindsdb"">
    <img src=""/docs/assets/mindsdb_logo.jpg"" alt=""MindsDB"" width=""300"">
  </a>

  <p align=""center"">
    <br />
    <a href=""https://www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo"">"
llama3,"<p align=""center"">
  <img src=""https://github.com/meta-llama/llama3/blob/main/Llama3_Repo.jpeg"" width=""400""/>
</p>

<p align=""center"">
        🤗 <a href=""https://huggingface.co/meta-Llama""> Models on Hugging Face</a>&nbsp | <a href=""https://ai.meta.com/blog/""> Blog</a>&nbsp |  <a href=""https://llama.meta.com/"">Website</a>&nbsp | <a href=""https://llama.meta.com/get-started/"">Get Started</a>&nbsp
<br>

---

## **Note of deprecation**

Thank you for developing with Llama models. As part of the Llama 3.1 release, we’ve consolidated GitHub repos and added some additional repos as we’ve expanded Llama’s functionality into being an e2e Llama Stack. Please use the following repos going forward:
- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mit"
Detectron,"**Detectron is deprecated. Please see [detectron2](https://github.com/facebookresearch/detectron2), a ground-up rewrite of Detectron in PyTorch.**

# Detectron

Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.

At FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](http"
DeepFaceLive,"<table align=""center"" border=""0"">

<tr><td colspan=2 align=""center"">

![](doc/deepfacelive_intro.png)

![](doc/logo_onnx.png)![](doc/logo_directx.png)![](doc/logo_python.png)

</td></tr>
</table>
<table align=""center"" border=""0"">

<tr><td colspan=2 align=""center"">

## Face Swap (DFM)

You can swap your face from a webcam or the face in the video using trained face models.

Here is a list of available ready-to-use public face models.

These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he's breathtaking!
</td></tr>

<tr><td colspan=2 align=""center"">

<table align=""center"" border=""0"">
<tr><td align=""center"">
Keanu Reeves

<img src=""doc/celebs/Keanu_Reeves/Keanu_Reeves.png"" width=128></img>

<a href=""doc/celebs/Keanu_Reeves/examples.md"">examples</a>
</td><td align=""center"">
Irina Arty

<img src=""doc/celebs/Irina_Arty/Irina_Arty.png"" width=128></img>

examples
</td><td align=""center"">
Millie Park

<img src=""doc/celebs/Millie_Park/M"
redash,"<p align=""center"">
  <img title=""Redash"" src='https://redash.io/assets/images/logo.png' width=""200px""/>
</p>

[![Documentation](https://img.shields.io/badge/docs-redash.io/help-brightgreen.svg)](https://redash.io/help/)
[![GitHub Build](https://github.com/getredash/redash/actions/workflows/ci.yml/badge.svg)](https://github.com/getredash/redash/actions)

Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.

Redash features:

1. **Browser-based**: Everything in your browser, with a shareable URL.
2. **Ease-of-use**: Become immediately productive with data without the need to master complex software.
3. **Qu"
python-telegram-bot,".. image:: https://raw.githubusercontent.com/python-telegram-bot/logos/master/logo-text/png/ptb-logo-text_768.png
   :align: center
   :target: https://python-telegram-bot.org
   :alt: python-telegram-bot Logo

.. image:: https://img.shields.io/pypi/v/python-telegram-bot.svg
   :target: https://pypi.org/project/python-telegram-bot/
   :alt: PyPi Package Version

.. image:: https://img.shields.io/pypi/pyversions/python-telegram-bot.svg
   :target: https://pypi.org/project/python-telegram-bot/
   :alt: Supported Python versions

.. image:: https://img.shields.io/badge/Bot%20API-7.10-blue?logo=telegram
   :target: https://core.telegram.org/bots/api-changelog
   :alt: Supported Bot API version

.. image:: https://img.shields.io/pypi/dm/python-telegram-bot
   :target: https://pypistats.org/packages/python-telegram-bot
   :alt: PyPi Package Monthly Download

.. image:: https://readthedocs.org/projects/python-telegram-bot/badge/?version=stable
   :target: https://docs.python-telegram-bot.org/"
Depix,"# Depix

Depix is a PoC for a technique to recover plaintext from pixelized screenshots.

This implementation works on pixelized images that were created with a linear box filter.
In [this article](https://www.spipm.nl/2030.html) I cover background information on pixelization and similar research.

## Example

![image](docs/img/Recovering_prototype_latest.png)

## Updates

* 27 nov '23: Refactored and removed all this pip stuff. I like scripts I can just run. If a package can't be found, just install it. Also added `tool_show_boxes.py` to show how bad the box detector is (you have to really cut out the pixels exactly). Made a TODO to create a version that just cuts out boxes of static size.

## Installation

* Install the dependencies
* Run Depix:

```sh
python3 depix.py \
    -p /path/to/your/input/image.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png \
    -o /path/to/your/output.png
```

## Example usage

* Depixelize example image created with Notep"
Umi-OCR,"<p align=""left"">
    <span>
        <b>中文</b>
    </span>
    <span> • </span>
    <a href=""README_en.md"">
        English
    </a>
    <span> • </span>
    <a href=""README_ja.md"">
        日本語
    </a>
</p>

<p align=""center"">
  <a href=""https://github.com/hiroi-sora/Umi-OCR"">
    <img width=""200"" height=""128"" src=""https://tupian.li/images/2022/10/27/icon---256.png"" alt=""Umi-OCR"">
  </a>
</p>

<h1 align=""center"">Umi-OCR 文字识别工具</h1>

<p align=""center"">
  <a href=""https://github.com/hiroi-sora/Umi-OCR/releases/latest"">
    <img src=""https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square"" alt=""Umi-OCR"">
  </a>
  <a href=""https://github.com/hiroi-sora/Umi-OCR/blob/main/LICENSE"">
    <img src=""https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square"" alt=""LICENSE"">
  </a>
  <a href=""#下载发行版"">
    <img src=""https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square"" alt=""forks"">
  </a>
  <a href=""https://star-history.com/#hiroi-s"
Hello-Python,"# Hello Python

[![Python](https://img.shields.io/badge/Python-3.10+-yellow?style=for-the-badge&logo=python&logoColor=white&labelColor=101010)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.88.0+-00a393?style=for-the-badge&logo=fastapi&logoColor=white&labelColor=101010)](https://fastapi.tiangolo.com)
[![MongoDB](https://img.shields.io/badge/MongoDB-6.0+-00684A?style=for-the-badge&logo=mongodb&logoColor=white&labelColor=101010)](https://www.mongodb.com)
[![ChatGPT](https://img.shields.io/badge/ChatGPT-GPT--4-7CF178?style=for-the-badge&logo=openai&logoColor=white&labelColor=101010)](https://platform.openai.com)
[![Reflex](https://img.shields.io/badge/Reflex-0.4.6+-5646ED?style=for-the-badge&logo=reflex&logoColor=white&labelColor=101010)](https://reflex.dev)

## Curso para aprender el lenguaje de programación Python desde cero y para principiantes

![](./Images/header.jpg)

### Proyecto realizado durante emisiones en directo desde [Twitch](https://twitch.tv/moured"
cascadia-code,"![Cascadia Code](images/cascadia-code.png)

# About Cascadia Code
Cascadia is a fun new coding font that comes bundled with [Windows Terminal](https://github.com/microsoft/terminal), and is now the default font in Visual Studio as well. 

# Font Variants
-  `Cascadia Code`: standard version of Cascadia
-  `Cascadia Mono`: a version of Cascadia that doesn't have ligatures
-  `Cascadia (Code|Mono) PL`: a version of Cascadia that has embedded Powerline symbols
-  `Cascadia (Code|Mono) NF`: a version of Cascadia that has Nerd Font symbols

For the italic, there is a standard `italic` and a `cursive` variant accessible via `ss01` (see [below](https://github.com/microsoft/cascadia-code/blob/main/README.md#to-enable-the-cursive-form-of-the-italic-heres-the-code-you-should-use)). 

# Font features
![Coding Ligatures](images/ligatures.png)

![Arrow Support](images/arrow_support.png)

![Stylistic Sets](images/stylistic_set.png)

Enabling stylistic sets will [vary between appl"
spleeter,"<img src=""https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png"" height=""80"" />

[![Github actions](https://github.com/deezer/spleeter/workflows/pytest/badge.svg)](https://github.com/deezer/spleeter/actions) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spleeter) [![PyPI version](https://badge.fury.io/py/spleeter.svg)](https://badge.fury.io/py/spleeter) [![Conda](https://img.shields.io/conda/vn/deezer-research/spleeter)](https://anaconda.org/deezer-research/spleeter) [![Docker Pulls](https://img.shields.io/docker/pulls/deezer/spleeter)](https://hub.docker.com/r/deezer/spleeter) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/spleeter/community) [![status](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b/status.svg)](https://joss.theoj.org/papers/"
ItChat,"# itchat

[![Gitter][gitter-picture]][gitter] ![py27][py27] ![py35][py35] [English version][english-version]

itchat是一个开源的微信个人号接口，使用python调用微信从未如此简单。

使用不到三十行的代码，你就可以完成一个能够处理所有信息的微信机器人。

当然，该api的使用远不止一个机器人，更多的功能等着你来发现，比如[这些][tutorial2]。

该接口与公众号接口[itchatmp][itchatmp]共享类似的操作方式，学习一次掌握两个工具。

如今微信已经成为了个人社交的很大一部分，希望这个项目能够帮助你扩展你的个人的微信号、方便自己的生活。

## 安装

可以通过本命令安装itchat：

```python
pip install itchat
```

## 简单入门实例

有了itchat，如果你想要给文件传输助手发一条信息，只需要这样：

```python
import itchat

itchat.auto_login()

itchat.send('Hello, filehelper', toUserName='filehelper')
```

如果你想要回复发给自己的文本消息，只需要这样：

```python
import itchat

@itchat.msg_register(itchat.content.TEXT)
def text_reply(msg):
    return msg.text

itchat.auto_login()
itchat.run()
```

一些进阶应用可以在下面的开源机器人的源码和进阶应用中看到，或者你也可以阅览[文档][document]。

## 试一试

这是一个基于这一项目的[开源小机器人][robot-source-code]，百闻不如一见，有兴趣可以尝试一下。

由于好友数量实在增长过快，自动通过好友验证的功能演示暂时关闭。

![QRCode][robot-qr]

## 截屏

![file-autoreply][robot-demo-file] ![login-page][robot-demo-login]

## 进阶应用

### 特殊的字典使用方式
"
YouCompleteMe,"YouCompleteMe: a code-completion engine for Vim
===============================================

[![Gitter room](https://img.shields.io/gitter/room/Valloric/YouCompleteMe.svg)](https://gitter.im/Valloric/YouCompleteMe)
[![Build status](https://dev.azure.com/YouCompleteMe/YCM/_apis/build/status/ycm-core.YouCompleteMe?branchName=master)](https://dev.azure.com/YouCompleteMe/YCM/_build?definitionId=3&branchName=master)
[![Coverage status](https://img.shields.io/codecov/c/github/ycm-core/YouCompleteMe/master.svg)](https://codecov.io/gh/ycm-core/YouCompleteMe)

Help, Advice, Support
---------------------

Looking for help, advice, or support? Having problems getting YCM to work?

First carefully read the [installation instructions](#installation) for your OS.
We recommend you use the supplied `install.py` - the ""full"" installation guide
is for rare, advanced use cases and most users should use `install.py`.

If the server isn't starting and you're getting a ""YouCompleteMe unavailable""
error,"
so-vits-svc,"<div align=""center"">
<img alt=""LOGO"" src=""https://avatars.githubusercontent.com/u/127122328?s=400&u=5395a98a4f945a3a50cb0cc96c2747505d190dbc&v=4"" width=""300"" height=""300"" />
  
# SoftVC VITS Singing Voice Conversion

[**English**](./README.md) | [**中文简体**](./README_zh_CN.md)

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/svc-develop-team/so-vits-svc/blob/4.1-Stable/sovits4_for_colab.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-AGPL3.0-green.svg?style=for-the-badge)](https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/LICENSE)

This round of limited time update is coming to an end, the warehouse will enter the Archieve state, please know

</div>

> ✨ A studio that contains visible f0 editor, speaker mix timeline editor and other features (Where the Onnx models are used) : [MoeVoiceStudio](https://github.com/NaruseMioShirakana/MoeVoiceStudio)

"
MiniGPT-4,"# MiniGPT-V

<font size='5'>**MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning**</font>

Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong☨, Mohamed Elhoseiny☨

☨equal last author

<a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/abs/2310.09478.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>  <a href='https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'> <a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Gradio-Demo-blue'></a> [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=atFCwV2hSY4)


<font size='5'> **MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models**</font>

Deyao Zhu*, J"
diffusers,"<!---
Copyright 2022 - The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg"" width=""400""/>
    <br>
<p>
<p align=""center"">
    <a href=""https://github.com/huggingface/diffusers/blob/main/LICENSE""><img alt=""GitHub"" src=""https://img.shields.io/github/license/huggingface/datasets.svg?color=blue""></a>
    <a href=""https://github.com/hugg"
jumpserver,"<div align=""center"">
  <a name=""readme-top""></a>
  <a href=""https://jumpserver.org/index-en.html""><img src=""https://download.jumpserver.org/images/jumpserver-logo.svg"" alt=""JumpServer"" width=""300"" /></a>
  
## An open-source PAM tool (Bastion Host)

[![][license-shield]][license-link]
[![][discord-shield]][discord-link]
[![][docker-shield]][docker-link]
[![][github-release-shield]][github-release-link]
[![][github-stars-shield]][github-stars-link]

**English** · [简体中文](./README.zh-CN.md)
</div>
<br/>

## What is JumpServer?

JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.

![JumpServer Overview](https://github.com/jumpserver/jumpserver/assets/32935519/35a371cb-8590-40ed-88ec-f351f8cf9045)

## Quickstart

Prepare a clean Linux Server ( 64 bit, >= 4c8g )

```sh
curl -sSL https://github.com/jumpserver/jumpserver/releases/l"
textual,"


![Textual splash image](https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png)

[![Discord](https://img.shields.io/discord/1026214085173461072)](https://discord.gg/Enf6Z3qhVr)


# Textual

Textual is a *Rapid Application Development* framework for Python.

Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a [web browser](https://github.com/Textualize/textual-web)!


<details>
  <summary> 🎬 Demonstration </summary>
  <hr>

A quick run through of some Textual features.



https://user-images.githubusercontent.com/554369/197355913-65d3c125-493d-4c05-a590-5311f16c40ff.mov



 </details>


## About

Textual adds interactivity to [Rich](https://github.com/Textualize/rich) with an API inspired by modern web development.

On modern terminal software (installed by default on most systems), Textual apps can use **16.7 million** colors with mouse support and smooth flicker-free animation. A powerful layout engine and re-usable "
pipenv,"Pipenv: Python Development Workflow for Humans
==============================================

[![image](https://img.shields.io/pypi/v/pipenv.svg)](https://python.org/pypi/pipenv)
[![image](https://img.shields.io/pypi/l/pipenv.svg)](https://python.org/pypi/pipenv)
[![CI](https://github.com/pypa/pipenv/actions/workflows/ci.yaml/badge.svg)](https://github.com/pypa/pipenv/actions/workflows/ci.yaml)
[![image](https://img.shields.io/pypi/pyversions/pipenv.svg)](https://python.org/pypi/pipenv)

------------------------------------------------------------------------

**Pipenv** is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip, python (using system python, pyenv or asdf) and virtualenv.
*Linux, macOS, and Windows are all first-class citizens in pipenv.*

Pipenv automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your `Pipfile` as you install/uninstall packages. It also genera"
locust,"# Locust

[![PyPI](https://img.shields.io/pypi/v/locust.svg)](https://pypi.org/project/locust/)<!--![Python Version from PEP 621 TOML](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Flocustio%2Flocust%2Fmaster%2Fpyproject.toml)-->[![Downloads](https://pepy.tech/badge/locust/week)](https://pepy.tech/project/locust)
[![Build Status](https://github.com/locustio/locust/workflows/Tests/badge.svg)](https://github.com/locustio/locust/actions?query=workflow%3ATests)
[![GitHub contributors](https://img.shields.io/github/contributors/locustio/locust.svg)](https://github.com/locustio/locust/graphs/contributors)
[![Support Ukraine Badge](https://bit.ly/support-ukraine-now)](https://github.com/support-ukraine/support-ukraine)

Locust is an open source performance/load testing tool for HTTP and other protocols. Its developer-friendly approach lets you define your tests in regular Python code.

Locust tests can be run from command line or usi"
celery,".. image:: https://docs.celeryq.dev/en/latest/_images/celery-banner-small.png

|build-status| |coverage| |license| |wheel| |semgrep| |pyversion| |pyimp| |ocbackerbadge| |ocsponsorbadge|

:Version: 5.5.0b3 (immunity)
:Web: https://docs.celeryq.dev/en/stable/index.html
:Download: https://pypi.org/project/celery/
:Source: https://github.com/celery/celery/
:Keywords: task, queue, job, async, rabbitmq, amqp, redis,
  python, distributed, actors

Donations
=========

This project relies on your generous donations.

If you are using Celery to create a commercial product, please consider becoming our `backer`_ or our `sponsor`_ to ensure Celery's future.

.. _`backer`: https://opencollective.com/celery#backer
.. _`sponsor`: https://opencollective.com/celery#sponsor

For enterprise
==============

Available as part of the Tidelift Subscription.

The maintainers of ``celery`` and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open sour"
Mask_RCNN,"# Mask R-CNN for Object Detection and Segmentation

This is an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870) on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.

![Instance Segmentation Sample](assets/street.png)

The repository includes:
* Source code of Mask R-CNN built on FPN and ResNet101.
* Training code for MS COCO
* Pre-trained weights for MS COCO
* Jupyter notebooks to visualize the detection pipeline at every step
* ParallelModel class for multi-GPU training
* Evaluation on MS COCO metrics (AP)
* Example of training on your own dataset


The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released [Matterport3D](https://matterport.com/blog/2017/09/20/announcin"
vnpy,"# VeighNa - By Traders, For Traders.

<p align=""center"">
  <img src =""https://vnpy.oss-cn-shanghai.aliyuncs.com/veighna-logo.png""/>
</p>

💬 Want to read this in **english** ? Go [**here**](README_ENG.md)

<p align=""center"">
    <img src =""https://img.shields.io/badge/version-3.9.2-blueviolet.svg""/>
    <img src =""https://img.shields.io/badge/platform-windows|linux|macos-yellow.svg""/>
    <img src =""https://img.shields.io/badge/python-3.10|3.11.|3.12-blue.svg"" />
    <img src =""https://img.shields.io/github/actions/workflow/status/vnpy/vnpy/pythonapp.yml?branch=master""/>
    <img src =""https://img.shields.io/github/license/vnpy/vnpy.svg?color=orange""/>
</p>

VeighNa是一套基于Python的开源量化交易系统开发框架，在开源社区持续不断的贡献下一步步成长为多功能量化交易平台，自发布以来已经积累了众多来自金融机构或相关领域的用户，包括私募基金、证券公司、期货公司等。

:rocket: :rocket: :rocket: **面向专业交易员的【VeighNa Elite量化终端】已经正式发布，针对专业交易员群体在海量策略并发、智能移仓换月、算法拆单执行、多账户交易支持等方面的需求提供了完善支持。了解更详细的信息请扫描下方二维码关注后，点击菜单栏的【社区交流 -> Elite会员服务】即可**：

<p align=""center"">
  <img src =""https://vnpy.oss-cn-shangha"
wttr.in,"
*wttr.in — the right way to ~check~ `curl` the weather!*

wttr.in is a console-oriented weather forecast service that supports various information
representation methods like terminal-oriented ANSI-sequences for console HTTP clients
(curl, httpie, or wget), HTML for web browsers, or PNG for graphical viewers.

Originally started as a small project, a wrapper for [wego](https://github.com/schachmat/wego),
intended to demonstrate the power of the console-oriented services,
*wttr.in* became a popular weather reporting service, handling tens of millions of queries daily.

You can see it running here: [wttr.in](https://wttr.in).

[Documentation](https://wttr.in/:help) | [Usage](https://github.com/chubin/wttr.in#usage) | [One-line output](https://github.com/chubin/wttr.in#one-line-output) | [Data-rich output format](https://github.com/chubin/wttr.in#data-rich-output-format-v2) | [Map view](https://github.com/chubin/wttr.in#map-view-v3) | [Output formats](https://github.com/chubin/wttr.in#di"
generative-models,"# Generative Models by Stability AI

![sample1](assets/000.jpg)

## News


**July 24, 2024**
- We are releasing **[Stable Video 4D (SV4D)](https://huggingface.co/stabilityai/sv4d)**, a video-to-4D diffusion model for novel-view video synthesis. For research purposes:
    - **SV4D** was trained to generate 40 frames (5 video frames x 8 camera views) at 576x576 resolution, given 5 context frames (the input video), and 8 reference views (synthesised from the first frame of the input video, using a multi-view diffusion model like SV3D) of the same size, ideally white-background images with one object.
    - To generate longer novel-view videos (21 frames), we propose a novel sampling method using SV4D, by first sampling 5 anchor frames and then densely sampling the remaining frames while maintaining temporal consistency.
    - To run the community-build gradio demo locally, run `python -m scripts.demo.gradio_app_sv4d`.
    - Please check our [project page](https://sv4d.github.io), [tech re"
algorithms,"[![PyPI version](https://badge.fury.io/py/algorithms.svg)](https://badge.fury.io/py/algorithms)
[![Open Source Helpers](https://www.codetriage.com/keon/algorithms/badges/users.svg)](https://www.codetriage.com/keon/algorithms)
[![Build Status](https://travis-ci.org/keon/algorithms.svg?branch=master)](https://travis-ci.org/keon/algorithms)
[![Coverage Status](https://coveralls.io/repos/github/keon/algorithms/badge.svg?branch=master)](https://coveralls.io/github/keon/algorithms?branch=master)

<p align=""center""><img src=""https://raw.githubusercontent.com/keon/algorithms/master/docs/source/_static/logo/logotype1blue.png""></p>

Pythonic Data Structures and Algorithms
=========================================

Minimal and clean example implementations of data structures and algorithms in Python 3.

## Contributing
Thanks for your interest in contributing! There are many ways to contribute to this project. [Get started here](CONTRIBUTING.md)

## Tests

### Use unittest
For running all tests w"
kitty,"= kitty - the fast, feature-rich, cross-platform, GPU based terminal

See https://sw.kovidgoyal.net/kitty/[the kitty website].

image:https://github.com/kovidgoyal/kitty/workflows/CI/badge.svg[""Build status"", link=""https://github.com/kovidgoyal/kitty/actions?query=workflow%3ACI""]

https://sw.kovidgoyal.net/kitty/faq/[Frequently Asked Questions]

To ask other questions about kitty usage, use either the https://github.com/kovidgoyal/kitty/discussions/[discussions on GitHub] or the
https://www.reddit.com/r/KittyTerminal[Reddit community]

Packaging status in various repositories:

image:https://repology.org/badge/vertical-allrepos/kitty.svg?columns=3&header=kitty[""Packaging status"", link=""https://repology.org/project/kitty/versions""]
"
ML-From-Scratch,"# Machine Learning From Scratch

## About
Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.

The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.

## Table of Contents
- [Machine Learning From Scratch](#machine-learning-from-scratch)
  * [About](#about)
  * [Table of Contents](#table-of-contents)
  * [Installation](#installation)
  * [Examples](#examples)
    + [Polynomial Regression](#polynomial-regression)
    + [Classification With CNN](#classification-with-cnn)
    + [Density-Based Clustering](#density-based-clustering)
    + [Generating Handwritten Digits](#generating-handwritten-digits)
    + [Deep Reinforcement Learning](#deep-reinforcement-learning)
    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)
    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-"
EasyOCR,"# EasyOCR

[![PyPI Status](https://badge.fury.io/py/easyocr.svg)](https://badge.fury.io/py/easyocr)
[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/JaidedAI/EasyOCR/blob/master/LICENSE)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.to/easyocr)
[![Tweet](https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR)
[![Twitter](https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat)](https://twitter.com/JaidedAI)

Ready-to-use OCR with 80+ [supported languages](https://www.jaided.ai/easyocr) and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.

[Try Demo on our website](https://www.jaided.ai/easyocr)

Integrated into [Huggingface Spaces 🤗](https://huggingface.co/spaces) using [Gradio](https://githu"
JARVIS,"# JARVIS


[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2303.17580)
[![Open in Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/microsoft/HuggingGPT)

The mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community.

## What's New

+  [2024.01.15] We release Easytool for easier tool usage.
   + The code and datasets are available at [EasyTool](/easytool).
   + The paper is available at [EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction](https://arxiv.org/abs/2401.06201).
+  [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs.
   + The code and datasets are available at [TaskBench](/taskbench).
   + The paper is available at [TaskBench: Benchmarking Large Language Models for Task Automation](https://arxiv.org/abs/2311.18760).
+  [2023.07.28] We are now in the process of plann"
d2l-en,"<div align=""left"">
  <img src=""https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png"" width=""350"">
</div>

# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions

[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)

[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)

<h5 align=""center""><i>The best way to understand deep learning is learning by doing.</i></h5>

<p align=""center"">
  <img width=""200""  src=""static/frontpage/_images/eq.jpg"">
  <img width=""200""  src=""static/frontpage/_images/figure.jpg"">
  <img width=""200""  src=""static/frontpage/_images/code.jpg"">
  <img width=""200""  src=""static/frontpage/_images/notebook.gif"">
</p>

This open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the c"
Retrieval-based-Voice-Conversion-WebUI,"<div align=""center"">

<h1>Retrieval-based-Voice-Conversion-WebUI</h1>
一个基于VITS的简单易用的变声框架<br><br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange
)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)

<img src=""https://counter.seku.su/cmoe?name=rvc&theme=r34"" /><br>

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/Retrieval_based_Voice_Conversion_WebUI.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/🤗%20-Spaces-yellow.svg?style=for-the-badge)](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)

[![Discord](https://img.shields.io/badge/RVC%20Developers-Discor"
insightface,"
# InsightFace: 2D and 3D Face Analysis Project

<div align=""left"">
  <img src=""https://insightface.ai/assets/img/custom/logo3.jpg"" width=""240""/>
</div>

[InsightFace](https://insightface.ai) project is mainly maintained By [Jia Guo](mailto:guojia@gmail.com?subject=[GitHub]%20InsightFace%20Project) and [Jiankang Deng](https://jiankangdeng.github.io/). 

For all main contributors, please check [contributing](#contributing).

## License

The code of InsightFace is released under the MIT License. There is no limitation for both academic and commercial usage.

The training data containing the annotation (and the models trained with these data) are available for non-commercial research purposes only.

Both manual-downloading models from our github repo and auto-downloading models with our [python-library](python-package) follow the above license policy(which is for non-commercial research purposes only).

## Top News

**`2024-08-01`** We have integrated our most advanced face-swapping model"
algo,"# 数据结构和算法必知必会的50个代码实现
### 微信搜索我的公众号“小争哥”，或者微信扫描下面二维码关注
### 关注微信公众号，回复”PDF“获取独家算法资料。
### 前Google工程师，10万人跟着学的《数据结构和算法之美》《设计模式之美》专栏作者
![t2](https://github.com/wangzheng0822/markdownphotos/blob/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg)

## 数组
* 实现一个支持动态扩容的数组
* 实现一个大小固定的有序数组，支持动态增删改操作
* 实现两个有序数组合并为一个有序数组

## 链表
* 实现单链表、循环链表、双向链表，支持增删操作
* 实现单链表反转
* 实现两个有序的链表合并为一个有序链表
* 实现求链表的中间结点

## 栈
* 用数组实现一个顺序栈
* 用链表实现一个链式栈
* 编程模拟实现一个浏览器的前进、后退功能

## 队列
* 用数组实现一个顺序队列
* 用链表实现一个链式队列
* 实现一个循环队列

## 递归
* 编程实现斐波那契数列求值f(n)=f(n-1)+f(n-2)
* 编程实现求阶乘n!
* 编程实现一组数据集合的全排列

## 排序
* 实现归并排序、快速排序、插入排序、冒泡排序、选择排序
* 编程实现O(n)时间复杂度内找到一组数据的第K大元素

## 二分查找
* 实现一个有序数组的二分查找算法
* 实现模糊二分查找算法（比如大于等于给定值的第一个元素）

## 散列表
* 实现一个基于链表法解决冲突问题的散列表
* 实现一个LRU缓存淘汰算法

## 字符串
* 实现一个字符集，只包含a～z这26个英文字母的Trie树
* 实现朴素的字符串匹配算法

## 二叉树
* 实现一个二叉查找树，并且支持插入、删除、查找操作
* 实现查找二叉查找树中某个节点的后继、前驱节点
* 实现二叉树前、中、后序以及按层遍历

## 堆
* 实现一个小顶堆、大顶堆、优先级队列
* 实现堆排序
* 利用优先级队列合并K个有序数组
* 求一组动态数据集合的最大Top K

## 图
* 实现有向图、无向图、有权图、无权图的邻接矩阵和邻接表表示方法
* 实现图的深度优先搜索、广度优先搜索
* 实现Dijkstra算法、"
PythonRobotics,"<img src=""https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"" align=""right"" width=""300"" alt=""header pic""/>

# PythonRobotics
![GitHub_Action_Linux_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg)
![GitHub_Action_MacOS_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg)
![GitHub_Action_Windows_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)
[![codecov](https://codecov.io/gh/AtsushiSakai/PythonRobotics/branch/master/graph/badge.svg)](https://codecov.io/gh/AtsushiSakai/PythonRobotics)

Python codes for robotics algorithm.


# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * "
pytorch-CycleGAN-and-pix2pix,"
<img src='imgs/horse2zebra.gif' align=""right"" width=384>

<br><br><br>

# CycleGAN and pix2pix in PyTorch

**New**:  Please check out [img2img-turbo](https://github.com/GaParmar/img2img-turbo) repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDiffusion-Turbo model. The inference time for 512x512 image is 0.29 sec on A6000 and 0.11 sec on A100.

Please check out [contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.

We provide PyTorch implementations for both unpaired and paired image-to-image translation.

The code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesungp), and supported by [Tongzhou Wang](https://github.com/Ss"
NLP-progress,"# Tracking Progress in Natural Language Processing

## Table of contents

### English

- [Automatic speech recognition](english/automatic_speech_recognition.md)
- [CCG](english/ccg.md)
- [Common sense](english/common_sense.md)
- [Constituency parsing](english/constituency_parsing.md)
- [Coreference resolution](english/coreference_resolution.md)
- [Data-to-Text Generation](english/data_to_text_generation.md)
- [Dependency parsing](english/dependency_parsing.md)
- [Dialogue](english/dialogue.md)
- [Domain adaptation](english/domain_adaptation.md)
- [Entity linking](english/entity_linking.md)
- [Grammatical error correction](english/grammatical_error_correction.md)
- [Information extraction](english/information_extraction.md)
- [Intent Detection and Slot Filling](english/intent_detection_slot_filling.md) 
- [Keyphrase Extraction and Generation](english/keyphrase_extraction_generation.md)
- [Language modeling](english/language_modeling.md)
- [Lexical normalization](english/lexical_normaliz"
deep-learning-for-image-processing,"# 深度学习在图像处理中的应用教程

## 前言
* 本教程是对本人研究生期间的研究内容进行整理总结，总结的同时也希望能够帮助更多的小伙伴。后期如果有学习到新的知识也会与大家一起分享。
* 本教程会以视频的方式进行分享，教学流程如下：  
1）介绍网络的结构与创新点  
2）使用Pytorch进行网络的搭建与训练  
3）使用Tensorflow（内部的keras模块）进行网络的搭建与训练 
* 课程中所有PPT都放在`course_ppt`文件夹下，需要的自行下载。


## 教程目录，点击跳转相应视频（后期会根据学习内容增加）

* 图像分类
  * LeNet（已完成）
    * [Pytorch官方demo(Lenet)](https://www.bilibili.com/video/BV187411T7Ye)
    * [Tensorflow2官方demo](https://www.bilibili.com/video/BV1n7411T7o6)

  * AlexNet（已完成）
    * [AlexNet网络讲解](https://www.bilibili.com/video/BV1p7411T7Pc)
    * [Pytorch搭建AlexNet](https://www.bilibili.com/video/BV1W7411T7qc)
    * [Tensorflow2搭建Alexnet](https://www.bilibili.com/video/BV1s7411T7vs)

  * VggNet（已完成）
    * [VggNet网络讲解](https://www.bilibili.com/video/BV1q7411T7Y6)
    * [Pytorch搭建VGG网络](https://www.bilibili.com/video/BV1i7411T7ZN)
    * [Tensorflow2搭建VGG网络](https://www.bilibili.com/video/BV1q7411T76b)

  * GoogLeNet（已完成）
    * [GoogLeNet网络讲解](https://www.bilibili.com/video/BV1z7411T7ie)
    * [Pytorch搭建GoogLeNet网络]"
labelImg,".. image:: /readme/images/labelimg.png
        :target: https://github.com/heartexlabs/label-studio

Label Studio is a modern, multi-modal data annotation tool
=======

LabelImg, the popular image annotation tool created by Tzutalin with the help of dozens contributors, is no longer actively being developed and has become part of the Label Studio community. Check out `Label Studio <https://github.com/heartexlabs/label-studio>`__, the most flexible open source data labeling tool for images, text, hypertext, audio, video and time-series data. `Install <https://labelstud.io/guide/install.html>`__ Label Studio and join the `slack community <https://label-studio.slack.com/>`__ to get started.

.. image:: /readme/images/label-studio-1-6-player-screenshot.png
        :target: https://github.com/heartexlabs/label-studio

About LabelImg
========

.. image:: https://img.shields.io/pypi/v/labelimg.svg
        :target: https://pypi.python.org/pypi/labelimg

.. image:: https://img.shields.io/github"
cookiecutter,"<h1 align=""center"">
    <img alt=""cookiecutter Logo"" width=""200px"" src=""https://raw.githubusercontent.com/cookiecutter/cookiecutter/3ac078356adf5a1a72042dfe72ebfa4a9cd5ef38/logo/cookiecutter_medium.png"">
</h1>

<div align=""center"">

[![pypi](https://img.shields.io/pypi/v/cookiecutter.svg)](https://pypi.org/project/cookiecutter/)
[![python](https://img.shields.io/pypi/pyversions/cookiecutter.svg)](https://pypi.org/project/cookiecutter/)
[![Build Status](https://github.com/cookiecutter/cookiecutter/actions/workflows/tests.yml/badge.svg?branch=main)](https://github.com/cookiecutter/cookiecutter/actions)
[![codecov](https://codecov.io/gh/cookiecutter/cookiecutter/branch/main/graphs/badge.svg?branch=main)](https://codecov.io/github/cookiecutter/cookiecutter?branch=main)
[![discord](https://img.shields.io/badge/Discord-cookiecutter-5865F2?style=flat&logo=discord&logoColor=white)](https://discord.gg/9BrxzPKuEW)
[![docs](https://readthedocs.org/projects/cookiecutter/badge/?version=latest)](htt"
gpt-2,"**Status:** Archive (code is provided as-is, no updates expected)

# gpt-2

Code and models from the paper [""Language Models are Unsupervised Multitask Learners""](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).

You can read about GPT-2 and its staged release in our [original blog post](https://openai.com/research/better-language-models/), [6 month follow-up post](https://openai.com/blog/gpt-2-6-month-follow-up/), and [final post](https://www.openai.com/blog/gpt-2-1-5b-release/).

We have also [released a dataset](https://github.com/openai/gpt-2-output-dataset) for researchers to study their behaviors.

<sup>*</sup> *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.*

## Usage

This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.

For basic information, see our [mode"
supervision,"<div align=""center"">
  <p>
    <a align=""center"" href="""" target=""https://supervision.roboflow.com"">
      <img
        width=""100%""
        src=""https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529""
      >
    </a>
  </p>

  <br>

[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)

  <br>

[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)
[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)
[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)
[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)
[![python-version](https://img.shields.io/pypi/pyversions/supervi"
examples,"# PyTorch Examples

![Run Examples](https://github.com/pytorch/examples/workflows/Run%20Examples/badge.svg)

https://pytorch.org/examples/

`pytorch/examples` is a repository showcasing examples of using [PyTorch](https://github.com/pytorch/pytorch). The goal is to have curated, short, few/no dependencies _high quality_ examples that are substantially different from each other that can be emulated in your existing work.

- For tutorials: https://github.com/pytorch/tutorials
- For changes to pytorch.org: https://github.com/pytorch/pytorch.github.io
- For a general model hub: https://pytorch.org/hub/ or https://huggingface.co/models
- For recipes on how to run PyTorch in production: https://github.com/facebookresearch/recipes
- For general Q&A and support: https://discuss.pytorch.org/

## Available models

- [Image classification (MNIST) using Convnets](./mnist/README.md)
- [Word-level Language Modeling using RNN and Transformer](./word_language_model/README.md)
- [Training Imagenet Clas"
openai-python,"# OpenAI Python API library

[![PyPI version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+
application. The library includes type definitions for all request params and response fields,
and offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).

It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).

## Documentation

The REST API documentation can be found on [platform.openai.com](https://platform.openai.com/docs). The full API of this library can be found in [api.md](api.md).

## Installation

> [!IMPORTANT]
> The SDK was rewritten in v1, which was released November 6th 2023. See the [v1 migration guide](https://github.com/openai/openai-python/discussions/742), which includes scripts to automatically update your code.

```sh
# ins"
Awesome-Linux-Software,"# Awesome Linux Software

![Tux](img/tux.png)

🐧 This repo is a collection of **AWESOME** Linux applications and tools for **any users/developers**.

🐧 Feel free to **contribute** / **star** / **fork** / **pull request** . Any **recommendations** and **suggestions** are welcome.

**Acknowledgement:** _Everything written below is from my own experience in college and after reading various materials. I am neither a professional nor an expert, but a passionate user. Anyone can open a discussion in the issue section, or a pull request if something should be modified or added._

- Brazilian Portuguese version : [here](https://github.com/LewisVo/Awesome-Linux-Software/blob/master/README_pt-BR.md).
- Chinese version : [here](https://github.com/LewisVo/Awesome-Linux-Software/blob/master/README_zh-CN.md) or [here](https://github.com/eniqiz/Awesome-Linux-Software-zh_CN).
- Spanish version : [here](https://github.com/LewisVo/Awesome-Linux-Software/blob/master/README_es-ES.md) or [here](https://gi"
mem0,"<p align=""center"">
  <a href=""https://github.com/mem0ai/mem0"">
  <img src=""docs/images/banner-sm.png"" width=""800px"" alt=""Mem0 - The Memory Layer for Personalized AI"">
  </a>
<p align=""center""><a href=https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps target='_blank'><img alt=Launch YC: Mem0 - Open Source Memory Layer for AI Apps src=https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps/upvote_embed.svg/></a></p>


  <p align=""center"">
    <a href=""https://mem0.ai"">Learn more</a>
    ·
    <a href=""https://mem0.ai/discord"">Join Discord</a>
  </p>
</p>

<p align=""center"">
  <a href=""https://mem0.ai/discord"">
    <img src=""https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat"" alt=""Mem0 Discord"">
  </a>
  <a href=""https://pepy.tech/project/mem0ai"">
    <img src=""https://img.shields.io/pypi/dm/mem0ai"" alt=""Mem0 PyPI - Downloads"" >
  </a>
  <a href=""https://pypi.org/project/mem0ai"" target=""_blank"">
        <img src=""https://"
Hitomi-Downloader,"<p align=""center"">
  <img src=""imgs/card_crop.png"" width=""50%""/>
  <br>
</p>

[![GitHub release](https://img.shields.io/github/release/KurtBestor/Hitomi-Downloader.svg?logo=github)](https://github.com/KurtBestor/Hitomi-Downloader/releases/latest)
[![GitHub downloads](https://img.shields.io/github/downloads/KurtBestor/Hitomi-Downloader/latest/total.svg?logo=github)](https://github.com/KurtBestor/Hitomi-Downloader/releases/latest)
[![GitHub downloads](https://img.shields.io/github/downloads/KurtBestor/Hitomi-Downloader/total.svg?logo=github)](https://github.com/KurtBestor/Hitomi-Downloader/releases)

## Links
- [Download](https://github.com/KurtBestor/Hitomi-Downloader/releases/latest)
- [Issues](https://github.com/KurtBestor/Hitomi-Downloader/issues)
- [Scripts & Plugins](https://github.com/KurtBestor/Hitomi-Downloader/wiki/Scripts-&-Plugins)
- [Chrome Extension](https://github.com/KurtBestor/Hitomi-Downloader/wiki/Chrome-Extension)

## Demo
<img src=""imgs/how_to_download.gif"">

## Feat"
Open-Sora,"<p align=""center"">
    <img src=""./assets/readme/icon.png"" width=""250""/>
</p>
<div align=""center"">
    <a href=""https://github.com/hpcaitech/Open-Sora/stargazers""><img src=""https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social""></a>
    <a href=""https://hpcaitech.github.io/Open-Sora/""><img src=""https://img.shields.io/badge/Gallery-View-orange?logo=&amp""></a>
    <a href=""https://discord.gg/kZakZzrSUT""><img src=""https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp""></a>
    <a href=""https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA""><img src=""https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp""></a>
    <a href=""https://twitter.com/yangyou1991/status/1769411544083996787?s=61&t=jT0Dsx2d-MS5vS9rNM5e5g""><img src=""https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp""></a>
    <a href=""https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png""><img src"
tornado,"Tornado Web Server
==================

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/tornadoweb/tornado
   :target: https://gitter.im/tornadoweb/tornado?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

`Tornado <http://www.tornadoweb.org>`_ is a Python web framework and
asynchronous networking library, originally developed at `FriendFeed
<http://friendfeed.com>`_.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
`long polling <http://en.wikipedia.org/wiki/Push_technology#Long_Polling>`_,
`WebSockets <http://en.wikipedia.org/wiki/WebSocket>`_, and other
applications that require a long-lived connection to each user.

Hello, world
------------

Here is a simple ""Hello, world"" example web app for Tornado:

.. code-block:: python

    import asyncio
    import tornado

    class MainHandler(tornado.web.RequestHandler):
        def get(self):
       "
GPT_API_free,"<div align=""center"">
<img src=""./images/logo.png"" alt=""icon"" width=""50px""/>
<h1 align=""center"">GPT-API-free</h1>

支持 **GPT-4** / GPT-3.5-Turbo / GPT-3.5-Turbo-16K / embeddings / DALL·E / whisper / text-davinci

国内动态加速 直连无需代理

[快速开始](#如何使用) / [API文档](https://chatanywhere.apifox.cn/) / [申请内测免费Key](https://api.chatanywhere.org/v1/oauth/free/github/render) / [支持付费Key](https://buyca.shop/) / [服务可用性](https://status.chatanywhere.tech/)

[QQ群: 1009368550](https://qm.qq.com/cgi-bin/qm/qr?k=IUo12Iwbb9X8FGdAJevKIanOkAb7EcAV&jump_from=webapi&authKey=mUS0pJ45r7qiVufMlylLOdi6FmL9M4PdMc6wz6Jk8r2Yr7DZGk0QcjsCedNOShRq)

[![](https://status.chatanywhere.org/api/badge/6/uptime/24?labelPrefix=付费API:gpt-4:)](https://status.chatanywhere.tech/)
[![](https://status.chatanywhere.org/api/badge/3/uptime/24?labelPrefix=付费API:gpt-3.5-turbo:)](https://status.chatanywhere.tech/)
[![](https://status.chatanywhere.org/api/badge/8/uptime/24?labelPrefix=付费API:gpt-3.5-turbo(Azure):)](https://status.chatanywhere.tech/)

[!"
proxy_pool,"
ProxyPool 爬虫代理IP池
=======
[![Build Status](https://travis-ci.org/jhao104/proxy_pool.svg?branch=master)](https://travis-ci.org/jhao104/proxy_pool)
[![](https://img.shields.io/badge/Powered%20by-@j_hao104-green.svg)](http://www.spiderpy.cn/blog/)
[![Packagist](https://img.shields.io/packagist/l/doctrine/orm.svg)](https://github.com/jhao104/proxy_pool/blob/master/LICENSE)
[![GitHub contributors](https://img.shields.io/github/contributors/jhao104/proxy_pool.svg)](https://github.com/jhao104/proxy_pool/graphs/contributors)
[![](https://img.shields.io/badge/language-Python-green.svg)](https://github.com/jhao104/proxy_pool)

    ______                        ______             _
    | ___ \_                      | ___ \           | |
    | |_/ / \__ __   __  _ __   _ | |_/ /___   ___  | |
    |  __/|  _// _ \ \ \/ /| | | ||  __// _ \ / _ \ | |
    | |   | | | (_) | >  < \ |_| || |  | (_) | (_) || |___
    \_|   |_|  \___/ /_/\_\ \__  |\_|   \___/ \___/ \_____\
                           __ / "
zulip,"# Zulip overview

[Zulip](https://zulip.com) is an open-source team collaboration tool with unique
[topic-based threading][why-zulip] that combines the best of email and chat to
make remote work productive and delightful. Fortune 500 companies, [leading open
source projects][rust-case-study], and thousands of other organizations use
Zulip every day. Zulip is the only [modern team chat app][features] that is
designed for both live and asynchronous conversations.

Zulip is built by a distributed community of developers from all around the
world, with 74+ people who have each contributed 100+ commits. With
over 1000 contributors merging over 500 commits a month, Zulip is the
largest and fastest growing open source team chat project.

Come find us on the [development community chat](https://zulip.com/development-community/)!

[![GitHub Actions build status](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml/badge.svg)](https://github.com/zulip/zulip/actions/workflows/zulip-ci.ym"
dash,"# Dash

[![CircleCI](https://img.shields.io/circleci/project/github/plotly/dash/master.svg)](https://circleci.com/gh/plotly/dash)
[![GitHub](https://img.shields.io/github/license/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/blob/master/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/dash.svg?color=dark-green)](https://pypi.org/project/dash/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dash.svg?color=dark-green)](https://pypi.org/project/dash/)
[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/graphs/contributors)

#### *Dash is the most downloaded, trusted Python framework for building ML & data science web apps*.

Built on top of [Plotly.js](https://github.com/plotly/plotly.js), [React](https://reactjs.org/) and [Flask](https://palletsprojects.com/p/flask/), Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Pyth"
GitHub520,"# GitHub520

<p align=""center"">
<a href=""https://hellogithub.com/repository/d05ff820bf36470581c02cda5cbd17ea"" target=""_blank""><img src=""https://api.hellogithub.com/v1/widgets/recommend.svg?rid=d05ff820bf36470581c02cda5cbd17ea&claim_uid=8MKvZoxaWt"" alt=""Featured｜HelloGitHub"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a><br>
😘 让你“爱”上 GitHub，解决访问时图裂、加载慢的问题。
</p>

> 服务器已续费到 2024.12 共花了：1500+💰 [点击扫码赞助](https://raw.hellogithub.com/code.png)，感谢🙏

## 一、介绍
对 GitHub 说""爱""太难了：访问慢、图片加载不出来。

**本项目无需安装任何程序，仅需 5 分钟。**

通过修改本地 hosts 文件，试图解决：
- GitHub 访问速度慢的问题
- GitHub 项目中的图片显示不出的问题

让你""爱""上 GitHub。



*注：* 本项目还处于测试阶段，仅在本机测试通过，如有问题欢迎提 [issues](https://github.com/521xueweihan/GitHub520/issues/new)


## 二、使用方法

下面的地址无需访问 GitHub 即可获取到最新的 hosts 内容：

- 文件：`https://raw.hellogithub.com/hosts`
- JSON：`https://raw.hellogithub.com/hosts.json`

### 2.1 手动方式

#### 2.1.1 复制下面的内容

```bash
# GitHub520 Host Start
140.82.113.25                 alive.github.com
140.82.113.6                  api.github"
manim,"<p align=""center"">
    <a href=""https://www.manim.community/""><img src=""https://raw.githubusercontent.com/ManimCommunity/manim/main/logo/cropped.png""></a>
    <br />
    <br />
    <a href=""https://pypi.org/project/manim/""><img src=""https://img.shields.io/pypi/v/manim.svg?style=flat&logo=pypi"" alt=""PyPI Latest Release""></a>
    <a href=""https://hub.docker.com/r/manimcommunity/manim""><img src=""https://img.shields.io/docker/v/manimcommunity/manim?color=%23099cec&label=docker%20image&logo=docker"" alt=""Docker image""> </a>
    <a href=""https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb""><img src=""https://mybinder.org/badge_logo.svg""></a>
    <a href=""http://choosealicense.com/licenses/mit/""><img src=""https://img.shields.io/badge/license-MIT-red.svg?style=flat"" alt=""MIT License""></a>
    <a href=""https://www.reddit.com/r/manim/""><img src=""https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=orange&label=reddit&logo=reddit"" alt="""
posthog,"<p align=""center"">
  <img alt=""posthoglogo"" src=""https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png"">
</p>
<p align=""center"">
  <!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
<a href='https://posthog.com/contributors'><img src='https://img.shields.io/badge/all_contributors-251-orange.svg?style=flat-square' /></a>
<!-- ALL-CONTRIBUTORS-BADGE:END -->
  <a href='http://makeapullrequest.com'><img alt='PRs Welcome' src='https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields'/></a>
  <img alt=""Docker Pulls"" src=""https://img.shields.io/docker/pulls/posthog/posthog""/>
  <img alt=""GitHub commit activity"" src=""https://img.shields.io/github/commit-activity/m/posthog/posthog""/>
  <img alt=""GitHub closed issues"" src=""https://img.shields.io/github/issues-closed/posthog/posthog""/>
</p>

<p align=""center"">
  <a href=""https://posthog.com/docs"">Docs</a> - <a href=""https://posthog.com/community"">Community</a>"
pytorch_geometric,"<p align=""center"">
  <img height=""150"" src=""https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true"" />
</p>

______________________________________________________________________

[![PyPI Version][pypi-image]][pypi-url]
[![Testing Status][testing-image]][testing-url]
[![Linting Status][linting-image]][linting-url]
[![Docs Status][docs-image]][docs-url]
[![Contributing][contributing-image]][contributing-url]
[![Slack][slack-image]][slack-url]

**[Documentation](https://pytorch-geometric.readthedocs.io)** | **[Paper](https://arxiv.org/abs/1903.02428)** | **[Colab Notebooks and Video Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)** | **[External Resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html)** | **[OGB Examples](https://github.com/snap-stanford/ogb/tree/master/examples)**

**PyG** *(PyTorch Geometric)* is a library built upon [PyTorch](ht"
chatgpt-retrieval-plugin,"# ChatGPT Retrieval Plugin

Build Custom GPTs with a Retrieval Plugin backend to give ChatGPT access to personal documents.
![Example Custom GPT Screenshot](/assets/example.png)

## Introduction

The ChatGPT Retrieval Plugin repository provides a flexible solution for semantic search and retrieval of personal or organizational documents using natural language queries. It is a standalone retrieval backend, and can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.com/?model=gpt-4-plugins). ChatGPT and the Assistants API both natively support retrieval from uploaded files, so you should use the Retrieval Plugin as a backend only if you want more granular contro"
jina,"<p align=""center"">
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/docs/_static/logo-light.svg?raw=true"" alt=""Jina logo: Build multimodal AI services via cloud native technologies · Model Serving · Generative AI · Neural Search · Cloud Native"" width=""150px""></a>
</p>

<p align=""center"">
<b>Build multimodal AI applications with cloud-native technologies</b>
</p>

<p align=center>
<a href=""https://pypi.org/project/jina/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/jina?label=Release&style=flat-square""></a>
<!--<a href=""https://codecov.io/gh/jina-ai/jina""><img alt=""Codecov branch"" src=""https://img.shields.io/codecov/c/github/jina-ai/jina/master?&logo=Codecov&logoColor=white&style=flat-square""></a>-->
<a href=""https://discord.jina.ai""><img src=""https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square""></a>
<a href=""https://pypistats.org/packages/jina""><img alt=""PyPI - Downloads from official pypistats"" src"
ArchiveBox,"<div align=""center"" style=""text-align: center; width: 100%"">
<img src=""https://archivebox.io/icon.png"" height=""90px""/>
<h1>ArchiveBox<br/><sub>Open-source self-hosted web archiving.</sub></h1>

<br/>

▶️ <a href=""https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart"">Quickstart</a> | <a href=""https://demo.archivebox.io"">Demo</a> | <a href=""https://github.com/ArchiveBox/ArchiveBox"">GitHub</a> | <a href=""https://github.com/ArchiveBox/ArchiveBox/wiki"">Documentation</a> | <a href=""#background--motivation"">Info & Motivation</a> | <a href=""https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community"">Community</a>

<br/>

<!--<a href=""http://webchat.freenode.net?channels=ArchiveBox&uio=d4""><img src=""https://img.shields.io/badge/Community_chat-IRC-%2328A745.svg""/></a>-->

<a href=""https://github.com/ArchiveBox/ArchiveBox/blob/dev/LICENSE""><img src=""https://img.shields.io/badge/Open_source-MIT-green.svg?logo=git&logoColor=green""/></a> <a href=""https://github.com/ArchiveBox/ArchiveB"
audiocraft,"# AudioCraft
![docs badge](https://github.com/facebookresearch/audiocraft/workflows/audiocraft_docs/badge.svg)
![linter badge](https://github.com/facebookresearch/audiocraft/workflows/audiocraft_linter/badge.svg)
![tests badge](https://github.com/facebookresearch/audiocraft/workflows/audiocraft_tests/badge.svg)

AudioCraft is a PyTorch library for deep learning research on audio generation. AudioCraft contains inference and training code
for two state-of-the-art AI generative models producing high-quality audio: AudioGen and MusicGen.


## Installation
AudioCraft requires Python 3.9, PyTorch 2.1.0. To install AudioCraft, you can run the following:

```shell
# Best to make sure you have torch installed first, in particular before installing xformers.
# Don't run this if you already have PyTorch installed.
python -m pip install 'torch==2.1.0'
# You might need the following before trying to install the packages
python -m pip install setuptools wheel
# Then proceed to one of the following
"
erpnext,"<div align=""center"">
    <a href=""https://erpnext.com"">
        <img src=""https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/erpnext-logo.png"" height=""128"">
    </a>
    <h2>ERPNext</h2>
    <p align=""center"">
        <p>ERP made simple</p>
    </p>

[![CI](https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule)](https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![codecov](https://codecov.io/gh/frappe/erpnext/branch/develop/graph/badge.svg?token=0TwvyUg3I5)](https://codecov.io/gh/frappe/erpnext)
[![docker pulls](https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg)](https://hub.docker.com/r/frappe/erpnext-worker)

[https://erpnext.com](https://erpnext.com)

</div>

ERPNext as a monolith includes the following areas for managing businesses:

1. [Acco"
saleor,"<div align=""center"" width=""100px"">
 <picture>
   <source media=""(prefers-color-scheme: dark)"" srcset=""https://user-images.githubusercontent.com/4006792/214640818-fd4de9e6-bdee-47f0-ae66-e69ee9ec84bb.png"">
   <source media=""(prefers-color-scheme: light)"" srcset=""https://user-images.githubusercontent.com/4006792/214636328-8e4f83e8-66cb-4114-a3d8-473eb908b9c3.png"">
   <img width=""200"" alt=""saleor-commerce-logo"" src=""https://user-images.githubusercontent.com/4006792/214636328-8e4f83e8-66cb-4114-a3d8-473eb908b9c3.png"">

 </picture>
</div>

<div align=""center"">
  <strong>Commerce that works with your language and stack</strong>
</div>

<div align=""center"">
  GraphQL native, API-only platform for scalable composable commerce.
</div>

<br>

<div align=""center"">
  Join our community: <br>
  <a href=""https://saleor.io/"">Website</a>
  <span> | </span>
  <a href=""https://twitter.com/getsaleor"">Twitter</a>
  <span> | </span>
  <a href=""https://github.com/saleor/saleor/discussions"">GitHub Discussion"
pydantic,"# Pydantic
[![CI](https://img.shields.io/github/actions/workflow/status/pydantic/pydantic/ci.yml?branch=main&logo=github&label=CI)](https://github.com/pydantic/pydantic/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic)
[![pypi](https://img.shields.io/pypi/v/pydantic.svg)](https://pypi.python.org/pypi/pydantic)
[![CondaForge](https://img.shields.io/conda/v/conda-forge/pydantic.svg)](https://anaconda.org/conda-forge/pydantic)
[![downloads](https://static.pepy.tech/badge/pydantic/month)](https://pepy.tech/project/pydantic)
[![versions](https://img.shields.io/pypi/pyversions/pydantic.svg)](https://github.com/pydantic/pydantic)
[![license](https://img.shields.io/github/license/pydantic/pydantic.svg)](https://github.com/pydantic/pydantic/blob/main/LICENSE)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubus"
Gooey,"# Gooey 
  

Turn (almost) any Python 3 Console Program into a GUI application with one line

<p align=""center"">
    <img src=""https://github.com/chriskiehl/GooeyImages/raw/images/readme-images/1-0-4-title-card.png"" />
</p>


Table of Contents
-----------------  

- [Gooey](#gooey)
- [Table of contents](#table-of-contents)
- [Latest Update](#latest-update)
- [Quick Start](#quick-start)
    - [Installation Instructions](#installation-instructions)
    - [Usage](#usage)
    - [Examples](#examples)
- [What It Is](#what-is-it)
- [Why Is It](#why)
- [Who is this for](#who-is-this-for)
- [How does it work](#how-does-it-work)
- [Internationalization](#internationalization)
- [Global Configuration](#global-configuration)
- [Layout Customization](#layout-customization)
- [Run Modes](#run-modes)
    - [Full/Advanced](#advanced)
    - [Basic](#basic)
    - [No Config](#no-config)
- [Menus](#menus)    
- [Dynamic Validation](#dynamic-validation)
- [Lifecycle Events and UI control](#lifecycle-event"
chinese-independent-blogs,"# 中文独立博客列表

  [![](https://badgen.net/badge/icon/Website?icon=chrome&label)](https://feeds.pub/cn-indie)  [![](https://badgen.net/badge/icon/Telegram?icon=telegram&label)](https://t.me/indieBlogs)  [![](https://badgen.net/badge/icon/Blog?icon=chrome&label)](https://blog.t9t.io/cn-indie-blogs-2019-10-29/)

## Sponsors

[琚致远](https://github.com/juzhiyuan) | [Bytebase](https://bytebase.com/) | [Madao](https://madao.me/) | [SecondState](https://bit.ly/3gfWwps)

[Become a sponsor](https://github.com/sponsors/timqian)

## 目录

- [博客列表](#博客列表)
- [什么是独立博客](#什么是独立博客)
  - [如何提交](#如何提交)
- [为什么要收集这张列表](#为什么要收集这张列表)

## 博客列表

> 暂时根据各 RSS 服务订阅数据排了个先后顺序。 欢迎加入 [Telegram 群](https://t.me/indieBlogs) 讨论如何更好地组织和利用这个列表

| RSS feed | Introduction | Address | tags |
| --- | --- | --- | --- |
| [Feed](https://blog.t9t.io/atom.xml) | 透明创业实验 | https://blog.t9t.io | 创业; 编程; 开源 |
| [Feed](http://feeds.feedburner.com/ruanyifeng) | 阮一峰的网络日志 | https://www.ruanyifeng.com/blog/ | 创业; 编程; 前端 |
| [Feed](http://coolshell."
ungoogled-chromium,"# ungoogled-chromium

*A lightweight approach to removing Google web service dependency*

**Help is welcome!** See the [docs/contributing.md](docs/contributing.md) document for more information.

## Objectives

In descending order of significance (i.e. most important objective first):

1. **ungoogled-chromium is Google Chromium, sans dependency on Google web services**.
2. **ungoogled-chromium retains the default Chromium experience as closely as possible**. Unlike other Chromium forks that have their own visions of a web browser, ungoogled-chromium is essentially a drop-in replacement for Chromium.
3. **ungoogled-chromium features tweaks to enhance privacy, control, and transparency**. However, almost all of these features must be manually activated or enabled. For more details, see [Feature Overview](#feature-overview).

In scenarios where the objectives conflict, the objective of higher significance should take precedence.

## Content Overview

* [Objectives](#objectives)
* [Motivat"
ddia,"# 设计数据密集型应用 - 中文翻译版

[![Webite: ddia](https://img.shields.io/badge/v1-ddia.pigsty.io-slategray?style=flat)](https://ddia.pigsty.io)
[![Webite: ddia2](https://img.shields.io/badge/v2-ddia2.pigsty.io-slategray?style=flat)](https://ddia2.pigsty.io)
[![GitHub Stars](https://img.shields.io/github/stars/Vonng/ddia?style=flat&logo=github&logoColor=black&color=slategray)](https://star-history.com/#Vonng/ddia&Date)

**作者**： [Martin Kleppmann](https://martin.kleppmann.com)，[《Designing Data-Intensive Applications 2nd Edition》](https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781098119058/ch01.html) ： 英国剑桥大学分布式系统研究员，演讲者，博主和开源贡献者，软件工程师和企业家，曾在 LinkedIn 和 Rapportive 负责数据基础架构。

**译者**：[冯若航](https://vonng.com) / [Vonng](https://github.com/Vonng) (rh@vonng.com)： 创业者，[开源贡献者](https://gitstar-ranking.com/Vonng)，PostgreSQL Hacker。开源 RDS PG [Pigsty](https://pigsty.cc/zh/) 与公众号《[非法加冯](https://mp.weixin.qq.com/s/p4Ys10ZdEDAuqNAiRmcnIQ)》作者，[数据库老司机](https://pigsty.cc/zh/blog/db)，["
OSX-KVM,"### Note

This `README.md` documents the process of creating a `Virtual Hackintosh`
system.

Note: All blobs and resources included in this repository are re-derivable (all
instructions are included!).

:green_heart: Looking for **commercial** support with this stuff? I am [available
over email](mailto:dhiru.kholia@gmail.com?subject=[GitHub]%20OSX-KVM%20Commercial%20Support%20Request&body=Hi%20-%20We%20are%20interested%20in%20purchasing%20commercial%20support%20options%20for%20your%20project.) for a chat for **commercial support options only**. Note: Project sponsors get access to the `Private OSX-KVM` repository, and direct support.

Struggling with `Content Caching` stuff? We can help.

Working with `Proxmox` and macOS? See [Nick's blog for sure](https://www.nicksherlock.com/).

Yes, we support offline macOS installations now - see [this document](./run_offline.md) 🎉


### Contributing Back

This project can always use your help, time and attention. I am looking for
help (pull-reques"
magic-wormhole,"# Magic Wormhole
[![PyPI](http://img.shields.io/pypi/v/magic-wormhole.svg)](https://pypi.python.org/pypi/magic-wormhole)
![Tests](https://github.com/magic-wormhole/magic-wormhole/workflows/Tests/badge.svg)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/w1bdniovwm4egfyg/branch/master?svg=true)](https://ci.appveyor.com/project/warner/magic-wormhole)
[![codecov.io](https://codecov.io/github/magic-wormhole/magic-wormhole/coverage.svg?branch=master)](https://codecov.io/github/magic-wormhole/magic-wormhole?branch=master)
[![Docs](https://readthedocs.org/projects/magic-wormhole/badge/?version=latest)](https://magic-wormhole.readthedocs.io)
[![Irc](https://img.shields.io/badge/irc.libera.chat-%23magic--wormhole-brightgreen)](https://web.libera.chat/)
[![Matrix](https://img.shields.io/badge/matrix.org-%23magic--wormhole-brightgreen)](https://matrix.to/#/#magic-wormhole:matrix.org)


Get things from one computer to another, safely.

This package provides a library and a com"
matplotlib,"[![PyPi](https://img.shields.io/pypi/v/matplotlib)](https://pypi.org/project/matplotlib/)
[![Conda](https://img.shields.io/conda/vn/conda-forge/matplotlib)](https://anaconda.org/conda-forge/matplotlib)
[![Downloads](https://img.shields.io/pypi/dm/matplotlib)](https://pypi.org/project/matplotlib)
[![NUMFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)

[![Discourse help forum](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.matplotlib.org)
[![Gitter](https://badges.gitter.im/matplotlib/matplotlib.svg)](https://gitter.im/matplotlib/matplotlib)
[![GitHub issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/matplotlib/matplotlib/issues)
[![Contributing](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://matplotlib.org/stable/devel/index.html)

[![GitHub actions status](https://github.com/matplotlib/matplotlib/workflows/Tests/badg"
babyagi,"# Translations:

[<img title=""عربي"" alt=""عربي"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/sa.svg"" width=""30"">](docs/README-ar.md)
[<img title=""Français"" alt=""Français"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/fr.svg"" width=""30"">](docs/README-fr.md)
[<img title=""Polski"" alt=""Polski"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/pl.svg"" width=""30"">](docs/README-pl.md)
[<img title=""Portuguese"" alt=""Portuguese"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/br.svg"" width=""30"">](docs/README-pt-br.md)
[<img title=""Romanian"" alt=""Romanian"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ro.svg"" width=""30"">](docs/README-ro.md)
[<img title=""Russian"" alt=""Russian"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ru.svg"" width=""30"">](docs/README-ru.md)
[<img title=""Slovenian"" alt=""Slovenian"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg"
minGPT,"
# minGPT

![mingpt](mingpt.jpg)

A PyTorch re-implementation of [GPT](https://github.com/openai/gpt-2), both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see [mingpt/model.py](mingpt/model.py)). All that's going on is that a sequence of indices feeds into a [Transformer](https://arxiv.org/abs/1706.03762), and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.

**note (Jan 2023)**: though I may continue to accept and change some details, minGPT is in a semi-archived state. For more recent developments see my rewrite [nanoGPT](https://github.com/karpathy/nanoGPT). Basically, minGPT became referenced across a wide varie"
localGPT,"# LocalGPT: Secure, Local Conversations with Your Documents 🌐


[![GitHub Stars](https://img.shields.io/github/stars/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/pulls)
[![License](https://img.shields.io/github/license/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/blob/main/LICENSE)

🚨🚨 You can run localGPT on a pre-configured [Virtual Machine](https://bit.ly/localGPT). Make sure to use the code: PromptEngineering to get 50% off. I will get a small commision!

**LocalGPT** is an open-source initiative that allows y"
vit-pytorch,"<img src=""./images/vit.gif"" width=""500px""></img>

## Table of Contents

- [Vision Transformer - Pytorch](#vision-transformer---pytorch)
- [Install](#install)
- [Usage](#usage)
- [Parameters](#parameters)
- [Simple ViT](#simple-vit)
- [NaViT](#navit)
- [Distillation](#distillation)
- [Deep ViT](#deep-vit)
- [CaiT](#cait)
- [Token-to-Token ViT](#token-to-token-vit)
- [CCT](#cct)
- [Cross ViT](#cross-vit)
- [PiT](#pit)
- [LeViT](#levit)
- [CvT](#cvt)
- [Twins SVT](#twins-svt)
- [CrossFormer](#crossformer)
- [RegionViT](#regionvit)
- [ScalableViT](#scalablevit)
- [SepViT](#sepvit)
- [MaxViT](#maxvit)
- [NesT](#nest)
- [MobileViT](#mobilevit)
- [XCiT](#xcit)
- [Masked Autoencoder](#masked-autoencoder)
- [Simple Masked Image Modeling](#simple-masked-image-modeling)
- [Masked Patch Prediction](#masked-patch-prediction)
- [Masked Position Prediction](#masked-position-prediction)
- [Adaptive Token Sampling](#adaptive-token-sampling)
- [Patch Merger](#patch-merger)
- [Vision Transformer for Smal"
unilm,"<!--# Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities-->
## [aka.ms/GeneralAI](https://aka.ms/GeneralAI)
# Hiring
We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and General AI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to <a href=""mailto:fuwei@microsoft.com"" class=""x-hidden-focus"">fuwei@microsoft.com</a>.

# Foundation Architecture
### TorchScale - A Library of Foundation Architectures ([repo](https://github.com/microsoft/torchscale))

Fundamental research to develop new architectures for foundation models and AI, focusing on modeling generality and capability, as well as training stability and efficiency.

> Stability - [**DeepNet**](https://github.com/microsoft/unilm/tree/master/deepnet): scaling Transformers to 1,000 Layers and beyond

> Generality - [**Foundation Transformers (Magneto)**](https://arxi"
loguru,".. raw:: html

    <p align=""center"">
        <a href=""#readme"">
            <img alt=""Loguru logo"" src=""https://raw.githubusercontent.com/Delgan/loguru/master/docs/_static/img/logo.png"">
            <!-- Logo credits: Sambeet from Pixaday -->
            <!-- Logo fonts: Comfortaa + Raleway -->
        </a>
    </p>
    <p align=""center"">
        <a href=""https://pypi.python.org/pypi/loguru""><img alt=""Pypi version"" src=""https://img.shields.io/pypi/v/loguru.svg""></a>
        <a href=""https://pypi.python.org/pypi/loguru""><img alt=""Python versions"" src=""https://img.shields.io/badge/python-3.5%2B%20%7C%20PyPy-blue.svg""></a>
        <a href=""https://loguru.readthedocs.io/en/stable/index.html""><img alt=""Documentation"" src=""https://img.shields.io/readthedocs/loguru.svg""></a>
        <a href=""https://github.com/Delgan/loguru/actions/workflows/tests.yml?query=branch:master""><img alt=""Build status"" src=""https://img.shields.io/github/actions/workflow/status/Delgan/loguru/tests.yml?branch=master"""
crewAI,"<div align=""center"">

![Logo of crewAI, two people rowing on a boat](./docs/crewai_logo.png)

# **crewAI**

🤖 **crewAI**: Cutting-edge framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.

<h3>

[Homepage](https://www.crewai.com/) | [Documentation](https://docs.crewai.com/) | [Chat with Docs](https://chatg.pt/DWjSBZn) | [Examples](https://github.com/crewAIInc/crewAI-examples) | [Discourse](https://community.crewai.com)

</h3>

[![GitHub Repo stars](https://img.shields.io/github/stars/joaomdmoura/crewAI)](https://github.com/crewAIInc/crewAI)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

</div>

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Description"
LLaVA,"# 🌋 LLaVA: Large Language and Vision Assistant

*Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.*

[📢 [LLaVA-NeXT Blog](https://llava-vl.github.io/blog/2024-01-30-llava-next/)] [[Project Page](https://llava-vl.github.io/)] [[Demo](https://llava.hliu.cc/)]  [[Data](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)] [[Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)]

🤝Community Contributions: [[llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3436)] [[Colab](https://github.com/camenduru/LLaVA-colab)] [[🤗Space](https://huggingface.co/spaces/badayvedat/LLaVA)] [[Replicate](https://replicate.com/yorickvp/llava-13b)] [[AutoGen](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb)]  [[BakLLaVA](https://github.com/SkunkworksAI/BakLLaVA)]

**Improved Baselines with Visual Instruction Tuning** [[Paper](https://arxiv.org/abs/2310.03744)] [[HF](https://huggingface.co/papers"
calibre,"# calibre

<img align=""left"" src=""https://raw.githubusercontent.com/kovidgoyal/calibre/master/resources/images/lt.png"" height=""200"" width=""200""/>

calibre is an e-book manager. It can view, convert, edit and catalog e-books 
in all of the major e-book formats. It can also talk to e-book reader 
devices. It can go out to the internet and fetch metadata for your books. 
It can download newspapers and convert them into e-books for convenient 
reading. It is cross platform, running on Linux, Windows and macOS.

For more information, see the [calibre About page](https://calibre-ebook.com/about).

[![Build Status](https://github.com/kovidgoyal/calibre/workflows/CI/badge.svg)](https://github.com/kovidgoyal/calibre/actions?query=workflow%3ACI)

## Screenshots  

[Screenshots page](https://calibre-ebook.com/demo)

## Usage

See the [User Manual](https://manual.calibre-ebook.com).

## Development

[Setting up a development environment for calibre](https://manual.calibre-ebook.com/develop.html).
"
reflex,"```diff
+ Searching for Pynecone? You are in the right repo. Pynecone has been renamed to Reflex. +
```

<div align=""center"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_dark.svg#gh-light-mode-only"" alt=""Reflex Logo"" width=""300px"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_light.svg#gh-dark-mode-only"" alt=""Reflex Logo"" width=""300px"">

<hr>

### **✨ Performant, customizable web apps in pure Python. Deploy in seconds. ✨**
[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&label=Discord)](https://discord.gg/T5WSbC2YtQ)
</div>

---

[English](https://github.com/reflex-dev/reflex/blob"
paperless-ngx,"[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png"" w"
aider,"
<!-- Edit README.md, not index.md -->

# Aider is AI pair programming in your terminal

Aider lets you pair program with LLMs,
to edit code in your local git repository.
Start a new project or work with an existing git repo.
Aider works best with GPT-4o & Claude 3.5 Sonnet and can 
[connect to almost any LLM](https://aider.chat/docs/llms.html).

<!-- SCREENCAST START -->
<p align=""center"">
  <img
    src=""https://aider.chat/assets/screencast.svg""
    alt=""aider screencast""
  >
</p>
<!-- SCREENCAST END -->

<!-- VIDEO START
<p align=""center"">
  <video style=""max-width: 100%; height: auto;"" autoplay loop muted playsinline>
    <source src=""/assets/shell-cmds-small.mp4"" type=""video/mp4"">
    Your browser does not support the video tag.
  </video>
</p>
VIDEO END -->

<p align=""center"">
  <a href=""https://discord.gg/Tv2uQnR88V"">
    <img src=""https://img.shields.io/badge/Join-Discord-blue.svg""/>
  </a>
  <a href=""https://aider.chat/docs/install.html"">
    <img src=""https://img.shields.io/b"
mkdocs,"# MkDocs

> *Project documentation with Markdown*

[![PyPI Version][pypi-v-image]][pypi-v-link]
[![Build Status][GHAction-image]][GHAction-link]
[![Coverage Status][codecov-image]][codecov-link]

MkDocs is a **fast**, **simple** and **downright gorgeous** static site
generator that's geared towards building project documentation. Documentation
source files are written in Markdown, and configured with a single YAML
configuration file. It is designed to be easy to use and can be extended with
third-party themes, plugins, and Markdown extensions.

Please see the [Documentation][mkdocs] for an introductory tutorial and a full
user guide.

## Features

- Build static HTML files from Markdown files.
- Use Plugins and Markdown Extensions to enhance MkDocs.
- Use the built-in themes, third party themes or create your own.
- Publish your documentation anywhere that static files can be served.
- Much more!

## Support

If you need help with MkDocs, do not hesitate to get in contact with us!

-  "
magenta,"# Status

This repository is currently inactive and serves only as a supplement some of our papers. We have transitioned to using individual repositories for new projects. For our current work, see the [Magenta website](https://g.co/magenta) and [Magenta GitHub Organization](https://github.com/magenta).

# Magenta

<img src=""magenta-logo-bg.png"" height=""75"">

[![Build Status](https://github.com/magenta/magenta/workflows/build/badge.svg)](https://github.com/magenta/magenta/actions?query=workflow%3Abuild)
 [![PyPI version](https://badge.fury.io/py/magenta.svg)](https://badge.fury.io/py/magenta)

**Magenta** is a research project exploring the role of machine learning
in the process of creating art and music.  Primarily this
involves developing new deep learning and reinforcement learning
algorithms for generating songs, images, drawings, and other materials. But it's also
an exploration in building smart tools and interfaces that allow
artists and musicians to extend (not replace!) their"
datasets,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg"">
    <img alt=""Hugging Face Datasets Library"" src=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg"" width=""352"" height=""59"" style=""max-width: 100%;"">
  </picture>
  <br/>
  <br/>
</p>

<p align=""center"">
    <a href=""https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain""><img alt=""Build"" src=""https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main""></a>
    <a href=""https://github.com/huggingface/datasets/blob/main/LICENSE""><img alt=""GitHub"" src=""https://img.shields.io/github/license/huggingface/datasets.svg?color=blue""></a>
    <a href="""
IOPaint,"<h1 align=""center"">IOPaint</h1>
<p align=""center"">A free and open-source inpainting & outpainting tool powered by SOTA AI model.</p>

<p align=""center"">
  <a href=""https://github.com/Sanster/IOPaint"">
    <img alt=""total download"" src=""https://pepy.tech/badge/iopaint"" />
  </a>
  <a href=""https://pypi.org/project/iopaint"">
    <img alt=""version"" src=""https://img.shields.io/pypi/v/iopaint"" />
  </a>
  <a href="""">
    <img alt=""python version"" src=""https://img.shields.io/pypi/pyversions/iopaint"" />
  </a>
  <a href=""https://huggingface.co/spaces/Sanster/iopaint-lama"">
    <img alt=""HuggingFace Spaces"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Spaces-blue"" />
  </a>
  <a href=""https://colab.research.google.com/drive/1TKVlDZiE3MIZnAUMpv2t_S4hLr6TUY1d?usp=sharing"">
    <img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg"" />
  </a>
</p>

|Erase([LaMa](https://www.iopaint.com/models/erase/lama))|Replace Object([PowerPaint](https://www.iopa"
recommenders,"<!--
Copyright (c) Recommenders contributors.
Licensed under the MIT License.
-->
<img src=""https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg"" width=""800"">


[![Documentation status](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment)
[![License](https://img.shields.io/github/license/recommenders-team/recommenders.svg)](https://github.com/recommenders-team/recommenders/blob/main/LICENSE)
[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PyPI Version](https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&logoColor=white)](https://pypi.org/project/recommenders)
[![Python Versions](https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&logoColor=white)](https://pypi.org/project/recommenders)

[<img align=""left"" width=""300"" s"
mlc-llm,"<div align=""center"">

# MLC LLM

[![Installation](https://img.shields.io/badge/docs-latest-green)](https://llm.mlc.ai/docs/)
[![License](https://img.shields.io/badge/license-apache_2-blue)](https://github.com/mlc-ai/mlc-llm/blob/main/LICENSE)
[![Join Discoard](https://img.shields.io/badge/Join-Discord-7289DA?logo=discord&logoColor=white)](""https://discord.gg/9Xpy2HGBuD"")
[![Related Repository: WebLLM](https://img.shields.io/badge/Related_Repo-WebLLM-fafbfc?logo=github)](https://github.com/mlc-ai/web-llm/)

**Universal LLM Deployment Engine with ML Compilation**

[Get Started](https://llm.mlc.ai/docs/get_started/quick_start) | [Documentation](https://llm.mlc.ai/docs) | [Blog](https://blog.mlc.ai/)

</div>

## About

MLC LLM is a machine learning compiler and high-performance deployment engine for large language models.  The mission of this project is to enable everyone to develop, optimize, and deploy AI models natively on everyone's platforms. 

<div align=""center"">
<table style=""width"
rasa,"<h1 align=""center"">Rasa Open Source</h1>

<div align=""center"">

[![Join the chat on Rasa Community Forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![PyPI version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)
[![Build Status](https://github.com/RasaHQ/rasa/workflows/Continuous%20Integration/badge.svg)](https://github.com/RasaHQ/rasa/actions)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=RasaHQ_rasa&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=RasaHQ_rasa)
[![Documentation Status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)
![Documentation Build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%2"
nginx-proxy,"[![Test](https://github.com/nginx-proxy/nginx-proxy/actions/workflows/test.yml/badge.svg)](https://github.com/nginx-proxy/nginx-proxy/actions/workflows/test.yml)
[![GitHub release](https://img.shields.io/github/v/release/nginx-proxy/nginx-proxy)](https://github.com/nginx-proxy/nginx-proxy/releases)
[![nginx 1.27.1](https://img.shields.io/badge/nginx-1.27.1-brightgreen.svg?logo=nginx)](https://nginx.org/en/CHANGES)
[![Docker Image Size](https://img.shields.io/docker/image-size/nginxproxy/nginx-proxy?sort=semver)](https://hub.docker.com/r/nginxproxy/nginx-proxy ""Click to view the image on Docker Hub"")
[![Docker stars](https://img.shields.io/docker/stars/nginxproxy/nginx-proxy.svg)](https://hub.docker.com/r/nginxproxy/nginx-proxy ""DockerHub"")
[![Docker pulls](https://img.shields.io/docker/pulls/nginxproxy/nginx-proxy.svg)](https://hub.docker.com/r/nginxproxy/nginx-proxy ""DockerHub"")

nginx-proxy sets up a container running nginx and [docker-gen](https://github.com/nginx-proxy/docker-gen)."
mlflow,"=============================================
MLflow: A Machine Learning Lifecycle Platform
=============================================

MLflow is a platform to streamline machine learning development, including tracking experiments, packaging code
into reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs that can be
used with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you
currently run ML code (e.g. in notebooks, standalone applications or the cloud). MLflow's current components are:

* `MLflow Tracking <https://mlflow.org/docs/latest/tracking.html>`_: An API to log parameters, code, and
  results in machine learning experiments and compare them using an interactive UI.
* `MLflow Projects <https://mlflow.org/docs/latest/projects.html>`_: A code packaging format for reproducible
  runs using Conda and Docker, so you can share your ML code with others.
* `MLflow Models <https://mlflow.org/d"
devika,"<p align=""center"">
  <img src="".assets/devika-avatar.png"" alt=""Devika Logo"" width=""250"">
</p>

<h1 align=""center"">🚀 Devika - Agentic AI Software Engineer 👩‍💻</h1>

![devika screenshot](.assets/devika-screenshot.png)

> [!IMPORTANT]  
> This project is currently in a very early development/experimental stage. There are a lot of unimplemented/broken features at the moment. Contributions are welcome to help out with the progress!

## Table of Contents

- [About](#about)
- [Key Features](#key-features)
- [System Architecture](#system-architecture)
- [Getting Started](#getting-started)
  - [Requirements](#requirements)
  - [Installation](#installation)
  - [How to use](#how-to-use)
- [Configuration](#configuration)
- [Contributing](#contributing)
- [Help and Support](#help-and-support)
- [License](#license)

## About

Devika is an advanced AI software engineer that can understand high-level human instructions, break them down into steps, research relevant information, and write code to achi"
prophet,"
# Prophet: Automatic Forecasting Procedure

![Build](https://github.com/facebook/prophet/workflows/Build/badge.svg)

[![PyPI Version](https://img.shields.io/pypi/v/prophet.svg)](https://pypi.python.org/pypi/prophet)
[![PyPI Downloads Monthly](https://pepy.tech/badge/prophet/month)](https://pepy.tech/project/prophet)
[![PyPI Downloads All](https://pepy.tech/badge/prophet)](https://pepy.tech/project/prophet)

[![CRAN Version](https://www.r-pkg.org/badges/version/prophet)](https://CRAN.R-project.org/package=prophet)
[![CRAN Downloads Monthly](https://cranlogs.r-pkg.org/badges/prophet?color=brightgreen)](https://cran.r-project.org/package=prophet)
[![CRAN Downloads All](https://cranlogs.r-pkg.org/badges/grand-total/prophet?color=brightgreen)](https://cranlogs.r-pkg.org/badges/grand-total/prophet)

[![Conda_Version](https://anaconda.org/conda-forge/prophet/badges/version.svg)](https://anaconda.org/conda-forge/prophet/)

-----

**2023 Update:** We discuss our plans for the future of Prophet"
ChatPaper,"<div style=""font-size: 1.5rem;"">
  <a href=""./README.md"">中文</a> |
  <a href=""./readme_en.md"">English</a>
</div>
</br>


💥💥💥<strong>7.23 [MasterYip](https://github.com/MasterYip) 同学开源了 [ChatPaper2Xmind](https://github.com/MasterYip/ChatPaper2Xmind)! 
将论文PDF通过Chat一键生成 图片+公式的简要XMind笔记。
 </strong>

💥💥💥<strong>7.22 仓库的文件做了一个整理，可能会有些路径和bug，正在修复中。
增加全新的本地PDF全文翻译功能！[⛏️PDF全文翻译配置教程](https://github.com/kaixindelele/ChatPaper#%E4%BB%BB%E6%84%8Fpdf%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B)
 </strong>

<details><summary><code><b>历史重大更新</b></code></summary>

- 🌟*2023.07.23*: [MasterYip](https://github.com/MasterYip) 同学开源了 [ChatPaper2Xmind](https://github.com/MasterYip/ChatPaper2Xmind)! 
将论文PDF通过Chat一键生成 图片+公式的简要XMind笔记。
- 🌟*2023.07.22*: 增加全新的本地PDF全文翻译功能！[⛏️PDF全文翻译配置教程](#任意PDF全文翻译配置教程)
- 🌟*2023.07.21*: 仓库的文件做了一个整理，可能会有些路径和bug，正在修复中。
- 🌟*2023.07.09*: 师弟[red-tie](https://github.com/red-tie)在[auto-draft](https://github.com/CCCBora/auto-draft)的基础上，优化了一款[一键文献综述](https://githu"
mypy,"<img src=""docs/source/mypy_light.svg"" alt=""mypy logo"" width=""300px""/>

Mypy: Static Typing for Python
=======================================

[![Stable Version](https://img.shields.io/pypi/v/mypy?color=blue)](https://pypi.org/project/mypy/)
[![Downloads](https://img.shields.io/pypi/dm/mypy)](https://pypistats.org/packages/mypy)
[![Build Status](https://github.com/python/mypy/actions/workflows/test.yml/badge.svg)](https://github.com/python/mypy/actions)
[![Documentation Status](https://readthedocs.org/projects/mypy/badge/?version=latest)](https://mypy.readthedocs.io/en/latest/?badge=latest)
[![Chat at https://gitter.im/python/typing](https://badges.gitter.im/python/typing.svg)](https://gitter.im/python/typing?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/blac"
Chinese-LLaMA-Alpaca,"# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)项目启动！

[**🇨🇳中文**](./README.md) | [**🌐English**](./README_EN.md) | [**📖文档/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki) | [**❓提问/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues) | [**💬讨论/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions) | [**⚔️竞技场/Arena**](http://llm-arena.ymcui.com/)

<p align=""center"">
    <br>
    <img src=""./pics/banner.png"" width=""700""/>
    <br>
</p>
<p align=""center"">
    <img alt=""GitHub"" src=""https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square"">
    <img alt=""GitHub release (latest by date)"" src=""https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca"">
    <img alt=""GitHub top language"" src=""https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca"">
    <img alt=""GitHub last commit"" src=""https://img.shields.io/github/last-commit/ymcui/Chinese-LLaMA-Alpaca"">
    <a "
facefusion,"FaceFusion
==========

> Industry leading face manipulation platform.

[![Build Status](https://img.shields.io/github/actions/workflow/status/facefusion/facefusion/ci.yml.svg?branch=master)](https://github.com/facefusion/facefusion/actions?query=workflow:ci)
[![Coverage Status](https://img.shields.io/coveralls/facefusion/facefusion.svg)](https://coveralls.io/r/facefusion/facefusion)
![License](https://img.shields.io/badge/license-MIT-green)


Preview
-------

![Preview](https://raw.githubusercontent.com/facefusion/facefusion/master/.github/preview.png?sanitize=true)


Installation
------------

Be aware, the [installation](https://docs.facefusion.io/installation) needs technical skills and is not recommended for beginners. In case you are not comfortable using a terminal, our [Windows Installer](https://windows-installer.facefusion.io) and [macOS Installer](https://macos-installer.facefusion.io) get you started.


Usage
-----

Run the command:

```
python facefusion.py [commands] [opti"
python-spider,"# 注：2020年最新连载教程请移步：[Python Spider 2020](https://github.com/Jack-Cherish/python-spider/tree/master/2020 ""Python Spider 2020"")

免责声明：

大家请以学习为目的使用本仓库，爬虫违法违规的案件：https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China

本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。

# Python Spider

原创文章每周最少两篇，**后续最新文章**会在[【公众号】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)首发，视频[【B站】](https://space.bilibili.com/331507846)首发，大家可以加我[【微信】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)进**交流群**，技术交流或提意见都可以，欢迎**Star**！

<p align=""center"">
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/weChat-微信群-blue.svg"" alt=""微信群""></a>
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Jack%20Cui"
ragflow,"<div align=""center"">
<a href=""https://demo.ragflow.io/"">
<img src=""web/src/assets/logo-with-text.png"" width=""520"" alt=""ragflow logo"">
</a>
</div>

<p align=""center"">
  <a href=""./README.md"">English</a> |
  <a href=""./README_zh.md"">简体中文</a> |
  <a href=""./README_ja.md"">日本語</a> |
  <a href=""./README_ko.md"">한국어</a>
</p>

<p align=""center"">
    <a href=""https://github.com/infiniflow/ragflow/releases/latest"">
        <img src=""https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&label=Latest%20Release"" alt=""Latest Release"">
    </a>
    <a href=""https://demo.ragflow.io"" target=""_blank"">
        <img alt=""Static Badge"" src=""https://img.shields.io/badge/Online-Demo-4e6b99""></a>
    <a href=""https://hub.docker.com/r/infiniflow/ragflow"" target=""_blank"">
        <img src=""https://img.shields.io/badge/docker_pull-ragflow:v0.11.0-brightgreen"" alt=""docker pull infiniflow/ragflow:v0.11.0""></a>
    <a href=""https://github.com/infiniflow/ragflow/blob/main/LICENSE"">
    <img height=""21"
sanic,".. image:: https://raw.githubusercontent.com/sanic-org/sanic-assets/master/png/sanic-framework-logo-400x97.png
    :alt: Sanic | Build fast. Run fast.

Sanic | Build fast. Run fast.
=============================

.. start-badges

.. list-table::
    :widths: 15 85
    :stub-columns: 1

    * - Build
      - | |Tests|
    * - Docs
      - | |UserGuide| |Documentation|
    * - Package
      - | |PyPI| |PyPI version| |Wheel| |Supported implementations| |Code style ruff|
    * - Support
      - | |Forums| |Discord| |Awesome|
    * - Stats
      - | |Monthly Downloads| |Weekly Downloads| |Conda downloads|

.. |UserGuide| image:: https://img.shields.io/badge/user%20guide-sanic-ff0068
   :target: https://sanic.dev/
.. |Forums| image:: https://img.shields.io/badge/forums-community-ff0068.svg
   :target: https://community.sanicframework.org/
.. |Discord| image:: https://img.shields.io/discord/812221182594121728?logo=discord&label=Discord&color=5865F2
   :target: https://discord.gg/FARQzAEMAA
.."
wagtail,"<h1 align=""center"">
    <picture>
        <source media=""(prefers-color-scheme: light)"" srcset="".github/wagtail.svg"">
        <source media=""(prefers-color-scheme: dark)"" srcset="".github/wagtail-inverse.svg"">
        <img width=""343"" src="".github/wagtail.svg"" alt=""Wagtail"">
    </picture>
</h1>
<p align=""center"">
    <br>
    <a href=""https://github.com/wagtail/wagtail/actions"">
        <img src=""https://github.com/wagtail/wagtail/workflows/Wagtail%20CI/badge.svg"" alt=""Build Status"" />
    </a>
    <a href=""https://opensource.org/licenses/BSD-3-Clause"">
        <img src=""https://img.shields.io/badge/license-BSD-blue.svg"" alt=""License"" />
    </a>
    <a href=""https://pypi.python.org/pypi/wagtail/"">
        <img src=""https://img.shields.io/pypi/v/wagtail.svg"" alt=""Version"" />
    </a>
    <a href=""https://pypi.python.org/pypi/wagtail/"">
        <img src=""https://img.shields.io/pypi/dm/wagtail?logo=Downloads"" alt=""Monthly downloads"" />
    </a>
    <a href=""https://x.com/WagtailCMS"">
   "
DeOldify,"
# DeOldify

**Quick Start**: The easiest way to colorize images using open source DeOldify
(for free!) is here: [DeOldify Image Colorization on DeepAI](https://deepai.org/machine-learning-model/colorizer)

**Desktop**: Want to run open source DeOldify for photos and videos on the desktop?
* Stable Diffusion Web UI Plugin- Photos and video, cross-platform (NEW!). <https://github.com/SpenserCai/sd-webui-deoldify>
* ColorfulSoft Windows GUI- No GPU required! Photos/Windows only. <https://github.com/ColorfulSoft/DeOldify.NET>.
No GPU required!

The **most advanced** version of DeOldify image colorization is available here,
exclusively.  Try a few images for free! [MyHeritage In Color](https://www.myheritage.com/incolor)

**Replicate:** Image: <a href=""https://replicate.com/arielreplicate/deoldify_image""><img src=""https://replicate.com/arielreplicate/deoldify_image/badge""></a> | Video: <a href=""https://replicate.com/arielreplicate/deoldify_video""><img src=""https://replicate.com/arielreplic"
learn_python3_spider,"
# learn_python3_spider
接下来就是，学习python的正确姿势！

[等下，阿里云服务器/2核2G/3M/40g，99元/年？？？](https://t.aliyun.com/U/DYTxRF)

peace.

# 如果你也想要会 Python

可以加入我的 [Python 会员网站](https://fxxkpython.com)!!

# python爬虫教程从0到1

## 爬虫负基础

- [python爬虫系列教程-1 ｜ 不会代码也想爬数据？这就教你！](https://mp.weixin.qq.com/s?__biz=MzkyNTExNzY4NA==&mid=2247484935&idx=1&sn=ad9f68845455ca35c08c0e11f92aa4a6&chksm=c1ca3b9cf6bdb28a8647bc911079221b790780611e019e628613657ebfbc38e1e317f53ab00f&token=1453775207&lang=zh_CN#rd)


## python爬虫前，抓包

- [python爬虫系列教程00 | 什么是爬虫，怎么玩爬虫？](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489892&amp;idx=1&amp;sn=40f3f6b70d467ca72b838939aa63d720&amp;chksm=ceb9e378f9ce6a6e089459fad40e2ef8bdce9f46a0a7b9e8332cdbe6d2bc09a47879dc99dd4c&amp;scene=27#wechat_redirect)
- [python爬虫系列教程01 | 教你在 Chrome 浏览器轻松抓包](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489893&amp;idx=1&amp;sn=32cc4fe30066a148485f40629aff598a&amp;chksm=ceb9e379f9ce6a6f609b95a729d01ff1745c101c14fe005fd2ed73e32dec08e1ed4d102b"
awesome-free-chatgpt,"# Awesome Free ChatGPT

![Awesome](https://cdn.jsdelivr.net/gh/LiLittleCat/PicBed/svg/awesome/badge.svg) [![English](https://cdn.jsdelivr.net/gh/LiLittleCat/PicBed/svg/lang/english.svg)](README_en.md) ![website count](https://img.shields.io/badge/websites-298-blue?style=flat) ![last-commit](https://img.shields.io/github/last-commit/LiLittleCat/awesome-free-chatgpt?style=flat&amp;label=last&nbsp;commit)

> 4 月 1 日，OpenAI 宣布可以不登录即可使用 ChatGPT 3.5，参阅 [Start using ChatGPT instantly](https://openai.com/blog/start-using-chatgpt-instantly)。

🎁 免费的 ChatGPT (<https://chatgpt.com>)(<https://chat.openai.com>) 镜像网站列表，以及更多免费资源，持续更新。

此处列出的网站均来源于互联网，请注意不要在这些网站上输入任何个人敏感信息。

🌈 欢迎贡献

- [添加镜像站点](https://github.com/LiLittleCat/awesome-free-chatgpt/issues/new?assignees=LiLittleCat&labels=&projects=&template=%E6%B7%BB%E5%8A%A0%E9%95%9C%E5%83%8F%E7%AB%99%E7%82%B9.md&title=%E6%B7%BB%E5%8A%A0%E9%95%9C%E5%83%8F%E7%AB%99%E7%82%B9)
- [反馈站点失效](https://github.com/LiLittleCat/awesome-free-chatgpt/issues/new?assignee"
Ciphey,"<p align=""center"">
Translations <br>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/de/README.md>🇩🇪 DE   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/fr/README.md>🇫🇷 FR   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/hu/README.md>🇭🇺 HU   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/id/README.md>🇮🇩 ID   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/it/README.md>🇮🇹 IT   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/nl/README.md>🇳🇱 NL   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/pt-br/README.md>🇧🇷 PT-BR   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/ru/README.md>🇷🇺 RU   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/zh/README.md>🇨🇳 ZH   </a>
<a href=""https://github.com/Ciphey/Ciphey/tree/master/translations/th/README.md"">🇹🇭 TH   </a>
 <br><br>
➡️
<a href=""https://githu"
pyscript,"# PyScript

## What is PyScript

### Summary

PyScript is a framework that allows users to create rich Python applications in the browser using HTML's interface and the power of [Pyodide](https://pyodide.org/en/stable/), [MicroPython](https://micropython.org/) and [WASM](https://webassembly.org/), and modern web technologies.

To get started see the [Beginning PyScript tutorial](https://docs.pyscript.net/latest/beginning-pyscript/).

For examples see [here](https://pyscript.com/@examples).

Other useful resources:

-   The [official technical docs](https://docs.pyscript.net/).
-   Our current [Home Page](https://pyscript.net/) on the web.
-   A free-to-use [online editor](https://pyscript.com/) for trying PyScript.
-   Our community [Discord Channel](https://discord.gg/BYB2kvyFwm), to keep in touch .

Every Tuesday at 15:30 UTC there is the _PyScript Community Call_ on zoom, where we can talk about PyScript development in the open. Most of the maintainers regularly participate in the c"
luigi,".. figure:: https://raw.githubusercontent.com/spotify/luigi/master/doc/luigi.png
   :alt: Luigi Logo
   :align: center

.. image:: https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fspotify%2Fluigi%2Fbadge&label=build&logo=none&%3Fref%3Dmaster&style=flat
    :target: https://actions-badge.atrox.dev/spotify/luigi/goto?ref=master

.. image:: https://img.shields.io/codecov/c/github/spotify/luigi/master.svg?style=flat
    :target: https://codecov.io/gh/spotify/luigi?branch=master

.. image:: https://img.shields.io/pypi/v/luigi.svg?style=flat
   :target: https://pypi.python.org/pypi/luigi

.. image:: https://img.shields.io/pypi/l/luigi.svg?style=flat
   :target: https://pypi.python.org/pypi/luigi

.. image:: https://readthedocs.org/projects/luigi/badge/?version=stable
    :target: https://luigi.readthedocs.io/en/stable/?badge=stable
    :alt: Documentation Status

Luigi is a Python (3.6, 3.7, 3.8, 3.9, 3.10, 3.11, 3.12 tested) package that helps you build comple"
onnx,"<!--
Copyright (c) ONNX Project Contributors

SPDX-License-Identifier: Apache-2.0
-->

<p align=""center""><img width=""40%"" src=""https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png"" /></p>

[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)
[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)
[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://g"
awesome-quant,"# Awesome Quant

A curated list of insanely awesome libraries, packages and resources for Quants (Quantitative Finance).

[![](https://awesome.re/badge.svg)](https://awesome.re)

## Languages

- [Python](#python)
- [R](#r)
- [Matlab](#matlab)
- [Julia](#julia)
- [Java](#java)
- [JavaScript](#javascript)
- [Haskell](#haskell)
- [Scala](#scala)
- [Ruby](#ruby)
- [Elixir/Erlang](#elixirerlang)
- [Golang](#golang)
- [CPP](#cpp)
- [CSharp](#csharp)
- [Rust](#rust)
- [Frameworks](#frameworks)
- [Reproducing Works](#reproducing-works)

## Python

### Numerical Libraries & Data Structures

- [numpy](https://www.numpy.org) - NumPy is the fundamental package for scientific computing with Python.
- [scipy](https://www.scipy.org) - SciPy (pronounced “Sigh Pie”) is a Python-based ecosystem of open-source software for mathematics, science, and engineering.
- [pandas](https://pandas.pydata.org) - pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures an"
faker,"*Faker* is a Python package that generates fake data for you. Whether
you need to bootstrap your database, create good-looking XML documents,
fill-in your persistence to stress test it, or anonymize data taken from
a production service, Faker is for you.

Faker is heavily inspired by `PHP Faker`_, `Perl Faker`_, and by `Ruby Faker`_.

----

::

    _|_|_|_|          _|
    _|        _|_|_|  _|  _|      _|_|    _|  _|_|
    _|_|_|  _|    _|  _|_|      _|_|_|_|  _|_|
    _|      _|    _|  _|  _|    _|        _|
    _|        _|_|_|  _|    _|    _|_|_|  _|

|pypi| |build| |coverage| |license|

----

Compatibility
-------------

Starting from version ``4.0.0``, ``Faker`` dropped support for Python 2 and from version ``5.0.0``
only supports Python 3.7 and above. If you still need Python 2 compatibility, please install version ``3.0.1`` in the
meantime, and please consider updating your codebase to support Python 3 so you can enjoy the
latest features ``Faker`` has to offer. Please see the `"
inter,"# Inter

Inter is a typeface carefully crafted & designed for computer screens.
Inter features a tall x-height to aid in readability of mixed-case and lower-case text.
Inter is a [variable font](https://rsms.me/inter/#variable) with
several [OpenType features](https://rsms.me/inter/#features), like contextual alternates that adjusts punctuation depending on the shape of surrounding glyphs, slashed zero for when you need to disambiguate ""0"" from ""o"", tabular numbers, etc.

[**Download Inter font files…**](https://github.com/rsms/inter/releases/latest)

<br>

[![Sample](misc/readme/intro.png)](https://rsms.me/inter/samples/)


### Quick questions

- **Where can I get Inter?** [Here](https://github.com/rsms/inter/releases/latest)
- **I think I found a bug. How can I let you know?** [Open an issue here](https://github.com/rsms/inter/issues/new?template=bug_report.md)
- **I have a question. Where can I get help?** [Post in Discussions Q&A](https://github.com/rsms/inter/discussions/categorie"
ultimatevocalremovergui,"# Ultimate Vocal Remover GUI v5.6
<img src=""https://raw.githubusercontent.com/Anjok07/ultimatevocalremovergui/master/gui_data/img/UVR_v5.6.png?raw=true"" />

[![Release](https://img.shields.io/github/release/anjok07/ultimatevocalremovergui.svg)](https://github.com/anjok07/ultimatevocalremovergui/releases/latest)
[![Downloads](https://img.shields.io/github/downloads/anjok07/ultimatevocalremovergui/total.svg)](https://github.com/anjok07/ultimatevocalremovergui/releases)

## About

This application uses state-of-the-art source separation models to remove vocals from audio files. UVR's core developers trained all of the models provided in this package (except for the Demucs v3 and v4 4-stem models).

- **Core Developers**
    - [Anjok07](https://github.com/anjok07)
    - [aufr33](https://github.com/aufr33)

- **Support the Project**
    - [Donate](https://www.buymeacoffee.com/uvr5)

## Installation

These bundles contain the UVR interface, Python, PyTorch, and other depen"
game-programmer,"* English [svg](https://miloyip.github.io/game-programmer/game-programmer.svg) [pdf](https://miloyip.github.io/game-programmer/game-programmer.pdf) [jpg](https://miloyip.github.io/game-programmer/game-programmer.jpg) [png](https://miloyip.github.io/game-programmer/game-programmer.png)
* 简体中文 [svg](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.svg) [pdf](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.pdf) [jpg](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.jpg) [png](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.png) by [tkchu](https://github.com/tkchu)

![ ](game-programmer.jpg)

## Disclaimer

1. This work (the WORK) was created by Milo Yip (the AUTHOR), who has been a game developer for more than 20 years.
2. The books shown in the WORK represent knowledge/skills that may/should be acquired by game programmers. There are other important ways of learning, such as practicing, courses, industrial/academic conferences"
kivy,"Kivy
====

<img align=""right"" height=""256"" src=""https://raw.githubusercontent.com/kivy/kivy/master/kivy/data/logo/kivy-icon-256.png""/>

[Kivy](https://www.kivy.org) is an open-source [Python](https://python.org) framework
for developing GUI apps that work cross-platform, including desktop, mobile and
embedded platforms.

The aim is to allow for quick and easy interaction design and rapid prototyping
whilst making your code reusable and deployable: Innovative user interfaces made
easy.

Kivy is written in Python and [Cython](https://cython.org/) and is built on
[OpenGL ES 2.0](https://www.khronos.org/opengles/). It supports various input 
devices and has an extensive (and extensible) widget library. With the
same codebase, you can target Windows, macOS, Linux (including Raspberry Pi OS),
Android, and iOS. All Kivy widgets are built with multitouch support.

Kivy is [MIT licensed](LICENSE), actively developed by a great community and is
supported by many projects managed by the 
[Kivy Or"
zipline,".. image:: https://media.quantopian.com/logos/open_source/zipline-logo-03_.png
    :target: https://www.zipline.io
    :width: 212px
    :align: center
    :alt: Zipline

=============

|Gitter|
|pypi version status|
|pypi pyversion status|
|travis status|
|appveyor status|
|Coverage Status|

Zipline is a Pythonic algorithmic trading library. It is an event-driven
system for backtesting. Zipline is currently used in production as the backtesting and live-trading
engine powering `Quantopian <https://www.quantopian.com>`_ -- a free,
community-centered, hosted platform for building and executing trading
strategies. Quantopian also offers a `fully managed service for professionals <https://factset.quantopian.com>`_
that includes Zipline, Alphalens, Pyfolio, FactSet data, and more.

- `Join our Community! <https://groups.google.com/forum/#!forum/zipline>`_
- `Documentation <https://www.zipline.io>`_
- Want to Contribute? See our `Development Guidelines <https://www.zipline.io/development-gu"
graphrag,"# GraphRAG

👉 [Use the GraphRAG Accelerator solution](https://github.com/Azure-Samples/graphrag-accelerator) <br/>
👉 [Microsoft Research Blog Post](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)<br/>
👉 [Read the docs](https://microsoft.github.io/graphrag)<br/>
👉 [GraphRAG Arxiv](https://arxiv.org/pdf/2404.16130)

<div align=""left"">
  <a href=""https://pypi.org/project/graphrag/"">
    <img alt=""PyPI - Version"" src=""https://img.shields.io/pypi/v/graphrag"">
  </a>
  <a href=""https://pypi.org/project/graphrag/"">
    <img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dm/graphrag"">
  </a>
  <a href=""https://github.com/microsoft/graphrag/issues"">
    <img alt=""GitHub Issues"" src=""https://img.shields.io/github/issues/microsoft/graphrag"">
  </a>
  <a href=""https://github.com/microsoft/graphrag/discussions"">
    <img alt=""GitHub Discussions"" src=""https://img.shields.io/github/discussions/microsoft/graphrag"">
  </a>
</div>

## O"
dspy,"<p align=""center"">
  <img align=""center"" src=""docs/images/DSPy8.png"" width=""460px"" />
</p>
<p align=""left"">


## DSPy: _Programming_—not prompting—Foundation Models

**[Jun'24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct'23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul'24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun'24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb'24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan'24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec'23] [DSPy Assertions: Computational Constraints for Self-Refining Language Mod"
spotify-downloader,"
<!--- mdformat-toc start --slug=github --->

<!---
!!! IF EDITING THE README, ENSURE TO COPY THE WHOLE FILE TO index.md in `/docs/` AND REMOVE THE REFERENCES TO ReadTheDocs THERE.
--->

<div align=""center"">

# spotDL v4

**spotDL** finds songs from Spotify playlists on YouTube and downloads them - along with album art, lyrics and metadata.


[![MIT License](https://img.shields.io/github/license/spotdl/spotify-downloader?color=44CC11&style=flat-square)](https://github.com/spotDL/spotify-downloader/blob/master/LICENSE)
[![PyPI version](https://img.shields.io/pypi/pyversions/spotDL?color=%2344CC11&style=flat-square)](https://pypi.org/project/spotdl/)
[![PyPi downloads](https://img.shields.io/pypi/dw/spotDL?label=downloads@pypi&color=344CC11&style=flat-square)](https://pypi.org/project/spotdl/)
![Contributors](https://img.shields.io/github/contributors/spotDL/spotify-downloader?style=flat-square)
[![Discord](https://img.shields.io/discord/771628785447337985?label=discord&logo=discord&styl"
changedetection.io,"## Web Site Change Detection, Restock monitoring and notifications.

**_Detect website content changes and perform meaningful actions - trigger notifications via Discord, Email, Slack, Telegram, API calls and many more._**

_Live your data-life pro-actively._ 


[<img src=""https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot.png"" style=""max-width:100%;"" alt=""Self-hosted web site page change monitoring""  title=""Self-hosted web site page change monitoring""  />](https://changedetection.io?src=github)

[![Release Version][release-shield]][release-link] [![Docker Pulls][docker-pulls]][docker-link] [![License][license-shield]](LICENSE.md)

![changedetection.io](https://github.com/dgtlmoon/changedetection.io/actions/workflows/test-only.yml/badge.svg?branch=master)

[**Get started with website page change monitoring straight away. Don't have time? Try our $8.99/month subscription, use our proxies and support!**](https://changedetection.io) , _half the price of o"
haystack,"<div align=""center"">
  <a href=""https://haystack.deepset.ai/""><img src=""https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/banner_20.png"" alt=""Green logo of a stylized white 'H' with the text 'Haystack, by deepset. Haystack 2.0 is live 🎉' Abstract green and yellow diagrams in the background.""></a>

|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
sd-webui-controlnet,"# ControlNet for Stable Diffusion WebUI

The WebUI extension for ControlNet and other injection-based SD controls.
![image](https://github.com/Mikubill/sd-webui-controlnet/assets/20929282/261f9a50-ba9c-472f-b398-fced61929c4a)

This extension is for AUTOMATIC1111's [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui), allows the Web UI to add [ControlNet](https://github.com/lllyasviel/ControlNet) to the original Stable Diffusion model to generate images. The addition is on-the-fly, the merging is not required.

# News

- [2024-07-09] 🔥[v1.1.454] ControlNet union model support [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2989]
- [2024-07-01] 🔥[v1.1.452] Depth Anything V2 - UDAV2 depth Preprocessor [Pull thread: https://github.com/Mikubill/sd-webui-controlnet/pull/2969]
- [2024-05-19] 🔥[v1.1.449] Anyline Preprocessor & MistoLine SDXL model [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2907]
"
reddit,"## This repository is archived.

This repository is archived and will not receive any updates or accept issues or pull requests.

To report bugs in reddit.com please make a post in [/r/bugs](http://www.reddit.com/r/bugs).

If you have found a bug that can in some way compromise the security of the
site or its users, please exercise [responsible
disclosure](http://www.reddit.com/wiki/whitehat) and e-mail
security@reddit.com.

---

### API

For notices about reddit API changes and discussion of reddit API client development, subscribe to the [/r/redditdev](http://www.reddit.com/r/redditdev) and [/r/changelog](http://www.reddit.com/r/changelog) subreddits.

To learn more about reddit's API, check out our [automated API documentation](http://www.reddit.com/dev/api) and the [API wiki page](https://github.com/reddit/reddit/wiki/API). Please use a unique User-Agent string and take care to abide by our [API rules](https://github.com/reddit/reddit/wiki/API#wiki-rules).

### Quickstart

To set u"
InstaPy,"<p align=""center"">
  <img src=""https://i.imgur.com/sJzfZsL.jpg"" width=""154"">
  <h1 align=""center"">InstaPy</h1>
  <p align=""center"">Tooling that <b>automates</b> your social media interactions to “farm” Likes, Comments, and Followers on Instagram
Implemented in Python using the Selenium module.<p>
  <p align=""center"">
    <a href=""https://github.com/timgrossmann/InstaPy/blob/master/LICENSE"">
      <img src=""https://img.shields.io/badge/license-GPLv3-blue.svg"" />
    </a>
    <a href=""https://github.com/SeleniumHQ/selenium"">
      <img src=""https://img.shields.io/badge/built%20with-Selenium-yellow.svg"" />
    </a>
    <a href=""https://www.python.org/"">
    	<img src=""https://img.shields.io/badge/built%20with-Python3-red.svg"" />
    </a>
    <a href=""https://www.github.com/timgrossmann/InstaPy#backer"">
	<img src=""https://opencollective.com/instapy/backers/badge.svg"">
    </a>
    <a href=""https://www.github.com/timgrossmann/InstaPy#sponsors"">
	<img src=""https://opencollective.com/instapy/"
MediaCrawler,"> **免责声明：**
> 
> 大家请以学习为目的使用本仓库，爬虫违法违规的案件：https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China  <br>
>
>本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。

> 点击查看更为详细的免责声明。[点击跳转](#disclaimer)
# 仓库描述

**小红书爬虫**，**抖音爬虫**， **快手爬虫**， **B站爬虫**， **微博爬虫**，**百度贴吧爬虫**，**知乎爬虫**...。  
目前能抓取小红书、抖音、快手、B站、微博、贴吧、知乎等平台的公开信息。

原理：利用[playwright](https://playwright.dev/)搭桥，保留登录成功后的上下文浏览器环境，通过执行JS表达式获取一些加密参数
通过使用此方式，免去了复现核心加密JS代码，逆向难度大大降低

[MediaCrawlerPro](https://github.com/MediaCrawlerPro) 版本已经迭代出来了，相较于开源版本的优势：
- 多账号+IP代理支持（重点！）
- 去除Playwright依赖，使用更加简单
- 支持linux部署（Docker docker-compose）
- 代码重构优化，更加易读易维护（解耦JS签名逻辑）
- 完美的架构设计，更加易扩展，源码学习的价值更大


MediaCrawler仓库白金赞助商:
<a href=""https://mangoproxy.com/?utm_source=mediacrawler&utm_medium=repository&utm_campaign=default"">【MangoProxy】全球IP代理白金推荐，支持210+国家 [MangoProxy](https://mangoproxy.com/?utm_source=mediacrawler&utm_medium=repository&utm_c"
ml-stable-diffusion,"# Core ML Stable Diffusion

Run Stable Diffusion on Apple Silicon with Core ML

[\[Blog Post\]](https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon) [\[BibTeX\]](#bibtex)


This repository comprises:

- `python_coreml_stable_diffusion`, a Python package for converting PyTorch models to Core ML format and performing image generation with Hugging Face [diffusers](https://github.com/huggingface/diffusers) in Python
- `StableDiffusion`, a Swift package that developers can add to their Xcode projects as a dependency to deploy image generation capabilities in their apps. The Swift package relies on the Core ML model files generated by `python_coreml_stable_diffusion`

If you run into issues during installation or runtime, please refer to the [FAQ](#faq) section. Please refer to the [System Requirements](#system-requirements) section before getting started.

<img src=""assets/readme_reel.png"">

## <a name=""system-requirements""></a> System Requirements

<details>
  "
marker,"# Marker

Marker converts PDF to markdown quickly and accurately.

- Supports a wide range of documents (optimized for books and scientific papers)
- Supports all languages
- Removes headers/footers/other artifacts
- Formats tables and code blocks
- Extracts and saves images along with the markdown
- Converts most equations to latex
- Works on GPU, CPU, or MPS

## How it works

Marker is a pipeline of deep learning models:

- Extract text, OCR if necessary (heuristics, [surya](https://github.com/VikParuchuri/surya), tesseract)
- Detect page layout and find reading order ([surya](https://github.com/VikParuchuri/surya))
- Clean and format each block (heuristics, [texify](https://github.com/VikParuchuri/texify)
- Combine blocks and postprocess complete text (heuristics, [pdf_postprocessor](https://huggingface.co/vikp/pdf_postprocessor_t5))

It only uses models where necessary, which improves speed and accuracy.

## Examples

| PDF                                                           "
awesome-oss-alternatives,"# Awesome open-source alternatives to SaaS
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Awesome list of open-source startup alternatives to established SaaS products. Maintained by folks at [![Runa Capital](https://img.shields.io/static/v1?label=&message=%20&style=social&logoWidth=50&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD8AAAAUCAYAAAA6NOUqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAUpSURBVHgBtVhNUuNGFH7d/gkzmSrEKjGzGPkEmKpAUtkgdkBSBTkB4gSYE9icAHMCzAniqWI8kxXKJsXAVGFOgLLBZDWeRSapBNTpT/2EZCEbmcl8VV2S+v97f/1agnLi0qlZU1PldZLCIaUcIZSlSFhoU4p6+rsX3MrO81/evqTHY0kXVxebvwe6dHQ5pM8A8VCH/krNJllq6I4bEdmxUOTr0qy8OZ10w+tkiPq67JAhvklGGI4uv9L/jLHkr9cWtvWjmYt0Gkp5Sty4s93z3ycYda5LT5etRN2lLh7XRfsY8BPf07ok15jTpcZ9eok2i9usqL6YsYHQxJ88LR5o0hv3GoXwVAAzDy7CT0nTKpB6MeWm+jlClTw913zV6w0oH9L9bN7sgAlBOG2KhQPluLpU+b3JfT0y1kLcZvPYCJiznkn+iyflY2UWYx5qoJRoUVkeVjonftaY"
pyspider,"pyspider [![Build Status]][Travis CI] [![Coverage Status]][Coverage]
========

A Powerful Spider(Web Crawler) System in Python.

- Write script in Python
- Powerful WebUI with script editor, task monitor, project manager and result viewer
- [MySQL](https://www.mysql.com/), [MongoDB](https://www.mongodb.org/), [Redis](http://redis.io/), [SQLite](https://www.sqlite.org/), [Elasticsearch](https://www.elastic.co/products/elasticsearch); [PostgreSQL](http://www.postgresql.org/) with [SQLAlchemy](http://www.sqlalchemy.org/) as database backend
- [RabbitMQ](http://www.rabbitmq.com/), [Redis](http://redis.io/) and [Kombu](http://kombu.readthedocs.org/) as message queue
- Task priority, retry, periodical, recrawl by age, etc...
- Distributed architecture, Crawl Javascript pages, Python 2.{6,7}, 3.{3,4,5,6} support, etc...

Tutorial: [http://docs.pyspider.org/en/latest/tutorial/](http://docs.pyspider.org/en/latest/tutorial/)  
Documentation: [http://docs.pyspider.org/](http://docs.pyspider.org/)"
rembg,"# Rembg

[![Downloads](https://img.shields.io/pypi/dm/rembg.svg)](https://img.shields.io/pypi/dm/rembg.svg)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://img.shields.io/badge/License-MIT-blue.svg)
[![Hugging Face Spaces](https://img.shields.io/badge/🤗%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/KenjieDec/RemBG)
[![Streamlit App](https://img.shields.io/badge/🎈%20Streamlit%20Community-Cloud-blue)](https://bgremoval.streamlit.app/)

Rembg is a tool to remove images background.

<p style=""display: flex;align-items: center;justify-content: center;"">
  <img alt=""example car-1"" src=""https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.jpg"" width=""100"" />
  <img alt=""example car-1.out"" src=""https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.out.png"" width=""100"" />
  <img alt=""example car-2"" src=""https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-2.jpg"" width=""100"" />
  <img alt=""example "
PySnooper,"# PySnooper - Never use print for debugging again

**PySnooper** is a poor man's debugger. If you've used Bash, it's like `set -x` for Python, except it's fancier.

Your story: You're trying to figure out why your Python code isn't doing what you think it should be doing. You'd love to use a full-fledged debugger with breakpoints and watches, but you can't be bothered to set one up right now.

You want to know which lines are running and which aren't, and what the values of the local variables are.

Most people would use `print` lines, in strategic locations, some of them showing the values of variables.

**PySnooper** lets you do the same, except instead of carefully crafting the right `print` lines, you just add one decorator line to the function you're interested in. You'll get a play-by-play log of your function, including which lines ran and   when, and exactly when local variables were changed.

What makes **PySnooper** stand out from all other code intelligence tools? You can us"
PyTorch-GAN,"<p align=""center""><img src=""assets/logo.png"" width=""480""\></p>

**This repository has gone stale as I unfortunately do not have the time to maintain it anymore. If you would like to continue the development of it as a collaborator send me an email at eriklindernoren@gmail.com.**

## PyTorch-GAN
Collection of PyTorch implementations of Generative Adversarial Network varieties presented in research papers. Model architectures will not always mirror the ones proposed in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GANs to implement are very welcomed.

<b>See also:</b> [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)

## Table of Contents
  * [Installation](#installation)
  * [Implementations](#implementations)
    + [Auxiliary Classifier GAN](#auxiliary-classifier-gan)
    + [Adversarial Autoencoder](#adversarial-autoencoder)
    + [BEGAN](#began)
    + [BicycleGAN](#b"
learn-python,"# Playground and Cheatsheet for Learning Python

> 🇺🇦 UKRAINE [IS BEING ATTACKED](https://war.ukraine.ua/) BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED.
> - Help Ukraine via:
>   - [Serhiy Prytula Charity Foundation](https://prytulafoundation.org/en/)
>   - [Come Back Alive Charity Foundation](https://savelife.in.ua/en/donate-en/)
>   - [National Bank of Ukraine](https://bank.gov.ua/en/news/all/natsionalniy-bank-vidkriv-spetsrahunok-dlya-zboru-koshtiv-na-potrebi-armiyi)
> - More info on [war.ukraine.ua](https://war.ukraine.ua/) and [MFA of Ukraine](https://twitter.com/MFA_Ukraine)

<hr/>

[![Build Status](https://travis-ci.org/trekhleb/learn-python.svg?branch=master)](https://travis-ci.org/trekhleb/learn-python)

> This is a collection of Python scripts that are split by [topics](#table-of-contents) and contain 
code examples with explanations, different use cases and links to further readings.

> _Read this in:_ [_Português_](README.pt-BR.md), [_"
MoneyPrinterTurbo,"<div align=""center"">
<h1 align=""center"">MoneyPrinterTurbo 💸</h1>

<p align=""center"">
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/stargazers""><img src=""https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""Stargazers""></a>
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/issues""><img src=""https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""Issues""></a>
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/network/members""><img src=""https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""Forks""></a>
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE""><img src=""https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""License""></a>
</p>
<br>
<h3>简体中文 | <a href=""README-en.md"">English</a></h3>
<div align=""center"">
  <a href=""https://trendshift.io/repositories/8731"" target=""_blank""><i"
ipython,".. image:: https://codecov.io/github/ipython/ipython/coverage.svg?branch=main
    :target: https://codecov.io/github/ipython/ipython?branch=main

.. image:: https://img.shields.io/pypi/v/IPython.svg
    :target: https://pypi.python.org/pypi/ipython

.. image:: https://github.com/ipython/ipython/actions/workflows/test.yml/badge.svg
    :target: https://github.com/ipython/ipython/actions/workflows/test.yml

.. image:: https://www.codetriage.com/ipython/ipython/badges/users.svg
    :target: https://www.codetriage.com/ipython/ipython/

.. image:: https://raster.shields.io/badge/Follows-SPEC--0000-brightgreen.png
    :target: https://scientific-python.org/specs/spec-0000/

.. image:: https://tidelift.com/badges/package/pypi/ipython?style=flat
    :target: https://tidelift.com/subscription/pkg/pypi-ipython


===========================================
 IPython: Productive Interactive Computing
===========================================

Overview
========

Welcome to IPython.  Our full docum"
avatarify-python,"![](docs/mona.gif)

# Avatarify Python

Photorealistic avatars for video-conferencing.

Avatarify Python requires manually downloading and installing some dependencies, and is therefore best suited for users who have some experience with command line applications. [Avatarify Desktop](https://github.com/alievk/avatarify-desktop), which aims to be easier to install and use, is recommended for most users. If you still want to use Avatarify Python, proceed to the [install instructions](docs/).

Based on [First Order Motion Model](https://github.com/AliaksandrSiarohin/first-order-model).

## News
- **7 Mar 2021.** Renamed project to Avatarify Python to distinguish it from other versions of Avatarify
- **14 December 2020.** Released Avatarify Desktop. Check it out [here](https://github.com/alievk/avatarify-desktop).
- **11 July 2020.** Added Docker support. Now you can run Avatarify from Docker on [Linux](https://github.com/alievk/avatarify-python/blob/master/docs/README.md#docker). Thanks t"
autojump,"NAME
----

autojump - a faster way to navigate your filesystem

DESCRIPTION
-----------

autojump is a faster way to navigate your filesystem. It works by
maintaining a database of the directories you use the most from the
command line.

*Directories must be visited first before they can be jumped to.*

USAGE
-----

`j` is a convenience wrapper function around `autojump`. Any option that
can be used with `autojump` can be used with `j` and vice versa.

-   Jump To A Directory That Contains `foo`:

        j foo

-   Jump To A Child Directory:

    Sometimes it's convenient to jump to a child directory
    (sub-directory of current directory) rather than typing out the
    full name.

        jc bar

-   Open File Manager To Directories (instead of jumping):

    Instead of jumping to a directory, you can open a file explorer
    window (Mac Finder, Windows Explorer, GNOME Nautilus, etc.) to the
    directory instead.

        jo music

    Opening a file manager to a child directory is"
voice-changer,"## VC Client

[English](/README_en.md) [Korean](/README_ko.md) [Russian](/README_ru.md)

## What's New!
- 姉妹品のText To Speechのクライアントをリリースしました。
  - 簡単なIFで音声生成を楽しむことができます。
  - 詳細は[こちら](https://github.com/w-okada/ttsclient)。
- Beatrice V2 トレーニングコード公開!!!
  - [トレーニングコードリポジトリ](https://huggingface.co/fierce-cats/beatrice-trainer)
  - [コラボ版](https://github.com/w-okada/beatrice-trainer-colab)
- v.2.0.61-alpha
  - [こちらを参照](https://github.com/w-okada/voice-changer/tree/v.2)
  - feature:
    - クロスフェードの時間を指定できるようになりました。
  - bugfix:
    - モデルマージの際に、使用しないモデルの要素を0にしても動くようになりました。
- v.2.0.60-alpha
  - [こちらを参照](https://github.com/w-okada/voice-changer/tree/v.2)
  - feature:
    - [darkmode](https://github.com/w-okada/voice-changer/issues/1306)
    - [re-introduce pytorch rmvpe](https://github.com/w-okada/voice-changer/issues/1319)
    - [wasapi 排他モード選択](https://github.com/w-okada/voice-changer/issues/1305)
- v.2.0.58-alpha
  - [こちらを参照](https://github.com/w-okada/voice-changer/tree/v.2)
  - feature:
    - "
plotly.py,"# plotly.py

<table>
    <tr>
        <td>Latest Release</td>
        <td>
            <a href=""https://pypi.org/project/plotly/""/>
            <img src=""https://badge.fury.io/py/plotly.svg""/>
        </td>
    </tr>
    <tr>
        <td>User forum</td>
        <td>
            <a href=""https://community.plotly.com/""/>
            <img src=""https://img.shields.io/badge/help_forum-discourse-blue.svg""/>
        </td>
    </tr>
    <tr>
        <td>PyPI Downloads</td>
        <td>
            <a href=""https://pepy.tech/project/plotly""/>
            <img src=""https://pepy.tech/badge/plotly/month""/>
        </td>
    </tr>
    <tr>
        <td>License</td>
        <td>
            <a href=""https://opensource.org/licenses/MIT""/>
            <img src=""https://img.shields.io/badge/License-MIT-yellow.svg""/>
        </td>
    </tr>
</table>

<div align=""center"">
  <a href=""https://dash.plotly.com/project-maintenance"">
    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" "
unsloth,"<div align=""center"">

  <a href=""https://unsloth.ai""><picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"">
    <img alt=""unsloth logo"" src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"" height=""110"" style=""max-width: 100%;"">
  </picture></a>
  
<a href=""https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png"" height=""48""></a>
<a href=""https://discord.gg/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
<a href=""https://ko-fi.com/unsloth""><img src=""https://raw.github"
vision,"# torchvision

[![total torchvision downloads](https://pepy.tech/badge/torchvision)](https://pepy.tech/project/torchvision)
[![documentation](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https://pytorch.org/vision/stable/index.html)

The torchvision package consists of popular datasets, model architectures, and common image transformations for computer
vision.

## Installation

Please refer to the [official
instructions](https://pytorch.org/get-started/locally/) to install the stable
versions of `torch` and `torchvision` on your system.

To build source, refer to our [contributing
page](https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md#development-installation).

The following is the corresponding `torchvision` versions and supported Python
versions.

| `torch`            | `torchvision`      | Python              |
| ------------------ | ------------------ "
peft,"<!---
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<h1 align=""center""> <p>🤗 PEFT</p></h1>
<h3 align=""center"">
    <p>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods</p>
</h3>

Fine-tuning large pretrained models is often prohibitively costly due to their scale. Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a small number of (ext"
prefect,"<p align=""center""><img src=""https://github.com/PrefectHQ/prefect/assets/3407835/c654cbc6-63e8-4ada-a92a-efd2f8f24b85"" width=1000></p>

<p align=""center"">
    <a href=""https://pypi.python.org/pypi/prefect/"" alt=""PyPI version"">
        <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/prefect?color=0052FF&labelColor=090422""></a>
    <a href=""https://github.com/prefecthq/prefect/"" alt=""Stars"">
        <img src=""https://img.shields.io/github/stars/prefecthq/prefect?color=0052FF&labelColor=090422"" /></a>
    <a href=""https://pepy.tech/badge/prefect/"" alt=""Downloads"">
        <img src=""https://img.shields.io/pypi/dm/prefect?color=0052FF&labelColor=090422"" /></a>
    <a href=""https://github.com/prefecthq/prefect/pulse"" alt=""Activity"">
        <img src=""https://img.shields.io/github/commit-activity/m/prefecthq/prefect?color=0052FF&labelColor=090422"" /></a>
    <br>
    <a href=""https://prefect.io/slack"" alt=""Slack"">
        <img src=""https://img.shields.io/badge/slack-join_community-red.svg?c"
neural-networks-and-deep-learning,"# Code samples for ""Neural Networks and Deep Learning""

This repository contains code samples for my book on [""Neural Networks
and Deep Learning""](http://neuralnetworksanddeeplearning.com).

The code is written for Python 2.6 or 2.7. There is a version for 
Python 3.8-3.10 [here](https://github.com/unexploredtest/neural-networks-and-deep-learning). 
I will not be updating the current repository for Python 3 compatibility.

The program `src/network3.py` uses version 0.6 or 0.7 of the Theano
library.  It needs modification for compatibility with later versions
of the library.  I will not be making such modifications.

As the code is written to accompany the book, I don't intend to add
new features. However, bug reports are welcome, and you should feel
free to fork and modify the code.

## License

MIT License

Copyright (c) 2012-2022 Michael Nielsen

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated docume"
codellama,"# Introducing Code Llama

Code Llama is a family of large language models for code based on [Llama 2](https://github.com/facebookresearch/llama) providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model train"
netbox,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/netbox_logo.svg"" width=""400"" alt=""NetBox logo"" />
  <p><strong>The cornerstone of every automated network</strong></p>
  <a href=""https://github.com/netbox-community/netbox/releases""><img src=""https://img.shields.io/github/v/release/netbox-community/netbox"" alt=""Latest release"" /></a>
  <a href=""https://github.com/netbox-community/netbox/blob/master/LICENSE.txt""><img src=""https://img.shields.io/badge/license-Apache_2.0-blue.svg"" alt=""License"" /></a>
  <a href=""https://github.com/netbox-community/netbox/graphs/contributors""><img src=""https://img.shields.io/github/contributors/netbox-community/netbox?color=blue"" alt=""Contributors"" /></a>
  <a href=""https://github.com/netbox-community/netbox/stargazers""><img src=""https://img.shields.io/github/stars/netbox-community/netbox?style=flat"" alt=""GitHub stars"" /></a>
  <a href=""https://explore.transifex.com/netbox-community/netbox/""><img src=""h"
awesome-python-login-model,"<h2 align=""center""><code>🎉Life is fantastic🥳!~</code></h2>

<br>
<p align=""center"">
    <img src=""https://github.com/CriseLYJ/flask-video-streaming-recorder/blob/master/img/main.jpg?raw=true"" 
        alt=""Master"">
</p>

<br>

<p align=""center"">""<i>Did you know all your doors were locked?</i>"" - Riddick (The Chronicles of Riddick)</p>

<br>

<p align=""center"">
  <a href=""https://github.com/CriseLYJ/awesome-python-login-model/tree/master"">
    <img src=""https://img.shields.io/badge/Branch-master-green.svg?longCache=true""
        alt=""Branch"">
  </a>
  <a href=""https://github.com/CriseLYJ/awesome-python-login-model/stargazers"">
    <img src=""https://img.shields.io/github/stars/CriseLYJ/awesome-python-login-model.svg?label=Stars&style=social""
        alt=""Stars"">
  </a>
    <a href=""https://github.com/CriseLYJ/awesome-python-login-model/network/members"">
    <img src=""https://img.shields.io/github/forks/CriseLYJ/awesome-python-login-model.svg?label=Forks&style=social""
        alt=""Forks"">"
twint,"# TWINT - Twitter Intelligence Tool
![2](https://i.imgur.com/iaH3s7z.png)
![3](https://i.imgur.com/hVeCrqL.png)

[![PyPI](https://img.shields.io/pypi/v/twint.svg)](https://pypi.org/project/twint/) [![Build Status](https://travis-ci.org/twintproject/twint.svg?branch=master)](https://travis-ci.org/twintproject/twint) [![Python 3.6|3.7|3.8](https://img.shields.io/badge/Python-3.6%2F3.7%2F3.8-blue.svg)](https://www.python.org/download/releases/3.0/) [![GitHub license](https://img.shields.io/github/license/haccer/tweep.svg)](https://github.com/haccer/tweep/blob/master/LICENSE) [![Downloads](https://pepy.tech/badge/twint)](https://pepy.tech/project/twint) [![Downloads](https://pepy.tech/badge/twint/week)](https://pepy.tech/project/twint/week) [![Patreon](https://img.shields.io/endpoint.svg?url=https:%2F%2Fshieldsio-patreon.herokuapp.com%2Ftwintproject)](https://www.patreon.com/twintproject) ![](https://img.shields.io/twitter/follow/noneprivacy.svg?label=Follow&style=social) 

>No authenticat"
ChatGLM2-6B,"# ChatGLM2-6B

<p align=""center"">
🤗 <a href=""https://huggingface.co/THUDM/chatglm2-6b"" target=""_blank"">HF Repo</a> • 🐦 <a href=""https://twitter.com/thukeg"" target=""_blank"">Twitter</a> • 📃 <a href=""https://arxiv.org/abs/2103.10360"" target=""_blank"">[GLM@ACL 22]</a> <a href=""https://github.com/THUDM/GLM"" target=""_blank"">[GitHub]</a> • 📃 <a href=""https://arxiv.org/abs/2210.02414"" target=""_blank"">[GLM-130B@ICLR 23]</a> <a href=""https://github.com/THUDM/GLM-130B"" target=""_blank"">[GitHub]</a> <br>
</p>
<p align=""center"">
    👋 加入我们的  <a href=""https://discord.gg/fK2dz4bg"" target=""_blank"">Discord</a> 和 <a href=""resources/WECHAT.md"" target=""_blank"">WeChat</a>
</p>
<p align=""center"">
📍在 <a href=""https://www.chatglm.cn"">chatglm.cn</a> 体验更大规模的 ChatGLM 模型。
</p>


*Read this in [English](README_EN.md)*

## GLM-4 开源模型和API

我们已经发布最新的 **GLM-4** 模型，该模型在多个指标上有了新的突破，您可以在以下两个渠道体验我们的最新模型。

+ [GLM-4 开源模型](https://github.com/THUDM/GLM-4) 我们已经开源了 GLM-4-9B 系列模型，在各项指标的ce是上有明显提升，欢迎尝试。
+ [智谱清言](https://chatglm.cn/m"
baselines,"**Status:** Maintenance (expect bug fixes and minor updates)

<img src=""data/logo.jpg"" width=25% align=""right"" /> [![Build status](https://travis-ci.org/openai/baselines.svg?branch=master)](https://travis-ci.org/openai/baselines)

# Baselines

OpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.

These algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. 

## Prerequisites 
Baselines requires python3 (>=3.5) with the development headers. You'll also need system packages CMake, OpenMPI and zlib. Those can be installed as follows
### Ubuntu 
    
```bash
sudo apt-get update && sudo apt-get install cmake "
airbyte,"<p align=""center"">
  <a href=""https://airbyte.com""><img src=""https://assets.website-files.com/605e01bc25f7e19a82e74788/624d9c4a375a55100be6b257_Airbyte_logo_color_dark.svg"" alt=""Airbyte""></a>
</p>
<p align=""center"">
    <em>Data integration platform for ELT pipelines from APIs, databases & files to databases, warehouses & lakes</em>
</p>
<p align=""center"">
<a href=""https://github.com/airbytehq/airbyte/stargazers/"" target=""_blank"">
    <img src=""https://img.shields.io/github/stars/airbytehq/airbyte?style=social&label=Star&maxAge=2592000"" alt=""Test"">
</a>
<a href=""https://github.com/airbytehq/airbyte/releases"" target=""_blank"">
    <img src=""https://img.shields.io/github/v/release/airbytehq/airbyte?color=white"" alt=""Release"">
</a>
<a href=""https://airbytehq.slack.com/"" target=""_blank"">
    <img src=""https://img.shields.io/badge/slack-join-white.svg?logo=slack"" alt=""Slack"">
</a>
<a href=""https://www.youtube.com/c/AirbyteHQ/?sub_confirmation=1"" target=""_blank"">
    <img alt=""YouTube Channel"
Shadowrocket-ADBlock-Rules,"## 最完善的 iOS 翻墙规则

### 停止更新公告

维护该项目已花费了我过多的时间，而生活中值得花费时间的东西太多，所以从即日起停止更新该项目。

------------------------------------------------------

这里是一系列好用的翻墙规则，针对 [Shadowrocket](https://liguangming.com/Shadowrocket) 开发，支持广告过滤。规则定义了哪些网站可以直连，哪些必须走代理，规则是一个纯文本文件，无法提供翻墙功能。使用 Python 按照一定的规则和模板定期自动生成，并且使用开源的力量，集众人之力逐渐完善。

**正在使用手机浏览本页面的用户 [请点击这里](https://github.com/h2y/Shadowrocket-ADBlock-Rules/blob/master/readme.md)，查看完整的说明文档。**

**本规则具有以下特点：**

- 黑名单由最新版 GFWList 自动转换；白名单针对全球 top500 站点的连通情况定期自动生成。
- 自动转换最新版本的 `EasyList, Eaylist China, 乘风规则` 为 SR 规则，全面去除广告且去除重复。
- 也包括自定义的广告过滤规则，针对 iOS 端的网页广告、App 广告和视频广告。（[常见广告过滤效果统计](https://github.com/h2y/Shadowrocket-ADBlock-Rules/issues/40)）
- 提供多个规则文件让大家自由选择或者自由切换使用。
- 专门针对 ShadowRocket 开发，可以保证与 SR 的兼容性。


## 规则列表

![规则选择指南](https://h2y.github.io/Shadowrocket-ADBlock-Rules/figure/guide.png)

规则 | 规定代理的网站 | 规定直连的网站 
--- | ----------- | ------------- 
[黑名单规则 + 去广告](#黑名单过滤--广告) |  被墙的网站（GFWList） | 正常的网站 
[黑名单规则](#黑名单过滤) |   |  
[白名单规则 + 去广告](#白名单过滤--广告) | 其他网站 | top500"
click,"# $ click_

Click is a Python package for creating beautiful command line interfaces
in a composable way with as little code as necessary. It's the ""Command
Line Interface Creation Kit"". It's highly configurable but comes with
sensible defaults out of the box.

It aims to make the process of writing command line tools quick and fun
while also preventing any frustration caused by the inability to
implement an intended CLI API.

Click in three points:

-   Arbitrary nesting of commands
-   Automatic help page generation
-   Supports lazy loading of subcommands at runtime


## A Simple Example

```python
import click

@click.command()
@click.option(""--count"", default=1, help=""Number of greetings."")
@click.option(""--name"", prompt=""Your name"", help=""The person to greet."")
def hello(count, name):
    """"""Simple program that greets NAME for a total of COUNT times.""""""
    for _ in range(count):
        click.echo(f""Hello, {name}!"")

if __name__ == '__main__':
    hello()
```

```
$ python hello"
gensim,"gensim – Topic Modelling in Python
==================================

<!--
The following image URLs are obfuscated = proxied and cached through
Google because of Github's proxying issues. See:
https://github.com/RaRe-Technologies/gensim/issues/2805
-->

[![Build Status](https://github.com/RaRe-Technologies/gensim/actions/workflows/tests.yml/badge.svg?branch=develop)](https://github.com/RaRe-Technologies/gensim/actions)
[![GitHub release](https://img.shields.io/github/release/rare-technologies/gensim.svg?maxAge=3600)](https://github.com/RaRe-Technologies/gensim/releases)
[![Downloads](https://img.shields.io/pypi/dm/gensim?color=blue)](https://pepy.tech/project/gensim/)
[![DOI](https://zenodo.org/badge/DOI/10.13140/2.1.2393.1847.svg)](https://doi.org/10.13140/2.1.2393.1847)
[![Mailing List](https://img.shields.io/badge/-Mailing%20List-blue.svg)](https://groups.google.com/g/gensim)
[![Follow](https://img.shields.io/twitter/follow/gensim_py.svg?style=social&style=flat&logo=twitter&label=F"
mihomo,"# mihomo
A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.

API url: https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}

## Installation
```
pip install -U git+https://github.com/KT-Yeh/mihomo.git
```

## Usage

### Basic
There are two parsed data formats:
- V1:
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en&version=v1
  - Fetching: use `client.fetch_user_v1(800333171)`
  - Data model: `mihomo.models.v1.StarrailInfoParsedV1`
  - All models defined in `mihomo/models/v1` directory.
- V2: 
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en
  - Fetching: use `client.fetch_user(800333171)`
  - Data model: `mihomo.models.StarrailInfoParsed`
  - All models defined in `mihomo/models` directory.

If you don't want to use `client.get_icon_url` to get the image url everytime, you can use `client.fetch_user(800333171, replace_icon_name_with_url=True)` to get the parsed data with asset urls."
GHunt,"![](assets/long_banner.png)

<br>

#### 🌐 GHunt Online version : https://osint.industries

<br>

![Python minimum version](https://img.shields.io/badge/Python-3.10%2B-brightgreen)

# 😊 Description

GHunt (v2) is an offensive Google framework, designed to evolve efficiently.\
It's currently focused on OSINT, but any use related with Google is possible.

Features :
- CLI usage and modules
- Python library usage
- Fully async
- JSON export
- Browser extension to ease login

# ✔️ Requirements
- Python >= 3.10

# ⚙️ Installation

```bash
$ pip3 install pipx
$ pipx ensurepath
$ pipx install ghunt
```
It will automatically use venvs to avoid dependency conflicts with other projects.

# 💃 Usage

## Login

First, launch the listener by doing `ghunt login` and choose between 1 of the 2 first methods :
```bash
$ ghunt login

[1] (Companion) Put GHunt on listening mode (currently not compatible with docker)
[2] (Companion) Paste base64-encoded cookies
[3] Enter manually all cookies

Choice =>
```
"
typer,"<p align=""center"">
  <a href=""https://typer.tiangolo.com""><img src=""https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg#only-light"" alt=""Typer""></a>

</p>
<p align=""center"">
    <em>Typer, build great CLIs. Easy to code. Based on Python type hints.</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/typer/actions?query=workflow%3ATest"" target=""_blank"">
    <img src=""https://github.com/fastapi/typer/workflows/Test/badge.svg"" alt=""Test"">
</a>
<a href=""https://github.com/fastapi/typer/actions?query=workflow%3APublish"" target=""_blank"">
    <img src=""https://github.com/fastapi/typer/workflows/Publish/badge.svg"" alt=""Publish"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/typer"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/typer.svg"" alt=""Coverage"">
<a href=""https://pypi.org/project/typer"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/typer?color=%2334D058&label=pypi%20package"
aws-cli,"aws-cli
=======

.. image:: https://github.com/aws/aws-cli/actions/workflows/run-tests.yml/badge.svg
   :target: https://github.com/aws/aws-cli/actions/workflows/run-tests.yml
   :alt: Build Status

This package provides a unified command line interface to Amazon Web
Services.

Jump to:

-  `Getting Started <#getting-started>`__
-  `Getting Help <#getting-help>`__
-  `More Resources <#more-resources>`__

Getting Started
---------------

This README is for the AWS CLI version 1. If you are looking for
information about the AWS CLI version 2, please visit the `v2
branch <https://github.com/aws/aws-cli/tree/v2>`__.

Requirements
~~~~~~~~~~~~

The aws-cli package works on Python versions:

-  3.8.x and greater
-  3.9.x and greater
-  3.10.x and greater
-  3.11.x and greater
-  3.12.x and greater

Notices
~~~~~~~

On 2022-05-30, support for Python 3.6 was ended. This follows the
Python Software Foundation `end of support <https://www.python.org/dev/peps/pep-0494/#lifespan>`__
for the runtim"
ranger,"ranger 1.9.3
============

<img src=""https://ranger.github.io/ranger_logo.png"" width=""150"">

[![Build Status](https://travis-ci.org/ranger/ranger.svg?branch=master)](https://travis-ci.org/ranger/ranger)
<a href=""https://repology.org/metapackage/ranger/versions""><img src=""https://repology.org/badge/latest-versions/ranger.svg"" alt=""latest packaged version(s)""></a>
[![Donate via Liberapay](https://img.shields.io/liberapay/patrons/ranger)](https://liberapay.com/ranger)

ranger is a console file manager with VI key bindings.  It provides a
minimalistic and nice curses interface with a view on the directory hierarchy.
It ships with `rifle`, a file launcher that is good at automatically finding
out which program to use for what file type.

![screenshot](https://raw.githubusercontent.com/ranger/ranger-assets/master/screenshots/screenshot.png)

For `mc` aficionados there's also the multi-pane viewmode.

<p>
<img src=""https://raw.githubusercontent.com/ranger/ranger-assets/master/screenshots/twop"
tensor2tensor,"# Tensor2Tensor

[![PyPI
version](https://badge.fury.io/py/tensor2tensor.svg)](https://badge.fury.io/py/tensor2tensor)
[![GitHub
Issues](https://img.shields.io/github/issues/tensorflow/tensor2tensor.svg)](https://github.com/tensorflow/tensor2tensor/issues)
[![Contributions
welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/tensor2tensor/Lobby)
[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)
[![Travis](https://img.shields.io/travis/tensorflow/tensor2tensor.svg)](https://travis-ci.org/tensorflow/tensor2tensor)
[![Run on FH](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run)

[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor), or
[T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library
of deep learning models and datasets designed to m"
SMSBoom,"# SMSBoom - Deprecate

> **Due to judicial reasons, the repository has been suspended!**  
> **倉庫は司法上の理由で停止された。**-- Japenese  
> **由于司法原因，此仓库被停用！**-- Simplified Chinese  
> **由於司法原因，此倉庫被停用！** -- Traditional Chinese

For more details, please check: [HK JUDICIARY](https://www.judiciary.hk/zh/home/index.html)

![HK JUDICIARY](https://www.judiciary.hk/images/logo_big.png)

HK JUDICIARY 2024/3/20

<!--
![test](img/test2.gif)

## 免责声明

1. 使用此程序请遵守当地的法律法规，禁止滥用、恶意使用，**触犯法律所造成的问题均由使用者承担**。  
2. 本程序仅供娱乐,源码全部开源,**禁止滥用** 和二次 **禁止用于商业用途**.

## Repair - TODO

1. 修改文档
2. 修缮主要功能
3. 修缮后端使用 FastAPI 前端使用 vue3 elementUI
4. GUI 使用 web 技术

## Feature

1. 通过自定义 `api.json` 的方式定义接口.  
2. 支持关键字替换. **时间戳** `[timestamp]` **手机号** `[phone]`  
3. 多线程/异步 请求.  
4. 通过 Flask 提供网页测试/添加接口.  
5. 友好的命令行参数支持.  
6. 采用方便的 pipenv 包管理.  
7. 通过代理调用短信接口, 支持 http, socks4, socks5代理.
8. 使用随机的 User-Agent.
9. 可指定轰炸次数, 轰炸间隔时间.

## Quick Start

### 适用于小白

✨本项目已经使用 `pyinstaller` 打包成 `EXE` 可执行文件!免去部署 Python 环境的烦恼,适合用于小白白.  

🔨作者的打包环境为: `Wi"
SuperAGI,"<p align=""center"">
  <a href=""https://superagi.com//#gh-light-mode-only"">
    <img src=""https://superagi.com/wp-content/uploads/2023/05/Logo-dark.svg"" width=""318px"" alt=""SuperAGI logo"" />
  </a>
  <a href=""https://superagi.com//#gh-dark-mode-only"">
    <img src=""https://superagi.com/wp-content/uploads/2023/05/Logo-light.svg"" width=""318px"" alt=""SuperAGI logo"" />
  </a>

</p>

<p align=""center""><i>Open-source framework to build, manage and run useful Autonomous AI Agents</i></p>
    

<p align=""center"">
<a href=""https://superagi.com""> <img src=""https://superagi.com/wp-content/uploads/2023/08/Website.svg""></a>
<a href=""https://app.superagi.com""> <img src=""https://superagi.com/wp-content/uploads/2023/07/Cloud.svg""></a>
<a href=""https://marketplace.superagi.com/""> <img src=""https://superagi.com/wp-content/uploads/2023/08/Marketplace.svg""></a>
<a href=""https://superagi.com/docs/""> <img src=""https://superagi.com/wp-content/uploads/2023/08/Docs.svg""></a>
<a href=""https://documenter.getpostman."
numpy-ml,"# numpy-ml
Ever wish you had an inefficient but somewhat legible collection of machine
learning algorithms implemented exclusively in NumPy? No?

## Installation

### For rapid experimentation
To use this code as a starting point for ML prototyping / experimentation, just clone the repository, create a new [virtualenv](https://pypi.org/project/virtualenv/), and start hacking:

```sh
$ git clone https://github.com/ddbourgin/numpy-ml.git
$ cd numpy-ml && virtualenv npml && source npml/bin/activate
$ pip3 install -r requirements-dev.txt
```

### As a package
If you don't plan to modify the source, you can also install numpy-ml as a
Python package: `pip3 install -u numpy_ml`.

The reinforcement learning agents train on environments defined in the [OpenAI
gym](https://github.com/openai/gym). To install these alongside numpy-ml, you
can use `pip3 install -u 'numpy_ml[rl]'`.

## Documentation
For more details on the available models, see the [project documentation](https://numpy-ml.readthedoc"
CodeFormer,"<p align=""center"">
  <img src=""assets/CodeFormer_logo.png"" height=110>
</p>

## Towards Robust Blind Face Restoration with Codebook Lookup Transformer (NeurIPS 2022)

[Paper](https://arxiv.org/abs/2206.11253) | [Project Page](https://shangchenzhou.com/projects/CodeFormer/) | [Video](https://youtu.be/d3VDpkXlueI)


<a href=""https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""google colab logo""></a> [![Hugging Face](https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/sczhou/CodeFormer) [![Replicate](https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue)](https://replicate.com/sczhou/codeformer) [![OpenXLab](https://img.shields.io/badge/Demo-%F0%9F%90%BC%20OpenXLab-blue)](https://openxlab.org.cn/apps/detail/ShangchenZhou/CodeFormer) ![Visitors](https://api.infinitescript.com/badgen/count?name=sczhou/CodeFormer&ltext=Visitors"
qlib,"[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?"
ChuanhuChatGPT,"<div align=""right"">
  <!-- 语言: -->
  简体中文 | <a title=""English"" href=""./readme/README_en.md"">English</a> | <a title=""Japanese"" href=""./readme/README_ja.md"">日本語</a> | <a title=""Russian"" href=""./readme/README_ru.md"">Russian</a> | <a title=""Korean"" href=""./readme/README_ko.md"">한국어</a>
</div>

<h1 align=""center"">川虎 Chat 🐯 Chuanhu Chat</h1>
<div align=""center"">
  <a href=""https://github.com/GaiZhenBiao/ChuanhuChatGPT"">
    <img src=""https://github.com/GaiZhenbiao/ChuanhuChatGPT/assets/70903329/aca3a7ec-4f1d-4667-890c-a6f47bf08f63"" alt=""Logo"" height=""156"">
  </a>

<p align=""center"">
    <h3>为ChatGPT等多种LLM提供了一个轻快好用的Web图形界面和众多附加功能</h3>
    <p align=""center"">
      <a href=""https://github.com/GaiZhenbiao/ChuanhuChatGPT/blob/main/LICENSE"">
        <img alt=""Tests Passing"" src=""https://img.shields.io/github/license/GaiZhenbiao/ChuanhuChatGPT"" />
      </a>
      <a href=""https://gradio.app/"">
        <img alt=""GitHub Contributors"" src=""https://img.shields.io/badge/Base-Gradio-fb7d1a?style=flat"" />"
aiohttp,"==================================
Async http client/server framework
==================================

.. image:: https://raw.githubusercontent.com/aio-libs/aiohttp/master/docs/aiohttp-plain.svg
   :height: 64px
   :width: 64px
   :alt: aiohttp logo

|

.. image:: https://github.com/aio-libs/aiohttp/workflows/CI/badge.svg
   :target: https://github.com/aio-libs/aiohttp/actions?query=workflow%3ACI
   :alt: GitHub Actions status for master branch

.. image:: https://codecov.io/gh/aio-libs/aiohttp/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/aio-libs/aiohttp
   :alt: codecov.io status for master branch

.. image:: https://badge.fury.io/py/aiohttp.svg
   :target: https://pypi.org/project/aiohttp
   :alt: Latest PyPI package version

.. image:: https://img.shields.io/pypi/dm/aiohttp
   :target: https://pypistats.org/packages/aiohttp
   :alt: Downloads count

.. image:: https://readthedocs.org/projects/aiohttp/badge/?version=latest
   :target: https://docs.aiohttp.org/
"
Bringing-Old-Photos-Back-to-Life,"# Old Photo Restoration (Official PyTorch Implementation)

<img src='imgs/0001.jpg'/>

### [Project Page](http://raywzy.com/Old_Photo/) | [Paper (CVPR version)](https://arxiv.org/abs/2004.09484) | [Paper (Journal version)](https://arxiv.org/pdf/2009.07047v1.pdf) | [Pretrained Model](https://hkustconnect-my.sharepoint.com/:f:/g/personal/bzhangai_connect_ust_hk/Em0KnYOeSSxFtp4g_dhWdf0BdeT3tY12jIYJ6qvSf300cA?e=nXkJH2) | [Colab Demo](https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing)  | [Replicate Demo & Docker Image](https://replicate.ai/zhangmozhe/bringing-old-photos-back-to-life) :fire:

**Bringing Old Photos Back to Life, CVPR2020 (Oral)**

**Old Photo Restoration via Deep Latent Space Translation, TPAMI 2022**

[Ziyu Wan](http://raywzy.com/)<sup>1</sup>,
[Bo Zhang](https://www.microsoft.com/en-us/research/people/zhanbo/)<sup>2</sup>,
[Dongdong Chen](http://www.dongdongchen.bid/)<sup>3</sup>,
[Pan Zhang](https://panzhang0212.github.io/)<sup>4</sup>,
"
sentence-transformers,"<!--- BADGES: START --->
[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-models-yellow)](https://huggingface.co/models?library=sentence-transformers)
[![GitHub - License](https://img.shields.io/github/license/UKPLab/sentence-transformers?logo=github&style=flat&color=green)][#github-license]
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/sentence-transformers?logo=pypi&style=flat&color=blue)][#pypi-package]
[![PyPI - Package Version](https://img.shields.io/pypi/v/sentence-transformers?logo=pypi&style=flat&color=orange)][#pypi-package]
[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=sentence-transformers)][#docs-package]
<!-- [![PyPI - Downloads](https://img.shields.io/pypi/dm/sentence-transformers?logo=pypi&style=flat&color=green)][#pypi-package] -->

[#github-license]: https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE
[#pypi-package]: https://pypi.org/project/sentence-transformers/
[#"
jupyter,"# Jupyter

*Read this in other languages: [English](README.md), [Español](README.es-ES.md), [Português](README.pt-BR.md), [Français](README.fr-FR.md)*

Jupyter metapackage for installation and documents

## Documentation structure

This documentation uses the [Sphinx](https://sphinx-doc.org) documentation engine.

The documentation is located in the `docs/source` folder. When you build the documentation, it will be placed in the `docs/build` folder.
It is written in a combination of [reStructuredText](https://docutils.sourceforge.io/rst.html) and [MyST Markdown](https://myst-parser.readthedocs.io/).

## Build the documentation locally

There are a few ways to build the documentation; see below for instructions:

### Build the documentation automatically with `nox`

The easiest way to build the documentation locally is by using the [`nox` command line tool](https://nox.thea.codes/). This tool makes it easy to automate commands in a repository, and we have included a `docs` command to qu"
fabric,"|version| |python| |license| |ci| |coverage|

.. |version| image:: https://img.shields.io/pypi/v/fabric
    :target: https://pypi.org/project/fabric/
    :alt: PyPI - Package Version
.. |python| image:: https://img.shields.io/pypi/pyversions/fabric
    :target: https://pypi.org/project/fabric/
    :alt: PyPI - Python Version
.. |license| image:: https://img.shields.io/pypi/l/fabric
    :target: https://github.com/fabric/fabric/blob/main/LICENSE
    :alt: PyPI - License
.. |ci| image:: https://img.shields.io/circleci/build/github/fabric/fabric/main
    :target: https://app.circleci.com/pipelines/github/fabric/fabric
    :alt: CircleCI
.. |coverage| image:: https://img.shields.io/codecov/c/gh/fabric/fabric
    :target: https://app.codecov.io/gh/fabric/fabric
    :alt: Codecov

Welcome to Fabric!
==================

Fabric is a high level Python (2.7, 3.4+) library designed to execute shell
commands remotely over SSH, yielding useful Python objects in return. It builds
on top of `Invoke <"
pyecharts,"<p align=""center"">
    <img src=""https://user-images.githubusercontent.com/19553554/71825144-2d568180-30d6-11ea-8ee0-63c849cfd934.png"" alt=""pyecharts logo"" width=200 height=200 />
</p>
<h1 align=""center"">pyecharts</h1>
<p align=""center"">
    <em>Python ❤️ ECharts = pyecharts</em>
</p>
<p align=""center"">
    <a href=""https://github.com/pyecharts/pyecharts/actions"">
        <img src=""https://github.com/pyecharts/pyecharts/actions/workflows/python-app.yml/badge.svg"" alt=""Github Actions Status"">
    </a>
    <a href=""https://codecov.io/gh/pyecharts/pyecharts"">
        <img src=""https://codecov.io/gh/pyecharts/pyecharts/branch/master/graph/badge.svg"" alt=""Codecov"">
    </a>
    <a href=""https://badge.fury.io/py/pyecharts"">
        <img src=""https://badge.fury.io/py/pyecharts.svg"" alt=""Package version"">
    </a>
    <a href=""https://pypi.org/project/pyecharts/"">
        <img src=""https://img.shields.io/pypi/pyversions/pyecharts.svg?colorB=brightgreen"" alt=""PyPI - Python Version"">
    </a>
</"
networkx,"NetworkX
========


.. image::
    https://github.com/networkx/networkx/workflows/test/badge.svg?branch=main
    :target: https://github.com/networkx/networkx/actions?query=workflow%3Atest

.. image::
    https://codecov.io/gh/networkx/networkx/branch/main/graph/badge.svg?
    :target: https://app.codecov.io/gh/networkx/networkx/branch/main

.. image::
    https://img.shields.io/pypi/v/networkx.svg?
    :target: https://pypi.python.org/pypi/networkx

.. image::
    https://img.shields.io/pypi/l/networkx.svg?
    :target: https://github.com/networkx/networkx/blob/main/LICENSE.txt

.. image::
    https://img.shields.io/pypi/pyversions/networkx.svg?
    :target: https://pypi.python.org/pypi/networkx

.. image::
    https://img.shields.io/github/labels/networkx/networkx/good%20first%20issue?color=green&label=contribute
    :target: https://github.com/networkx/networkx/contribute


NetworkX is a Python package for the creation, manipulation,
and study of the structure, dynamics, and functio"
dalle-mini,"# DALL·E Mini

<a href=""https://www.craiyon.com/""><img src=""https://www.craiyon.com/thumbnail.png"" width=""300""></a>

## How to use it?

You can use the model on [🖍️ craiyon](https://www.craiyon.com/)

## How does it work?

Refer to our reports:

* [DALL·E mini - Generate Images from Any Text Prompt](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy)
* [DALL·E mini - Explained](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA)
* [DALL·E mega - Training Journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2)

## Development

### Dependencies Installation

For inference only, use `pip install dalle-mini`.

For development, clone the repo and use `pip install -e "".[dev]""`.
Before making a PR, check style with `make style`.

You can experiment with the pipeline step by step through our [`inference pipeline notebook`](tools/inference/i"
discord.py,"discord.py
==========

.. image:: https://discord.com/api/guilds/336642139381301249/embed.png
   :target: https://discord.gg/r3sSKJJ
   :alt: Discord server invite
.. image:: https://img.shields.io/pypi/v/discord.py.svg
   :target: https://pypi.python.org/pypi/discord.py
   :alt: PyPI version info
.. image:: https://img.shields.io/pypi/pyversions/discord.py.svg
   :target: https://pypi.python.org/pypi/discord.py
   :alt: PyPI supported Python versions

A modern, easy to use, feature-rich, and async ready API wrapper for Discord written in Python.

Key Features
-------------

- Modern Pythonic API using ``async`` and ``await``.
- Proper rate limit handling.
- Optimised in both speed and memory.

Installing
----------

**Python 3.8 or higher is required**

To install the library without full voice support, you can just run the following command:

.. code:: sh

    # Linux/macOS
    python3 -m pip install -U discord.py

    # Windows
    py -3 -m pip install -U discord.py

Otherwise to ge"
python-mini-projects,"<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
[![forthebadge](https://forthebadge.com/images/badges/built-by-developers.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/built-with-swag.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://forthebadge.com)

# Python-Mini-Projects

[![All Contributors](https://img.shields.io/github/contributors/Python-World/python-mini-projects)](#contributors-)
![Issues](https://img.shields.io/github/issues/Python-World/python-mini-projects)
![Pull Requests](https://img.shields.io/github/issues-pr/Python-World/python-mini-projects?)
![Forks](https://img.shields.io/github/forks/Python-World/python-mini-projects)
![Stars](https://img.shields.io/github/stars/Python-World/python-mini-projects)
![License](https://img.sh"
evals,"# OpenAI Evals

Evals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.

If you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of [OpenAI's President Greg Brockman](https://twitter.com/gdb/status/1733553161884127435):

<img width=""596"" alt=""https://x.com/gdb/status/1733553161884127435?s=20"" src=""https://github.com/openai/evals/assets/35577566/ce7840ff-43a8-4d88-bb2f-6b207410333b"">

## Setup

To run evals, you will need to set up and specify yo"
DocsGPT,"<h1 align=""center"">
  DocsGPT  🦖
</h1>

<p align=""center"">
  <strong>Open-Source Documentation Assistant</strong>
</p>

<p align=""left"">
  <strong><a href=""https://www.docsgpt.cloud/"">DocsGPT</a></strong> is a cutting-edge open-source solution that streamlines the process of finding information in the project documentation. With its integration of the powerful <strong>GPT</strong> models, developers can easily ask questions about a project and receive accurate answers.
  
Say goodbye to time-consuming manual searches, and let <strong><a href=""https://www.docsgpt.cloud/"">DocsGPT</a></strong> help you quickly find the information you need. Try it out and see how it revolutionizes your project documentation experience. Contribute to its development and be a part of the future of AI-powered assistance.
</p>

<div align=""center"">
  
  <a href=""https://github.com/arc53/DocsGPT"">![link to main GitHub showing Stars number](https://img.shields.io/github/stars/arc53/docsgpt?style=social)</a>
  <"
Scrapegraph-ai,"
# 🕷️ ScrapeGraphAI: You Only Scrape Once
[English](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/README.md) | [中文](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/chinese.md) | [日本語](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/japanese.md)
| [한국어](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/korean.md)
| [Русский](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/russian.md)


[![Downloads](https://img.shields.io/pepy/dt/scrapegraphai?style=for-the-badge)](https://pepy.tech/project/scrapegraphai)
[![linting: pylint](https://img.shields.io/badge/linting-pylint-yellowgreen?style=for-the-badge)](https://github.com/pylint-dev/pylint)
[![Pylint](https://img.shields.io/github/actions/workflow/status/VinciGit00/Scrapegraph-ai/pylint.yml?label=Pylint&logo=github&style=for-the-badge)](https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/pylint.yml)
[![CodeQL](https://img.shields.io/github/actions/workflow/status/VinciGit00"
fauxpilot,"
# FauxPilot

This is an attempt to build a locally hosted alternative to [GitHub Copilot](https://copilot.github.com/). It uses the [SalesForce CodeGen](https://github.com/salesforce/CodeGen) models inside of NVIDIA's [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) with the [FasterTransformer backend](https://github.com/triton-inference-server/fastertransformer_backend/).

<p align=""right"">
  <img width=""50%"" align=""right"" src=""./img/fauxpilot.png"">
</p>

## Prerequisites

You'll need:

* Docker
* `docker compose` >= 1.28
* An NVIDIA GPU with Compute Capability >= 6.0 and enough VRAM to run the model you want.
* [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker)
* `curl` and `zstd` for downloading and unpacking the models.

Note that the VRAM requirements listed by `setup.sh` are *total* -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you *should* be able to run the 6B model by "
mackup,"# Mackup™

Keep your application settings in sync.

## Table of contents

- [Mackup](#mackup)
  - [Table of contents](#table-of-contents)
  - [WARNING](#warning)
  - [Quickstart](#quickstart)
  - [Usage](#usage)
  - [What does it do](#what-does-it-do)
  - [Bullsh\*t, what does it really do to my files](#bullsht-what-does-it-really-do-to-my-files)
    - [Backup](#backup)
    - [Restore](#restore)
    - [Uninstall](#uninstall)
  - [Supported Storages](#supported-storages)
  - [Unsupported Storages](#unsupported-storages)
  - [Supported Applications](#supported-applications)
  - [Can you support application X](#can-you-support-application-x)
  - [Personalization \& configuration](#personalization--configuration)
  - [Why did you do this](#why-did-you-do-this)
  - [What platforms are supported](#what-platforms-are-supported)
  - [What's up with the weird name](#whats-up-with-the-weird-name)
  - [Where can I find more information](#where-can-i-find-more-information)

## WARNING

⚠️ Mackup d"
DeDRM_tools,"# No Longer Maintained
I have not had the time to devote to this project in recent years that I would have liked. I am delighted to find that someone else has taken on the task of keeping the tools updated, and making releases. I shall be using noDRM's version of the tools from now on. 

[Guide] How to remove DRM
Refer to [Wiki Page](https://github.com/apprenticeharper/DeDRM_tools/wiki/Exactly-how-to-remove-DRM)

# DeDRM_tools
DeDRM tools for ebooks

This is a repository that tracks all the scripts and other tools for removing DRM from ebooks that I could find, committed in date order as best as I could manage. (Except for the Requiem tools for Apple's iBooks, and Convert LIT for Microsoft's .lit ebooks.) This includes the tools from a time before Apprentice Alf had a blog, and continues through to when Apprentice Harper (with help) took over maintenance of the tools.

The individual scripts are now released as two plugins for calibre: DeDRM and Obok. 
The DeDRM plugin handles books th"
py12306,"# 🚂 py12306 购票助手
分布式，多账号，多任务购票

## Features
- [x] 多日期查询余票
- [x] 自动打码下单
- [x] 用户状态恢复
- [x] 电话语音通知
- [x] 多账号、多任务、多线程支持
- [x] 单个任务多站点查询 
- [x] 分布式运行
- [x] Docker 支持
- [x] 动态修改配置文件
- [x] 邮件通知
- [x] Web 管理页面
- [x] 微信消息通知
- [ ] 代理池支持 ([pyproxy-async](https://github.com/pjialin/pyproxy-async))

## 使用
py12306 需要运行在 python 3.6 以上版本（其它版本暂未测试)

**1. 安装依赖**
```bash
git clone https://github.com/pjialin/py12306

pip install -r requirements.txt
```

**2. 配置程序**
```bash
cp env.py.example env.py
```
自动打码

（若快已停止服务，目前只能设置**free**打码模式）
free 已对接到打码共享平台，[https://py12306-helper.pjialin.com](https://py12306-helper.pjialin.com/)，欢迎参与分享

语音通知

语音验证码使用的是阿里云 API 市场上的一个服务商，需要到 [https://market.aliyun.com/products/56928004/cmapi026600.html](https://market.aliyun.com/products/56928004/cmapi026600.html) 购买后将 appcode 填写到配置中

**3. 启动前测试**

目前提供了一些简单的测试，包括用户账号检测，乘客信息检测，车站检测等

开始测试 -t 
```bash
python main.py -t
```

测试通知消息 (语音, 邮件) -t -n
```bash
# 默认不会进行通知测试，要对通知进行测试需要加上 -n 参数 
python main.py -t -n
```

**4. 运行程序**
```ba"
imgaug,"# imgaug

This python library helps you with augmenting images for your machine learning projects.
It converts a set of input images into a new, much larger set of slightly altered images.

[![Build Status](https://travis-ci.org/aleju/imgaug.svg?branch=master)](https://travis-ci.org/aleju/imgaug)
[![codecov](https://codecov.io/gh/aleju/imgaug/branch/master/graph/badge.svg)](https://codecov.io/gh/aleju/imgaug)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/1370ce38e99e40af842d47a8dd721444)](https://www.codacy.com/app/aleju/imgaug?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=aleju/imgaug&amp;utm_campaign=Badge_Grade)

<table>

<tr>
<th>&nbsp;</th>
<th>Image</th>
<th>Heatmaps</th>
<th>Seg. Maps</th>
<th>Keypoints</th>
<th>Bounding Boxes,<br>Polygons</th>
</tr>

<!-- Line 1: Original Input -->
<tr>
<td><em>Original Input</em></td>
<td><img src=""https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_image.jpg?raw=true"" heigh"
powerline,"Powerline
=========

**Powerline is a statusline plugin for vim, and provides statuslines and 
prompts for several other applications, including zsh, bash, fish, tmux,
IPython, Awesome, i3 and Qtile.**

+---------+---------------------------------------------------+
| Author  | Kim Silkebækken (kim.silkebaekken+vim@gmail.com)  |
+---------+---------------------------------------------------+
| Source  | https://github.com/powerline/powerline            |
+---------+---------------------------------------------------+
| Version | beta                                              |
+---------+---------------------------------------------------+

**Powerline does not support python2 anymore and powerline will stop working with python2 in the near future.**

Features
--------

* **Extensible and feature rich, written in Python.** Powerline was 
  completely rewritten in Python to get rid of as much vimscript as 
  possible. This has allowed much better extensibility, leaner and better 
  c"
the-gan-zoo,"# The GAN Zoo

<p align=""center""><img width=""40%"" src=""The_GAN_Zoo.jpg"" /></p>

Every week, new GAN papers are coming out and it's hard to keep track of them all, not to mention the incredibly creative ways in which researchers are naming these GANs! So, here's a list of what started as a fun activity compiling all named GANs!

<p align=""center""><img width=""50%"" src=""cumulative_gans.jpg"" /></p>

You can also check out the same data in a tabular format with functionality to filter by year or do a quick search by title [here](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv).

Contributions are welcome. Add links through pull requests in gans.tsv file in the same format or create an issue to lemme know something I missed or to start a discussion.

Check out [Deep Hunt](https://deephunt.in) - my weekly AI newsletter for this repo as [blogpost](https://medium.com/deep-hunt/the-gan-zoo-79597dc8c347) and follow me on [Twitter](https://www.twitter.com/hindupuravinash).

* 3"
YYeTsBot,"# YYeTsBot

[![build docker image](https://github.com/tgbot-collection/YYeTsBot/actions/workflows/docker.yaml/badge.svg)](https://github.com/tgbot-collection/YYeTsBot/actions/workflows/docker.yaml)
[![Docker Pulls](https://img.shields.io/docker/pulls/bennythink/yyetsbot)](https://hub.docker.com/r/bennythink/yyetsbot)

![](assets/index.png)

👉 前端[在这里](https://github.com/tgbot-collection/YYeTsFE) 👈

# 使用说明

直接发送想要看的剧集名称就可以了，可选分享网页或者链接（ed2k和磁力链接）。


搜索资源时，会按照我预定的优先级（人人影视离线、字幕侠）进行搜索，当然也可以使用命令强制某个字幕组，如 `/yyets_offline 逃避可耻`

## 命令

```
start - 开始使用
help - 帮助
credits - 致谢
ping - 运行状态
settings - 获取公告
zimuxia_online - 字幕侠在线数据  
newzmz_online - new字幕组在线数据 
yyets_offline - 人人影视离线数据
```

# 截图

## 常规搜索

![](assets/1.png)

## 资源分享站截图

本网站永久免费，并且没有任何限制。
![](assets/new_resource.png)

![](assets/2.png)

支持收藏功能，会跨设备同步
![](assets/like.png)

## 指定字幕组搜索

目前只支持YYeTsOffline、ZimuxiaOnline和NewzmzOnline

![](assets/3.png)

# 如何下载磁力和电驴资源？迅雷提示资源敏感

## 电驴资源

"
horovod,".. raw:: html

    <p align=""center""><img src=""https://user-images.githubusercontent.com/16640218/34506318-84d0c06c-efe0-11e7-8831-0425772ed8f2.png"" alt=""Logo"" width=""200""/></p>
    <br/>

Horovod
=======

.. raw:: html

   <div align=""center"">

.. image:: https://badge.fury.io/py/horovod.svg
   :target: https://badge.fury.io/py/horovod
   :alt: PyPI Version

.. image:: https://badge.buildkite.com/6f976bc161c69d9960fc00de01b69deb6199b25680a09e5e26.svg?branch=master
   :target: https://buildkite.com/horovod/horovod
   :alt: Build Status

.. image:: https://readthedocs.org/projects/horovod/badge/?version=latest
   :target: https://horovod.readthedocs.io/en/latest/
   :alt: Documentation Status

.. image:: https://img.shields.io/badge/slack-chat-green.svg?logo=slack
   :target: https://forms.gle/cPGvty5hp31tGfg79
   :alt: Slack

.. raw:: html

   </div>

.. raw:: html

   <div align=""center"">

.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg
   :target: https://img.sh"
gpt-researcher,"<div align=""center"">
<!--<h1 style=""display: flex; align-items: center; gap: 10px;"">
  <img src=""https://github.com/assafelovic/gpt-researcher/assets/13554167/a45bac7c-092c-42e5-8eb6-69acbf20dde5"" alt=""Logo"" width=""25"">
  GPT Researcher
</h1>-->
<img src=""https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3"" alt=""Logo"" width=""80"">


####
[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)
[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)
[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted&?compact=true)](https://discord.gg/QgZXvJAccX)
<!--[![Discord Follow](https://img.shields.io/discord/1127851779011391548?style=for-the-badge&logo=discord&label=Chat%20on%20Discord)](https://discord.gg/"
sqlmodel,"<p align=""center"">
  <a href=""https://sqlmodel.tiangolo.com""><img src=""https://sqlmodel.tiangolo.com/img/logo-margin/logo-margin-vector.svg#only-light"" alt=""SQLModel""></a>

</p>
<p align=""center"">
    <em>SQLModel, SQL databases in Python, designed for simplicity, compatibility, and robustness.</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/sqlmodel/actions?query=workflow%3ATest"" target=""_blank"">
    <img src=""https://github.com/fastapi/sqlmodel/workflows/Test/badge.svg"" alt=""Test"">
</a>
<a href=""https://github.com/fastapi/sqlmodel/actions?query=workflow%3APublish"" target=""_blank"">
    <img src=""https://github.com/fastapi/sqlmodel/workflows/Publish/badge.svg"" alt=""Publish"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/sqlmodel"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/sqlmodel.svg"" alt=""Coverage"">
<a href=""https://pypi.org/project/sqlmodel"" target=""_blank"">
    <img src=""https://img.shield"
flux,"# FLUX
by Black Forest Labs: https://blackforestlabs.ai

![grid](assets/grid.jpg)

This repo contains minimal inference code to run text-to-image and image-to-image with our Flux latent rectified flow transformers.

### Inference partners

We are happy to partner with [Replicate](https://replicate.com/), [FAL](https://fal.ai/) and [Mystic](https://www.mystic.ai). You can sample our models using their services.
Below we list relevant links.

Replicate:

- https://replicate.com/collections/flux
- https://replicate.com/collections/flux-fine-tunes
- https://replicate.com/black-forest-labs/flux-pro
- https://replicate.com/black-forest-labs/flux-dev
- https://replicate.com/black-forest-labs/flux-schnell

FAL:

- https://fal.ai/models/fal-ai/flux-pro
- https://fal.ai/models/fal-ai/flux/dev
- https://fal.ai/models/fal-ai/flux/schnell

Mystic:

- https://www.mystic.ai/black-forest-labs
- https://www.mystic.ai/black-forest-labs/flux1-pro
- https://www.mystic.ai/black-forest-labs/flux1-dev
- http"
salt,".. image:: https://img.shields.io/github/license/saltstack/salt
   :alt: Salt Project License: Apache v2.0
   :target: https://github.com/saltstack/salt/blob/master/LICENSE

.. image:: https://img.shields.io/pypi/dm/salt?label=pypi%20downloads
   :alt: PyPi Package Downloads
   :target: https://pypi.org/project/salt

.. image:: https://img.shields.io/lgtm/grade/python/github/saltstack/salt
   :alt: PyPi Package Downloads
   :target: https://lgtm.com/projects/g/saltstack/salt/context:python

.. image:: https://img.shields.io/badge/slack-SaltProject-blue.svg?logo=slack
   :alt: Salt Project Slack Community
   :target: https://via.vmw.com/salt-slack

.. image:: https://img.shields.io/twitch/status/saltprojectoss
   :alt: Salt Project Twitch Channel
   :target: https://www.twitch.tv/saltprojectoss

.. image:: https://img.shields.io/reddit/subreddit-subscribers/saltstack?style=social
   :alt: Salt Project subreddit
   :target: https://www.reddit.com/r/saltstack/

.. image:: https://img.shie"
stylegan,"## StyleGAN &mdash; Official TensorFlow Implementation
![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg?style=plastic)
![TensorFlow 1.10](https://img.shields.io/badge/tensorflow-1.10-green.svg?style=plastic)
![cuDNN 7.3.1](https://img.shields.io/badge/cudnn-7.3.1-green.svg?style=plastic)
![License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-green.svg?style=plastic)

![Teaser image](./stylegan-teaser.png)
**Picture:** *These people are not real &ndash; they were produced by our generator that allows control over different aspects of the image.*

This repository contains the official TensorFlow implementation of the following paper:

> **A Style-Based Generator Architecture for Generative Adversarial Networks**<br>
> Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)<br>
> https://arxiv.org/abs/1812.04948
>
> **Abstract:** *We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literatur"
backtrader,"backtrader
==========

.. image:: https://img.shields.io/pypi/v/backtrader.svg
   :alt: PyPi Version
   :scale: 100%
   :target: https://pypi.python.org/pypi/backtrader/

..  .. image:: https://img.shields.io/pypi/dm/backtrader.svg
       :alt: PyPi Monthly Donwloads
       :scale: 100%
       :target: https://pypi.python.org/pypi/backtrader/

.. image:: https://img.shields.io/pypi/l/backtrader.svg
   :alt: License
   :scale: 100%
   :target: https://github.com/backtrader/backtrader/blob/master/LICENSE
.. image:: https://travis-ci.org/backtrader/backtrader.png?branch=master
   :alt: Travis-ci Build Status
   :scale: 100%
   :target: https://travis-ci.org/backtrader/backtrader
.. image:: https://img.shields.io/pypi/pyversions/backtrader.svg
   :alt: Python versions
   :scale: 100%
   :target: https://pypi.python.org/pypi/backtrader/

**Yahoo API Note**:

  [2018-11-16] After some testing it would seem that data downloads can be
  again relied upon over the web interface (or API ``v7``)
"
newspaper,"Newspaper3k: Article scraping & curation
========================================

.. image:: https://badge.fury.io/py/newspaper3k.svg
    :target: http://badge.fury.io/py/newspaper3k.svg
        :alt: Latest version

.. image:: https://travis-ci.org/codelucas/newspaper.svg
        :target: http://travis-ci.org/codelucas/newspaper/
        :alt: Build status

.. image:: https://coveralls.io/repos/github/codelucas/newspaper/badge.svg?branch=master
        :target: https://coveralls.io/github/codelucas/newspaper
        :alt: Coverage status


Inspired by `requests`_ for its simplicity and powered by `lxml`_ for its speed:

    ""Newspaper is an amazing python library for extracting & curating articles.""
    -- `tweeted by`_ Kenneth Reitz, Author of `requests`_

    ""Newspaper delivers Instapaper style article extraction."" -- `The Changelog`_

.. _`tweeted by`: https://twitter.com/kennethreitz/status/419520678862548992
.. _`The Changelog`: http://thechangelog.com/newspaper-delivers-instap"
albumentations,"# Albumentations

[![PyPI version](https://badge.fury.io/py/albumentations.svg)](https://badge.fury.io/py/albumentations)
![CI](https://github.com/albumentations-team/albumentations/workflows/CI/badge.svg)
[![PyPI Downloads](https://img.shields.io/pypi/dm/albumentations.svg?label=PyPI%20downloads)](
https://pypi.org/project/albumentations/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/albumentations.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/albumentations)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/albumentations)
[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)

[Docs](https://albumentations.ai/docs/) | [Discord](https://discord.gg/AKPrrDYNAt) | [Twitter](https://twitter.com/albumentations) | [LinkedIn](https://www.linkedin.com/company/100504475/)

Albumentations is a Python library for image "
ChatterBot,"![ChatterBot: Machine learning in Python](https://i.imgur.com/b3SCmGT.png)

# ChatterBot

ChatterBot is a machine-learning based conversational dialog engine build in
Python which makes it possible to generate responses based on collections of
known conversations. The language independent design of ChatterBot allows it
to be trained to speak any language.

[![Package Version](https://img.shields.io/pypi/v/chatterbot.svg)](https://pypi.python.org/pypi/chatterbot/)
[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)
[![Django 2.0](https://img.shields.io/badge/Django-2.0-blue.svg)](https://docs.djangoproject.com/en/2.1/releases/2.0/)
[![Requirements Status](https://requires.io/github/gunthercox/ChatterBot/requirements.svg?branch=master)](https://requires.io/github/gunthercox/ChatterBot/requirements/?branch=master)
[![Build Status](https://travis-ci.org/gunthercox/ChatterBot.svg?branch=master)](https://travis-ci.org/gunthe"
ivy,"<div style=""display: block;"" align=""center"">
    <a href=""https://ivy.dev/"">
        <img class=""dark-light"" width=""50%"" src=""https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/ivy-long.svg""/>
    </a>
</div>

------------------------------------------------------------------------

<table align=""center"">
  <tr>
    <td align=""center"">
      <a href=""https://ivy.dev/"">
          <img class=""dark-light"" width=""75"" src=""https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/website.svg"" alt=""Website"">
      </a>
      <br>
      <a href=""https://ivy.dev/"" style=""text-decoration: none;"">Website</a>
    </td>
    <td align=""center"">
      <a href=""https://docs.ivy.dev/"">
          <img class=""dark-light"" width=""70"" src=""https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/docs.svg"" alt=""Docs"">
      </a>
      <br>
      <a href=""https://docs.ivy.dev/"" style=""text-decoration: none;"">Docs</a>
    </td>
  "
nni,"<div align=""center"">
<img src=""docs/img/nni_logo.png"" width=""600""/>
</div>

<br/>

[![MIT licensed](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)
[![Issues](https://img.shields.io/github/issues-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen)
[![Bugs](https://img.shields.io/github/issues/Microsoft/nni/bug.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen+label%3Abug)
[![Pull Requests](https://img.shields.io/github/issues-pr-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/pulls?q=is%3Apr+is%3Aopen)
[![Version](https://img.shields.io/github/release/Microsoft/nni.svg)](https://github.com/Microsoft/nni/releases)
[![Documentation Status](https://readthedocs.org/projects/nni/badge/?version=stable)](https://nni.readthedocs.io/en/stable/?badge=stable)
[![](https://img.shields.io/github/contributors-anon/microsoft/nni)](https://github.com/microsoft/nni/graphs/contributors)



[<img src=""docs/img/readme_bann"
wxpy,"wxpy: 用 Python 玩微信
==============================

.. image:: https://badge.fury.io/py/wxpy.svg
    :target: https://badge.fury.io/py/wxpy

.. image:: https://img.shields.io/pypi/pyversions/wxpy.svg
        :target: https://github.com/youfou/wxpy

.. image:: https://readthedocs.org/projects/wxpy/badge/?version=latest
    :target: http://wxpy.readthedocs.io/zh/latest/?badge=latest

微信机器人 / 可能是最优雅的微信个人号 API
    wxpy 在 itchat 的基础上，通过大量接口优化提升了模块的易用性，并进行丰富的功能扩展


..  attention::

    | **强烈建议仅使用小号运行机器人！**

    | 从近期 (17年6月下旬) 反馈来看，使用机器人存在一定概率被限制登录的可能性。
    | 主要表现为无法登陆 Web 微信 (但不影响手机等其他平台)。



用来干啥
----------------

一些常见的场景

* 控制路由器、智能家居等具有开放接口的玩意儿
* 运行脚本时自动把日志发送到你的微信
* 加群主为好友，自动拉进群中
* 跨号或跨群转发消息
* 自动陪人聊天
* 逗人玩
* ...

总而言之，可用来实现各种微信个人号的自动化操作


..
    体验一下
    ----------------

    **这有一个现成的微信机器人，想不想调戏一下？**

    记得填写入群口令 👉 [ **wxpy** ]，与群里的大神们谈笑风生 😏

    ..  image:: https://github.com/youfou/wxpy/raw/master/docs/wechat-group.png


轻松安装
----------------

wxpy 支持 Python 3.4-3.6，以及 2.7 版本

将下方命令中"
awx,"[![CI](https://github.com/ansible/awx/actions/workflows/ci.yml/badge.svg?branch=devel)](https://github.com/ansible/awx/actions/workflows/ci.yml) [![codecov](https://codecov.io/github/ansible/awx/graph/badge.svg?token=4L4GSP9IAR)](https://codecov.io/github/ansible/awx) [![Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-yellow.svg)](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html) [![Apache v2 License](https://img.shields.io/badge/license-Apache%202.0-brightgreen.svg)](https://github.com/ansible/awx/blob/devel/LICENSE.md) [![AWX on the Ansible Forum](https://img.shields.io/badge/mailing%20list-AWX-orange.svg)](https://forum.ansible.com/tag/awx)
[![Ansible Matrix](https://img.shields.io/badge/matrix-Ansible%20Community-blueviolet.svg?logo=matrix)](https://chat.ansible.im/#/welcome) [![Ansible Discourse](https://img.shields.io/badge/discourse-Ansible%20Community-yellowgreen.svg?logo=discourse)](https://forum.ansible.com)

<img src=""https://r"
mailinabox,"Mail-in-a-Box
=============

By [@JoshData](https://github.com/JoshData) and [contributors](https://github.com/mail-in-a-box/mailinabox/graphs/contributors).

Mail-in-a-Box helps individuals take back control of their email by defining a one-click, easy-to-deploy SMTP+everything else server: a mail server in a box.

**Please see [https://mailinabox.email](https://mailinabox.email) for the project's website and setup guide!**

* * *

Our goals are to:

* Make deploying a good mail server easy.
* Promote [decentralization](http://redecentralize.org/), innovation, and privacy on the web.
* Have automated, auditable, and [idempotent](https://web.archive.org/web/20190518072631/https://sharknet.us/2014/02/01/automated-configuration-management-challenges-with-idempotency/) configuration.
* **Not** make a totally unhackable, NSA-proof server.
* **Not** make something customizable by power users.

Additionally, this project has a [Code of Conduct](CODE_OF_CONDUCT.md), which supersedes the goals"
flair,"![alt text](resources/docs/flair_logo_2020_FINAL_day_dpi72.png#gh-light-mode-only)
![alt text](resources/docs/flair_logo_2020_FINAL_night_dpi72.png#gh-dark-mode-only)

[![PyPI version](https://badge.fury.io/py/flair.svg)](https://badge.fury.io/py/flair)
[![GitHub Issues](https://img.shields.io/github/issues/flairNLP/flair.svg)](https://github.com/flairNLP/flair/issues)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)

A very simple framework for **state-of-the-art NLP**. Developed by [Humboldt University of Berlin](https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en/) and friends.

---

Flair is:

* **A powerful NLP library.** Flair allows you to apply our state-of-the-art natural language processing (NLP)
models to your text, such as named entity recognition (NER), sentiment analysis, part-of-speec"
wechat_jump_game,"# 教你用 Python 来玩微信跳一跳
[![GitHub stars](https://img.shields.io/github/stars/wangshub/wechat_jump_game.svg)](https://github.com/wangshub/wechat_jump_game/stargazers) [![GitHub forks](https://img.shields.io/github/forks/wangshub/wechat_jump_game.svg)](https://github.com/wangshub/wechat_jump_game/network) [![GitHub license](https://img.shields.io/github/license/wangshub/wechat_jump_game.svg)](https://github.com/wangshub/wechat_jump_game/blob/master/LICENSE)

[![Throughput Graph](https://graphs.waffle.io/wangshub/wechat_jump_game/throughput.svg)](https://waffle.io/wangshub/wechat_jump_game/metrics/throughput) 

## 游戏模式

> 2017 年 12 月 28 日下午，微信发布了 6.6.1 版本，加入了「小游戏」功能，并提供了官方 DEMO「跳一跳」。这是一个 2.5D 插画风格的益智游戏，玩家可以通过按压屏幕时间的长短来控制这个「小人」跳跃的距离。分数越高，那么在好友排行榜更加靠前。通过 Python 脚本自动运行，让你轻松霸榜。

![](./resource/image/jump.gif)

可能刚开始上手的时候，因为时间距离之间的关系把握不恰当，只能跳出几个就掉到了台子下面。**如果能利用图像识别精确测量出起始和目标点之间测距离，就可以估计按压的时间来精确跳跃。**

## 原理说明

##### 由于微信检测非常严厉，这里的防禁代码可能已经不起作用，主要供学习用途

1. 将手机点击到《跳一跳》小程序界面

2. 用 ADB 工具获取当前手机截图，并用 AD"
examples-of-web-crawlers,"# <p align=""center"">一些非常有趣的python爬虫例子,对新手比较友好</p>


<p align=""center"">
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers""><img src=""https://img.shields.io/badge/status-updating-brightgreen.svg""></a>
    <a href=""https://github.com/python/cpython""><img src=""https://img.shields.io/badge/Python-3.7-FF1493.svg""></a>
    <a href=""https://opensource.org/licenses/mit-license.php""><img src=""https://badges.frapsoft.com/os/mit/mit.svg""></a>
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers/graphs/contributors""><img src=""https://img.shields.io/github/contributors/shengqiangzhang/examples-of-web-crawlers?color=blue""></a>
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers/stargazers""><img src=""https://img.shields.io/github/stars/shengqiangzhang/examples-of-web-crawlers.svg?logo=github""></a>
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers/network/members""><img src=""https://img.shields.io/github/forks/shengqi"
gaussian-splatting,"# 3D Gaussian Splatting for Real-Time Radiance Field Rendering
Bernhard Kerbl*, Georgios Kopanas*, Thomas Leimkühler, George Drettakis (* indicates equal contribution)<br>
| [Webpage](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) | [Full Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf) | [Video](https://youtu.be/T_kXY43VZnk) | [Other GRAPHDECO Publications](http://www-sop.inria.fr/reves/publis/gdindex.php) | [FUNGRAPH project page](https://fungraph.inria.fr) |<br>
| [T&T+DB COLMAP (650MB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/datasets/input/tandt_db.zip) | [Pre-trained Models (14 GB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/datasets/pretrained/models.zip) | [Viewers for Windows (60MB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/binaries/viewers.zip) | [Evaluation Images (7 GB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/evaluation/images.zip) |<br>
![Teaser image"
yapf,"# YAPF

<p align=""center"">
<a href=""https://badge.fury.io/py/yapf""><img alt=""PyPI Version"" src=""https://badge.fury.io/py/yapf.svg""></a>
<a href=""https://github.com/google/yapf/actions/workflows/ci.yml""><img alt=""Build Status"" src=""https://github.com/google/yapf/actions/workflows/ci.yml/badge.svg""></a>
<a href=""https://github.com/google/yapf/actions/workflows/pre-commit.yml""><img alt=""Actions Status"" src=""https://github.com/google/yapf/actions/workflows/pre-commit.yml/badge.svg""></a>
<a href=""https://coveralls.io/github/google/yapf?branch=main""><img alt=""Coverage Status"" src=""https://coveralls.io/repos/github/google/yapf/badge.svg?branch=main""></a>
</p>


## Introduction

YAPF is a Python formatter based on [`clang-format`](https://clang.llvm.org/docs/ClangFormat.html)
(developed by Daniel Jasper). In essence, the algorithm takes the code and
calculates the best formatting that conforms to the configured style. It takes
away a lot of the drudgery of maintaining your code.

The ultimate "
facenet,"# Face Recognition using Tensorflow [![Build Status][travis-image]][travis]

[travis-image]: http://travis-ci.org/davidsandberg/facenet.svg?branch=master
[travis]: http://travis-ci.org/davidsandberg/facenet

This is a TensorFlow implementation of the face recognizer described in the paper
[""FaceNet: A Unified Embedding for Face Recognition and Clustering""](http://arxiv.org/abs/1503.03832). The project also uses ideas from the paper [""Deep Face Recognition""](http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf) from the [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/) at Oxford.

## Compatibility
The code is tested using Tensorflow r1.7 under Ubuntu 14.04 with Python 2.7 and Python 3.5. The test cases can be found [here](https://github.com/davidsandberg/facenet/tree/master/test) and the results can be found [here](http://travis-ci.org/davidsandberg/facenet).

## News
| Date     | Update |
|----------|--------|
| 2018-04-10 | Added new models trained on Casia-"
OCRmyPDF,"<!-- SPDX-FileCopyrightText: 2014 Julien Pfefferkorn -->
<!-- SPDX-FileCopyrightText: 2015 James R. Barlow -->
<!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->

<img src=""docs/images/logo.svg"" width=""240"" alt=""OCRmyPDF"">

[![Build Status](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg)](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml) [![PyPI version][pypi]](https://pypi.org/project/ocrmypdf/) ![Homebrew version][homebrew] ![ReadTheDocs][docs] ![Python versions][pyversions]

[pypi]: https://img.shields.io/pypi/v/ocrmypdf.svg ""PyPI version""
[homebrew]: https://img.shields.io/homebrew/v/ocrmypdf.svg ""Homebrew version""
[docs]: https://readthedocs.org/projects/ocrmypdf/badge/?version=latest ""RTD""
[pyversions]: https://img.shields.io/pypi/pyversions/ocrmypdf ""Supported Python versions""

OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.

```bash
ocrmypdf                      # it's a scriptable comman"
requests-html,"Requests-HTML: HTML Parsing for Humans™
=======================================

.. image:: https://farm5.staticflickr.com/4695/39152770914_a3ab8af40d_k_d.jpg

.. image:: https://travis-ci.com/psf/requests-html.svg?branch=master
    :target: https://travis-ci.com/psf/requests-html

This library intends to make parsing HTML (e.g. scraping the web) as
simple and intuitive as possible.

When using this library you automatically get:

- **Full JavaScript support**! (Using Chromium, thanks to pyppeteer)
- *CSS Selectors* (a.k.a jQuery-style, thanks to PyQuery).
- *XPath Selectors*, for the faint of heart.
- Mocked user-agent (like a real web browser).
- Automatic following of redirects.
- Connection–pooling and cookie persistence.
- The Requests experience you know and love, with magical parsing abilities.
- **Async Support**

.. Other nice features include:

    - Markdown export of pages and elements.


Tutorial & Usage
================

Make a GET request to 'python.org', using Requests:"
Llama-Chinese,"<p align=""left"">
    <a href=""README_EN.md"">English</a> ｜ 中文
</p>

<h1 align=""center"">
  Llama中文社区
</h1>
<p align=""center"" width=""100%"">
  <img src=""assets/llama.jpg"" alt=""Llama"" style=""width: 20%; display: block; margin: auto;""></a>
</p>
<p align=""center"">
  <font face=""黑体"" color=orange size=""6""> Llama3体验和微调已开放，最好的中文Llama大模型 </font>
</p>

<p align=""center"">
🤗 <a href=""https://huggingface.co/FlagAlpha"" target=""_blank"">Hugging Face</a> • 🤖 <a href=""https://www.modelscope.cn/organization/FlagAlpha/"" target=""_blank"">ModelScope</a> • ✡️ <a href=""https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat"" target=""_blank"">WiseModel</a>
</p> 

<p align=""center"">
  <a href=""https://llama.family"">Llama3.1 在线体验（包含Llama2）：https://llama.family</a>
</p>
<p align=""center"">
  <a href=""https://huggingface.co/FlagAlpha/Atom-7B-Chat"">基于Llama的开源中文预训练大模型Atom</a>
</p>

</br></br>


## 🗂️ 目录
- [📌 Llama中文社区](#-llama中文社区)
  * [🔥 社区介绍：Llama中文社区](#-社区介绍llama中文社区)
  * [📢 最新动态](#-最新动态)
  * [🤗 模型](#-模型)
    + [🤗 中文预训练模型At"
Swin-Transformer,"# Swin Transformer

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/object-detection-on-coco)](https://paperswithcode.com/sota/object-detection-on-coco?p=swin-transformer-v2-scaling-up-capacity-and)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/instance-segmentation-on-coco)](https://paperswithcode.com/sota/instance-segmentation-on-coco?p=swin-transformer-v2-scaling-up-capacity-and)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/semantic-segmentation-on-ade20k)](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=swin-transformer-v2-scaling-up-capacity-and)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/action-classification-on-kinetics-400)](https://paperswithcode."
dvc,"🚀 Check out our new product `DataChain <https://github.com/iterative/datachain>`_ (and give it a ⭐!) if you need to version and process a large number of files. Contact us at support@iterative.ai to discuss commercial solutions and support for AI reproducibility and data management scenarios.

--------------------------

`Website <https://dvc.org>`_
• `Docs <https://dvc.org/doc>`_
• `Blog <http://blog.dataversioncontrol.com>`_
• `Tutorial <https://dvc.org/doc/get-started>`_
• `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
• `How DVC works`_
• `VS Code Extension`_
• `Installation`_
• `Contributing`_
• `Community and Support`_

|CI| |Python Version| |Coverage| |VS Code| |DOI|

|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|

|

**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:

#. **Version** your data and models.
   Store them in your cloud storag"
flash-attention,"# FlashAttention
This repository provides the official implementation of FlashAttention and
FlashAttention-2 from the
following papers.

**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**  
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré  
Paper: https://arxiv.org/abs/2205.14135  
IEEE Spectrum [article](https://spectrum.ieee.org/mlperf-rankings-2022) about our submission to the MLPerf 2.0 benchmark using FlashAttention.
![FlashAttention](assets/flashattn_banner.jpg)

**FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning**  
Tri Dao

Paper: https://tridao.me/publications/flash2/flash2.pdf

![FlashAttention-2](assets/flashattention_logo.png)


## Usage

We've been very happy to see FlashAttention being widely adopted in such a short
time after its release. This [page](https://github.com/Dao-AILab/flash-attention/blob/main/usage.md)
contains a partial list of places where FlashAttention is being used.

FlashAttent"
Qwen,"<p align=""left"">
    <a href=""README_CN.md"">中文</a>&nbsp ｜ &nbspEnglish&nbsp ｜ &nbsp<a href=""README_JA.md"">日本語</a> ｜ &nbsp<a href=""README_FR.md"">Français</a> ｜ &nbsp<a href=""README_ES.md"">Español</a>
</p>
<br><br>

<p align=""center"">
    <img src=""https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg"" width=""400""/>
<p>
<br>

<p align=""center"">
        🤗 <a href=""https://huggingface.co/Qwen"">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=""https://modelscope.cn/organization/qwen"">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp 📑 <a href=""https://arxiv.org/abs/2309.16609"">Paper</a> &nbsp&nbsp ｜ &nbsp&nbsp🖥️ <a href=""https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary"">Demo</a>
<br>
<a href=""assets/wechat.png"">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp<a href=""https://discord.gg/CV4E9rpNSD"">Discord</a>&nbsp&nbsp ｜  &nbsp&nbsp<a href=""https://dashscope.aliyun.com"">API</a> 
</p>
<br><br>

> [!Important]
> Qwen2 is here! You are welcome to follow [QwenLM/Qwen2](https://github.com/QwenLM/"
speedtest-cli,"speedtest-cli
=============

Command line interface for testing internet bandwidth using
speedtest.net

.. image:: https://img.shields.io/pypi/v/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: Latest Version
.. image:: https://img.shields.io/travis/sivel/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: Travis
.. image:: https://img.shields.io/pypi/l/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: License

Versions
--------

speedtest-cli works with Python 2.4-3.7

.. image:: https://img.shields.io/pypi/pyversions/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: Versions

Installation
------------

pip / easy\_install
~~~~~~~~~~~~~~~~~~~

::

    pip install speedtest-cli

or

::

    easy_install speedtest-cli

Github
~~~~~~

::

    pip install git+https://github.com/sivel/speedtest-cli.git

or

::

    git cl"
zhao,"= 俺整理的《太子党关系网络》 =

== 简介 ==

此项目创建于2016年2月，专门用来揭露天朝的权贵（也就是传说中的“赵家人”）。

俺把这几年收集整理的数据开源到 GitHub，便于多人协作——大伙儿群策群力，一起来曝光权贵家族。

初次上传的数据包括：700多个数据文件（ '''对应700多人，130多个家族''' ），另有200多张图片（人物头像）。随着俺不断完善，数据会越来越多。

对这个项目，俺会【持续更新】。比如朝廷每次换届的时候，俺都会补充新的素材。

为了确保数据的可信度，俺主要参考“维基百科”以及一些国际权威媒体的报道（比如《纽约时报》、《华尔街日版》、《金融时报》等等）。

另外，对于某些客观事实（比如：生卒年月、简历、亲戚关系），俺也参考了天朝政府的官方网站，以及墙内的“百度百科”。


== 下载说明 ==

GitHub 提供了“下载整个项目”的功能，但是会比较大。

如果你仅仅想看《太子党关系网络》这份文档，只需在首页上方点击进入 '''download''' 这个目录。

该目录下有 '''pdf''' 和 '''jpg''' 两个子目录，分别存放对应的 '''【文件类型】''' 。你想要看哪一种文件格式，就进入哪个子目录里面。

进入【文件类型】的子目录之后，会看到一个文件列表（目前有13个文件）。先点击你想要的某个文件，会进入该文件的页面。

然后在【右上方】你会看到一个 '''Raw 按钮''' ，在这个按钮上点【右键】，在【右键菜单】里面选“保存”或“另存为”，就可以把这个文件下载到你本机。


== 多人协作说明 ==

俺非常希望有更多的网友参与该项目，大伙儿一起来完善天朝权贵家族的资料。

想要参与的同学，可以通过如下方式：

* 到[https://program-think.blogspot.com/ 俺博客]留言进行反馈，补充信息或反馈错误。

* 在[https://github.com/programthink/zhao/issues 本项目发一个 issue]，补充信息或反馈错误。

* Fork 该项目，进行修改，然后向俺发一个 Pull Request

（后面两种方式，你需要有 GitHub 的帐号）


== 数据格式说明 ==

本项目的数据文件，全部采用[https://zh.wikiped"
reinforcement-learning-an-introduction,"# Reinforcement Learning: An Introduction

[![Build Status](https://travis-ci.org/ShangtongZhang/reinforcement-learning-an-introduction.svg?branch=master)](https://travis-ci.org/ShangtongZhang/reinforcement-learning-an-introduction)

Python replication for Sutton & Barto's book [*Reinforcement Learning: An Introduction (2nd Edition)*](http://incompleteideas.net/book/the-book-2nd.html)

> If you have any confusion about the code or want to report a bug, please open an issue instead of emailing me directly, and unfortunately I do not have exercise answers for the book.

# Contents 

### Chapter 1
1. Tic-Tac-Toe

### Chapter 2
1. [Figure 2.1: An exemplary bandit problem from the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_1.png)
2. [Figure 2.2: Average performance of epsilon-greedy action-value methods on the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-intr"
nltk,"# Natural Language Toolkit (NLTK)
[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk)
![CI](https://github.com/nltk/nltk/actions/workflows/ci.yaml/badge.svg?branch=develop)

NLTK -- the Natural Language Toolkit -- is a suite of open source Python
modules, data sets, and tutorials supporting research and development in Natural
Language Processing. NLTK requires Python version 3.8, 3.9, 3.10, 3.11 or 3.12.

For documentation, please visit [nltk.org](https://www.nltk.org/).


## Contributing

Do you want to contribute to NLTK development? Great!
Please read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.

See also [how to contribute to NLTK](https://www.nltk.org/contribute.html).


## Donate

Have you found the toolkit helpful?  Please support NLTK development by donating
to the project via PayPal, using the link on the NLTK homepage.


## Citing

If you publish work that uses NLTK, please cite the NLTK book, as follows:

    Bird, Steven, Edward Lope"
detr,"**DE⫶TR**: End-to-End Object Detection with Transformers
========

[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)

PyTorch training code and pretrained models for **DETR** (**DE**tection **TR**ansformer).
We replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining **42 AP** on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.

![DETR](.github/DETR.png)

**What it is**. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. 
Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image conte"
dgl,"<p align=""center"">
  <img src=""http://data.dgl.ai/asset/logo.jpg"" height=""200"">
</p>

[![Latest Release](https://img.shields.io/github/v/release/dmlc/dgl)](https://github.com/dmlc/dgl/releases)
[![Conda Latest Release](https://anaconda.org/dglteam/dgl/badges/version.svg)](https://anaconda.org/dglteam/dgl)
[![Build Status](https://ci.dgl.ai/buildStatus/icon?job=DGL/master)](https://ci.dgl.ai/job/DGL/job/master/)
[![Benchmark by ASV](http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat)](https://asv.dgl.ai/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)
[![Twitter](https://img.shields.io/twitter/follow/DGLGraph?style=social)](https://twitter.com/GraphDeep)

[Website](https://www.dgl.ai) | [A Blitz Introduction to DGL](https://docs.dgl.ai/tutorials/blitz/index.html) | Documentation ([Latest](https://docs.dgl.ai/en/latest/) | [Stable](https://docs.dgl.ai)) | [Official Examples](examples/README.md) | [Discussion Forum](https://discuss.d"
searx,".. SPDX-License-Identifier: AGPL-3.0-or-later

Searx is no longer maintained. Thank you for your support and all your contributions.

.. figure:: https://raw.githubusercontent.com/searx/searx/master/searx/static/themes/oscar/img/logo_searx_a.png
   :target: https://searx.github.io/searx/
   :alt: searX
   :width: 100%
   :align: center

-------

|searx install|
|searx homepage|
|searx wiki|
|AGPL License|
|Issues|
|commits|
|OpenCollective searx backers|
|OpenCollective searx sponsors|

Privacy-respecting, hackable `metasearch engine`_ / *pronunciation* **sɜːks**.

.. _metasearch engine: https://en.wikipedia.org/wiki/Metasearch_engine

.. |searx install| image:: https://img.shields.io/badge/-install-blue
   :target: https://searx.github.io/searx/admin/installation.html

.. |searx homepage| image:: https://img.shields.io/badge/-homepage-blue
   :target: https://searx.github.io/searx

.. |searx wiki| image:: https://img.shields.io/badge/-wiki-blue
   :target: https://github.com/searx/sea"
PySimpleGUI,"<p align=""center"">
    <img src=""https://pysimplegui.net/images/big_news_emoji2.png"">
    <br>
    For more information visit <a href=""https://home.PySimpleGUI.com"">PySimpleGUI.com</a>
</p>


##

<p align=""center"">
    <img height=""250"" src=""https://pysimplegui.net/images/logos/Logo_Full_Transparent_Cropped.png"">
    <h2 align=""center"">User Interfaces for Humans<sup>TM</sup></h2>
</p>

# Welcome to PySimpleGUI 5 !!

Do you use PySimpleGUI 4? [Here is what you need to know.](https://docs.pysimplegui.com/en/latest/readme/sunset/)

**PySimpleGUI creates desktop applications easily**, enhancing the tkinter, Qt, WxPython, and Remi frameworks with a much simpler programming interface:

1. PySimpleGUI user interfaces are defined using core Python data types (lists and dictionaries) that are easily understood by beginners.
2. PySimpleGUI event handling changes from a complex callback-based model to a simple message passing one.
3. PySimpleGUI uses simple Python code and has no requirement for "
DB-GPT,"# DB-GPT: Revolutionizing Database Interactions with Private LLM Technology
 
<p align=""left"">
  <img src=""./assets/LOGO.png"" width=""100%"" />
</p>

<div align=""center"">
  <p>
    <a href=""https://github.com/eosphoros-ai/DB-GPT"">
        <img alt=""stars"" src=""https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social"" />
    </a>
    <a href=""https://github.com/eosphoros-ai/DB-GPT"">
        <img alt=""forks"" src=""https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social"" />
    </a>
    <a href=""https://opensource.org/licenses/MIT"">
      <img alt=""License: MIT"" src=""https://img.shields.io/badge/License-MIT-yellow.svg"" />
    </a>
     <a href=""https://github.com/eosphoros-ai/DB-GPT/releases"">
      <img alt=""Release Notes"" src=""https://img.shields.io/github/release/eosphoros-ai/DB-GPT"" />
    </a>
    <a href=""https://github.com/eosphoros-ai/DB-GPT/issues"">
      <img alt=""Open Issues"" src=""https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT"" />
    </a>
  "
ChatGLM3,"# ChatGLM3

<p align=""center"">
📄<a href=""https://arxiv.org/pdf/2406.12793"" target=""_blank""> Report </a> • 🤗 <a href=""https://huggingface.co/THUDM/chatglm3-6b"" target=""_blank"">HF Repo</a> • 🤖 <a href=""https://modelscope.cn/models/ZhipuAI/chatglm3-6b"" target=""_blank"">ModelScope</a> • 🟣 <a href=""https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b"" target=""_blank"">WiseModel</a> • 📔 <a href=""https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof"" target=""_blank"">Document</a> •  🧰 <a href=""https://openxlab.org.cn/models/hot/THUDM"" target=""_blank"">OpenXLab</a> • 🐦 <a href=""https://twitter.com/thukeg"" target=""_blank"">Twitter</a><br>
</p>
<p align=""center"">
    👋 加入我们的 <a href=""https://discord.gg/fK2dz4bg"" target=""_blank"">Discord</a> 和 <a href=""resources/WECHAT.md"" target=""_blank"">微信</a>
</p>
<p align=""center"">
📍在 <a href=""https://www.chatglm.cn"">chatglm.cn</a> 体验更大规模的 ChatGLM 模型。
</p>

[Read this in English.](./README_en.md)

📔 关于`ChatGLM3-6B` 更为详细的使用信息，可以参考

+ [ChatGLM3 开放技术文档](https:/"
SWE-agent,"<p align=""center"">
  <a href=""https://www.swe-agent.com/"">
    <img src=""assets/swe-agent-banner.png"" alt=""swe-agent.com"" />
  </a>
</p>

<p align=""center"">
  <a href=""#enigma""><img src=""https://github.com/user-attachments/assets/70ba3fdf-ee7b-474f-8fdc-de4ffde51eef"" height=""35px""></a>
</p>

<p align=""center"">
  <a href=""https://princeton-nlp.github.io/SWE-agent/""><strong>Documentation</strong></a>&nbsp; | &nbsp;
  <a href=""https://discord.gg/AVEFbBn2rH""><strong>Discord</strong></a>&nbsp; | &nbsp;
  <a href=""https://arxiv.org/abs/2405.15793""><strong>Preprint</strong></a>&nbsp; | &nbsp;
  <a href=""https://arxiv.org/abs/2409.16165""><strong>EnIGMA preprint</strong></a>
</p>

**SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can resolve issues in real GitHub repositories and more.**

On [SWE-bench][], SWE-agent resolves 12.47% of issues of the full test set and 23% of issues of SWE-bench lite.
[SWE-agent EnIGMA][enigma] solves more than **3x more** challenges of the "
impacket,"Impacket
========

[![Latest Version](https://img.shields.io/pypi/v/impacket.svg)](https://pypi.python.org/pypi/impacket/)
[![Build and test Impacket](https://github.com/fortra/impacket/actions/workflows/build_and_test.yml/badge.svg)](https://github.com/fortra/impacket/actions/workflows/build_and_test.yml)

Copyright Fortra, LLC and its affiliated companies. All rights reserved.

Impacket was originally created by [SecureAuth](https://www.secureauth.com/labs/open-source-tools/impacket), and now maintained by Fortra's Core Security.

Impacket is a collection of Python classes for working with network
protocols. Impacket is focused on providing low-level
programmatic access to the packets and for some protocols (e.g.
SMB1-3 and MSRPC) the protocol implementation itself.
Packets can be constructed from scratch, as well as parsed from 
raw data, and the object-oriented API makes it simple to work with 
deep hierarchies of protocols. The library provides a set of tools
as examples of what c"
transferlearning,"[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

<h1 align=""center"">
  <br>
  <img src=""png/logo.jpg"" alt=""Transfer Leanring"" width=""500"">
</h1>

<h4 align=""center"">Everything about Transfer Learning. 迁移学习.</h4>

<p align=""center"">
  <strong><a href=""#0papers-论文"">Papers</a></strong> •
  <strong><a href=""#1introduction-and-tutorials-简介与教程"">Tutorials</a></strong> •
  <a href=""#2transfer-learning-areas-and-papers-研究领域与相关论文"">Research areas</a> •
  <a href=""#3theory-and-survey-理论与综述"">Theory</a> •
  <a href=""#3theory-and-survey-理论与综述"">Survey</a> •
  <strong><a href=""https://github.com/jindongwang/transferlearning/tree/master/code"">Code</a></strong> •
  <strong><a href=""#7datasets-and-benchmarks-数据集与评测结果"">Dataset & benchmark</a></strong>
</p>
<p align=""center"">
  <a href=""#6transfer-learning-thesis-硕博士论文"">Thesis</a> •
  <a href=""#5transfer-learning-scholars-著名学者"">Schola"
labelme,"<h1 align=""center"">
  <img src=""labelme/icons/icon.png""><br/>labelme
</h1>

<h4 align=""center"">
  Image Polygonal Annotation with Python
</h4>

<div align=""center"">
  <a href=""https://pypi.python.org/pypi/labelme""><img src=""https://img.shields.io/pypi/v/labelme.svg""></a>
  <a href=""https://pypi.org/project/labelme""><img src=""https://img.shields.io/pypi/pyversions/labelme.svg""></a>
  <a href=""https://github.com/labelmeai/labelme/actions""><img src=""https://github.com/labelmeai/labelme/workflows/ci/badge.svg?branch=main&event=push""></a>
</div>

<div align=""center"">
  <a href=""#starter-guide""><b>Starter Guide</b></a>
  | <a href=""#installation""><b>Installation</b></a>
  | <a href=""#usage""><b>Usage</b></a>
  | <a href=""#examples""><b>Examples</b></a>
  <!-- | <a href=""https://github.com/labelmeai/labelme/discussions""><b>Community</b></a> -->
  <!-- | <a href=""https://www.youtube.com/playlist?list=PLI6LvFw0iflh3o33YYnVIfOpaO0hc5Dzw""><b>Youtube FAQ</b></a> -->
</div>

<br/>

<div align=""center"
XSStrike,"<h1 align=""center"">
  <br>
  <a href=""https://github.com/s0md3v/XSStrike""><img src=""https://image.ibb.co/cpuYoA/xsstrike-logo.png"" alt=""XSStrike""></a>
  <br>
  XSStrike
  <br>
</h1>

<h4 align=""center"">Advanced XSS Detection Suite</h4>

<p align=""center"">
  <a href=""https://github.com/s0md3v/XSStrike/releases"">
    <img src=""https://img.shields.io/github/release/s0md3v/XSStrike.svg"">
  </a>
  <a href=""https://travis-ci.com/s0md3v/XSStrike"">
    <img src=""https://img.shields.io/travis/com/s0md3v/XSStrike.svg"">
  </a>
  <a href=""https://github.com/s0md3v/XSStrike/issues?q=is%3Aissue+is%3Aclosed"">
      <img src=""https://img.shields.io/github/issues-closed-raw/s0md3v/XSStrike.svg"">
  </a>
</p>

![multi xss](https://image.ibb.co/gOCV5L/Screenshot-2018-11-19-13-33-49.png)

<p align=""center"">
  <a href=""https://github.com/s0md3v/XSStrike/wiki"">XSStrike Wiki</a> •
  <a href=""https://github.com/s0md3v/XSStrike/wiki/Usage"">Usage</a> •
  <a href=""https://github.com/s0md3v/XSStrike/wiki/FAQ"">FAQ<"
AutoEq,"# AutoEq
AutoEq is a tool for automatically equalizing headphones.

Go to **[autoeq.app](https://autoeq.app)** to get started.

This Github repository now mainly serves developers. The contributions of this project are:
* Web application for easily equalize and tweak headphone frequency responses without needing to install anything
* Library for working with (headphone) frequency responses and optimizing parametric equalizers
* [PyPi package](https://pypi.org/project/autoeq/) for installing the library on your projects
* Collection of headphone [measurements](./measurements) as numerical data from
[oratory1990](https://www.reddit.com/r/oratory1990/wiki/index/list_of_presets/),
[crinacle](https://crinacle.com),
[Innerfidelity](https://www.stereophile.com/content/innerfidelity-headphone-measurements),
[Rtings](https://www.rtings.com/headphones/1-5/graph) and legacy
headphone.com measurements (which are not the same as what the company produces today).
* Collection of different headphone "
pyright,"![Pyright](https://github.com/microsoft/pyright/blob/main/docs/img/PyrightLarge.png)

# Static Type Checker for Python

Pyright is a full-featured, standards-based static type checker for Python. It is designed for high performance and can be used with large Python source bases.

Pyright includes both a [command-line tool](https://microsoft.github.io/pyright/#/command-line) and an [extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-pyright.pyright).


## Pyright Playground

Try Pyright in your browser using the [Pyright Playground](https://pyright-play.net/?code=MQAgKgFglgziMEMC2AHANgUxAEw0g9gHYwAuATgiRnBPgO4gDG%2BSBhIGZZ%2BZcjC7AEZZcVRlWzwSlKPzRoAniEFKUCslADmEEgDoAUPtwAzEAmzYAFAA8AXCGNp8lADQgF9x85IBKW-pBAkDIMEgBXMnZrEABqd0NQAAUEGBgoQk0zKTIQdNIBRiwUkBIILBgMZkJJBDJNMKQMQhJg6jC0Ejh0rLIw5qhGjmtClBIoIgNzKwBGNwAiOZ99IA).


## Documentation

Refer to [the documentation](https://microsoft.github.io/pyright) for installation, configuration,"
memray,"<p align=""center"">
<img src=""https://raw.githubusercontent.com/bloomberg/memray/main/docs/_static/images/logo.png"" width=""70%"">
</p>

---

[![OS Linux](https://img.shields.io/badge/OS-Linux-blue)](https://pypi.org/project/memray)
[![OS MacOS](https://img.shields.io/badge/OS-macOS-blue)](https://pypi.org/project/memray)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/memray)](https://pypi.org/project/memray)
[![PyPI - Implementation](https://img.shields.io/pypi/implementation/memray)](https://pypi.org/project/memray)
[![PyPI](https://img.shields.io/pypi/v/memray)](https://pypi.org/project/memray)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/memray)](https://pypistats.org/packages/memray)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/memray.svg)](https://anaconda.org/conda-forge/memray)
[![Tests](https://github.com/bloomberg/memray/actions/workflows/build.yml/badge.svg)](https://github.com/bloomberg/memray/actions/workflows/build.yml)
[![Code Style"
explainshell,"# [explainshell.com](http://www.explainshell.com) - match command-line arguments to their help text

explainshell is a tool (with a web interface) capable of parsing man pages, extracting options and
explaining a given command-line by matching each argument to the relevant help text in the man page.

## How?

explainshell is built from the following components:

1. man page reader which converts a given man page from raw format to html (manpage.py)
2. classifier which goes through every paragraph in the man page and classifies
   it as contains options or not (algo/classifier.py)
3. an options extractor that scans classified paragraphs and looks for options (options.py)
4. a storage backend that saves processed man pages to mongodb (store.py)
5. a matcher that walks the command's AST (parsed by [bashlex](https://github.com/idank/bashlex)) and contextually matches each node
   to the relevant help text (matcher.py)

When querying explainshell, it:

1. parses the query into an AST
2. vis"
wifiphisher,"[![Build Status](https://travis-ci.org/wifiphisher/wifiphisher.svg?branch=master)](https://travis-ci.org/wifiphisher/wifiphisher)
[![Documentation Status](https://readthedocs.org/projects/wifiphisher/badge/?version=latest)](http://wifiphisher.readthedocs.io/en/latest/?badge=latest)
![Python Version](https://img.shields.io/badge/python-3.7-blue.svg)
![License](https://img.shields.io/badge/license-GPL-blue.svg)

<p align=""center""><img src=""https://wifiphisher.github.io/wifiphisher/wifiphisher.png"" /></p>

## About
<a href=""https://wifiphisher.org"">Wifiphisher</a> is a rogue Access Point framework for conducting red team engagements or Wi-Fi security testing. Using Wifiphisher, penetration testers can easily achieve a man-in-the-middle position against wireless clients by performing targeted Wi-Fi association attacks. Wifiphisher can be further used to mount victim-customized web phishing attacks against the connected clients in order to capture credentials (e.g. from third party login pa"
yfinance,"# Download market data from Yahoo! Finance's API

<table border=1 cellpadding=10><tr><td>

#### \*\*\* IMPORTANT LEGAL DISCLAIMER \*\*\*

---

**Yahoo!, Y!Finance, and Yahoo! finance are registered trademarks of
Yahoo, Inc.**

yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It's
an open-source tool that uses Yahoo's publicly available APIs, and is
intended for research and educational purposes.

**You should refer to Yahoo!'s terms of use**
([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm),
[here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and
[here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) **for
details on your rights to use the actual data downloaded. Remember - the
Yahoo! finance API is intended for personal use only.**

</td></tr></table>

---

<a target=""new"" href=""https://pypi.python.org/pypi/yfinance""><img border=0 src=""https://img.shields.io/badge/python-2.7,%203.6+-blue.svg?style=flat"" alt"
httpx,"<p align=""center"">
  <a href=""https://www.python-httpx.org/""><img width=""350"" height=""208"" src=""https://raw.githubusercontent.com/encode/httpx/master/docs/img/butterfly.png"" alt='HTTPX'></a>
</p>

<p align=""center""><strong>HTTPX</strong> <em>- A next-generation HTTP client for Python.</em></p>

<p align=""center"">
<a href=""https://github.com/encode/httpx/actions"">
    <img src=""https://github.com/encode/httpx/workflows/Test%20Suite/badge.svg"" alt=""Test Suite"">
</a>
<a href=""https://pypi.org/project/httpx/"">
    <img src=""https://badge.fury.io/py/httpx.svg"" alt=""Package version"">
</a>
</p>

HTTPX is a fully featured HTTP client library for Python 3. It includes **an integrated
command line client**, has support for both **HTTP/1.1 and HTTP/2**, and provides both **sync
and async APIs**.

---

Install HTTPX using pip:

```shell
$ pip install httpx
```

Now, let's get started:

```pycon
>>> import httpx
>>> r = httpx.get('https://www.example.org/')
>>> r
<Response [200 OK]>
>>> r.status_co"
chatgpt-mirai-qq-bot,"![cover](https://user-images.githubusercontent.com/117586514/230783378-34ddb86a-c8d3-47a6-baa5-86e39200b258.png)

------------------------------------
<p align=""center"">
  <h2 align=""center"">ChatGPT for Bot</h2>
  <p align=""center"">
    一款支持各种主流语言模型的聊天的机器人！
    <br/>
    <br/>
    <a href=""https://chatgpt-qq.lss233.com/""><strong>» 查看使用教程 »</strong></a>
    <br/>
  </p>
</p>

<p align=""center"">
  <a href=""https://github.com/lss233/chatgpt-mirai-qq-bot/stargazers""><img src=""https://img.shields.io/github/stars/lss233/chatgpt-mirai-qq-bot?color=E2CDBC&amp;logo=github&amp;style=for-the-badge"" alt=""Github stars""></a>
  <a href=""https://github.com/lss233/chatgpt-mirai-qq-bot/actions/workflows/docker-latest.yml""><img src=""https://img.shields.io/github/actions/workflow/status/lss233/chatgpt-mirai-qq-bot/docker-latest.yml?color=E2CDBC&amp;logo=docker&amp;logoColor=white&amp;style=for-the-badge"" alt=""Docker build latest""></a>
  <a href=""https://hub.docker.com/r/lss233/chatgpt-mirai-qq-bot/""><img "
authentik,"<p align=""center"">
    <img src=""https://goauthentik.io/img/icon_top_brand_colour.svg"" height=""150"" alt=""authentik logo"">
</p>

---

[![Join Discord](https://img.shields.io/discord/809154715984199690?label=Discord&style=for-the-badge)](https://goauthentik.io/discord)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&label=core%20build&style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&label=outpost%20build&style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&label=web%20build&style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml)
[![Cod"
edgedb,"<p align=""center"">
  <a href=""https://www.edgedb.com"">
    <img src=""https://www.edgedb.com/github_banner.png"">
  </a>
</p>

<div align=""center"">
  <h1>EdgeDB</h1>
  <a href=""https://github.com/edgedb/edgedb"" rel=""nofollow"">
    <img src=""https://img.shields.io/github/stars/edgedb/edgedb"" alt=""Stars"">
  </a>
  <a href=""https://github.com/edgedb/edgedb/actions"">
    <img src=""https://github.com/edgedb/edgedb/workflows/Tests/badge.svg?event=push&branch=master"" />
  </a>
  <a href=""https://github.com/edgedb/edgedb/blob/master/LICENSE"">
    <img alt=""license"" src=""https://img.shields.io/badge/license-Apache%202.0-blue"" />
  </a>
  <a href=""https://discord.gg/umUueND6ag"">
    <img alt=""discord"" src=""https://img.shields.io/discord/841451783728529451?color=5865F2&label=discord&logo=discord&logoColor=8a9095"">
  </a>
  <br />
  <br />
  <a href=""https://www.edgedb.com/docs/guides/quickstart"">Quickstart</a>
  <span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>
  <a href=""https://www.edgedb.com"">Website</a>
 "
scipy,".. image:: https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg
  :target: https://scipy.org
  :width: 110
  :height: 110
  :align: left 

.. image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
  :target: https://numfocus.org

.. image:: https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads
  :target: https://pypi.org/project/scipy/

.. image:: https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads
  :target: https://anaconda.org/conda-forge/scipy

.. image:: https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg
  :target: https://stackoverflow.com/questions/tagged/scipy

.. image:: https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue.svg
  :target: https://www.nature.com/articles/s41592-019-0686-2

SciPy (pronounced ""Sigh Pie"") is an open-source software for mathematics,
science, and engineering. It includes modules for statistics, optimizat"
EIPs,"# Ethereum Improvement Proposals (EIPs)

> **_ATTENTION_**: The EIPs repository has recently [undergone](https://github.com/ethereum/EIPs/pull/7206) a separation of ERCs and EIPs. ERCs are now accessible at [https://github.com/ethereum/ercs](https://github.com/ethereum/ercs). All new ERCs and updates to existing ones must be directed at this new repository. The editors apologize for this inconvenience.

The goal of the EIP project is to standardize and provide high-quality documentation for Ethereum itself and conventions built upon it. This repository tracks past and ongoing improvements to Ethereum in the form of Ethereum Improvement Proposals (EIPs). [EIP-1](https://eips.ethereum.org/EIPS/eip-1) governs how EIPs are published.

The [status page](https://eips.ethereum.org/) tracks and lists EIPs, which can be divided into the following categories:

- [Core EIPs](https://eips.ethereum.org/core) are improvements to the Ethereum consensus protocol.
- [Networking EIPs](https://eips.ether"
sympy,"# SymPy

[![pypi version](https://img.shields.io/pypi/v/sympy.svg)](https://pypi.python.org/pypi/sympy)
[![Join the chat at https://gitter.im/sympy/sympy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/sympy/sympy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Zenodo Badge](https://zenodo.org/badge/18918/sympy/sympy.svg)](https://zenodo.org/badge/latestdoi/18918/sympy/sympy)
[![Downloads](https://pepy.tech/badge/sympy/month)](https://pepy.tech/project/sympy)
[![GitHub Issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/sympy/sympy/issues)
[![Git Tutorial](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project)
[![Powered by NumFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)
[![Commits since last release](https://img.shields.io/github/commits-since/sympy/symp"
tushare,"TuShare


Tushare Pro版已发布，请访问新的官网了解和查询数据接口！ [https://tushare.pro](https://tushare.pro)

TuShare是实现对股票/期货等金融数据从**数据采集**、**清洗加工** 到 **数据存储**过程的工具，满足金融量化分析师和学习数据分析的人在数据获取方面的需求，它的特点是数据覆盖范围广，接口调用简单,响应快速。

![](http://tushare.org/_images/main_pic_min.png)

欢迎关注扫描TuShare的微信公众号“挖地兔”，更多资源和信息与您分享。另外，由于tushare官网在重新设计和开发，最新接口的使用文档都会在挖地兔公众号发布，所以，请扫码关注，谢谢！

![](http://tushare.org/_images/ts.jpg)

QQ交流群：

- 一群（已满）：14934432
- 二群（付费高级用户群，可获得更多支持及参与圈子活动）：658562506
- 三群（免费）：665480579
- 四群 (免费) ：527416821



Dependencies
=========
python 2.x/3.x   

[pandas](http://pandas.pydata.org/ ""pandas"")


Installation
====

- 方式1：pip install tushare
- 方式2：python setup.py install
- 方式3：访问[https://pypi.python.org/pypi/tushare/](https://pypi.python.org/pypi/tushare/)下载安装


Upgrade
=======

	pip install tushare --upgrade

Quick Start
======
**Example 1.** 获取个股历史交易数据（包括均线数据）：

    import tushare as ts

	ts.get_hist_data('600848') #一次性获取全部数据
	另外，参考get_k_data函数

结果显示：

>"
beets,".. image:: https://img.shields.io/pypi/v/beets.svg
    :target: https://pypi.python.org/pypi/beets

.. image:: https://img.shields.io/codecov/c/github/beetbox/beets.svg
    :target: https://codecov.io/github/beetbox/beets

.. image:: https://github.com/beetbox/beets/workflows/ci/badge.svg?branch=master
    :target: https://github.com/beetbox/beets/actions

.. image:: https://repology.org/badge/tiny-repos/beets.svg
    :target: https://repology.org/project/beets/versions


beets
=====

Beets is the media library management system for obsessive music geeks.

The purpose of beets is to get your music collection right once and for all.
It catalogs your collection, automatically improving its metadata as it goes.
It then provides a bouquet of tools for manipulating and accessing your music.

Here's an example of beets' brainy tag corrector doing its thing::

  $ beet import ~/music/ladytron
  Tagging:
      Ladytron - Witching Hour
  (Similarity: 98.4%)
   * Last One Standing      -> The La"
spiderfoot,"<a href=""https://www.spiderfoot.net/r.php?u=aHR0cHM6Ly93d3cuc3BpZGVyZm9vdC5uZXQv&s=os_gh""><img src=""https://www.spiderfoot.net/wp-content/themes/spiderfoot/img/spiderfoot-wide.png""></a>


[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/smicallef/spiderfoot/master/LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.7+-green)](https://www.python.org)
[![Stable Release](https://img.shields.io/badge/version-4.0-blue.svg)](https://github.com/smicallef/spiderfoot/releases/tag/v4.0)
[![CI status](https://github.com/smicallef/spiderfoot/workflows/Tests/badge.svg)](https://github.com/smicallef/spiderfoot/actions?query=workflow%3A""Tests"")
[![Last Commit](https://img.shields.io/github/last-commit/smicallef/spiderfoot)](https://github.com/smicallef/spiderfoot/commits/master)
[![Codecov](https://codecov.io/github/smicallef/spiderfoot/coverage.svg)](https://codecov.io/github/smicallef/spiderfoot)
[![Twitter Follow](https://img.shields.i"
pre-commit,"[![build status](https://github.com/pre-commit/pre-commit/actions/workflows/main.yml/badge.svg)](https://github.com/pre-commit/pre-commit/actions/workflows/main.yml)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/pre-commit/pre-commit/main.svg)](https://results.pre-commit.ci/latest/github/pre-commit/pre-commit/main)

## pre-commit

A framework for managing and maintaining multi-language pre-commit hooks.

For more information see: https://pre-commit.com/
"
httpbin,"# httpbin(1): HTTP Request & Response Service


A [Kenneth Reitz](http://kennethreitz.org/bitcoin) Project.

![ice cream](http://farm1.staticflickr.com/572/32514669683_4daf2ab7bc_k_d.jpg)

Run locally:
```sh
docker pull kennethreitz/httpbin
docker run -p 80:80 kennethreitz/httpbin
```

See http://httpbin.org for more information.

## Officially Deployed at:

- http://httpbin.org
- https://httpbin.org
- https://hub.docker.com/r/kennethreitz/httpbin/


## SEE ALSO

- http://requestb.in
- http://python-requests.org
- https://grpcb.in/

## Build Status

[![Build Status](https://travis-ci.org/requests/httpbin.svg?branch=master)](https://travis-ci.org/requests/httpbin)
"
Auto_Jobs_Applier_AIHawk,"<div align=""center"">
<img src=""./assets/AIHawk.png"">

<!-- At first glance, the branding and messaging clearly conveys what to expect -->


  <!-- [![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/AIhawkCommunity) -->
 
  [![Gmail](https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:federico.elia.majo@gmail.com)

  # Auto_Jobs_Applier_AIHawk
  ![CI](https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk/actions/workflows/ci.yml/badge.svg)

  #### 🤖🔍 Your AI-powered job search assistant. Automate applications, get personalized recommendations, and land your dream job faster.



<br />

<!-- Message Clarity -->
## 🚀 Join the AIHawk Community 🚀 

Connect with like-minded individuals and get the most out of AIHawk.

💡 **Get support:** Ask questions, troubleshoot issues, and find solutions.

🗣️ **Share knowledge:** Share your experiences, tips, and best practices.

🤝 **Networ"
PaddleHub,"English | [简体中文](README_ch.md)

<p align=""center"">
 <img src=""./docs/imgs/paddlehub_logo.jpg"" align=""middle"" width=""400"" />
<p align=""center"">
<div align=""center"">  
  <h3> <a href=#QuickStart> Quick Start </a> | <a href=""./modules""> Model List </a> | <a href=#demos> Demos </a> </h3>
</div>

------------------------------------------------------------------------------------------

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.6.2+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""""><img src=""https://img.shields.io/pypi/format/paddlehub?color=c77""></a>
    <a href=""https://pypi.org/project/paddlehub/""><img src=""https://img.shields.io/pypi/dm/paddlehub?color=9cf""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleHub/stargazers""><img src=""https://img.shields.io/github/stars/Paddle"
mamba,"# Mamba

![Mamba](assets/selection.png ""Selective State Space"")
> **Mamba: Linear-Time Sequence Modeling with Selective State Spaces**\
> Albert Gu*, Tri Dao*\
> Paper: https://arxiv.org/abs/2312.00752

![Mamba-2](assets/ssd_algorithm.png ""State Space Dual Model"")
> **Transformers are SSMs: Generalized Models and Efficient Algorithms**\
>     **Through Structured State Space Duality**\
> Tri Dao*, Albert Gu*\
> Paper: https://arxiv.org/abs/2405.21060

## About

Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of Transformers.
It is based on the line of progress on [structured state space models](https://github.com/state-spaces/s4),
with an efficient hardware-aware design and implementation in the spirit of [FlashAttention](https://github.com/Dao-AILab/flash-attention).

## Installation

- [Option] `pip install causal-conv1d>=1.4.0`: an efficient implementation o"
openage,"[![openage](/assets/logo/banner.svg)](http://openage.dev)
=========================================================

**openage**: a volunteer project to create a free engine clone of the *Genie Engine* used by *Age of Empires*, *Age of Empires II (HD)* and *Star Wars: Galactic Battlegrounds*, comparable to projects like [OpenMW](https://openmw.org/), [OpenRA](http://openra.net/),  [OpenSAGE](https://github.com/OpenSAGE/OpenSAGE/), [OpenTTD](https://openttd.org/) and [OpenRCT2](https://openrct2.org/).

openage uses the original game assets (such as sounds and graphics), but (for obvious reasons) doesn't ship them.
To play, you require *[any of the original games (AoE1, AoE2)](/doc/media_convert.md)* or their *Definitive Edition* releases.

[![github stars](https://img.shields.io/github/stars/SFTtech/openage.svg)](https://github.com/SFTtech/openage/stargazers)
[![#sfttech on matrix.org](/assets/doc/matrixroom.svg)](https://matrix.to/#/#sfttech:matrix.org)
[![GPL licensed](/assets/doc/lic"
khoj,"<p align=""center""><img src=""src/khoj/interface/web/assets/icons/khoj-logo-sideways-500.png"" width=""230"" alt=""Khoj Logo""></p>

<div align=""center"">

[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)
[![dockerize](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)
[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)
![Discord](https://img.shields.io/discord/1112065956647284756?style=plastic&label=discord)

</div>

<div align=""center"">
<b>The open-source, personal AI for your digital brain</b>
</div>

<br />

<div align=""center"">

[📑 Docs](https://docs.khoj.dev)
<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>
[🏮 App](https://khoj.dev)
<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>
[💬 Discord](https://discord.gg/BDgyabRM6e)
<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>
[📚 Blog](https://blog"
MHDDoS,"<p align=""center""><img src=""https://i.ibb.co/3F6V9JQ/MHDDoS.png"" width=""400px"" height=""150px"" alt=""ddos""></p>

<h1 align=""center"">MHDDoS - DDoS Attack Script With 56 Methods</h1>
<em><h5 align=""center"">(Programming Language - Python 3)</h5></em>

<p align=""center"">
<a href=""#""><img alt=""MH-DDoS forks"" src=""https://img.shields.io/github/forks/MatrixTM/MHDDoS?style=for-the-badge""></a>
<a href=""#""><img alt=""MH-DDoS last commit (main)"" src=""https://img.shields.io/github/last-commit/MatrixTM/MHDDoS/main?color=green&style=for-the-badge""></a>
<a href=""#""><img alt=""MH-DDoS Repo stars"" src=""https://img.shields.io/github/stars/MatrixTM/MHDDoS?style=for-the-badge&color=yellow""></a>
<a href=""#""><img alt=""MH-DDoS License"" src=""https://img.shields.io/github/license/MatrixTM/MHDDoS?color=orange&style=for-the-badge""></a>
<a href=""https://github.com/MatrixTM/MHDDoS/issues""><img alt=""MatrixTM issues"" src=""https://img.shields.io/github/issues/MatrixTM/MHDDoS?color=purple&style=for-the-badge""></a>
  
<p a"
pandas-ai,"# ![PandasAI](assets/logo.png)

[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&style=flat-square)](https://pypi.org/project/pandasai/)
[![CI](https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg)](https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg)
[![CD](https://github.com/gventuri/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/gventuri/pandas-ai/actions/workflows/cd.yml/badge.svg)
[![Coverage](https://codecov.io/gh/gventuri/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/gventuri/pandas-ai)
[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&compact=true)](https://discord.gg/kF7FqH2FwS)
[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab."
PaddleDetection,"简体中文 | [English](README_en.md)

<div align=""center"">
<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/48054808/160532560-34cf7a1f-d950-435e-90d2-4b0a679e5119.png"" align=""middle"" width = ""800"" />
</p>

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleDetection/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleDetection?color=ffa""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleDetection/stargazers""><img src=""https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection?color=ccf""></a>
</p>
</div>

## 💌目录
- [💌目录](#目录)
- [🌈简介](#简介)
- [📣最新进展](#最新进展)
- [👫开源社区](#开源社区)
- [✨主要特性](#主要特性)
    - [🧩模块化设计](#模块化设计)
    - [📱丰富的模型库](#丰富的模型库)
    - [🎗️"
searxng,".. SPDX-License-Identifier: AGPL-3.0-or-later

----

.. figure:: https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng.svg
   :target: https://docs.searxng.org/
   :alt: SearXNG
   :width: 100%
   :align: center

----

Privacy-respecting, hackable `metasearch engine`_

Searx.space_ lists ready-to-use running instances.

A user_, admin_ and developer_ handbook is available on the homepage_.

|SearXNG install|
|SearXNG homepage|
|SearXNG wiki|
|AGPL License|
|Issues|
|commits|
|weblate|
|SearXNG logo|

----

.. _searx.space: https://searx.space
.. _user: https://docs.searxng.org/user
.. _admin: https://docs.searxng.org/admin
.. _developer: https://docs.searxng.org/dev
.. _homepage: https://docs.searxng.org/
.. _metasearch engine: https://en.wikipedia.org/wiki/Metasearch_engine

.. |SearXNG logo| image:: https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng-wordmark.svg
   :target: https://docs.searxng.org/
   :width: 5%

.. |SearXNG install| imag"
redis-py,"# redis-py

The Python interface to the Redis key-value store.

[![CI](https://github.com/redis/redis-py/workflows/CI/badge.svg?branch=master)](https://github.com/redis/redis-py/actions?query=workflow%3ACI+branch%3Amaster)
[![docs](https://readthedocs.org/projects/redis/badge/?version=stable&style=flat)](https://redis-py.readthedocs.io/en/stable/)
[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)
[![pypi](https://badge.fury.io/py/redis.svg)](https://pypi.org/project/redis/)
[![pre-release](https://img.shields.io/github/v/release/redis/redis-py?include_prereleases&label=latest-prerelease)](https://github.com/redis/redis-py/releases)
[![codecov](https://codecov.io/gh/redis/redis-py/branch/master/graph/badge.svg?token=yenl5fzxxr)](https://codecov.io/gh/redis/redis-py)

[Installation](#installation) |  [Usage](#usage) | [Advanced Topics](#advanced-topics) | [Contributing](https://github.com/redis/redis-py/blob/master/CONTRIBUTING.md)

-------------------------"
pygwalker,"[English](README.md) | [Español](./docs/README.es.md) | [Français](./docs/README.fr.md) | [Deutsch](./docs/README.de.md) | [中文](./docs/README.zh.md) | [Türkçe](./docs/README.tr.md) | [日本語](./docs/README.ja.md) | [한국어](./docs/README.ko.md)

<p align=""center""><a href=""https://github.com/Kanaries/pygwalker""><img width=100% alt="""" src=""https://github.com/Kanaries/pygwalker/assets/22167673/bed8b3db-fda8-43e7-8ad2-71f6afb9dddd"" /></a></p>

<h2 align=""center"">PyGWalker: A Python Library for Exploratory Data Analysis with Visualization</h2>

<p align=""center"">
    <a href=""https://arxiv.org/abs/2406.11637"">
      <img src=""https://img.shields.io/badge/arXiv-2406.11637-b31b1b.svg"" height=""18"" align=""center"">
    </a>
    <a href=""https://badge.fury.io/py/pygwalker"">
        <img src=""https://badge.fury.io/py/pygwalker.svg"" alt=""PyPI version"" height=""18"" align=""center"" />
    </a>
    <a href=""https://mybinder.org/v2/gh/Kanaries/pygwalker/main"">
      <img src=""https://mybinder.org/badge_logo.sv"
fish-speech,"# Fish Speech

<div align=""center"">

**English** | [简体中文](README.zh.md) | [Portuguese](README.pt-BR.md) | [日本語](README.ja.md)

</div>

<div>
<a href=""https://www.producthunt.com/posts/fish-speech-1-4?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-fish&#0045;speech&#0045;1&#0045;4"" target=""_blank""><img src=""https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&theme=light"" alt=""Fish&#0032;Speech&#0032;1&#0046;4 - Open&#0045;Source&#0032;Multilingual&#0032;Text&#0045;to&#0045;Speech&#0032;with&#0032;Voice&#0032;Cloning | Product Hunt"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a>

<a href=""https://trendshift.io/repositories/7014"" target=""_blank"">
<img src=""https://trendshift.io/api/badge/repositories/7014"" alt=""fishaudio%2Ffish-speech | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/>
</a>
</div>

<div>
<a target=""_blank"" href=""https://discord.gg/Es5qTB9BcN"">
<img alt=""Discord"" src=""https://img."
pelican,"Pelican |build-status| |pypi-version| |downloads| |repology|
============================================================

Pelican is a static site generator, written in Python_, that allows you to create
web sites by composing text files in formats such as Markdown, reStructuredText, and HTML.

With Pelican, you can create web sites without worrying about databases or server-side programming.
Pelican generates static sites that can be served via any web server or hosting service.

You can perform the following functions with Pelican:

* Compose content in Markdown_ or reStructuredText_ using your editor of choice
* Simple command-line tool (re)generates HTML, CSS, and JS from your source content
* Easy to interface with version control systems and web hooks
* Completely static output is simple to host anywhere


Features
--------

Pelican’s feature highlights include:

* Chronological content (e.g., articles, blog posts) as well as static pages
* Integration with external services
* S"
OpenCore-Legacy-Patcher,"<div align=""center"">
             <img src=""docs/images/OC-Patcher.png"" alt=""OpenCore Patcher Logo"" width=""256"" />
             <h1>OpenCore Legacy Patcher</h1>
</div>

A Python-based project revolving around [Acidanthera's OpenCorePkg](https://github.com/acidanthera/OpenCorePkg) and [Lilu](https://github.com/acidanthera/Lilu) for both running and unlocking features in macOS on supported and unsupported Macs.

Our project's main goal is to breathe new life into Macs no longer supported by Apple, allowing for the installation and usage of macOS Big Sur and newer on machines as old as 2007.

----------

![GitHub all releases](https://img.shields.io/github/downloads/dortania/OpenCore-Legacy-Patcher/total?color=white&style=plastic) ![GitHub top language](https://img.shields.io/github/languages/top/dortania/OpenCore-Legacy-Patcher?color=4B8BBE&style=plastic) ![Discord](https://img.shields.io/discord/417165963327176704?color=7289da&label=discord&style=plastic)

----------

Noteworthy feature"
litellm,"<h1 align=""center"">
        🚅 LiteLLM
    </h1>
    <p align=""center"">
        <p align=""center"">
        <a href=""https://render.com/deploy?repo=https://github.com/BerriAI/litellm"" target=""_blank"" rel=""nofollow""><img src=""https://render.com/images/deploy-to-render-button.svg"" alt=""Deploy to Render""></a>
        <a href=""https://railway.app/template/HLP0Ub?referralCode=jch2ME"">
          <img src=""https://railway.app/button.svg"" alt=""Deploy on Railway"">
        </a>
        </p>
        <p align=""center"">Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        <br>
    </p>
<h4 align=""center""><a href=""https://docs.litellm.ai/docs/simple_proxy"" target=""_blank"">LiteLLM Proxy Server (LLM Gateway)</a> | <a href=""https://docs.litellm.ai/docs/hosted"" target=""_blank""> Hosted Proxy (Preview)</a> | <a href=""https://docs.litellm.ai/docs/enterprise""target=""_blank"">Enterprise Tier</a></h4>
<h4 align=""center"">
    <a href=""https://pypi"
RWKV-LM,"# RWKV: Parallelizable RNN with Transformer-level LM Performance (pronounced as ""RwaKuv"" (rʌkuv in IPA), from 4 major params: R W K V)

RWKV homepage: https://www.rwkv.com

**RWKV-5/6 Eagle/Finch paper**: https://arxiv.org/abs/2404.05892

**Awesome RWKV in Vision:** https://github.com/Yaziwel/Awesome-RWKV-in-Vision

RWKV-6 3B Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1

RWKV-6 7B Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2

**RWKV-6 GPT-mode demo code (with comments and explanations)**: https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/rwkv_v6_demo.py

RWKV-6 RNN-mode demo: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v6_demo.py

![MQAR](Research/RWKV-6-MQAR.png)

### HOW TO TEST TRAINING RWKV-5 on MiniPile (1.5G tokens) ###

For reference, use python 3.10, torch 2.3.1+cu121 (or latest), cuda 12.5+, **latest deepspeed**, but **keep pytorch-lightning==1.9.5**

```
pip install torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu121
pi"
dask,"Dask
====

|Build Status| |Coverage| |Doc Status| |Discourse| |Version Status| |NumFOCUS|

Dask is a flexible parallel computing library for analytics.  See
documentation_ for more information.


LICENSE
-------

New BSD. See `License File <https://github.com/dask/dask/blob/main/LICENSE.txt>`__.

.. _documentation: https://dask.org
.. |Build Status| image:: https://github.com/dask/dask/actions/workflows/tests.yml/badge.svg
   :target: https://github.com/dask/dask/actions/workflows/tests.yml
.. |Coverage| image:: https://codecov.io/gh/dask/dask/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/dask/dask/branch/main
   :alt: Coverage status
.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest
   :target: https://dask.org
   :alt: Documentation Status
.. |Discourse| image:: https://img.shields.io/discourse/users?logo=discourse&server=https%3A%2F%2Fdask.discourse.group
   :alt: Discuss Dask-related things and ask for help
   :target: https://dask."
awesome-aws,"<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png"">
</p>
<br/>

# Awesome AWS [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.

Inspired by the [awesome](https://github.com/sindresorhus/awesome) list.

## The Fiery Meter of AWSome

* Repo with 0100+ Stars: :fire:
* Repo with 0200+ Stars: :fire::fire:
* Repo with 0500+ Stars: :fire::fire::fire:
* Repo with 1000+ Stars: :fire::fire::fire::fire:
* Repo with 2000+ Stars: :fire::fire::fire::fire::fire:

Repos not on `The Fiery Meter of AWSome` can still be awesome, see [A Note on Repo AWSomeness](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md#a-note-on-repo-awsomeness).

### `awesome-aws` Python Module

[![Build Status](https://trav"
seaborn,"<img src=""https://raw.githubusercontent.com/mwaskom/seaborn/master/doc/_static/logo-wide-lightbg.svg""><br>

--------------------------------------

seaborn: statistical data visualization
=======================================

[![PyPI Version](https://img.shields.io/pypi/v/seaborn.svg)](https://pypi.org/project/seaborn/)
[![License](https://img.shields.io/pypi/l/seaborn.svg)](https://github.com/mwaskom/seaborn/blob/master/LICENSE.md)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.03021/status.svg)](https://doi.org/10.21105/joss.03021)
[![Tests](https://github.com/mwaskom/seaborn/workflows/CI/badge.svg)](https://github.com/mwaskom/seaborn/actions)
[![Code Coverage](https://codecov.io/gh/mwaskom/seaborn/branch/master/graph/badge.svg)](https://codecov.io/gh/mwaskom/seaborn)

Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.


Documentation
-------------

Online documentation is available at"
ydata-profiling,"# ydata-profiling

[![Build Status](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml)
[![PyPI download month](https://img.shields.io/pypi/dm/ydata-profiling.svg)](https://pypi.python.org/pypi/ydata-profiling/)
[![](https://pepy.tech/badge/pandas-profiling)](https://pypi.org/project/ydata-profiling/)
[![Code Coverage](https://codecov.io/gh/ydataai/pandas-profiling/branch/master/graph/badge.svg?token=gMptB4YUnF)](https://codecov.io/gh/ydataai/pandas-profiling)
[![Release Version](https://img.shields.io/github/release/ydataai/pandas-profiling.svg)](https://github.com/ydataai/pandas-profiling/releases)
[![Python Version](https://img.shields.io/pypi/pyversions/ydata-profiling)](https://pypi.org/project/ydata-profiling/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)
<img referrerpolicy=""no-referrer-when-"
moviepy,"MoviePy
=======

.. image:: https://badge.fury.io/py/moviepy.svg
    :target: PyPI_
    :alt: MoviePy page on the Python Package Index
.. image:: https://img.shields.io/gitter/room/movie-py/gitter?color=46BC99&logo=gitter
    :target: Gitter_
    :alt: Discuss MoviePy on Gitter
.. image:: https://img.shields.io/github/actions/workflow/status/Zulko/moviepy/test_suite.yml?logo=github
    :target: https://github.com/Zulko/moviepy/actions/workflows/test_suite.yml
    :alt: Build status on gh-actions
.. image:: https://img.shields.io/coveralls/github/Zulko/moviepy/master?logo=coveralls
    :target: https://coveralls.io/github/Zulko/moviepy?branch=master
    :alt: Code coverage from coveralls.io

MoviePy (full documentation_) is a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects. See the gallery_ for some examples of use.

MoviePy can read and write all the most common "
clip-as-service,"<p align=""center"">
<a href=""https://clip-as-service.jina.ai""><img src=""https://github.com/jina-ai/clip-as-service/blob/main/docs/_static/logo-light.svg?raw=true"" alt=""CLIP-as-service logo: The data structure for unstructured data"" width=""200px""></a>
<br><br><br>
</p>


<p align=center>
<a href=""https://pypi.org/project/clip_server/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/clip_server?label=Release&style=flat-square""></a>
<a href=""https://discord.jina.ai""><img src=""https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square""></a>
<a href=""https://codecov.io/gh/jina-ai/clip-as-service""><img alt=""Codecov branch"" src=""https://img.shields.io/codecov/c/github/jina-ai/clip-as-service/main?logo=Codecov&logoColor=white&style=flat-square""></a>
<a href=""https://colab.research.google.com/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb""><img src=""https://img.shields.io/badge/Host-on%20Google%20Colab%20(GPU/TPU)-brightgreen?st"
kotaemon,"# kotaemon

An open-source clean & customizable RAG UI for chatting with your documents. Built with both end users and
developers in mind.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)

[Live Demo](https://huggingface.co/spaces/cin-model/kotaemon-demo) |
[Source Code](https://github.com/Cinnamon/kotaemon)

[User Guide](https://cinnamon.github.io/kotaemon/) |
[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |
[Feedback](https://github.com/Cinnamon/kotaemon/issues)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
<a href=""https://github.com/Cinnamon/kotaemon"" target=""_blank"">
<img src=""https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen"" alt=""docker pull ghcr.io/cinnamon/kotaemon:latest""></a>
![download](https:/"
alphafold,"![header](imgs/header.jpg)

# AlphaFold

This package provides an implementation of the inference pipeline of AlphaFold
v2. For simplicity, we refer to this model as AlphaFold throughout the rest of
this document.

We also provide:

1.  An implementation of AlphaFold-Multimer. This represents a work in progress
    and AlphaFold-Multimer isn't expected to be as stable as our monomer
    AlphaFold system. [Read the guide](#updating-existing-installation) for how
    to upgrade and update code.
2.  The [technical note](docs/technical_note_v2.3.0.md) containing the models
    and inference procedure for an updated AlphaFold v2.3.0.
3.  A [CASP15 baseline](docs/casp15_predictions.zip) set of predictions along
    with documentation of any manual interventions performed.

Any publication that discloses findings arising from using this source code or
the model parameters should [cite](#citing-this-work) the
[AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2) and, if
applicable, the"
Pillow,"<p align=""center"">
    <img width=""248"" height=""250"" src=""https://raw.githubusercontent.com/python-pillow/pillow-logo/main/pillow-logo-248x250.png"" alt=""Pillow logo"">
</p>

# Pillow

## Python Imaging Library (Fork)

Pillow is the friendly PIL fork by [Jeffrey A. Clark and
contributors](https://github.com/python-pillow/Pillow/graphs/contributors).
PIL is the Python Imaging Library by Fredrik Lundh and contributors.
As of 2019, Pillow development is
[supported by Tidelift](https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=readme&utm_campaign=enterprise).

<table>
    <tr>
        <th>docs</th>
        <td>
            <a href=""https://pillow.readthedocs.io/?badge=latest""><img
                alt=""Documentation Status""
                src=""https://readthedocs.org/projects/pillow/badge/?version=latest""></a>
        </td>
    </tr>
    <tr>
        <th>tests</th>
        <td>
            <a href=""https://github.com/python-pillow/Pillow/actions/workflows/li"
routersploit,"# RouterSploit - Exploitation Framework for Embedded Devices

[![Python 3.6](https://img.shields.io/badge/Python-3.6-yellow.svg)](http://www.python.org/download/)
[![Build Status](https://travis-ci.org/threat9/routersploit.svg?branch=master)](https://travis-ci.org/threat9/routersploit)

The RouterSploit Framework is an open-source exploitation framework dedicated to embedded devices.

[![asciicast](https://asciinema.org/a/180370.png)](https://asciinema.org/a/180370)

It consists of various modules that aid penetration testing operations:

* exploits - modules that take advantage of identified vulnerabilities
* creds - modules designed to test credentials against network services
* scanners - modules that check if a target is vulnerable to any exploit
* payloads - modules that are responsible for generating payloads for various architectures and injection points
* generic - modules that perform generic attacks 

# Installation

## Requirements

Required:
* future
* requests
* paramiko
*"
pytube,"<div align=""center"">
  <p>
    <a href=""#""><img src=""https://assets.nickficano.com/gh-pytube.min.svg"" width=""456"" height=""143"" alt=""pytube logo"" /></a>
  </p>
  <p align=""center"">
	<a href=""https://pypi.org/project/pytube/""><img src=""https://img.shields.io/pypi/dm/pytube?style=flat-square"" alt=""pypi""/></a>
	<a href=""https://pytube.io/en/latest/""><img src=""https://readthedocs.org/projects/python-pytube/badge/?version=latest&style=flat-square"" /></a>
	<a href=""https://pypi.org/project/pytube/""><img src=""https://img.shields.io/pypi/v/pytube?style=flat-square"" /></a>
  </p>
</div>

### Actively soliciting contributors!

Have ideas for how pytube can be improved? Feel free to open an issue or a pull request!

# pytube

*pytube* is a genuine, lightweight, dependency-free Python library (and command-line utility) for downloading YouTube videos.

## Documentation

Detailed documentation about the usage of the library can be found at [pytube.io](https://pytube.io). This is recommended for most "
LaTeX-OCR,"# pix2tex - LaTeX OCR

[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces"
buzz,"# Buzz

[Documentation](https://chidiwilliams.github.io/buzz/) | [Buzz Captions on the App Store](https://apps.apple.com/us/app/buzz-captions/id6446018936?mt=12&itsct=apps_box_badge&itscg=30200)

Transcribe and translate audio offline on your personal computer. Powered by
OpenAI's [Whisper](https://github.com/openai/whisper).

![MIT License](https://img.shields.io/badge/license-MIT-green)
[![CI](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg)](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml)
[![codecov](https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP)](https://codecov.io/github/chidiwilliams/buzz)
![GitHub release (latest by date)](https://img.shields.io/github/v/release/chidiwilliams/buzz)
[![Github all releases](https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg)](https://GitHub.com/chidiwilliams/buzz/releases/)

<blockquote>
<p>Buzz is better on the App Store. Get a Mac-native versio"
MiniCPM-V,"<div align=""center"">

<img src=""./assets/minicpmv.png"" width=""300em"" ></img> 

**A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone**

  <strong>[中文](./README_zh.md) |
  English</strong>

Join our <a href=""docs/wechat.md"" target=""_blank""> 💬 WeChat</a> | View  MiniCPM-V <a href=""docs/best_practice_summary.md"" target=""_blank""> 📖 best practices</a>


<p align=""center"">
  MiniCPM-V 2.6 <a href=""https://huggingface.co/openbmb/MiniCPM-V-2_6"">🤗</a> <a href=""https://huggingface.co/spaces/openbmb/MiniCPM-V-2_6"">🤖</a> | MiniCPM-Llama3-V 2.5  <a href=""https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/"">🤗</a> <a href=""https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"">🤖</a> |
  <a href=https://arxiv.org/abs/2408.01800>MiniCPM-Llama3-V 2.5 Technical Report</a> 
</p>

</div>


**MiniCPM-V** is a series of end-side multimodal LLMs (MLLMs) designed for vision-language understanding. The models take image, video and text as inputs and provide high-quality text outputs."
taipy,"
![Hactoberfestnew](https://github.com/user-attachments/assets/149a5cee-6af1-4d4e-9c43-6fcf82a9b07e)


<div align=""center"">
  <a href=""https://taipy.io?utm_source=github"" target=""_blank"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/Avaiga/taipy/assets/100117126/509bf101-54c2-4321-adaf-a2af63af9682"">
    <img alt=""Taipy"" src=""https://github.com/Avaiga/taipy/assets/100117126/4df8a733-d8d0-4893-acf0-d24ef9e8b58a"" width=""300"" />
  </picture>
  </a>
</div>

<h1 align=""center"">
Build Python Data & AI web applications
</h1>

<div align=""center"">
From simple pilots to production-ready web applications in no time. <br />
No more compromise on performance, customization, and scalability.
</div>

<br />

<div align=""center"">

**Go beyond existing libraries**

</div>

<p align=""left"">
    <br />
    <a href=""https://docs.taipy.io/en/latest/""><strong>📚 Explore the docs </strong></a>
    <br />
    <a href=""https://discord.com/invite/SJyz2VJGxV""><strong>  "
pyodide,"<div align=""center"">
  <a href=""https://github.com/pyodide/pyodide"">
  <img src=""./docs/_static/img/pyodide-logo-readme.png"" alt=""Pyodide"">
  </a>
</div>

[![NPM Latest Release](https://img.shields.io/npm/v/pyodide)](https://www.npmjs.com/package/pyodide)
[![PyPI Latest Release](https://img.shields.io/pypi/v/pyodide-py.svg)](https://pypi.org/project/pyodide-py/)
[![Build Status](https://circleci.com/gh/pyodide/pyodide.png)](https://circleci.com/gh/pyodide/pyodide)
[![Documentation Status](https://readthedocs.org/projects/pyodide/badge/?version=stable)](https://pyodide.readthedocs.io/?badge=stable)

Pyodide is a Python distribution for the browser and Node.js based on WebAssembly.

## What is Pyodide?

Pyodide is a port of CPython to WebAssembly/[Emscripten](https://emscripten.org/).

Pyodide makes it possible to install and run Python packages in the browser with
[micropip](https://micropip.pyodide.org/). Any pure
Python package with a wheel available on PyPi is supported. Many package"
numpy-100,"## 100 numpy exercises

[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/rougier/numpy-100/notebooks/100%20Numpy%20exercises.ipynb)

This is a collection of numpy exercises from numpy mailing list, stack overflow, and numpy documentation. I've also created some problems myself to reach the 100 limit. The goal of this collection is to offer a quick reference for both old and new users but also to provide a set of exercises for those who teach. For extended exercises, make sure to read [From Python to NumPy](http://www.labri.fr/perso/nrougier/from-python-to-numpy/).

→ [Test them on Binder](http://mybinder.org:/repo/rougier/numpy-100/notebooks/100_Numpy_exercises.ipynb)  
→ [Read them on GitHub](100_Numpy_exercises.md)  

Note: markdown and ipython notebook are created programmatically from the source data in `source/exercises.ktx`.
To modify the content of these files, please change the text in the source and run the `generators.py` module with a python
interpreter w"
pgcli,"We stand with Ukraine
---------------------

Ukrainian people are fighting for their country. A lot of civilians, women and children, are suffering. Hundreds were killed and injured, and thousands were displaced.

This is an image from my home town, Kharkiv. This place is right in the old city center.

.. image:: screenshots/kharkiv-destroyed.jpg

Picture by @fomenko_ph (Telegram).

Please consider donating or volunteering.

* https://bank.gov.ua/en/
* https://savelife.in.ua/en/donate/
* https://www.comebackalive.in.ua/donate
* https://www.globalgiving.org/projects/ukraine-crisis-relief-fund/
* https://www.savethechildren.org/us/where-we-work/ukraine
* https://www.facebook.com/donate/1137971146948461/
* https://donate.wck.org/give/393234#!/donation/checkout
* https://atlantaforukraine.com/


A REPL for Postgres
-------------------

|Build Status| |CodeCov| |PyPI| |netlify|

This is a postgres client that does auto-completion and syntax highlighting.

Home Page: http://pgcli.com

MySQL "
cookiecutter-django,"# Cookiecutter Django

[![Build Status](https://img.shields.io/github/actions/workflow/status/cookiecutter/cookiecutter-django/ci.yml?branch=master)](https://github.com/cookiecutter/cookiecutter-django/actions/workflows/ci.yml?query=branch%3Amaster)
[![Documentation Status](https://readthedocs.org/projects/cookiecutter-django/badge/?version=latest)](https://cookiecutter-django.readthedocs.io/en/latest/?badge=latest)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/cookiecutter/cookiecutter-django/master.svg)](https://results.pre-commit.ci/latest/github/cookiecutter/cookiecutter-django/master)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

[![Updates](https://pyup.io/repos/github/cookiecutter/cookiecutter-django/shield.svg)](https://pyup.io/repos/github/cookiecutter/cookiecutter-django/)
[![Join our Discord](https://img.shields.io/badge/Discord-cookiecutter-5865F2?style=flat&logo=discord&logoColor=whi"
PaddleNLP,"**简体中文**🀄 | [English🌎](./README_en.md)

<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/1371212/175816733-8ec25eb0-9af3-4380-9218-27c154518258.png"" align=""middle""  width=""500"" />
</p>

------------------------------------------------------------------------------------------

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleNLP/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleNLP?color=ffa""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleNLP/graphs/contributors""><img src=""https://img.shields.io/github/contributors/PaddlePaddle/PaddleNLP?color=9ea""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleNLP/commits""><img src=""https://img"
LivePortrait,"<h1 align=""center"">LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</h1>

<div align='center'>
    <a href='https://github.com/cleardusk' target='_blank'><strong>Jianzhu Guo</strong></a><sup> 1†</sup>&emsp;
    <a href='https://github.com/Mystery099' target='_blank'><strong>Dingyun Zhang</strong></a><sup> 1,2</sup>&emsp;
    <a href='https://github.com/KwaiVGI' target='_blank'><strong>Xiaoqiang Liu</strong></a><sup> 1</sup>&emsp;
    <a href='https://github.com/zzzweakman' target='_blank'><strong>Zhizhou Zhong</strong></a><sup> 1,3</sup>&emsp;
    <a href='https://scholar.google.com.hk/citations?user=_8k1ubAAAAAJ' target='_blank'><strong>Yuan Zhang</strong></a><sup> 1</sup>&emsp;
</div>

<div align='center'>
    <a href='https://scholar.google.com/citations?user=P6MraaYAAAAJ' target='_blank'><strong>Pengfei Wan</strong></a><sup> 1</sup>&emsp;
    <a href='https://openreview.net/profile?id=~Di_ZHANG3' target='_blank'><strong>Di Zhang</strong></a><sup> 1<"
pwntools,"# pwntools - CTF toolkit
![pwntools logo](https://github.com/Gallopsled/pwntools/blob/stable/docs/source/logo.png?raw=true)

[![PyPI](https://img.shields.io/pypi/v/pwntools?style=flat)](https://pypi.python.org/pypi/pwntools/)
[![Docs](https://readthedocs.org/projects/pwntools/badge/?version=stable)](https://docs.pwntools.com/)
[![GitHub Workflow Status (dev)](https://img.shields.io/github/actions/workflow/status/Gallopsled/pwntools/ci.yml?branch=dev&logo=GitHub)](https://github.com/Gallopsled/pwntools/actions/workflows/ci.yml?query=branch%3Adev)
[![Coveralls](https://img.shields.io/coveralls/github/Gallopsled/pwntools/dev?logo=coveralls)](https://coveralls.io/github/Gallopsled/pwntools?branch=dev)
[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)
[![Packaging status](https://img.shields.io/repology/repositories/python:pwntools)](https://repology.org/project/python:pwntools/versions)
[![Discord](https://img.shields.io"
pix2code,"# pix2code
*Generating Code from a Graphical User Interface Screenshot*

[![License](http://img.shields.io/badge/license-APACHE2-blue.svg)](LICENSE.txt)

* A video demo of the system can be seen [here](https://youtu.be/pqKeXkhFA3I)
* The paper is available at [https://arxiv.org/abs/1705.07962](https://arxiv.org/abs/1705.07962)
* Official research page: [https://uizard.io/research#pix2code](https://uizard.io/research#pix2code)

## Abstract
Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).

## Citation

```
@article{beltramelli2017pix2code,
  title={pix2code: Generating Code from "
MOSS,"# MOSS
<p align=""center"" width=""100%"">
<a href=""https://txsun1997.github.io/blogs/moss.html"" target=""_blank""><img src=""https://txsun1997.github.io/images/moss.png"" alt=""MOSS"" style=""width: 50%; min-width: 300px; display: block; margin: auto;""></a>
</p>

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg)](https://github.com/OpenLMLab/MOSS/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20BY--NC%204.0-blue.svg)](https://github.com/OpenLMLab/MOSS/blob/main/DATA_LICENSE)
[![Model License](https://img.shields.io/badge/Model%20License-GNU%20AGPL%203.0-red.svg)](https://github.com/OpenLMLab/MOSS/blob/main/MODEL_LICENSE)

[[论文](https://link.springer.com/article/10.1007/s11633-024-1502-8)][[中文版](https://github.com/OpenLMLab/MOSS/blob/main/README.md)] [[English](https://github.com/OpenLMLab/MOSS/blob/main/README_en.md)] [[官方微信群](https://github.com/OpenLMLab/MOSS/blob/main/examples/WeChatGroupQR.jpg)]

## 目录

- [开源清单](#spira"
pytest,".. image:: https://github.com/pytest-dev/pytest/raw/main/doc/en/img/pytest_logo_curves.svg
   :target: https://docs.pytest.org/en/stable/
   :align: center
   :height: 200
   :alt: pytest


------

.. image:: https://img.shields.io/pypi/v/pytest.svg
    :target: https://pypi.org/project/pytest/

.. image:: https://img.shields.io/conda/vn/conda-forge/pytest.svg
    :target: https://anaconda.org/conda-forge/pytest

.. image:: https://img.shields.io/pypi/pyversions/pytest.svg
    :target: https://pypi.org/project/pytest/

.. image:: https://codecov.io/gh/pytest-dev/pytest/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/pytest-dev/pytest
    :alt: Code coverage Status

.. image:: https://github.com/pytest-dev/pytest/actions/workflows/test.yml/badge.svg
    :target: https://github.com/pytest-dev/pytest/actions?query=workflow%3Atest

.. image:: https://results.pre-commit.ci/badge/github/pytest-dev/pytest/main.svg
   :target: https://results.pre-commit.ci/latest/github/pytest-d"
deepface,"# deepface

<div align=""center"">

[![Downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&units=international_system&left_color=grey&right_color=blue&left_text=downloads)](https://pepy.tech/project/deepface)
[![Stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&style=flat&label=%E2%AD%90%20stars)](https://github.com/serengil/deepface/stargazers)
[![License](http://img.shields.io/:license-MIT-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/LICENSE)
[![Tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)
[![DOI](http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat)](https://doi.org/10.17671/gazibtd.1399077)

[![Blog](https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&logo=wordpress)](https://sefiks.com)
[![YouTube](https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&logo=youtube)](https:/"
dirsearch,"<img src=""static/logo.png#gh-light-mode-only"" alt=""dirsearch logo (light)"" width=""675px"">
<img src=""static/logo-dark.png#gh-dark-mode-only"" alt=""dirsearch logo (dark)"" width=""675px"">

dirsearch - Web path discovery
=========

![Build](https://img.shields.io/badge/Built%20with-Python-Blue)
![License](https://img.shields.io/badge/license-GNU_General_Public_License-_red.svg)
![Stars](https://img.shields.io/github/stars/maurosoria/dirsearch.svg)
[![Release](https://img.shields.io/github/release/maurosoria/dirsearch.svg)](https://github.com/maurosoria/dirsearch/releases)
[![Sponsors](https://img.shields.io/github/sponsors/maurosoria)](https://github.com/sponsors/maurosoria)
[![Discord](https://img.shields.io/discord/992276296669339678.svg?logo=discord)](https://discord.gg/2N22ZdAJRj)
[![Twitter](https://img.shields.io/twitter/follow/_dirsearch?label=Follow)](https://twitter.com/_dirsearch)


> An advanced web path brute-forcer

**dirsearch** is being actively developed by [@maurosoria](http"
activitywatch,"<img title=""ActivityWatch"" src=""https://activitywatch.net/img/banner.png"" align=""center"">

<p align=""center"">
  <b>Records what you do</b> so that you can <i>know how you've spent your time</i>.
  <br>
  All in a secure way where <i>you control the data</i>.
</p>

<p align=""center"">
  <a href=""https://twitter.com/ActivityWatchIt"">
    <img title=""Twitter follow"" src=""https://img.shields.io/twitter/follow/ActivityWatchIt.svg?style=social&label=Follow""/>
  </a>
  <a href=""https://github.com/ActivityWatch/activitywatch"">
    <img title=""Star on GitHub"" src=""https://img.shields.io/github/stars/ActivityWatch/activitywatch.svg?style=social&label=Star"">
  </a>

  <br>

  <b>
    <a href=""https://activitywatch.net/"">Website</a>
    — <a href=""https://forum.activitywatch.net/"">Forum</a>
    — <a href=""https://docs.activitywatch.net"">Documentation</a>
    — <a href=""https://github.com/ActivityWatch/activitywatch/releases"">Releases</a>
  </b>

  <br>

  <b>
    <a href=""https://activitywatch.net/"
tiktoken,"# ⏳ tiktoken

tiktoken is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokeniser for use with
OpenAI's models.

```python
import tiktoken
enc = tiktoken.get_encoding(""o200k_base"")
assert enc.decode(enc.encode(""hello world"")) == ""hello world""

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model(""gpt-4o"")
```

The open source version of `tiktoken` can be installed from PyPI:
```
pip install tiktoken
```

The tokeniser API is documented in `tiktoken/core.py`.

Example code using `tiktoken` can be found in the
[OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb).


## Performance

`tiktoken` is between 3-6x faster than a comparable open source tokeniser:

![image](https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg)

Performance measured on 1GB of text using the GPT-2 tokeniser, using `GPT2TokenizerFast` from
`tokenizers==0.13.2`, `trans"
Zappa,"Project moved to https://github.com/zappa/Zappa. Versions after 0.52.0 are published from there. Thank you Rich Jones for all your work on creating Zappa and maintaining it for years!
"
neural-enhance,"Neural Enhance
==============

.. image:: docs/OldStation_example.gif

**Example #1** — Old Station: `view comparison <http://enhance.nucl.ai/w/0f5177f4-9ce6-11e6-992c-c86000be451f/view>`_ in 24-bit HD, `original photo <https://flic.kr/p/oYhbBv>`_ CC-BY-SA @siv-athens.

----

`As seen on TV! <https://www.youtube.com/watch?v=LhF_56SxrGk>`_ What if you could increase the resolution of your photos using technology from CSI laboratories? Thanks to deep learning and ``#NeuralEnhance``, it's now possible to train a neural network to zoom in to your images at 2x or even 4x.  You'll get even better results by increasing the number of neurons or training with a dataset similar to your low resolution image.

The catch? The neural network is hallucinating details based on its training from example images. It's not reconstructing your photo exactly as it would have been if it was HD. That's only possible in Hollywood — but using deep learning as ""Creative AI"" works and it is just as cool!  Here's "
fashion-mnist,"# Fashion-MNIST

[![GitHub stars](https://img.shields.io/github/stars/zalandoresearch/fashion-mnist.svg?style=flat&label=Star)](https://github.com/zalandoresearch/fashion-mnist/)
[![Gitter](https://badges.gitter.im/zalandoresearch/fashion-mnist.svg)](https://gitter.im/fashion-mnist/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link)
[![Readme-CN](https://img.shields.io/badge/README-中文-green.svg)](README.zh-CN.md)
[![Readme-JA](https://img.shields.io/badge/README-日本語-green.svg)](README.ja.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Year-In-Review](https://img.shields.io/badge/%F0%9F%8E%82-Year%20in%20Review-orange.svg)](https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/)

<details><summary>Table of Contents</summary><p>

* [Why we made Fashion-MNIST](#why-we-made-fashion-mnist)
* [Get the Data](#get-the-data)
* [Usage](#usage)
* [Benchmark](#benchmark)
* [Visualization](#visualization"
walle-web,"![](https://raw.github.com/meolu/walle-web/master/screenshot/logo.jpg)

Walle 2.0 - [官方主页](https://www.walle-web.io)
=========================
<p align=""left"">
    <a href='https://travis-ci.org/meolu/walle-web'><img src='https://travis-ci.org/meolu/walle-web.svg?branch=master' alt=""Build Status""></a>  
    <a href='https://gitter.im/meolu/walle-web'><img src='https://badges.gitter.im/Join%20Chat.svg'></a>
</p>

**字节跳动内推**：ToB Lark 招聘大数据研发、数据分析师，机会极佳，请勿错过。请各位朋友扩散下有需要的同学，[直达内推链接](https://job.toutiao.com/referral/pc/position/detail/?token=MTsxNTcxMTA2MDM0NTkyOzY3MDQwNDI5MDQ2MTQzMDczMzY7NjcxODk1MDE2MDEzMjQ3NTE0OQ%3D%3D)，帮助内推，君子成人之美，谢谢。

功能强大，且免费开源的`walle-web 瓦力`终于更新`2.0.0`了！！！

walle 让用户代码发布终于可以不只能选择 jenkins！支持各种web代码发布，php、java、python、go等代码的发布、回滚可以通过web来一键完成。walle 一个可自由配置项目，更人性化，高颜值，支持git、多用户、多语言、多项目、多环境同时部署的开源上线部署系统。

`2.0.0` 占用了我几乎所有业余时间，精力与金钱付出换各位使用收益，望各位喜欢不吝顺手 `star` 以示支持，项目更好亦反馈予你。目前 `2.0.0` 已经发布，请保持关注，我会在公众号更新（在最下面）。  


有推广资源（开源文章推荐、大会分享）的同学，请微信联系我，强烈需要帮助。另外，老版本已迁移到 [walle 1.x](ht"
abu,"![](./img/head.png)

### 索引

| 内容 | 位置 | 
| ------| ------ | 
| 阿布量化系统源代码 | abupy目录 |
| 阿布量化使用教程 | abupy_lecture目录 |
| 阿布量化非编程界面操作 | abupy_ui目录 |
| 《量化交易之路》示例代码 | ipython／python目录| 
| 《机器学习之路》示例代码 | https://github.com/maxmon/abu_ml | 


###  🏆 [览器访问网址: https://www.abuquant.com](https://www.abuquant.com)

* 🇨🇳 [上证指数周报示例量化分析:](https://www.abuquant.com/abu_context/output_cn_week_2023-09-27/report/sh000001/index.html)
* 🇨🇳 [上证指数日报示例量化分析:](https://www.abuquant.com/abu_context/output_cn_day_2023-09-27-10-20-44/report/sh000001/index.html)

* 🚩 [深证成指周报示例量化分析:](https://www.abuquant.com/abu_context/output_cn_week_2023-09-27/report/sz399001/index.html)
* 🚩 [深证成指日报示例量化分析:](https://www.abuquant.com/abu_context/output_cn_day_2023-10-08-14-39-39/report/sz399001/index.html)

* 🇺🇸 [道琼斯周报示例量化分析:](https://www.abuquant.com/abu_context/output_us_week_2023-09-27/report/us.DJI/index.html)
* 🇺🇸 [道琼斯日报示例量化分析:](https://www.abuquant.com/abu_context/output_us_day_2023-09-27-09-39-11/report/us.DJI/index.html)

* 🚩"
fail2ban,"                         __      _ _ ___ _               
                        / _|__ _(_) |_  ) |__  __ _ _ _  
                       |  _/ _` | | |/ /| '_ \/ _` | ' \ 
                       |_| \__,_|_|_/___|_.__/\__,_|_||_|
                       v1.1.0.dev1            20??/??/??

## Fail2Ban: ban hosts that cause multiple authentication errors

Fail2Ban scans log files like `/var/log/auth.log` and bans IP addresses conducting
too many failed login attempts. It does this by updating system firewall rules
to reject new connections from those IP addresses, for a configurable amount
of time. Fail2Ban comes out-of-the-box ready to read many standard log files,
such as those for sshd and Apache, and is easily configured to read any log
file of your choosing, for any error you wish.

Though Fail2Ban is able to reduce the rate of incorrect authentication
attempts, it cannot eliminate the risk presented by weak authentication.
Set up services to use only two factor, or public/private a"
developer,"# 🐣 smol developer

<a href=""https://app.e2b.dev/agent/smol-developer"" target=""_blank"" rel=""noopener noreferrer"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://app.e2b.dev/api/badge_light"">
  <img alt=""Deploy agent on e2b button"" src=""https://app.e2b.dev/api/badge""/>
</picture>
</a>
<a href=""https://github.com/modal-labs/devlooper""><img src=""https://github.com/smol-ai/developer/assets/6764957/6af16d37-2494-4722-b3a2-6fc91c005451""></img>
</a>
<a href=""https://twitter.com/morph_labs/status/1689321673151979536""><img src=""https://avatars.githubusercontent.com/u/136536927?s=40&v=4"" alt=""Morph""></img> Morph
</a>

***Human-centric & Coherent Whole Program Synthesis*** aka your own personal junior developer

> [Build the thing that builds the thing!](https://twitter.com/swyx/status/1657578738345979905) a `smol dev` for every dev in every situation

This is a ""junior developer"" agent (aka `smol dev`) that either:

1. scaffolds an entire codebase out for you once you g"
synapse,"=========================================================================
Synapse |support| |development| |documentation| |license| |pypi| |python|
=========================================================================

Synapse is now actively maintained at `element-hq/synapse <https://github.com/element-hq/synapse>`_
=================================================================================================

Synapse is an open-source `Matrix <https://matrix.org/>`_ homeserver developed
from 2019 through 2023 as part of the Matrix.org Foundation. The Matrix.org
Foundation is not able to resource maintenance of Synapse and it
`continues to be developed by Element <https://github.com/element-hq/synapse>`_;
additionally you have the choice of `other Matrix homeservers <https://matrix.org/ecosystem/servers/>`_.

See `The future of Synapse and Dendrite <https://matrix.org/blog/2023/11/06/future-of-synapse-dendrite/>`_
blog post for more information.

==============================="
Nuitka,".. image:: https://img.shields.io/pypi/pyversions/Nuitka.svg
   :target: https://pypi.org/project/Nuitka

.. image:: https://badge.fury.io/py/Nuitka.svg
   :target: https://pypi.org/project/Nuitka

.. image:: https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg
   :target: CODE_OF_CONDUCT.md

####################
 Nuitka User Manual
####################

This document is the recommended first read when you start using
**Nuitka**. On this page, you will learn more about **Nuitka**
fundamentals, such as license type, use cases, requirements, and
credits.

.. contents:: Table of Contents
   :depth: 1
   :local:
   :class: page-toc

Nuitka is **the** Python compiler. It is written in Python. It is a
seamless replacement or extension to the Python interpreter and compiles
**every** construct that Python 2 (2.6, 2.7) and Python 3 (3.4 - 3.12)
have, when itself run with that Python version.

It then executes uncompiled code and compiled code together in an
extremely c"
Chinese-Word-Vectors,"# Chinese Word Vectors 中文词向量
[中文](https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md)

This project provides 100+ Chinese Word Vectors (embeddings) trained with different **representations** (dense and sparse), **context features** (word, ngram, character, and more), and **corpora**. One can easily obtain pre-trained vectors with different properties and use them for downstream tasks. 

Moreover, we provide a Chinese analogical reasoning dataset **CA8** and an evaluation toolkit for users to evaluate the quality of their word vectors.

## Reference
Please cite the paper, if using these embeddings and CA8 dataset.

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href=""http://aclweb.org/anthology/P18-2023""><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.

```
@InProceedings{P18-2023,
  author =  ""Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong"",
  tit"
pyinstaller,"PyInstaller Overview
====================

.. image:: https://img.shields.io/pypi/v/pyinstaller
   :alt: PyPI
   :target: https://pypi.org/project/pyinstaller
.. image:: https://img.shields.io/pypi/pyversions/pyinstaller
   :alt: PyPI - Python Version
   :target: https://pypi.org/project/pyinstaller
.. image:: https://img.shields.io/readthedocs/pyinstaller/stable
   :alt: Read the Docs (version)
   :target: https://pyinstaller.org
.. image:: https://img.shields.io/pypi/dm/pyinstaller
   :alt: PyPI - Downloads
   :target: https://pypistats.org/packages/pyinstaller


PyInstaller bundles a Python application and all its dependencies into a single
package. The user can run the packaged app without installing a Python
interpreter or any modules.

:Documentation: https://pyinstaller.org/
:Code:          https://github.com/pyinstaller/pyinstaller

PyInstaller reads a Python script written by you. It analyzes your code
to discover every other module and library your script needs in order to
ex"
schedule,"`schedule <https://schedule.readthedocs.io/>`__
===============================================


.. image:: https://github.com/dbader/schedule/workflows/Tests/badge.svg
        :target: https://github.com/dbader/schedule/actions?query=workflow%3ATests+branch%3Amaster

.. image:: https://coveralls.io/repos/dbader/schedule/badge.svg?branch=master
        :target: https://coveralls.io/r/dbader/schedule

.. image:: https://img.shields.io/pypi/v/schedule.svg
        :target: https://pypi.python.org/pypi/schedule

Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.

- A simple to use API for scheduling jobs, made for humans.
- In-process scheduler for periodic jobs. No extra processes needed!
- Very lightweight and no external dependencies.
- Excellent test coverage.
- Tested on Python and 3.7, 3.8, 3.9, 3.10, 3.11, 3.12

Usage
-----

.. code-block:: bash

    $ pip install schedule

.. code-block:: python

    import schedule"
tutorials,"<p align=""center"">
    <a href=""https://mofanpy.com/tutorials/"" target=""_blank"">
    <img width=""60%"" src=""%E7%89%87%E5%A4%B4.png"" style=""max-width:100%;"">
    </a>
</p>


<br>

我是 周沫凡, [莫烦Python](https://mofanpy.com/) 只是谐音, 我喜欢制作,
分享所学的东西, 所以你能在这里找到很多有用的东西, 少走弯路. 你能在[这里](https://mofanpy.com/about/)找到关于我的所有东西.

## 这个 Python tutorial 的一些内容:

* [Python 基础](https://mofanpy.com/tutorials/python-basic/)
  * [基础](https://mofanpy.com/tutorials/python-basic/basic/)
  * [多线程 threading](https://mofanpy.com/tutorials/python-basic/threading/)
  * [多进程 multiprocessing](https://mofanpy.com/tutorials/python-basic/multiprocessing/)
  * [简单窗口 tkinter](https://mofanpy.com/tutorials/python-basic/tkinter/)
* [机器学习](https://mofanpy.com/tutorials/machine-learning/)
  * [有趣的机器学习](https://mofanpy.com/tutorials/machine-learning/ML-intro/)
  * [强化学习 (Reinforcement Learning)](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/)
  * [进化算法 (Evolutionary Algorithm) 如遗传算法等](https://mofanpy.com/tut"
MemGPT,"# Letta (previously MemGPT)
[![Discord](https://img.shields.io/discord/1161736243340640419?label=Discord&logo=discord&logoColor=5865F2&style=flat-square&color=5865F2)](https://discord.gg/letta)
[![Twitter Follow](https://img.shields.io/badge/follow-%40Letta_AI-1DA1F2?style=flat-square&logo=x&logoColor=white)](https://twitter.com/Letta_AI)
[![arxiv 2310.08560](https://img.shields.io/badge/arXiv-2310.08560-B31B1B?logo=arxiv&style=flat-square)](https://arxiv.org/abs/2310.08560)

> [!NOTE]
> **Looking for MemGPT?**
>
> The MemGPT package and Docker image have been renamed to `letta` to clarify the distinction between **MemGPT agents** and the API server / runtime that runs LLM agents as *services*.
>
> You use the **Letta framework** to create **MemGPT agents**. Read more about the relationship between MemGPT and Letta [here](https://www.letta.com/blog/memgpt-and-letta).

See [documentation](https://docs.letta.com/introduction) for setup and usage.

## How to Get Involved
* **Contribute to"
allennlp,"<div align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/allenai/allennlp/main/docs/img/allennlp-logo-dark.png"" width=""400""/>
    <p>
    An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.
    </p>
    <hr/>
</div>
<p align=""center"">
    <a href=""https://github.com/allenai/allennlp/actions"">
        <img alt=""CI"" src=""https://github.com/allenai/allennlp/workflows/CI/badge.svg?event=push&branch=main"">
    </a>
    <a href=""https://pypi.org/project/allennlp/"">
        <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/allennlp"">
    </a>
    <a href=""https://github.com/allenai/allennlp/blob/main/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/github/license/allenai/allennlp.svg?color=blue&cachedrop"">
    </a>
    <a href=""https://codecov.io/gh/allenai/allennlp"">
        <img alt=""Codecov"" src=""https://codecov.io/gh/allenai/allennlp/branch/main/graph/badge.s"
SadTalker,"<div align=""center"">

<img src='https://user-images.githubusercontent.com/4397546/229094115-862c747e-7397-4b54-ba4a-bd368bfe2e0f.png' width='500px'/>


<!--<h2> 😭 SadTalker： <span style=""font-size:12px"">Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation </span> </h2> -->

  <a href='https://arxiv.org/abs/2211.12194'><img src='https://img.shields.io/badge/ArXiv-PDF-red'></a> &nbsp; <a href='https://sadtalker.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp; [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb) &nbsp; [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/vinthony/SadTalker) &nbsp; [![sd webui-colab](https://img.shields.io/badge/Automatic1111-Colab-green)](https://colab.research.google.com/github/camenduru/s"
sshuttle,"sshuttle: where transparent proxy meets VPN meets ssh
=====================================================

As far as I know, sshuttle is the only program that solves the following
common case:

- Your client machine (or router) is Linux, FreeBSD, MacOS or Windows.

- You have access to a remote network via ssh.

- You don't necessarily have admin access on the remote network.

- The remote network has no VPN, or only stupid/complex VPN
  protocols (IPsec, PPTP, etc). Or maybe you *are* the
  admin and you just got frustrated with the awful state of
  VPN tools.

- You don't want to create an ssh port forward for every
  single host/port on the remote network.

- You hate openssh's port forwarding because it's randomly
  slow and/or stupid.

- You can't use openssh's PermitTunnel feature because
  it's disabled by default on openssh servers; plus it does
  TCP-over-TCP, which has `terrible performance`_.

.. _terrible performance: https://sshuttle.readthedocs.io/en/stable/how-it-works"
apprise,"![Apprise Logo](https://raw.githubusercontent.com/caronc/apprise/master/apprise/assets/themes/default/apprise-logo.png)

<hr/>

**ap·prise** / *verb*<br/>
To inform or tell (someone). To make one aware of something.
<hr/>

*Apprise* allows you to send a notification to *almost* all of the most popular *notification* services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc.

* One notification library to rule them all.
* A common and intuitive notification syntax.
* Supports the handling of images and attachments (_to the notification services that will accept them_).
* It's incredibly lightweight.
* Amazing response times because all messages sent asynchronously.

Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the "
tvm,"<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- ""License""); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<img src=https://raw.githubusercontent.com/apache/tvm-site/main/images/logo/tvm-logo-small.png width=128"
owasp-mastg,"<img width=""180px"" align=""right"" style=""float: right;"" src=""cover.png"">

# OWASP Mobile Application Security Testing Guide (MASTG)

[![OWASP Flagship](https://img.shields.io/badge/owasp-flagship%20project-48A646.svg)](https://owasp.org/projects/)
[![Creative Commons License](https://img.shields.io/github/license/OWASP/owasp-mastg)](https://creativecommons.org/licenses/by-sa/4.0/ ""CC BY-SA 4.0"")

[![Document Build](https://github.com/OWASP/owasp-mastg/workflows/Documents%20Build/badge.svg)](https://github.com/OWASP/owasp-mastg/actions?query=workflow%3A%22Document+Build%22)
[![Markdown Linter](https://github.com/OWASP/owasp-mastg/workflows/Markdown%20Linter/badge.svg)](https://github.com/OWASP/owasp-mastg/actions?query=workflow%3A%22Markdown+Linter%22)
[![URL Checker](https://github.com/OWASP/owasp-mastg/workflows/URL%20Checker/badge.svg)](https://github.com/OWASP/owasp-mastg/actions?query=workflow%3A%22URL+Checker%22)

This is the official GitHub Repository of the OWASP Mobile Applicati"
MinerU,"<div align=""center"" xmlns=""http://www.w3.org/1999/html"">
<!-- logo -->
<p align=""center"">
  <img src=""docs/images/MinerU-logo.png"" width=""300px"" style=""vertical-align:middle;"">
</p>

<!-- icon -->

[![stars](https://img.shields.io/github/stars/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![forks](https://img.shields.io/github/forks/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![open issues](https://img.shields.io/github/issues-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![PyPI version](https://badge.fury.io/py/magic-pdf.svg)](https://badge.fury.io/py/magic-pdf)
[![Downloads](https://static.pepy.tech/badge/magic-pdf)](https://pepy.tech/project/magic-pdf)
[![Downloads](https://static.pepy.tech/badge/magic-pdf/month)](https://pepy.tech/project/magic-pdf)

[![OpenDataLab](https://img.shiel"
scalene,"![scalene](https://github.com/plasma-umass/scalene/raw/master/docs/scalene-icon-white.png)

# Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals

by [Emery Berger](https://emeryberger.com), [Sam Stern](https://samstern.me/), and [Juan Altmayer Pizzorno](https://github.com/jaltmayerpizzorno).

[![Scalene community Slack](https://github.com/plasma-umass/scalene/raw/master/docs/images/slack-logo.png)](https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg)[Scalene community Slack](https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg)

[![PyPI Latest Release](https://img.shields.io/pypi/v/scalene.svg)](https://pypi.org/project/scalene/)[![Anaconda-Server Badge](https://img.shields.io/conda/v/conda-forge/scalene)](https://anaconda.org/conda-forge/scalene) [![Downloads](https://static.pepy.tech/badge/scalene)](https://pepy.tech/project/scalene)[![Anaconda downloads](https://img"
NeMo,"[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)
[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)
[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolki"
playwright-python,"# 🎭 [Playwright](https://playwright.dev) for Python [![PyPI version](https://badge.fury.io/py/playwright.svg)](https://pypi.python.org/pypi/playwright/) [![Anaconda version](https://img.shields.io/conda/v/microsoft/playwright)](https://anaconda.org/Microsoft/playwright) [![Join Discord](https://img.shields.io/badge/join-discord-infomational)](https://aka.ms/playwright/discord)

Playwright is a Python library to automate [Chromium](https://www.chromium.org/Home), [Firefox](https://www.mozilla.org/en-US/firefox/new/) and [WebKit](https://webkit.org/) browsers with a single API. Playwright delivers automation that is **ever-green**, **capable**, **reliable** and **fast**. [See how Playwright is better](https://playwright.dev/python).

|          | Linux | macOS | Windows |
|   :---   | :---: | :---: | :---:   |
| Chromium <!-- GEN:chromium-version -->129.0.6668.29<!-- GEN:stop --> | ✅ | ✅ | ✅ |
| WebKit <!-- GEN:webkit-version -->18.0<!-- GEN:stop --> | ✅ | ✅ | ✅ |
| Firefox <!-- GEN:fire"
shap-e,"# Shap-E

This is the official code and model release for [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463).

 * See [Usage](#usage) for guidance on how to use this repository.
 * See [Samples](#samples) for examples of what our text-conditional model can generate.

# Samples

Here are some highlighted samples from our text-conditional model. For random samples on selected prompts, see [samples.md](samples.md).

<table>
    <tbody>
        <tr>
            <td align=""center"">
                <img src=""samples/a_chair_that_looks_like_an_avocado/2.gif"" alt=""A chair that looks like an avocado"">
            </td>
            <td align=""center"">
                <img src=""samples/an_airplane_that_looks_like_a_banana/3.gif"" alt=""An airplane that looks like a banana"">
            </td align=""center"">
            <td align=""center"">
                <img src=""samples/a_spaceship/0.gif"" alt=""A spaceship"">
            </td>
        </tr>
        <tr>
        "
whisperX,"<h1 align=""center"">WhisperX</h1>

<p align=""center"">
  <a href=""https://github.com/m-bain/whisperX/stargazers"">
    <img src=""https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&colorB=orange&logo=github""
         alt=""GitHub stars"">
  </a>
  <a href=""https://github.com/m-bain/whisperX/issues"">
        <img src=""https://img.shields.io/github/issues/m-bain/whisperx.svg""
             alt=""GitHub issues"">
  </a>
  <a href=""https://github.com/m-bain/whisperX/blob/master/LICENSE"">
        <img src=""https://img.shields.io/github/license/m-bain/whisperX.svg""
             alt=""GitHub license"">
  </a>
  <a href=""https://arxiv.org/abs/2303.00747"">
        <img src=""http://img.shields.io/badge/Arxiv-2303.00747-B31B1B.svg""
             alt=""ArXiv paper"">
  </a>
  <a href=""https://twitter.com/intent/tweet?text=&url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX"">
  <img src=""https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social"" alt=""Twitter"">
  </a>  "
faster-whisper,"[![CI](https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg)](https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI) [![PyPI version](https://badge.fury.io/py/faster-whisper.svg)](https://badge.fury.io/py/faster-whisper)

# Faster Whisper transcription with CTranslate2

**faster-whisper** is a reimplementation of OpenAI's Whisper model using [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models.

This implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.

## Benchmark

### Whisper

For reference, here's the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7tTptBo9I) of audio using different implementations:

* [openai/whisper](https://github.com/openai/whisper)@[6dea21fd](http"
storm,"<p align=""center"">
  <img src=""assets/logo.svg"" style=""width: 25%; height: auto;"">
</p>

# STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking

<p align=""center"">
| <a href=""http://storm.genie.stanford.edu""><b>Research preview</b></a> | <a href=""https://arxiv.org/abs/2402.14207""><b>STORM Paper</b></a>| <a href=""https://www.arxiv.org/abs/2408.15232""><b>Co-STORM Paper</b></a>  | <a href=""https://storm-project.stanford.edu/""><b>Website</b></a> |
</p>

**Latest News** 🔥

- [2024/09] Co-STORM codebase is now released and integrated into `knowledge-storm` python package v1.0.0. Run `pip install knowledge-storm --upgrade` to check it out.

- [2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! [Co-STORM Paper](https://www.arxiv.org/abs/2408.15232) has been accepted to EMNLP 2024 main conference.

- [2024/07] You can now install our package with `pip install knowledge-storm`!
- [2024/07] We add `Vecto"
QAnything,"<div align=""center"">

  <a href=""https://github.com/netease-youdao/QAnything"">
    <!-- Please provide path to your logo here -->
    <img src=""docs/images/qanything_logo.png"" alt=""Logo"" width=""800"">
  </a>

# **Q**uestion and **A**nswer based on **Anything**

<p align=""center"">
  <a href=""./README.md"">English</a> |
  <a href=""./README_zh.md"">简体中文</a>
</p>

</div>

<div align=""center"">

<a href=""https://qanything.ai""><img src=""https://img.shields.io/badge/try%20online-qanything.ai-purple""></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://read.youdao.com#/home""><img src=""https://img.shields.io/badge/try%20online-read.youdao.com-purple""></a>
&nbsp;&nbsp;&nbsp;&nbsp;

<a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-AGPL--3.0-yellow""></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://github.com/netease-youdao/QAnything/pulls""><img src=""https://img.shields.io/badge/PRs-welcome-red""></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://twitter.com/YDopensource""><img src=""https://img.shi"
gallery-dl,"==========
gallery-dl
==========

*gallery-dl* is a command-line program
to download image galleries and collections
from several image hosting sites
(see `Supported Sites <docs/supportedsites.md>`__).
It is a cross-platform tool
with many `configuration options <https://gdl-org.github.io/docs/configuration.html>`__
and powerful `filenaming capabilities <https://gdl-org.github.io/docs/formatting.html>`__.


|pypi| |build|

.. contents::


Dependencies
============

- Python_ 3.4+
- Requests_

Optional
--------

- yt-dlp_ or youtube-dl_: HLS/DASH video downloads, ``ytdl`` integration
- FFmpeg_: Pixiv Ugoira conversion
- mkvmerge_: Accurate Ugoira frame timecodes
- PySocks_: SOCKS proxy support
- brotli_ or brotlicffi_: Brotli compression support
- zstandard_: Zstandard compression support
- PyYAML_: YAML configuration file support
- toml_: TOML configuration file support for Python<3.11
- SecretStorage_: GNOME keyring passwords for ``--cookies-from-browser``


Installation
============
"
mycli,"# mycli

[![Build Status](https://github.com/dbcli/mycli/workflows/mycli/badge.svg)](https://github.com/dbcli/mycli/actions?query=workflow%3Amycli)

A command line client for MySQL that can do auto-completion and syntax highlighting.

HomePage: [http://mycli.net](http://mycli.net)
Documentation: [http://mycli.net/docs](http://mycli.net/docs)

![Completion](screenshots/tables.png)
![CompletionGif](screenshots/main.gif)

Postgres Equivalent: [http://pgcli.com](http://pgcli.com)

Quick Start
-----------

If you already know how to install python packages, then you can install it via pip:

You might need sudo on linux.

```
$ pip install -U mycli
```

or

```
$ brew update && brew install mycli  # Only on macOS
```

or

```
$ sudo apt-get install mycli # Only on debian or ubuntu
```

### Usage

    $ mycli --help
    Usage: mycli [OPTIONS] [DATABASE]

      A MySQL terminal client with auto-completion and syntax highlighting.

      Examples:
        - mycli my_database
        - mycli -u "
External-Attention-pytorch,"
<img src=""./FightingCVimg/LOGO.gif"" height=""200"" width=""400""/>

简体中文 | [English](./README_EN.md)

# FightingCV 代码库， 包含 [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)

![](https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen)
![](https://img.shields.io/badge/python->=v3.0-blue)
![](https://img.shields.io/badge/pytorch->=v1.4-red)

<!--
-------
*If this project is helpful to you, welcome to give a ***star***.* 

*Don't forget to ***follow*** me to learn about project updates.*

-->




Hello，大家好，我是小马🚀🚀🚀

***For 小白（Like Me）：***
最近在读论文的时候会发现一个问题，有时候论文核心思想非常简单，核心代码可能也就十几行。但是打开作者release的源码时，却发现提出的模块嵌入到分类、检测、分割等任务框架中，导致代码比较冗余，对于特定任务框架不熟悉的我，**很难找到核心代码**，导致在论文和网络思想的理解上会有一定困难。

***For 进阶者（Like You）：***
如果把Conv、FC、RNN这些基本单元看做小的Lego积木，把Transformer、ResNet这些结构看成已经搭好的Lego城堡。那么本项目提供的模块就是一个个具有完整语义信息的Lego组件。**让科研工作者们避免反复造轮子**，只需思考如何利用这些“Lego组件”，搭建出更多绚烂多彩的作品。

***F"
deep_learning_object_detection,"# deep learning object detection
A paper list of object detection using deep learning. I wrote this page with reference to [this survey paper](https://arxiv.org/pdf/1809.02165v1.pdf) and searching and searching.. 

*Last updated: 2020/09/22*

#### Update log
*2018/9/18* - update all of recent papers and make some diagram about history of object detection using deep learning. 
*2018/9/26* - update codes of papers. (official and unofficial)  
*2018/october* - update 5 papers and performance table.  
*2018/november* - update 9 papers.  
*2018/december* - update 8 papers and and performance table and add new diagram(**2019 version!!**).  
*2019/january* - update 4 papers and and add commonly used datasets.  
*2019/february* - update 3 papers.  
*2019/march* - update figure and code links.  
*2019/april* - remove author's names and update ICLR 2019 & CVPR 2019 papers.  
*2019/may* - update CVPR 2019 papers.  
*2019/june* - update CVPR 2019 papers and dataset paper.  
*2019/july* - update BM"
CustomTkinter,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""./documentation_images/CustomTkinter_logo_dark.png"">
    <img src=""./documentation_images/CustomTkinter_logo_light.png"">
  </picture>
</p>

<div align=""center"">

![PyPI](https://img.shields.io/pypi/v/customtkinter)
![PyPI - Downloads](https://img.shields.io/pypi/dm/customtkinter?color=green&label=downloads)
![Downloads last 6 month](https://static.pepy.tech/personalized-badge/customtkinter?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads%20last%206%20month)
![PyPI - License](https://img.shields.io/badge/license-MIT-blue)
![LOC](https://tokei.rs/b1/github/tomschimansky/customtkinter?category=lines)

</div>

---

<div align=""center"">
<a href=""https://www.paypal.com/donate/?hosted_button_id=LK5QAZYRN2R2A""><img src=""documentation_images/paypal_donate_button.png"" width=170 alt=""Paypal donation button""></a>

<a></a>

| Massive Thanks to all the People who Donat"
Open-Sora-Plan,"

<h1 align=""left""> <a href="""">Open-Sora Plan</a></h1>

This project aims to create a simple and scalable repo, to reproduce [Sora](https://openai.com/sora) (OpenAI, but we prefer to call it ""ClosedAI"" ). We wish the open-source community can contribute to this project. Pull requests are welcome! The current code supports complete training and inference using the Huawei Ascend AI computing system. Models trained on Huawei Ascend can also output video quality comparable to industry standards.

本项目希望通过开源社区的力量复现Sora，由北大-兔展AIGC联合实验室共同发起，当前版本离目标差距仍然较大，仍需持续完善和快速迭代，欢迎Pull request！目前代码同时支持使用国产AI计算系统（华为昇腾）进行完整的训练和推理。基于昇腾训练出的模型，也可输出持平业界的视频质量。

<h5 align=""left"">

[![slack badge](https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp)](https://discord.gg/FkFm5M2J)
[![WeChat badge](https://img.shields.io/badge/微信-加入-green?logo=wechat&amp)](https://github.com/PKU-YuanGroup/Open-Sora-Plan/issues/53#issuecomment-1987226516)
[![Twitter](https://img.shields.io/badge/-Twitter@LinBin46984-b"
h2ogpt,"# h2oGPT

Turn ★ into ⭐ (top-right corner) if you like the project!

Query and summarize your documents or just chat with local private GPT LLMs using h2oGPT, an Apache V2 open-source project.

Check out a long CoT Open-o1 open 🍓strawberry🍓 project: https://github.com/pseudotensor/open-strawberry

## Live Demo

[![img-small.png](docs/img-small.png) Gradio Demo](https://gpt.h2o.ai/)

[![img-small.png](docs/img-small.png) OpenWebUI Demo](https://gpt-docs.h2o.ai/)

## Video Demo

https://github.com/h2oai/h2ogpt/assets/2249614/2f805035-2c85-42fb-807f-fd0bca79abc6

[![img-small.png](docs/img-small.png) YouTube 4K Video](https://www.youtube.com/watch?v=_iktbj4obAI)

## Features

- **Private** offline database of any documents [(PDFs, Excel, Word, Images, Video Frames, YouTube, Audio, Code, Text, MarkDown, etc.)](docs/README_LangChain.md#supported-datatypes)
  - **Persistent** database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)
"
gorilla,"# Gorilla: Large Language Model Connected with Massive APIs [[Project Website](https://shishirpatil.github.io/gorilla/)]


<img src=""https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png"" width=50% height=50%>

**🚒  GoEx: A Runtime for executing LLM generated actions like code & API calls** GoEx presents “undo” and “damage confinement” abstractions for mitigating the risk of unintended actions taken in LLM-powered systems. [Release blog](https://gorilla.cs.berkeley.edu/blogs/10_gorilla_exec_engine.html) [Paper](https://arxiv.org/abs/2404.06921).

**🎉 Berkeley Function Calling Leaderboard** How do models stack up for function calling? :dart: Releasing the [Berkeley Function Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard). Read more in our [Release Blog](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html). 

**:trophy: Gorilla OpenFunctions v2** Sets new SoTA for open-source LLMs :muscle: On-par with GPT-4 :raised_hand"
dagster,"<div align=""center"">
  <!-- Note: Do not try adding the dark mode version here with the `picture` element, it will break formatting in PyPI -->
  <a target=""_blank"" href=""https://dagster.io"" style=""background:none"">
    <img alt=""dagster logo"" src=""https://raw.githubusercontent.com/dagster-io/dagster/master/.github/dagster-readme-header.svg"" width=""auto"" height=""100%"">
  </a>
  <a target=""_blank"" href=""https://github.com/dagster-io/dagster"" style=""background:none"">
    <img src=""https://img.shields.io/github/stars/dagster-io/dagster?labelColor=4F43DD&color=163B36&logo=github"">
  </a>
  <a target=""_blank"" href=""https://github.com/dagster-io/dagster/blob/master/LICENSE"" style=""background:none"">
    <img src=""https://img.shields.io/badge/License-Apache_2.0-blue.svg?label=license&labelColor=4F43DD&color=163B36"">
  </a>
  <a target=""_blank"" href=""https://pypi.org/project/dagster/"" style=""background:none"">
    <img src=""https://img.shields.io/pypi/v/dagster?labelColor=4F43DD&color=163B36"">
 "
phidata,"<h1 align=""center"">
  phidata
</h1>

<h3 align=""center"">
Build AI Assistants with memory, knowledge and tools
</h3>

![image](https://github.com/phidatahq/phidata/assets/22579644/295187f6-ac9d-41e0-abdb-38e3291ad1d1)

## What is phidata?

**Phidata is a framework for building Autonomous Assistants** (aka Agents) that have long-term memory, contextual knowledge and the ability to take actions using function calling.

Use phidata to turn any LLM into an AI Assistant that can:
- **Search the web** using DuckDuckGo, Google etc.
- **Analyze data** using SQL, DuckDb, etc.
- **Conduct research** and generate reports.
- **Answer questions** from PDFs, APIs, etc.
- **Write scripts** for movies, books, etc.
- **Summarize** articles, videos, etc.
- **Perform tasks** like sending emails, querying databases, etc.
- **And much more...**

## Why phidata?

**Problem:** We need to turn general-purpose LLMs into specialized assistants for our use-case.

**Solution:** Extend LLMs with memory, knowledge a"
theZoo,"# theZoo - A Live Malware Repository

[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=round)](https://github.com/ytisf/theZoo/issues)
[![HitCount](http://hits.dwyl.com/ytisf/theZoo.svg)](http://hits.dwyl.com/ytisf/theZoo)
[![GitHub stars](https://img.shields.io/github/stars/ytisf/theZoo.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/ytisf/theZoo/stargazers/)
[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)

![Logo](https://github.com/ytisf/theZoo/raw/gh-pages/MalDB-Logo-Thumb.png)

theZoo is a project created to make the possibility of malware analysis open and available to the public. Since we have found out that almost all versions of malware are very hard to come by in a way which will allow analysis, we have decided to gather all of them for you in an accessible and safe way.
theZoo was born by Yuval tisf Nativ and is now maintained by Shahak Shalev.

**theZ"
theHarvester,"![theHarvester](https://github.com/laramies/theHarvester/blob/master/theHarvester-logo.webp)

![TheHarvester CI](https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg) ![TheHarvester Docker Image CI](https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg)
[![Rawsec's CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg)](https://inventory.raw.pm/)

What is this?
-------------
theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red<br>
team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine<br>
a domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using<br>
multiple public resources that include:<br>

Passive modules:
----------------
* anubis: Anubis-DB - https://github.com/jonluca/anubis

* bevigil: Clou"
FastPhotoStyle,"[![License CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC4.0-blue.svg)](https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/LICENSE.md)
![Python 2.7](https://img.shields.io/badge/python-2.7-green.svg)
![Python 3.5](https://img.shields.io/badge/python-3.5-green.svg)

## FastPhotoStyle

### License
Copyright (C) 2018 NVIDIA Corporation.  All rights reserved.
Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

<img src=""https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/teaser.png"" width=""800"" title=""Teaser results""> 


### What's new
 
 | Date     | News |
 |----------|--------------|
 |2018-07-25| Migrate to pytorch 0.4.0. For pytorch 0.3.0 user, check out [FastPhotoStyle for pytorch 0.3.0](https://github.com/NVIDIA/FastPhotoStyle/releases/tag/f33e07f). |
 |          | Add a [tutorial](TUTORIAL.md) showing 3 ways of using the FastPhotoStyle algorithm.|
 |2018-07-10| Our paper is accepted by the ECCV "
ludwig,"<p align=""center"">
  <a href=""https://ludwig.ai"">
    <img src=""https://github.com/ludwig-ai/ludwig-docs/raw/master/docs/images/ludwig_hero_smaller.jpg"" height=""150"">
  </a>
</p>

<div align=""center"">

_Declarative deep learning framework built for scale and efficiency._

[![PyPI version](https://badge.fury.io/py/ludwig.svg)](https://badge.fury.io/py/ludwig)
[![Discord](https://dcbadge.vercel.app/api/server/CBgdrGnZjy?style=flat&theme=discord-inverted)](https://discord.gg/CBgdrGnZjy)
[![DockerHub](https://img.shields.io/docker/pulls/ludwigai/ludwig.svg)](https://hub.docker.com/r/ludwigai)
[![Downloads](https://pepy.tech/badge/ludwig)](https://pepy.tech/project/ludwig)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ludwig-ai/ludwig/blob/master/LICENSE)
[![X](https://img.shields.io/twitter/follow/ludwig_ai.svg?style=social&logo=twitter)](https://twitter.com/ludwig_ai)

</div>

> \[!IMPORTANT\]
> Our community has moved to [Discord](https://dis"
peewee,".. image:: https://media.charlesleifer.com/blog/photos/peewee3-logo.png

peewee
======

Peewee is a simple and small ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use.

* a small, expressive ORM
* python 2.7+ and 3.4+
* supports sqlite, mysql, mariadb, postgresql and cockroachdb
* tons of `extensions <http://docs.peewee-orm.com/en/latest/peewee/playhouse.html>`_

New to peewee? These may help:

* `Quickstart <http://docs.peewee-orm.com/en/latest/peewee/quickstart.html#quickstart>`_
* `Example twitter app <http://docs.peewee-orm.com/en/latest/peewee/example.html>`_
* `Using peewee interactively <http://docs.peewee-orm.com/en/latest/peewee/interactive.html>`_
* `Models and fields <http://docs.peewee-orm.com/en/latest/peewee/models.html>`_
* `Querying <http://docs.peewee-orm.com/en/latest/peewee/querying.html>`_
* `Relationships and joins <http://docs.peewee-orm.com/en/latest/peewee/relationships.html>`_

Examples
--------

Defining models is similar "
Meshroom,"# ![Meshroom - 3D Reconstruction Software](/docs/logo/banner-meshroom.png)

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2997/badge)](https://bestpractices.coreinfrastructure.org/projects/2997)

Meshroom is a free, open-source 3D Reconstruction Software based on the [AliceVision](https://github.com/alicevision/AliceVision) Photogrammetric Computer Vision framework.

Learn more details about the pipeline on [AliceVision website](http://alicevision.github.io).

See [results of the pipeline on sketchfab](http://sketchfab.com/AliceVision).

Continuous integration:
* Windows: [![Build status](https://ci.appveyor.com/api/projects/status/25sd7lfr3v0rnvni/branch/develop?svg=true)](https://ci.appveyor.com/project/AliceVision/meshroom/branch/develop)
* Linux: [![Build Status](https://travis-ci.org/alicevision/meshroom.svg?branch=develop)](https://travis-ci.org/alicevision/meshroom)


## Photogrammetry

Photogrammetry is the science of making measurements from phot"
DALLE2-pytorch,"<img src=""./dalle2.png"" width=""450px""></img>

## DALL-E 2 - Pytorch

Implementation of <a href=""https://openai.com/dall-e-2/"">DALL-E 2</a>, OpenAI's updated text-to-image synthesis neural network, in Pytorch.

<a href=""https://youtu.be/RJwPN4qNi_Y?t=555"">Yannic Kilcher summary</a> | <a href=""https://www.youtube.com/watch?v=F1X4fHzF4mQ"">AssemblyAI explainer</a>

The main novelty seems to be an extra layer of indirection with the prior network (whether it is an autoregressive transformer or a diffusion network), which predicts an image embedding based on the text embedding from CLIP. Specifically, this repository will only build out the diffusion prior network, as it is the best performing variant (but which incidentally involves a causal transformer as the denoising network 😂)

This model is SOTA for text-to-image for now.

Please join <a href=""https://discord.gg/xBPBXfcFHd""><img alt=""Join us on Discord"" src=""https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&lo"
ml-engineering,"# Machine Learning Engineering Open Book

This is an open collection of methodologies, tools and step by step instructions to help with successful training of large language models and multi-modal models.

This is a technical material suitable for LLM/VLM training engineers and operators. That is the content here contains lots of scripts and copy-n-paste commands to enable you to quickly address your needs.

This repo is an ongoing brain dump of my experiences training Large Language Models (LLM) (and VLMs); a lot of the know-how I acquired while training the open-source [BLOOM-176B](https://huggingface.co/bigscience/bloom) model in 2022 and [IDEFICS-80B](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) multi-modal model in 2023. Currently, I'm working on developing/training open-source Retrieval Augmented Generation (RAG) models at [Contextual.AI](https://contextual.ai/).

I've been compiling this information mostly for myself so that I could quickly find solutions I have al"
gdb-dashboard,"# GDB dashboard

GDB dashboard is a standalone `.gdbinit` file written using the [Python API][] that enables a modular interface showing relevant information about the program being debugged. Its main goal is to reduce the number of GDB commands needed to inspect the status of current program thus allowing the developer to primarily focus on the control flow.

![Screenshot](https://raw.githubusercontent.com/wiki/cyrus-and/gdb-dashboard/Screenshot.png)

[Python API]: https://sourceware.org/gdb/onlinedocs/gdb/Python-API.html

## Quickstart

Just place [`.gdbinit`][] in your home directory, for example with:

```
wget -P ~ https://github.com/cyrus-and/gdb-dashboard/raw/master/.gdbinit
```

Optionally install [Pygments][] to enable syntax highlighting:

```
pip install pygments
```

Then debug as usual, the dashboard will appear automatically every time the inferior program stops.

Keep in mind that no GDB command has been redefined, instead all the features are available via the main `das"
borg,"This is borg2!
--------------

Please note that this is the README for borg2 / master branch.

For the stable version's docs, please see there:

https://borgbackup.readthedocs.io/en/stable/

Borg2 is currently in beta testing and might get major and/or
breaking changes between beta releases (and there is no beta to
next-beta upgrade code, so you will have to delete and re-create repos).

Thus, **DO NOT USE BORG2 FOR YOUR PRODUCTION BACKUPS!** Please help with
testing it, but set it up *additionally* to your production backups.

TODO: the screencasts need a remake using borg2, see there:

https://github.com/borgbackup/borg/issues/6303


What is BorgBackup?
-------------------

BorgBackup (short: Borg) is a deduplicating backup program.
Optionally, it supports compression and authenticated encryption.

The main goal of Borg is to provide an efficient and secure way to back up data.
The data deduplication technique used makes Borg suitable for daily backups
since only changes are stored.
"
Statistical-Learning-Method_Code,"## 【广告】每日Arxiv（中文版）
每日Arxiv（中文版）立志paper**汉化**，目前翻译目前涵盖**标题**和**摘要**，AI学科近期支持论文**全文汉化**

一天阅读百篇paper不是梦！

链接： [学术巷子(xueshuxiangzi.com)](https://www.xueshuxiangzi.com/)


前言
====

力求每行代码都有注释，重要部分注明公式来源。具体会追求下方这样的代码，学习者可以照着公式看程序，让代码有据可查。

![image](https://github.com/Dod-o/Statistical-Learning-Method_Code/blob/master/CodePic.png)

    
如果时间充沛的话，可能会试着给每一章写一篇博客。先放个博客链接吧：[传送门](http://www.pkudodo.com/)。    

##### 注：其中Mnist数据集已转换为csv格式，由于体积为107M超过限制，改为压缩包形式。下载后务必先将Mnist文件内压缩包直接解压。  

### 【Updates】
**书籍出版**：目前已与**人民邮电出版社**签订合同，未来将结合该repo整理出版机器学习实践相关书籍。同时会在book分支中对代码进行重构，欢迎在issue中提建议！同时issue中现有的问题也会考虑进去。（Feb 12 2022）

**线下培训**：女朋友计划近期开办**ML/MLP/CV线下培训班**，地点**北上广深杭**，目标各方向**快速入门**，正在筹备。这里帮她打个广告，可以添加微信15324951814（备注线下培训）。本人也会被拉过去义务评估课程质量。。。（Feb 12 2022）

**无监督部分更新**：部分**无监督**算法已更新！！！ 该部分由[Harold-Ran](https://github.com/Harold-Ran)提供，在此感谢！ 有其他算法补充的同学也欢迎添加我微信并pr！（Jan 27 2021）
       
实现
======

## 监督部分

### 第二章 感知机：
博客：[统计学习方法|感知机原理剖析及实现](http://www.pkudodo.com/2018/11/18/1-4/)      
实现：[perceptron/"
PaddleSpeech,"([简体中文](./README_cn.md)|English)
<p align=""center"">
  <img src=""./docs/images/PaddleSpeech_logo.png"" />
</p>

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-red.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa""></a>
    <a href=""support os""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors""><img src=""https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/commits""><img src=""https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/issues""><img src=""https://img."
flet,"# Flet

<img src=""media/logo/flet-logo.svg"" width=""50%""/>

[![Build status](https://ci.appveyor.com/api/projects/status/xwablctxslvey576/branch/main?svg=true)](https://ci.appveyor.com/project/flet-dev/flet/branch/main)

Flet is a framework that enables you to easily build real-time web, mobile, and desktop apps in your favorite language and securely share them with your team. No frontend experience is required.

### ⚡From idea to app in minutes

An internal tool or a dashboard for your team, weekend project, data entry form, kiosk app, or high-fidelity prototype - Flet is an ideal framework to quickly hack great-looking interactive apps to serve a group of users.

### 📐 Simple architecture

No more complex architecture with JavaScript frontend, REST API backend, database, cache, etc. With Flet you just write a monolith stateful app in Python only and get multi-user, real-time Single-Page Application (SPA).

### 🔋Batteries included

To start developing with Flet, you just need your favo"
stylegan2,"## StyleGAN2 &mdash; Official TensorFlow Implementation

![Teaser image](./docs/stylegan2-teaser-1024x256.png)

**Analyzing and Improving the Image Quality of StyleGAN**<br>
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila<br>

Paper: http://arxiv.org/abs/1912.04958<br>
Video: https://youtu.be/c-NJtV9Jvp0<br>

Abstract: *The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it po"
vanna,"

| GitHub | PyPI | Documentation |
| ------ | ---- | ------------- |
| [![GitHub](https://img.shields.io/badge/GitHub-vanna-blue?logo=github)](https://github.com/vanna-ai/vanna) | [![PyPI](https://img.shields.io/pypi/v/vanna?logo=pypi)](https://pypi.org/project/vanna/) | [![Documentation](https://img.shields.io/badge/Documentation-vanna-blue?logo=read-the-docs)](https://vanna.ai/docs/) |

# Vanna
Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality.

https://github.com/vanna-ai/vanna/assets/7146154/1901f47a-515d-4982-af50-f12761a3b2ce

![vanna-quadrants](https://github.com/vanna-ai/vanna/assets/7146154/1c7c88ba-c144-4ecf-a028-cf5ba7344ca2)

## How Vanna works

![Screen Recording 2024-01-24 at 11 21 37 AM](https://github.com/vanna-ai/vanna/assets/7146154/1d2718ad-12a8-4a76-afa2-c61754462f93)


Vanna works in two easy steps - train a RAG ""model"" on your data, and then ask questions which will return SQL q"
Photon,"
<h1 align=""center"">
  <br>
  <a href=""https://github.com/s0md3v/Photon""><img src=""https://image.ibb.co/h5OZAK/photonsmall.png"" alt=""Photon""></a>
  <br>
  Photon
  <br>
</h1>

<h4 align=""center"">Incredibly fast crawler designed for OSINT.</h4>

<p align=""center"">
  <a href=""https://github.com/s0md3v/Photon/releases"">
    <img src=""https://img.shields.io/github/release/s0md3v/Photon.svg"">
  </a>
  <a href=""https://pypi.org/project/photon/"">
    <img src=""https://img.shields.io/badge/pypi-@photon-red.svg?style=style=flat-square""
         alt=""pypi"">
  </a>
  <a href=""https://github.com/s0md3v/Photon/issues?q=is%3Aissue+is%3Aclosed"">
      <img src=""https://img.shields.io/github/issues-closed-raw/s0md3v/Photon.svg"">
  </a>
  <a href=""https://travis-ci.com/s0md3v/Photon"">
    <img src=""https://img.shields.io/travis/com/s0md3v/Photon.svg"">
  </a>
</p>

Met a CAPTCHA? Try [CapSolver](https://www.capsolver.com/?utm_source=github&utm_medium=repo&utm_campaign=scraping&utm_term=photon) solving s"
InstantID,"<div align=""center"">
<h1>InstantID: Zero-shot Identity-Preserving Generation in Seconds</h1>

[**Qixun Wang**](https://github.com/wangqixun)<sup>12</sup> · [**Xu Bai**](https://huggingface.co/baymin0220)<sup>12</sup> · [**Haofan Wang**](https://haofanwang.github.io/)<sup>12*</sup> · [**Zekui Qin**](https://github.com/ZekuiQin)<sup>12</sup> · [**Anthony Chen**](https://antonioo-c.github.io/)<sup>123</sup>

Huaxia Li<sup>2</sup> · Xu Tang<sup>2</sup> · Yao Hu<sup>2</sup>

<sup>1</sup>InstantX Team · <sup>2</sup>Xiaohongshu Inc · <sup>3</sup>Peking University

<sup>*</sup>corresponding authors

<a href='https://instantid.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>
<a href='https://arxiv.org/abs/2401.07519'><img src='https://img.shields.io/badge/Technique-Report-red'></a>
<a href='https://huggingface.co/papers/2401.07519'><img src='https://img.shields.io/static/v1?label=Paper&message=Huggingface&color=orange'></a> 
[![GitHub](https://img.shields.io/github/st"
fast-style-transfer,"## Fast Style Transfer in [TensorFlow](https://github.com/tensorflow/tensorflow)

Add styles from famous paintings to any photo in a fraction of a second! [You can even style videos!](#video-stylization)

<p align = 'center'>
<img src = 'examples/style/udnie.jpg' height = '246px'>
<img src = 'examples/content/stata.jpg' height = '246px'>
<a href = 'examples/results/stata_udnie.jpg'><img src = 'examples/results/stata_udnie_header.jpg' width = '627px'></a>
</p>
<p align = 'center'>
It takes 100ms on a 2015 Titan X to style the MIT Stata Center (1024×680) like Udnie, by Francis Picabia.
</p>

Our implementation is based off of a combination of Gatys' [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576), Johnson's [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](http://cs.stanford.edu/people/jcjohns/eccv16/), and Ulyanov's [Instance Normalization](https://arxiv.org/abs/1607.08022). 

### Sponsorship
Please consider sponsoring my work on this project"
chinese-xinhua,"# chinese-xinhua

中华新华字典数据库和 API 。收录包括 14032 条歇后语，16142 个汉字，264434 个词语，31648 个成语。

## Project Structure

```
chinese-xinhua/
|
+- data/ <-- 数据文件夹
|  |
|  +- idiom.json <-- 成语
|  |
|  +- word.json <-- 汉字
|  |
|  +- xiehouyu.json <-- 歇后语
|  |
|  +- ci.json <-- 词语
```

## Database Introduction

### 成语 (idiom.json)

```json
[
    {
        ""derivation"": ""语出《法华经·法师功德品》下至阿鼻地狱。”"",
        ""example"": ""但也有少数意志薄弱的……逐步上当，终至堕入～。★《上饶集中营·炼狱杂记》"",
        ""explanation"": ""阿鼻梵语的译音，意译为无间”，即痛苦无有间断之意。常用来比喻黑暗的社会和严酷的牢狱。又比喻无法摆脱的极其痛苦的境地。"",
        ""pinyin"": ""ā bí dì yù"",
        ""word"": ""阿鼻地狱"",
        ""abbreviation"": ""abdy""
    },
    ...
]
```

### 词语 (ci.json)

```json
[
    { 
        ""ci"": ""宸纶"", 
        ""explanation"": ""1.帝王的诏书﹑制令。"" 
    },
    ...
]
```

### 汉字 (word.json)

```json
[
    {
        ""word"": ""嗄"",
        ""oldword"": ""嗄"",
        ""strokes"": ""13"",
        ""pinyin"": ""á"",
        ""radicals"": ""口"",
        ""explanation"": ""嗄〈叹〉\n\n 同啊”。表示省悟或惊奇\n\n 嗄!难道这里是没有地方官的么?--宋·佚名《新编五代史平话》\n\n 嗄á叹词。在句首，〈表〉疑问或反"
urh,"![URH image](https://raw.githubusercontent.com/jopohl/urh/master/data/icons/banner.png)

[![CI](https://github.com/jopohl/urh/actions/workflows/ci.yml/badge.svg)](https://github.com/jopohl/urh/actions/workflows/ci.yml)
[![Code style: black](https://img.shields.io/badge/code%20style-black-black)](https://github.com/psf/black)
[![PyPI version](https://badge.fury.io/py/urh.svg)](https://badge.fury.io/py/urh)
[![Packaging status](https://repology.org/badge/tiny-repos/urh.svg)](https://repology.org/project/urh/versions)
 [![Blackhat Arsenal 2017](https://rawgit.com/toolswatch/badges/master/arsenal/usa/2017.svg)](http://www.toolswatch.org/2017/06/the-black-hat-arsenal-usa-2017-phenomenal-line-up-announced/)
 [![Blackhat Arsenal 2018](https://rawgit.com/toolswatch/badges/master/arsenal/europe/2018.svg)](http://www.toolswatch.org/2018/09/black-hat-arsenal-europe-2018-lineup-announced/)


The Universal Radio Hacker (URH) is a complete suite for wireless protocol investigation with native suppor"
chia-blockchain,"# chia-blockchain

[![Chia Network logo][logo-chia]][link-chia]

| Releases                                                                                                                                        | Repo Stats                                                                                                                                                                                                           | Socials                                                                                                                                                                                   |
| ----------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------"
dolly,"# Dolly

Databricks’ [Dolly](https://huggingface.co/databricks/dolly-v2-12b) is an instruction-following large language model trained on the Databricks machine learning platform
that is licensed for commercial use. Based on `pythia-12b`, Dolly is trained on ~15k instruction/response fine tuning records
[`databricks-dolly-15k`](https://huggingface.co/datasets/databricks/databricks-dolly-15k) generated
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. `dolly-v2-12b` is not a state-of-the-art model, but does exhibit surprisingly
high quality instruction following behavior not characteristic of the foundation model on which it is based.

Databricks is committed to ensuring that every organization and individual benefits from the transformative power of artificial intelligence. The Dolly model family represents our first steps along this journey, and we’"
DALL-E,"# Overview

[[Blog]](https://openai.com/blog/dall-e/) [[Paper]](https://arxiv.org/abs/2102.12092) [[Model Card]](model_card.md) [[Usage]](notebooks/usage.ipynb)

This is the official PyTorch package for the discrete VAE used for DALL·E. The transformer used to generate the images from the text is not part of this code release.

# Installation

Before running [the example notebook](notebooks/usage.ipynb), you will need to install the package using

	pip install DALL-E
"
social-engineer-toolkit,"# The Social-Engineer Toolkit (SET)
* Copyright :copyright: 2020
* Written by: David Kennedy (ReL1K) @HackingDave 
* Company: [TrustedSec](https://www.trustedsec.com)

<br/>

## Description
The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC – an information security consulting firm located in Cleveland, Ohio.

DISCLAIMER: This is *only* for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period.
Please read the LICENSE under readme/LICENSE for the licensing of SET. 

#### Supported platforms:
* Linux
* Mac OS X (experimental)

# Installation

## Install via requirements.txt

```bash
pip3 install -r requirements.txt
python3 setup.py 
```

## Install SET
=======
#### Mac OS X
You will need to use a virtual environment for the Python instal"
opensnitch,"<p align=""center"">
  <small>Join the project community on our server!</small>
  <br/><br/>
  <a href=""https://discord.gg/https://discord.gg/btZpkp45gQ"" target=""_blank"" title=""Join our community!"">
    <img src=""https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ""/>
  </a>
</p>
<hr/>

<p align=""center"">
  <img alt=""opensnitch"" src=""https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png"" height=""160"" />
  <p align=""center"">
    <img src=""https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"" />
    <a href=""https://github.com/evilsocket/opensnitch/releases/latest""><img alt=""Release"" src=""https://img.shields.io/github/release/evilsocket/opensnitch.svg?style=flat-square""></a>
    <a href=""https://github.com/evilsocket/opensnitch/blob/master/LICENSE.md""><img alt=""Software License"" src=""https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square""></a>
    <a href=""https://goreportcard.com/report/github.c"
faceai,"[English Doc](README_en.md)
# 功能 #

1. 人脸检测、识别（图片、视频）
2. 轮廓标识
3. 头像合成（给人戴帽子）
4. 数字化妆（画口红、眉毛、眼睛等）
5. 性别识别
6. 表情识别（生气、厌恶、恐惧、开心、难过、惊喜、平静等七种情绪）
7. 视频对象提取
8. 图片修复（可用于水印去除）
9. 图片自动上色
10. 眼动追踪（待完善）
11. 换脸（待完善）

**查看功能预览↓↓↓**

# 开发环境 #

- Windows 10（x64）
- Python 3.6.4
- OpenCV 3.4.1
- Dlib 19.8.1
- face_recognition 1.2.2
- keras 2.1.6
- tensorflow 1.8.0
- Tesseract OCR 4.0.0-beta.1


# 教程 #

[OpenCV环境搭建](doc/settingup.md)

[Tesseract OCR文字识别](doc/tesseractOCR.md)

[图片人脸检测（OpenCV版）](doc/detectionOpenCV.md)

[图片人脸检测（Dlib版）](doc/detectionDlib.md)

[视频人脸检测（OpenCV版）](doc/videoOpenCV.md)

[视频人脸检测（Dlib版）](doc/videoDlib.md)

[脸部轮廓绘制](doc/faceRecognitionOutline.md)

[数字化妆](doc/faceRecognitionMakeup.md)

[视频人脸识别](doc/faceRecognition.md)

[头像特效合成](doc/compose.md)

[性别识别](doc/gender.md)

[表情识别](doc/emotion.md)

[视频对象提取](https://github.com/vipstone/faceai/blob/master/doc/hsv-opencv.md)

[图片修复](https://github.com/vipstone/faceai/blob/master/doc/inpaint.md)


# 其他教程 #

[Ubuntu apt-get和pip源更换](doc/ubuntuChan"
MLAlgorithms,"# Machine learning algorithms
A collection of minimal and clean implementations of machine learning algorithms.

### Why?
This project is targeting people who want to learn internals of ml algorithms or implement them from scratch.  
The code is much easier to follow than the optimized libraries and easier to play with.  
All algorithms are implemented in Python, using numpy, scipy and autograd.  

### Implemented:
* [Deep learning (MLP, CNN, RNN, LSTM)](mla/neuralnet)
* [Linear regression, logistic regression](mla/linear_models.py)
* [Random Forests](mla/ensemble/random_forest.py)
* [Support vector machine (SVM) with kernels (Linear, Poly, RBF)](mla/svm)
* [K-Means](mla/kmeans.py)
* [Gaussian Mixture Model](mla/gaussian_mixture.py)
* [K-nearest neighbors](mla/knn.py)
* [Naive bayes](mla/naive_bayes.py)
* [Principal component analysis (PCA)](mla/pca.py)
* [Factorization machines](mla/fm.py)
* [Restricted Boltzmann machine (RBM)](mla/rbm.py)
* [t-Distributed Stochastic Neighbor Embeddin"
binwalk,"# *** Binwalk v2 Deprecation Notice ***

### This version of Binwalk is largely unmaintained.

### [Binwalkv3](https://github.com/ReFirmLabs/binwalk/tree/binwalkv3) is currently under active development.
### While still experimental, we recommend trying it out; issues/bug reports welcome! :)
### A fork of Binwalk v2 is still actively maintained by [OSPG](https://github.com/OSPG/binwalk).

## 
[![Build Status](https://travis-ci.org/ReFirmLabs/binwalk.svg?branch=master)](https://travis-ci.org/ReFirmLabs/binwalk)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/ReFirmLabs/binwalk/graphs/commit-activity)
[![GitHub license](https://img.shields.io/github/license/ReFirmLabs/binwalk.svg)](https://github.com/ReFirmLabs/binwalk/blob/master/LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/badges/shields.svg?style=social&label=Stars)](https://github.com/ReFirmLabs/binwalk/stargazers)

Binwalk is a fast, easy to use tool for analyzing, rever"
python-mastery,"# Advanced Python Mastery

A course by David Beazley (https://www.dabeaz.com)  
Copyright (C) 2007-2024  

## Synopsis

An exercise-driven course on Advanced Python Programming that was
battle-tested several hundred times on the corporate-training circuit
for more than a decade.  Written by David Beazley, author of the
[Python Cookbook, 3rd Edition](https://www.dabeaz.com/cookbook.html) (O'Reilly) and 
[Python Distilled](https://www.dabeaz.com/python-distilled/index.html)
(Addison-Wesley).  Released under a Creative Commons license.  Free of
ads, tracking, pop-ups, newsletters, and AI.

Everything in this course should work with the latest version of
Python, but be aware that the course primarily targets the feature set
of Python 3.6.  As such, certain modern features don't get coverage. 
Honestly, this shouldn't affect you much unless you're trying to write code
that's freakishly clever.

## Target Audience 

This course is for Python programmers who want to move beyond 
short scripts"
prowler,"<p align=""center"">
  <img align=""center"" src=""https://github.com/prowler-cloud/prowler/blob/master/docs/img/prowler-logo-black.png#gh-light-mode-only"" width=""50%"" height=""50%"">
  <img align=""center"" src=""https://github.com/prowler-cloud/prowler/blob/master/docs/img/prowler-logo-white.png#gh-dark-mode-only"" width=""50%"" height=""50%"">
</p>
<p align=""center"">
  <b><i>Prowler SaaS </b> and <b>Prowler Open Source</b> are as dynamic and adaptable as the environment they’re meant to protect. Trusted by the leaders in security.
</p>
<p align=""center"">
<b>Learn more at <a href=""https://prowler.com"">prowler.com</i></b>
</p>

<p align=""center"">
<a href=""https://join.slack.com/t/prowler-workspace/shared_invite/zt-1hix76xsl-2uq222JIXrC7Q8It~9ZNog""><img width=""30"" height=""30"" alt=""Prowler community on Slack"" src=""https://github.com/prowler-cloud/prowler/assets/38561120/3c8b4ec5-6849-41a5-b5e1-52bbb94af73a""></a>
  <br>
  <a href=""https://join.slack.com/t/prowler-workspace/shared_invite/zt-2oinmgmw6-cl"
chalice,"===========
AWS Chalice
===========

.. image:: https://badges.gitter.im/awslabs/chalice.svg
   :target: https://gitter.im/awslabs/chalice?utm_source=badge&utm_medium=badge
   :alt: Gitter
.. image:: https://readthedocs.org/projects/chalice/badge/?version=latest
   :target: http://aws.github.io/chalice/?badge=latest
   :alt: Documentation Status


.. image:: https://aws.github.io/chalice/_images/chalice-logo-whitespace.png
   :target: https://aws.github.io/chalice/
   :alt: Chalice Logo


Chalice is a framework for writing serverless apps in python. It allows
you to quickly create and deploy applications that use AWS Lambda.  It provides:

* A command line tool for creating, deploying, and managing your app
* A decorator based API for integrating with Amazon API Gateway, Amazon S3,
  Amazon SNS, Amazon SQS, and other AWS services.
* Automatic IAM policy generation


You can create Rest APIs:

.. code-block:: python

    from chalice import Chalice

    app = Chalice(app_name=""helloworl"
scientific-visualization-book,"## Scientific Visualization: Python + Matplotlib
**Nicolas P. Rougier, Bordeaux, November 2021.**  

<img src=""images/book.png"" width=""25%"" alt=""Front cover"" align=""left""/>

The Python scientific visualisation landscape is huge. It is composed of a myriad of tools, ranging from the most versatile and widely used down to the more specialised and confidential. Some of these tools are community based while others are developed by companies. Some are made specifically for the web, others are for the desktop only, some deal with 3D and large data, while others target flawless 2D rendering. In this landscape, Matplotlib has a very special place. It is a versatile and powerful library that allows you to design very high quality figures, suitable for scientific publishing. It also offers a simple and intuitive interface as well as an object oriented architecture that allows you to tweak anything within a figure. Finally, it can be used as a regular graphic library in order to design non‐scient"
scapy,"<!-- start_ppi_description -->

# <img src=""https://github.com/secdev/scapy/raw/master/doc/scapy/graphics/scapy_logo.png"" width=""64"" valign=""middle"" alt=""Scapy"" />&nbsp;&nbsp; Scapy

[![Scapy unit tests](https://github.com/secdev/scapy/actions/workflows/unittests.yml/badge.svg?branch=master&event=push)](https://github.com/secdev/scapy/actions/workflows/unittests.yml?query=event%3Apush) <!-- ignore_ppi -->
[![AppVeyor Build status](https://ci.appveyor.com/api/projects/status/os03daotfja0wtp7/branch/master?svg=true)](https://ci.appveyor.com/project/secdev/scapy/branch/master) <!-- ignore_ppi -->
[![Codecov Status](https://codecov.io/gh/secdev/scapy/branch/master/graph/badge.svg)](https://codecov.io/gh/secdev/scapy) <!-- ignore_ppi -->
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/30ee6772bb264a689a2604f5cdb0437b)](https://www.codacy.com/app/secdev/scapy) <!-- ignore_ppi -->
[![PyPI Version](https://img.shields.io/pypi/v/scapy.svg)](https://pypi.python.org/pypi/scapy/)
[![Li"
optuna,"<div align=""center""><img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" width=""800""/></div>

# Optuna: A hyperparameter optimization framework

[![Python](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)](https://www.python.org)
[![pypi](https://img.shields.io/pypi/v/optuna.svg)](https://pypi.python.org/pypi/optuna)
[![conda](https://img.shields.io/conda/vn/conda-forge/optuna.svg)](https://anaconda.org/conda-forge/optuna)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna)
[![Read the Docs](https://readthedocs.org/projects/optuna/badge/?version=stable)](https://optuna.readthedocs.io/en/stable/)
[![Codecov](https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg)](https://codecov.io/gh/optuna/optuna)

:link: [**Website**](https://optuna.org/)
| :page_with_curl: [**Docs**](https://optuna.readthedocs.io/en/stable/)
| :gear: [**"
awesome-chatgpt-zh,"# 🤖 ChatGPT 中文指南 🤖

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 
[![Code License](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/yzfly/awesome-chatgpt-zh/blob/main/LICENSE)
[![slack badge](https://img.shields.io/badge/Telegrem-join-blueviolet?logo=telegrem&amp)](https://t.me/AwesomeChatGPT)

[GitHub 持续更新，欢迎关注，欢迎 star ~](https://github.com/yzfly/awesome-chatgpt-zh)

[为方便国内访问, GitLab 镜像同步更新~](https://gitlab.com/awesomeai/awesome-chatgpt-zh)


ChatGPT 中文指南项目旨在帮助中文用户了解和使用ChatGPT。我们收集了各种免费和付费的ChatGPT资源，以及如何更有效地使用中文与 ChatGPT 进行交流的方法。我们收集了收集了ChatGPT应用开发的各种相关资源，也收集了基于 ChatGPT能力开发的生产力工具。在这个仓库中，您将找到丰富的 ChatGPT工具、应用和示例。

- [🤖 ChatGPT 中文指南 🤖](#-chatgpt-中文指南-)
  - [什么是 ChatGPT ?](#什么是-chatgpt-)
  - [ChatGPT 使用途径](#chatgpt-使用途径)
  - [与 ChatGPT 高效对话？——Prompt工程指南](#与-chatgpt-高效对话prompt工程指南)
  - [OpenAI GPTs 指南](#openai-gpts-指南)
  - [ChatGPT 顶级爆款开源项目(10K+ Stars)](#chatgpt-顶级爆款开源项目10k-stars)
  - [ChatGPT 应用](#chatgpt-应用)
  - [ChatGPT 插件](#chatgpt-插件)
  - [Chat"
howdoi,"<p align=""center"">
    <a href=""https://pypi.python.org/pypi/howdoi"">
        <img src=""https://www.dropbox.com/s/dk13iy2uoufdwr7/HowDoIcolor512.png?raw=1"" alt=""Sherlock, your neighborhood command-line sloth sleuth"" />
    </a>
</p>
<h1 align=""center"">howdoi</h1>
<h2 align=""center"">Instant coding answers via the command line</h2>
<p align=""center""><strong>⚡ Never open your browser to look for help again ⚡</strong></p>

<p align=""center"">
    <a href=""https://github.com/gleitz/howdoi/actions?query=workflow%3A%22Python+CI%22""><img src=""https://img.shields.io/github/actions/workflow/status/gleitz/howdoi/python.yml?style=plastic&color=78dce8"" alt=""build status""></a>
    <a href=""https://pepy.tech/project/howdoi""><img src=""https://img.shields.io/pypi/dm/howdoi?style=plastic&color=ab9df2&maxAge=86400&label=downloads&query=%24.total_downloads&url=https%3A%2F%2Fapi.pepy.tech%2Fapi%2Fprojects%2Fhowdoi"" alt=""downloads""></a>
    <a href=""https://pypi.python.org/pypi/howdoi""><img src=""https://img."
fsociety,"# Fsociety Hacking Tools Pack

[![Python2.7](https://img.shields.io/badge/Python-2.7-green.svg?style=flat-square)](https://www.python.org/downloads/release/python-2714/) 
![OS](https://img.shields.io/badge/Tested%20On-Linux%20|%20OSX%20|%20Windows%20|%20Android-yellowgreen.svg?style=flat-square) 
![Docker](https://img.shields.io/docker/automated/jrottenberg/ffmpeg.svg?style=flat-square) 
[![License](https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square)](https://github.com/Manisso/fsociety/blob/master/LICENSE)

A Penetration Testing Framework, you will have every script that a hacker needs. Works with Python 2. For a Python 3 version see our updated version at [`fsociety-team/fsociety`](https://github.com/fsociety-team/fsociety).

## Fsociety Contains All Tools Used in Mr. Robot Series

[![Mr. Robot](http://nikolaskama.me/content/images/2016/07/mr-robot-1.gif)](https://wikipedia.org/wiki/Mr._Robot)

## Menu

- Information Gathering
- Password Attacks
- Wireless Testing
- "
english-words,"List Of English Words
=============

A text file containing over 466k English words.

While searching for a list of english words (for an auto-complete tutorial)
I found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to [https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable](https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable) (archived).

No idea why infochimps put the word list inside an excel (.xls) file.

I pulled out the words into a simple new-line-delimited text file.
Which is more useful when building apps or importing into databases etc.

Copyright still belongs to them.

Files you may be interested in:

-  [words.txt](words.txt) contains all words.
-  [words_alpha.txt](words_alpha.txt) contains only [[:alpha:]] words (words that only have letters, no numbers or symbols). If you want a quick solution choose this"
AnimatedDrawings,"# Animated Drawings

![Sequence 02](https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif)


This repo contains an implementation of the algorithm described in the paper, [A Method for Animating Children's Drawings of the Human Figure](https://dl.acm.org/doi/10.1145/3592788).

In addition, this repo aims to be a useful creative tool in its own right, allowing you to flexibly create animations starring your own drawn characters. If you do create something fun with this, let us know! Use hashtag **#FAIRAnimatedDrawings**, or tag me on twitter: [@hjessmith](https://twitter.com/hjessmith/).

Project website: [http://www.fairanimateddrawings.com](http://www.fairanimateddrawings.com)

Video overview of [Animated Drawings OS Project](https://www.youtube.com/watch?v=WsMUKQLVsOI)


## Installation
*This project has been tested with macOS Ventura 13.2.1 and Ubuntu 18.04. If you're installing on another operating system, you may encounter issues.*

W"
ParlAI,"<p align=""center"">
 <img width=""70%"" src=""docs/source/\_static/img/parlai.png"" />
</p>

<p align=""center"">
   <a href=""https://github.com/facebookresearch/ParlAI/blob/main/LICENSE"">
    <img src=""https://img.shields.io/badge/license-MIT-blue.svg"" alt=""CircleCI"" />
  </a>
   <a href=""https://pypi.org/project/parlai/"">
    <img src=""https://img.shields.io/pypi/v/parlai?color=blue&label=release"" alt=""CircleCI"" />
  </a>
    <a href=""https://circleci.com/gh/facebookresearch/ParlAI/tree/main"">
    <img src=""https://img.shields.io/circleci/build/github/facebookresearch/ParlAI/main"" alt=""Coverage"" />
  </a>
    <a href=""https://codecov.io/gh/facebookresearch/ParlAI"">
    <img src=""https://img.shields.io/codecov/c/github/facebookresearch/ParlAI"" alt=""GitHub contributors"" />
  </a>
    <a href=""https://img.shields.io/github/contributors/facebookresearch/ParlAI"">
    <img src=""https://img.shields.io/github/contributors/facebookresearch/ParlAI""/>
  </a>
    <a href=""https://twitter.com/parlai_par"
tweepy,"Tweepy: Twitter for Python!
======

[![PyPI Version](https://img.shields.io/pypi/v/tweepy?label=PyPI)](https://pypi.org/project/tweepy/)
[![Python Versions](https://img.shields.io/pypi/pyversions/tweepy?label=Python)](https://pypi.org/project/tweepy/)
[![DOI](https://zenodo.org/badge/244025.svg)](https://zenodo.org/badge/latestdoi/244025)

[![Documentation Status](https://readthedocs.org/projects/tweepy/badge/?version=latest)](https://tweepy.readthedocs.io/en/latest/)
[![Test Status](https://github.com/tweepy/tweepy/workflows/Test/badge.svg)](https://github.com/tweepy/tweepy/actions?query=workflow%3ATest)
[![Coverage Status](https://img.shields.io/coveralls/tweepy/tweepy/master.svg?style=flat)](https://coveralls.io/github/tweepy/tweepy?branch=master)

[![Discord Server](https://discord.com/api/guilds/432685901596852224/embed.png)](https://discord.gg/bJvqnhg)

Installation
------------

The easiest way to install the latest version from PyPI is by using
[pip](https://pip.pypa.io/):

   "
github-trends,"# GitHub Trends

## SPECIAL: GitHub Wrapped

Check out your GitHub Wrapped at `githubwrapped.io`!

![github-wrapped](https://github.com/avgupta456/github-trends/assets/16708871/bf9406a4-6a49-4dbf-8f60-af221bb84bd6)

---

## What is GitHub Trends

GitHub Trends dives deep into the GitHub API to bring you exciting and impactful metrics about your code contributions. Generate insights on lines written by language, repository, and time. Easily embed dynamic images into your GitHub profile to share your statistics with the world. Check out some of the examples below:

<a href=""https://githubtrends.io"">
  <img align=""center"" src=""https://api.githubtrends.io/user/svg/avgupta456/langs?time_range=one_year&include_private=True&loc_metric=changed"" />
</a>
<a href=""https://githubtrends.io"">
  <img align=""center"" src=""https://api.githubtrends.io/user/svg/avgupta456/repos?time_range=one_year&include_private=True&group=private&loc_metric=changed"" />
</a>

## Quickstart

First, visit `https://api.gith"
MaxKB,"[English](README_EN.md) | [中文](README.md)

<p align=""center""><img src= ""https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf"" alt=""MaxKB"" width=""300"" /></p>
<h3 align=""center"">基于大语言模型和 RAG 的知识库问答系统</h3>
<p align=""center""><a href=""https://trendshift.io/repositories/9113"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/9113"" alt=""1Panel-dev%2FMaxKB | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a></p>
<p align=""center"">
  <a href=""https://www.gnu.org/licenses/gpl-3.0.html#license-text""><img src=""https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF"" alt=""License: GPL v3""></a>
  <a href=""https://app.codacy.com/gh/1Panel-dev/maxkb?utm_source=github.com&utm_medium=referral&utm_content=1Panel-dev/maxkb&utm_campaign=Badge_Grade_Dashboard""><img src=""https://app.codacy.com/project/badge/Grade/da67574fd82b473992781d1386b937ef"" alt=""Codacy""></a>
  <a href=""https://github.com/1Panel-dev/maxkb/"
LoRA,"# LoRA: Low-Rank Adaptation of Large Language Models

This repo contains the source code of the Python package `loralib` and several examples of how to integrate it with PyTorch models, such as those in Hugging Face.
We only support PyTorch for now.
See our paper for a detailed description of LoRA.

**LoRA: Low-Rank Adaptation of Large Language Models** <br>
*Edward J. Hu\*, Yelong Shen\*, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen* <br>
Paper: https://arxiv.org/abs/2106.09685 <br>
Video explainer: https://www.youtube.com/watch?v=DhRoTONcyZE <br>

*Update 2/2023: LoRA is now supported by the [State-of-the-art Parameter-Efficient Fine-Tuning (PEFT)](https://github.com/huggingface/peft) library by Hugging Face.*

LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights.
This vastly reduces the storage requirement for large language models adapted to specific tasks and enables ef"
magic-animate,"<!-- # magic-edit.github.io -->

<p align=""center"">

  <h2 align=""center"">MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</h2>
  <p align=""center"">
    <a href=""https://scholar.google.com/citations?user=-4iADzMAAAAJ&hl=en""><strong>Zhongcong Xu</strong></a>
    ·
    <a href=""http://jeff95.me/""><strong>Jianfeng Zhang</strong></a>
    ·
    <a href=""https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ&hl=en""><strong>Jun Hao Liew</strong></a>
    ·
    <a href=""https://hanshuyan.github.io/""><strong>Hanshu Yan</strong></a>
    ·
    <a href=""https://scholar.google.com/citations?user=stQQf7wAAAAJ&hl=en""><strong>Jia-Wei Liu</strong></a>
    ·
    <a href=""https://zhangchenxu528.github.io/""><strong>Chenxu Zhang</strong></a>
    ·
    <a href=""https://sites.google.com/site/jshfeng/home""><strong>Jiashi Feng</strong></a>
    ·
    <a href=""https://sites.google.com/view/showlab""><strong>Mike Zheng Shou</strong></a>
    <br>
    <br>
        <a href=""https://ar"
amazing-qr,"# Amazing-QR

[![former name](https://img.shields.io/badge/old%20name-MyQR-yellow)](https://pypi.org/project/myqr/) [![PyPI - Downloads](https://img.shields.io/pypi/dm/myqr?label=downloads@myqr)](https://pypi.org/project/myqr/) [![](https://img.shields.io/badge/language-Python-blue)](https://www.python.org/) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/amzqr?logo=python&logoColor=ffffff&label=Python&labelColor=blue&color=ffffff) [![PyPI](https://img.shields.io/pypi/v/amzqr?logo=pypi&logoColor=ffffff&label=PyPI&labelColor=blue)](https://pypi.org/project/amzqr/) ![PyPI - Wheel](https://img.shields.io/pypi/wheel/amzqr) [![PyPI - Downloads](https://img.shields.io/pypi/dm/amzqr)](https://pypi.org/project/amzqr/) [![PyPI - License](https://img.shields.io/pypi/l/amzqr)](https://github.com/x-hw/amazing-qr/blob/master/LICENSE.md) ![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/x-hw/amazing-qr) ![macos](https://img.shields.io/badge/-macOS-black?l"
Wav2Lip,"# **Wav2Lip**: *Accurately Lip-syncing Videos In The Wild* 
### Wav2Lip is hosted for free at [Sync Labs](https://sync.so/)
Are you looking to integrate this into a product? We have a turn-key hosted API with new and improved lip-syncing models here: https://sync.so/
For any other commercial / enterprise requests, please contact us at pavan@synclabs.so and prady@sync.so
To reach out to the authors directly you can reach us at prajwal@synclabs.so, rudrabha@sync.so.
This code is part of the paper: _A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild_ published at ACM Multimedia 2020. 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs2)](https://paperswithcode.com/sota/lip-sync-on-lrs2?p=a-lip-sync-expert-is-all-you-need-for-speech)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs3)]("
AnimateDiff,"# AnimateDiff

This repository is the official implementation of [AnimateDiff](https://arxiv.org/abs/2307.04725) [ICLR2024 Spotlight].
It is a plug-and-play module turning most community text-to-image models into animation generators, without the need of additional training.

**[AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725)** 
</br>
[Yuwei Guo](https://guoyww.github.io/),
[Ceyuan Yang✝](https://ceyuan.me/),
[Anyi Rao](https://anyirao.com/),
[Zhengyang Liang](https://maxleung99.github.io/),
[Yaohui Wang](https://wyhsirius.github.io/),
[Yu Qiao](https://scholar.google.com.hk/citations?user=gFtI-8QAAAAJ),
[Maneesh Agrawala](https://graphics.stanford.edu/~maneesh/),
[Dahua Lin](http://dahua.site),
[Bo Dai](https://daibo.info)
(✝Corresponding Author)  
[![arXiv](https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg)](https://arxiv.org/abs/2307.04725)
[![Project Page](https://img.shields.io/badge/Project-We"
mvt,"<p align=""center"">
     <img src=""https://docs.mvt.re/en/latest/mvt.png"" width=""200"" />
</p>

# Mobile Verification Toolkit

[![](https://img.shields.io/pypi/v/mvt)](https://pypi.org/project/mvt/)
[![Documentation Status](https://readthedocs.org/projects/mvt/badge/?version=latest)](https://docs.mvt.re/en/latest/?badge=latest)
[![CI](https://github.com/mvt-project/mvt/actions/workflows/python-package.yml/badge.svg)](https://github.com/mvt-project/mvt/actions/workflows/python-package.yml)
[![Downloads](https://pepy.tech/badge/mvt)](https://pepy.tech/project/mvt)

Mobile Verification Toolkit (MVT) is a collection of utilities to simplify and automate the process of gathering forensic traces helpful to identify a potential compromise of Android and iOS devices.

It has been developed and released by the [Amnesty International Security Lab](https://securitylab.amnesty.org) in July 2021 in the context of the [Pegasus Project](https://forbiddenstories.org/about-the-pegasus-project/) along wit"
stanford-tensorflow-tutorials,"[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Join the https://gitter.im/stanford-tensorflow-tutorials](https://badges.gitter.im/tflearn/tflearn.svg)](https://gitter.im/stanford-tensorflow-tutorials)

# stanford-tensorflow-tutorials
This repository contains code examples for the course CS 20: TensorFlow for Deep Learning Research. <br>
It will be updated as the class progresses. <br>
Detailed syllabus and lecture notes can be found [here](http://cs20.stanford.edu).<br>
For this course, I use python3.6 and TensorFlow 1.4.1.

For the code and notes of the previous year's course, please see the folder 2017 and the website https://web.stanford.edu/class/cs20si/2017

For setup instruction and the list of dependencies, please see the setup folder of this repository."
danswer,"<!-- DANSWER_METADATA={""link"": ""https://github.com/danswer-ai/danswer/blob/main/README.md""} -->

<h2 align=""center"">
<a href=""https://www.danswer.ai/""> <img width=""50%"" src=""https://github.com/danswer-owners/danswer/blob/1fabd9372d66cd54238847197c33f091a724803b/DanswerWithName.png?raw=true)"" /></a>
</h2>

<p align=""center"">
<p align=""center"">Open Source Gen-AI Chat + Unified Search.</p>

<p align=""center"">
<a href=""https://docs.danswer.dev/"" target=""_blank"">
    <img src=""https://img.shields.io/badge/docs-view-blue"" alt=""Documentation"">
</a>
<a href=""https://join.slack.com/t/danswer/shared_invite/zt-2lcmqw703-071hBuZBfNEOGUsLa5PXvQ"" target=""_blank"">
    <img src=""https://img.shields.io/badge/slack-join-blue.svg?logo=slack"" alt=""Slack"">
</a>
<a href=""https://discord.gg/TDJ59cGV2X"" target=""_blank"">
    <img src=""https://img.shields.io/badge/discord-join-blue.svg?logo=discord&logoColor=white"" alt=""Discord"">
</a>
<a href=""https://github.com/danswer-ai/danswer/blob/main/README.md"" target=""_"
pytorch-grad-cam,"[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Build Status](https://github.com/jacobgil/pytorch-grad-cam/workflows/Tests/badge.svg)
[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=Monthly%20Downloads)](https://pepy.tech/project/grad-cam)
[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=total&units=international_system&left_color=black&right_color=blue&left_text=Total%20Downloads)](https://pepy.tech/project/grad-cam)

# Advanced AI explainability for PyTorch

`pip install grad-cam`

Documentation with advanced tutorials: [https://jacobgil.github.io/pytorch-gradcam-book](https://jacobgil.github.io/pytorch-gradcam-book)


This is a package with state of the art methods for Explainable AI for computer vision.
This can be used for diagnosing model predictions, either in production or while
devel"
jinja,"# Jinja

Jinja is a fast, expressive, extensible templating engine. Special
placeholders in the template allow writing code similar to Python
syntax. Then the template is passed data to render the final document.

It includes:

-   Template inheritance and inclusion.
-   Define and import macros within templates.
-   HTML templates can use autoescaping to prevent XSS from untrusted
    user input.
-   A sandboxed environment can safely render untrusted templates.
-   AsyncIO support for generating templates and calling async
    functions.
-   I18N support with Babel.
-   Templates are compiled to optimized Python code just-in-time and
    cached, or can be compiled ahead-of-time.
-   Exceptions point to the correct line in templates to make debugging
    easier.
-   Extensible filters, tests, functions, and even syntax.

Jinja's philosophy is that while application logic belongs in Python if
possible, it shouldn't make the template designer's job difficult by
restricting functionality"
HivisionIDPhotos,"<div align=""center"">

<img alt=""hivision_logo"" src=""assets/hivision_logo.png"" width=120 height=120>
<h1>HivisionIDPhoto</h1>

[English](README_EN.md) / 中文 / [日本語](README_JP.md) / [한국어](README_KO.md)

[![][release-shield]][release-link]
[![][dockerhub-shield]][dockerhub-link]
[![][github-stars-shield]][github-stars-link]
[![][github-issues-shield]][github-issues-link]
[![][github-contributors-shield]][github-contributors-link]
[![][github-forks-shield]][github-forks-link]
[![][license-shield]][license-link]  
[![][wechat-shield]][wechat-link]
[![][spaces-shield]][spaces-link]
[![][swanhub-demo-shield]][swanhub-demo-link]
[![][modelscope-shield]][modelscope-link]

[![][trendshift-shield]][trendshift-link]
[![][hellogithub-shield]][hellogithub-link]

<img src=""assets/demoImage.jpg"" width=900>

👋 加入我们的[微信群][wechat-link]

</div>

> **相关项目**：
>
> - [SwanLab](https://github.com/SwanHubX/SwanLab)：训练人像抠图模型全程用它来分析和监控，以及和实验室同学协作交流，大幅提升了训练效率。


<br>

# 目录

- [最近更新](#-最近更新)
- [项目简介](#-项目简介)
- [社区]("
psutil,"|  |downloads| |stars| |forks| |contributors| |coverage|
|  |version| |py-versions| |packages| |license|
|  |github-actions-wheels|  |github-actions-bsd| |appveyor| |doc| |twitter| |tidelift|

.. |downloads| image:: https://img.shields.io/pypi/dm/psutil.svg
    :target: https://pepy.tech/project/psutil
    :alt: Downloads

.. |stars| image:: https://img.shields.io/github/stars/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/stargazers
    :alt: Github stars

.. |forks| image:: https://img.shields.io/github/forks/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/network/members
    :alt: Github forks

.. |contributors| image:: https://img.shields.io/github/contributors/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/graphs/contributors
    :alt: Contributors

.. |github-actions-wheels| image:: https://img.shields.io/github/actions/workflow/status/giampaolo/psutil/.github/workflows/build.yml.svg?label=Linux%2C%20macOS%2C%20W"
pyautogui,"PyAutoGUI
=========

PyAutoGUI is a  cross-platform GUI automation Python module for human beings. Used to programmatically control the mouse & keyboard.

`pip install pyautogui`

Full documentation available at https://pyautogui.readthedocs.org

Simplified Chinese documentation available at https://github.com/asweigart/pyautogui/blob/master/docs/simplified-chinese.ipynb

Source code available at https://github.com/asweigart/pyautogui

If you need help installing Python, visit https://installpython3.com/

Dependencies
============

PyAutoGUI supports Python 2 and 3. If you are installing PyAutoGUI from PyPI using pip:

Windows has no dependencies. The Win32 extensions do not need to be installed.

macOS needs the pyobjc-core and pyobjc module installed (in that order).

Linux needs the python3-xlib (or python-xlib for Python 2) module installed.

Pillow needs to be installed, and on Linux you may need to install additional libraries to make sure Pillow's PNG/JPEG works correctly. See:
"
q,"[![Build and Package](https://github.com/harelba/q/workflows/BuildAndPackage/badge.svg?branch=master)](https://github.com/harelba/q/actions?query=branch%3Amaster)

# q - Text as Data
q's purpose is to bring SQL expressive power to the Linux command line and to provide easy access to text as actual data.

q allows the following:

* Performing SQL-like statements directly on tabular text data, auto-caching the data in order to accelerate additional querying on the same file. 
* Performing SQL statements directly on multi-file sqlite3 databases, without having to merge them or load them into memory

The following table shows the impact of using caching:

|    Rows   | Columns | File Size | Query time without caching | Query time with caching | Speed Improvement |
|:---------:|:-------:|:---------:|:--------------------------:|:-----------------------:|:-----------------:|
| 5,000,000 |   100   |   4.8GB   |    4 minutes, 47 seconds   |       1.92 seconds      |        x149       |
| 1,000"
yolov3,"<div align=""center"">
  <p>
    <a align=""center"" href=""https://ultralytics.com/yolov3"" target=""_blank"">
      <img width=""100%"" src=""https://raw.githubusercontent.com/ultralytics/assets/main/yolov3/banner-yolov3.png""></a>
  </p>

[中文](https://docs.ultralytics.com/zh) | [한국어](https://docs.ultralytics.com/ko) | [日本語](https://docs.ultralytics.com/ja) | [Русский](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [Français](https://docs.ultralytics.com/fr) | [Español](https://docs.ultralytics.com/es) | [Português](https://docs.ultralytics.com/pt) | [Türkçe](https://docs.ultralytics.com/tr) | [Tiếng Việt](https://docs.ultralytics.com/vi) | [العربية](https://docs.ultralytics.com/ar)

<div>
    <a href=""https://github.com/ultralytics/yolov3/actions/workflows/ci-testing.yml""><img src=""https://github.com/ultralytics/yolov3/actions/workflows/ci-testing.yml/badge.svg"" alt=""YOLOv3 CI""></a>
    <a href=""https://zenodo.org/badge/latestdoi/264818686""><img src=""https://zen"
pipx,"<p align=""center"">
<a href=""https://pipx.pypa.io"">
<img align=""center"" src=""https://github.com/pypa/pipx/raw/main/logo.svg"" width=""200""/>
</a>
</p>

# pipx — Install and Run Python Applications in Isolated Environments

<p align=""center"">
<a href=""https://github.com/pypa/pipx/raw/main/pipx_demo.gif"">
<img src=""https://github.com/pypa/pipx/raw/main/pipx_demo.gif""/>
</a>
</p>

<p align=""center"">
<a href=""https://github.com/pypa/pipx/actions"">
<img src=""https://github.com/pypa/pipx/workflows/tests/badge.svg?branch=main"" alt=""image"" /></a> <a href=""https://badge.fury.io/py/pipx""><img src=""https://badge.fury.io/py/pipx.svg"" alt=""PyPI version""></a> <a href=""https://badge.fury.io/py/pipx""><img src=""https://static.pepy.tech/badge/pipx""></a>

</p>

**Documentation**: <https://pipx.pypa.io>

**Source Code**: <https://github.com/pypa/pipx>

_For comparison to other tools including pipsi, see
[Comparison to Other Tools](https://pipx.pypa.io/stable/comparisons/)._

## Install pipx

> [!WARNING]
>
>"
eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee,"# eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
![eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee](https://img.shields.io/badge/eeeeee-eeeeee-eeeeee.svg)
eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"
django-cms,"##########
django CMS
##########
.. image:: https://img.shields.io/pypi/v/django-cms.svg
    :target: https://pypi.python.org/pypi/django-cms/
.. image:: https://img.shields.io/badge/wheel-yes-green.svg
    :target: https://pypi.python.org/pypi/django-cms/
.. image:: https://img.shields.io/pypi/l/django-cms.svg
    :target: https://pypi.python.org/pypi/django-cms/
.. image:: https://codeclimate.com/github/divio/django-cms/badges/gpa.svg
   :target: https://codeclimate.com/github/divio/django-cms
   :alt: Code Climate

Open source enterprise content management system based on the Django framework and backed by the non-profit django CMS Association (`Sponsor us! <https://www.django-cms.org/en/memberships/>`_).

*******************************************
Contribute to this project and win rewards
*******************************************

Because django CMS is a community-driven project, we welcome everyone to `get involved in the project <https://www.django-cms.org/en/contribute/>`_. "
maigret,"# Maigret

<p align=""center"">
  <p align=""center"">
    <a href=""https://pypi.org/project/maigret/"">
      <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/maigret?style=flat-square"">
    </a>
    <a href=""https://pypi.org/project/maigret/"">
      <img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dw/maigret?style=flat-square"">
    </a>
    <a href=""https://pypi.org/project/maigret/"">
      <img alt=""Views"" src=""https://komarev.com/ghpvc/?username=maigret&color=brightgreen&label=views&style=flat-square"">
    </a>
  </p>
  <p align=""center"">
    <img src=""https://raw.githubusercontent.com/soxoj/maigret/main/static/maigret.png"" height=""200""/>
  </p>
</p>

<i>The Commissioner Jules Maigret is a fictional French police detective, created by Georges Simenon. His investigation method is based on understanding the personality of different people and their interactions.</i>

<b>👉👉👉 [Online Telegram bot](https://t.me/osint_maigret_bot)</b>

## About

**Maigret** collects a dossier on"
MoneyPrinter,"# MoneyPrinter 💸

Automate the creation of YouTube Shorts, simply by providing a video topic to talk about.

<a href=""https://trendshift.io/repositories/7545"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/7545"" alt=""FujiwaraChoki%2FMoneyPrinter | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

> **Important** Please make sure you look through existing/closed issues before opening your own. If it's just a question, please join our [discord](https://dsc.gg/fuji-community) and ask there.

> **🎥** Watch the video on [YouTube](https://youtu.be/mkZsaDA2JnA?si=pNne3MnluRVkWQbE).

Check out the instructions for the local version [here](Local.md).

## FAQ 🤔

### How do I get the TikTok session ID?

You can obtain your TikTok session ID by logging into TikTok in your browser and copying the value of the `sessionid` cookie.

### My ImageMagick binary is not being detected

Make sure you set your path to the ImageMagick binary correctly in th"
Megatron-LM,"<div align=""center"">

Megatron-LM & Megatron-Core
===========================
<h4>GPU optimized techniques for training transformer models at-scale</h4>

[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html)
[![version](https://img.shields.io/badge/release-0.5.0-green)](./setup.py)
[![license](https://img.shields.io/badge/license-OpenBSD-blue)](./LICENSE)

<div align=""left"">

# Latest News

- **[2024/7]** Megatron-Core v0.7 improves scalability and training resiliency and adds support for multimodal training ([blog](https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-megatron-core-functionalities/)). 
- **[2024/6]** Megatron-Core added supports for Mamba-based models. Check out our paper [An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887) and [code example](https://github.com/NVIDIA/Megatron-LM/tree/ssm/ex"
bpytop,"# ![bpytop](https://github.com/aristocratos/bpytop/raw/master/Imgs/logo.png)

<a href=""https://repology.org/project/bpytop/versions"">
    <img src=""https://repology.org/badge/vertical-allrepos/bpytop.svg"" alt=""Packaging status"" align=""right"">
</a>

![Linux](https://img.shields.io/badge/-Linux-grey?logo=linux)
![OSX](https://img.shields.io/badge/-OSX-black?logo=apple)
![FreeBSD](https://img.shields.io/badge/-FreeBSD-red?logo=freebsd)
![Usage](https://img.shields.io/badge/Usage-System%20resource%20monitor-yellow)
![Python](https://img.shields.io/badge/Python-v3.7%5E-green?logo=python)
![bpytop_version](https://img.shields.io/github/v/tag/aristocratos/bpytop?label=version)
[![pypi_version](https://img.shields.io/pypi/v/bpytop?label=pypi)](https://pypi.org/project/bpytop)
[![Test Status](https://img.shields.io/github/workflow/status/aristocratos/bpytop/Testing?label=tests)](https://github.com/aristocratos/bpytop/actions?query=workflow%3Atesting)
[![Donate](https://img.shields.io/badge/-Don"
pyvideotrans,"简体中文 | [English](docs/EN/README_EN.md) | [pt-BR](docs/pt-BR/README_pt-BR.md) | [Italian](docs/IT/README_IT.md) | [Spanish](docs/ES/README_ES.md) / [捐助](docs/about.md) / [Discord](https://discord.gg/y9gUweVCCJ) / 微信公众号：`pyvideotrans`

# 视频翻译配音工具

这是一个视频翻译配音工具，可将一种语言的视频翻译为指定语言的视频，自动生成和添加该语言的字幕和配音。并支持API调用


语音识别支持 `faster-whisper`和`openai-whisper`本地离线模型 及 `OpenAI SpeechToText API`  `GoogleSpeech` `阿里中文语音识别模型`和豆包模型，并支持自定义语音识别api.

文字翻译支持 `微软翻译|Google翻译|百度翻译|腾讯翻译|ChatGPT|AzureAI|Gemini|DeepL|DeepLX|字节火山|离线翻译OTT`

文字合成语音支持 `Microsoft Edge tts` `Google tts` `Azure AI TTS` `Openai TTS` `Elevenlabs TTS` `自定义TTS服务器api` `GPT-SoVITS` [clone-voice](https://github.com/jianchang512/clone-voice)  [ChatTTS-ui](https://github.com/jianchang512/ChatTTS-ui)  [Fish TTS](https://github.com/fishaudio/fish-speech)  [CosyVoice](https://github.com/FunAudioLLM/CosyVoice)

允许保留背景伴奏音乐等(基于uvr5)

支持的语言：中文简繁、英语、韩语、日语、俄语、法语、德语、意大利语、西班牙语、葡萄牙语、越南语、泰国语、阿拉伯语、土耳其语、匈牙利语、印度语、乌克兰语、哈萨克语、印尼语、马来语、捷克语、波兰语、荷兰语、瑞典语


> **[赞助商]**
>
"
word_cloud,"[![licence](http://img.shields.io/badge/licence-MIT-blue.svg?style=flat)](https://github.com/amueller/word_cloud/blob/master/LICENSE)
[![DOI](https://zenodo.org/badge/21369/amueller/word_cloud.svg)](https://zenodo.org/badge/latestdoi/21369/amueller/word_cloud)


word_cloud
==========

A little word cloud generator in Python. Read more about it on the [blog
post][blog-post] or the [website][website].

The code is tested against Python 3.7, 3.8, 3.9, 3.10, 3.11, 3.12.

## Installation

If you are using pip:

    pip install wordcloud

If you are using conda, you can install from the `conda-forge` channel:

    conda install -c conda-forge wordcloud


#### Installation notes

wordcloud depends on `numpy`, `pillow`, and `matplotlib`.

If there are no wheels available for your version of python, installing the
package requires having a C compiler set up. Before installing a compiler, report
an issue describing the version of python and operating system being used.


## Examples

Check out ["
starlette,"<p align=""center"">
  <a href=""https://www.starlette.io/""><img width=""420px"" src=""https://raw.githubusercontent.com/encode/starlette/master/docs/img/starlette.svg"" alt='starlette'></a>
</p>
<p align=""center"">
    <em>✨ The little ASGI framework that shines. ✨</em>
</p>

---

[![Build Status](https://github.com/encode/starlette/workflows/Test%20Suite/badge.svg)](https://github.com/encode/starlette/actions)
[![Package version](https://badge.fury.io/py/starlette.svg)](https://pypi.python.org/pypi/starlette)
[![Supported Python Version](https://img.shields.io/pypi/pyversions/starlette.svg?color=%2334D058)](https://pypi.org/project/starlette)

---

**Documentation**: <a href=""https://www.starlette.io/"" target=""_blank"">https://www.starlette.io</a>

**Source Code**: <a href=""https://github.com/encode/starlette"" target=""_blank"">https://github.com/encode/starlette</a>

---

# Starlette

Starlette is a lightweight [ASGI][asgi] framework/toolkit,
which is ideal for building async web services in P"
LLMSurvey,"# LLMSurvey


> A collection of papers and resources related to Large Language Models. 
>
> The organization of papers refers to our survey [**""A Survey of Large Language Models""**](https://arxiv.org/abs/2303.18223). [![Paper page](https://huggingface.co/datasets/huggingface/badges/raw/main/paper-page-sm-dark.svg)](https://huggingface.co/papers/2303.18223)
>
> Please let us know if you find out a mistake or have any suggestions by e-mail: batmanfly@gmail.com
>
> (we suggest ccing another email francis_kun_zhou@163.com meanwhile, in case of any unsuccessful delivery issue.)
>
>
> If you find our survey useful for your research, please cite the following paper:

```
@article{LLMSurvey,
    title={A Survey of Large Language Models},
    author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao "
the-algorithm-ml,"This project open sources some of the ML models used at Twitter.

Currently these are:

1. The ""For You"" Heavy Ranker (projects/home/recap).

2. TwHIN embeddings (projects/twhin) https://arxiv.org/abs/2202.05387


This project can be run inside a python virtualenv. We have only tried this on Linux machines and because we use torchrec it works best with an Nvidia GPU. To setup run

`./images/init_venv.sh` (Linux only).

The READMEs of each project contain instructions about how to run each project.
"
thumbor,"<h4 align=""center"">Join <a href=""https://github.com/thumbor/thumbor-bootcamp"">thumbor-bootcamp</a> for a learning and contribution experience with ❤️ and 🤗 from the thumbor team</h4>

<p align=""center"">
  <a href=""http://www.thumbor.org"">
    <img title=""thumbor"" alt=""thumbor"" src=""https://github.com/thumbor/thumbor/blob/readme/docs/thumbor-logo.png?raw=true"" />
  </a>
</p>

<h3 align=""center"">
Crop, resize, transform and much more, all on-demand and AI Powered
</h3>

<p align=""center"">
  <img src='https://github.com/thumbor/thumbor/workflows/build/badge.svg' />
  <a href='https://coveralls.io/github/thumbor/thumbor?branch=master' target='_blank'>
    <img src='https://coveralls.io/repos/thumbor/thumbor/badge.svg?branch=master&service=github'/>
  </a>
  <a href='https://codeclimate.com/github/thumbor/thumbor' target='_blank'>
    <img src='https://codeclimate.com/github/thumbor/thumbor/badges/gpa.svg'/>
  </a>
  <a href='https://pypi.python.org/pypi/thumbor' target='_blank'>
    <img s"
video2x,"<p align=""center"">
   <img src=""https://user-images.githubusercontent.com/21986859/102733190-872a7880-4334-11eb-8e9e-0ca747f130b1.png""/>
   </br>
   <img src=""https://img.shields.io/github/v/release/k4yt3x/video2x?style=flat-square""/>
   <img src=""https://img.shields.io/github/actions/workflow/status/k4yt3x/video2x/ci.yml?label=CI&style=flat-square""/>
   <img src=""https://img.shields.io/github/downloads/k4yt3x/video2x/total?style=flat-square""/>
   <img src=""https://img.shields.io/github/license/k4yt3x/video2x?style=flat-square""/>
   <img src=""https://img.shields.io/badge/dynamic/json?color=%23e85b46&label=Patreon&query=data.attributes.patron_count&suffix=%20patrons&url=https%3A%2F%2Fwww.patreon.com%2Fapi%2Fcampaigns%2F4507807&style=flat-square""/>
</p>

## [💬 Telegram Discussion Group](https://t.me/video2x)

Join our Telegram discussion group to ask any questions you have about Video2X, chat directly with the developers, or discuss about upscaling technologies and the future of Video2X "
statsmodels,".. image:: docs/source/images/statsmodels-logo-v2-horizontal.svg
  :alt: Statsmodels logo

|PyPI Version| |Conda Version| |License| |Azure CI Build Status|
|Codecov Coverage| |Coveralls Coverage| |PyPI downloads| |Conda downloads|

About statsmodels
=================

statsmodels is a Python package that provides a complement to scipy for
statistical computations including descriptive statistics and estimation
and inference for statistical models.


Documentation
=============

The documentation for the latest release is at

https://www.statsmodels.org/stable/

The documentation for the development version is at

https://www.statsmodels.org/dev/

Recent improvements are highlighted in the release notes

https://www.statsmodels.org/stable/release/

Backups of documentation are available at https://statsmodels.github.io/stable/
and https://statsmodels.github.io/dev/.


Main Features
=============

* Linear regression models:

  - Ordinary least squares
  - Generalized least squares
  - W"
spinningup,"**Status:** Maintenance (expect bug fixes and minor updates)

Welcome to Spinning Up in Deep RL! 
==================================

This is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).

For the unfamiliar: [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with [deep learning](http://ufldl.stanford.edu/tutorial/).

This module contains a variety of helpful resources, including:

- a short [introduction](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) to RL terminology, kinds of algorithms, and basic theory,
- an [essay](https://spinningup.openai.com/en/latest/spinningup/spinningup.html) about how to grow into an RL research role,
- a [curated list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) of important papers "
litgpt,"<div align=""center"">


# ⚡ LitGPT

**20+ high-performance LLMs with recipes to pretrain, finetune, and deploy at scale.**

<pre>
✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   
✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter
✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            
</pre>


---


![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)
![cpu-tests](https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg) [![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/lit-stablelm/blob/master/LICENSE) [![Discord](https://img.shields.io/discord/1077906959069626439)](https://discord.gg/VptPCZkGNa)

<p align=""center"">
  <a href=""#quick-start"">Quick start</a> •
  <a href=""#choose-from-20-llms"">Models</a> •
  <a href=""#finetune-an-llm"">Finetune</a> • 
  <a href=""#deploy-an-llm"">Deploy</a> •    
  "
AudioGPT,"# AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head

[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2304.12995)
[![GitHub Stars](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social)](https://github.com/AIGC-Audio/AudioGPT)
![visitors](https://visitor-badge.glitch.me/badge?page_id=AIGC-Audio.AudioGPT)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/AIGC-Audio/AudioGPT)


We provide our implementation and pretrained models as open source in this repository.


## Get Started

Please refer to [run.md](run.md)


## Capabilities

Here we list the capability of AudioGPT at this time. More supported models and tasks are coming soon. For prompt examples, refer to [asset](assets/README.md).

Currently not every model has repository.
### Speech
|            Task            |   Supported Foundation Models   | Status |
|:--------------------------:|:-"
visdom,"

<h3 align=""center"">
    <br/>
    <img src=""https://user-images.githubusercontent.com/19650074/198746195-574bb828-026f-41cb-82a9-250fcbc4e090.png"" width=""300"" alt=""Logo""/><br/><br/>
    Creating, organizing & sharing visualizations of live, rich data. Supports <a href=""https://pypi.org/project/visdom/"">Python</a>.
</h3>


<p align=""center""> Jump To: <a href=""#setup"">Setup</a>, <a href=""#usage"">Usage</a>, <a href=""#api"">API</a>, <a href=""#customizing-visdom"">Customizing</a>, <a href=""#contributing"">Contributing</a>, <a href=""#license"">License</a>
</p>


<p align=""center"">
    <a href=""https://github.com/fossasia/visdom/releases""><img src=""https://img.shields.io/github/v/release/fossasia/visdom?colorA=363a4f&colorB=a6da95&style=for-the-badge""/></a>
    <a href=""https://pypi.org/project/visdom""><img src=""https://img.shields.io/pypi/dd/visdom?colorA=363a4f&colorB=156df1&style=for-the-badge""></a>
    <a href=""https://github.com/fossasia/visdom/commits""><img src=""https://img.shields.io/git"
EverydayWechat,![python_vesion](https://img.shields.io/badge/Python-3.5%2B-green.svg)   [![itchat_vesion](https://img.shields.io/badge/Itchat-1.3.10-brightgreen.svg)](https://github.com/littlecodersh/ItChat)   [![codebeat badge](https://codebeat.co/badges/0953014f-dbd3-41f4-bacd-60018e7d5065)](https://codebeat.co/projects/github-com-sfyc23-everydaywechat-master)   [![Codacy Badge](https://api.codacy.com/project/badge/Grade/a278078ba9a14e22bd86740b0807a78e)](https://www.codacy.com/app/sfyc23/EverydayWechat?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=sfyc23/EverydayWechat&amp;utm_campaign=Badge_Grade)   [![MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/sfyc23/EverydayWechat/blob/master/LICENSE)               [![weibo](https://img.shields.io/badge/weibo-@sfyc23-red.svg)](https://www.weibo.com/sfyc23)  [![GitHub issues](https://img.shields.io/github/issues/sfyc23/EverydayWechat.svg)](https://github.com/sfyc23/EverydayWechat/issues)  [![GitHub contributors](h
streamlink,"<h1 align=""center""><a href=""https://streamlink.github.io/"">Streamlink<br><img height=""150"" alt=""Streamlink"" src=""https://raw.githubusercontent.com/streamlink/streamlink/master/icon.svg""></a></h1>

<p align=""center"">
  <a href=""https://streamlink.github.io/install.html""><img alt=""Supported Python versions"" src=""https://img.shields.io/pypi/pyversions/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://streamlink.github.io/changelog.html""><img alt=""Latest release"" src=""https://img.shields.io/github/release/streamlink/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://github.com/streamlink/streamlink""><img alt=""License"" src=""https://img.shields.io/github/license/streamlink/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://github.com/streamlink/streamlink/issues""><img alt=""Open issues"" src=""https://img.shields.io/github/issues/streamlink/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://github.com/streamlink/st"
eat_tensorflow2_in_30_days,"# How to eat TensorFlow2 in 30 days ?🔥🔥

Click here for [Chinese Version（中文版）](#30天吃掉那只-tensorflow2)

**《10天吃掉那只pyspark》**
* 🚀 github项目地址: https://github.com/lyhue1991/eat_pyspark_in_10_days
* 🐳 和鲸专栏地址: https://www.heywhale.com/home/column/5fe6aa955e24ed00302304e0 【代码可直接fork后云端运行，无需配置环境】


**《20天吃掉那只Pytorch》**
* 🚀 github项目地址: https://github.com/lyhue1991/eat_pytorch_in_20_days
* 🐳 和鲸专栏地址: https://www.heywhale.com/home/column/5f2ac5d8af3980002cb1bc08 【代码可直接fork后云端运行，无需配置环境】


**《30天吃掉那只TensorFlow2》**
* 🚀 github项目地址: https://github.com/lyhue1991/eat_tensorflow2_in_30_days
* 🐳 和鲸专栏地址: https://www.heywhale.com/home/column/5d8ef3c3037db3002d3aa3a0 【代码可直接fork后云端运行，无需配置环境】

**极速通道** 
*  🚀 公众号 “**算法美食屋**” 后台回复暗号：""**吃货来了**""
*  😋 获取以上3套教程的jupyter notebook 源码文件以及全部数据集的百度云盘下载链接。
*   https://mp.weixin.qq.com/s/ymLtH5BqlWAkpOmCLQOYxw 


### 1. TensorFlow2 🍎 or Pytorch🔥

Conclusion first: 

**For the engineers, priority goes to TensorFlow2.**

**For the students and researchers，first choice should be"
surya,"# Surya

Surya is a document OCR toolkit that does:

- OCR in 90+ languages that benchmarks favorably vs cloud services
- Line-level text detection in any language
- Layout analysis (table, image, header, etc detection)
- Reading order detection

It works on a range of documents (see [usage](#usage) and [benchmarks](#benchmarks) for more details).

|                            Detection                             |                                   OCR                                   |
|:----------------------------------------------------------------:|:-----------------------------------------------------------------------:|
|  ![New York Times Article Detection](static/images/excerpt.png)  |  ![New York Times Article Recognition](static/images/excerpt_text.png)  |

|                               Layout                               |                               Reading Order                                |
|:------------------------------------------------------------------:|:"
ffmpeg-python,"# ffmpeg-python: Python bindings for FFmpeg

[![CI][ci-badge]][ci]

[ci-badge]: https://github.com/kkroening/ffmpeg-python/actions/workflows/ci.yml/badge.svg
[ci]: https://github.com/kkroening/ffmpeg-python/actions/workflows/ci.yml

<img src=""https://raw.githubusercontent.com/kkroening/ffmpeg-python/master/doc/formula.png"" alt=""ffmpeg-python logo"" width=""60%"" />

## Overview

There are tons of Python FFmpeg wrappers out there but they seem to lack complex filter support.  `ffmpeg-python` works well for simple as well as complex signal graphs.


## Quickstart

Flip a video horizontally:
```python
import ffmpeg
stream = ffmpeg.input('input.mp4')
stream = ffmpeg.hflip(stream)
stream = ffmpeg.output(stream, 'output.mp4')
ffmpeg.run(stream)
```

Or if you prefer a fluent interface:
```python
import ffmpeg
(
    ffmpeg
    .input('input.mp4')
    .hflip()
    .output('output.mp4')
    .run()
)
```

## [API reference](https://kkroening.github.io/ffmpeg-python/)

## Complex filter graphs
FFmpe"
Theano,"============================================================================================================
MILA has stopped developing Theano: https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ

The PyMC developers have forked Theano to a new project called PyTensor that is being actively developed: https://github.com/pymc-devs/pytensor
============================================================================================================
"
neural-doodle,"Neural Doodle
=============

.. image:: docs/Workflow.gif

Use a deep neural network to borrow the skills of real artists and turn your two-bit doodles into masterpieces! This project is an implementation of `Semantic Style Transfer <http://arxiv.org/abs/1603.01768>`_ (Champandard, 2016), based on the `Neural Patches <http://arxiv.org/abs/1601.04589>`_ algorithm (Li, 2016). Read more about the motivation in this `in-depth article <https://nucl.ai/blog/neural-doodles/>`_ and watch this `workflow video <https://www.youtube.com/watch?v=fu2fzx4w3mI>`_ for inspiration.

The ``doodle.py`` script generates a new image by using one, two, three or four images as inputs depending what you're trying to do: the original style and its annotation, and a target content image (optional) with its annotation (a.k.a. your doodle). The algorithm extracts annotated patches from the style image, and incrementally transfers them over to the target image based on how closely they match.

**NOTE**: Making a ``"
open_clip,"# OpenCLIP

[[Paper]](https://arxiv.org/abs/2212.07143) [[Citations]](#citing) [[Clip Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb) [[Coca Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)
[![pypi](https://img.shields.io/pypi/v/open_clip_torch.svg)](https://pypi.python.org/pypi/open_clip_torch)

Welcome to an open source implementation of OpenAI's [CLIP](https://arxiv.org/abs/2103.00020) (Contrastive Language-Image Pre-training).

Using this codebase, we have trained several models on a variety of data sources and compute budgets, ranging from [small-scale experiments](docs/LOW_ACC.md) to larger runs including models trained on datasets such as [LAION-400M](https://arxiv.org/abs/2111.02114), [LAION-2B](https://arxiv.org/abs/2210.08402) and [DataComp-1B](https://arxiv.org/abs/2304.14108).
Many of our models and their scaling properties a"
node-gyp,"# `node-gyp` - Node.js native addon build tool

[![Build Status](https://github.com/nodejs/node-gyp/workflows/Tests/badge.svg?branch=main)](https://github.com/nodejs/node-gyp/actions?query=workflow%3ATests+branch%3Amain)
![npm](https://img.shields.io/npm/dm/node-gyp)

`node-gyp` is a cross-platform command-line tool written in Node.js for
compiling native addon modules for Node.js. It contains a vendored copy of the
[gyp-next](https://github.com/nodejs/gyp-next) project that was previously used
by the Chromium team and extended to support the development of Node.js native
addons.

Note that `node-gyp` is _not_ used to build Node.js itself.

All current and LTS target versions of Node.js are supported. Depending on what version of Node.js is actually installed on your system
`node-gyp` downloads the necessary development files or headers for the target version. List of stable Node.js versions can be found on [Node.js website](https://nodejs.org/en/about/previous-releases).

## Features
"
kedro,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-light.png"">
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-dark.png"">
    <img src=""https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-light.png"" alt=""Kedro"">
  </picture>
</p>

[![Python version](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue.svg)](https://pypi.org/project/kedro/)
[![PyPI version](https://badge.fury.io/py/kedro.svg)](https://pypi.org/project/kedro/)
[![Conda version](https://img.shields.io/conda/vn/conda-forge/kedro.svg)](https://anaconda.org/conda-forge/kedro)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/kedro-org/kedro/blob/main/LICENSE.md)
[![Slack Organisation](https://img.shields.io/badge/slack-chat-blueviolet.svg?label=K"
great_expectations,"[![Python Versions](https://img.shields.io/pypi/pyversions/great_expectations.svg)](https://pypi.python.org/pypi/great_expectations)
[![PyPI](https://img.shields.io/pypi/v/great_expectations)](https://pypi.org/project/great-expectations/#history)
[![PyPI Downloads](https://img.shields.io/pypi/dm/great-expectations)](https://pypistats.org/packages/great-expectations)
[![Build Status](https://img.shields.io/azure-devops/build/great-expectations/bedaf2c2-4c4a-4b37-87b0-3877190e71f5/1)](https://dev.azure.com/great-expectations/great_expectations/_build/latest?definitionId=1&branchName=develop)
[![pre-commit.ci Status](https://results.pre-commit.ci/badge/github/great-expectations/great_expectations/develop.svg)](https://results.pre-commit.ci/latest/github/great-expectations/great_expectations/develop)
[![codecov](https://codecov.io/gh/great-expectations/great_expectations/graph/badge.svg?token=rbHxgTxYTs)](https://codecov.io/gh/great-expectations/great_expectations)
[![DOI](https://zenodo.o"
practical-python,"# Welcome!

When I first learned Python nearly 27 years ago, I was immediately
struck by how I could productively apply it to all sorts of messy work
projects. Fast-forward a decade and I found myself teaching others the
same fun.  The result of that teaching is this course--A no-nonsense
treatment of Python that has been actively taught to more than 400
in-person groups since 2007.  Traders, systems admins, astronomers,
tinkerers, and even a few hundred rocket scientists who used Python to
help land a rover on Mars--they've all taken this course. Now, I'm
pleased to make it available under a Creative Commons license--completely
free of spam, signups, and other nonsense. Enjoy!

[GitHub Pages](https://dabeaz-course.github.io/practical-python) | [GitHub Repo](https://github.com/dabeaz-course/practical-python).

--David Beazley ([https://dabeaz.com](https://dabeaz.com)), [@dabeaz](https://mastodon.social/@dabeaz)

(P.S. This course is about Python. If you want a Python course that's abou"
kornia,"<div align=""center"">
<p align=""center"">
  <img width=""55%"" src=""https://github.com/kornia/data/raw/main/kornia_banner_pixie.png"" />
</p>

---

English | [简体中文](README_zh-CN.md)

<!-- prettier-ignore -->
<a href=""https://kornia.readthedocs.io"">Docs</a> •
<a href=""https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/nbs/hello_world_tutorial.ipynb"">Try it Now</a> •
<a href=""https://kornia.github.io/tutorials/"">Tutorials</a> •
<a href=""https://github.com/kornia/kornia-examples"">Examples</a> •
<a href=""https://kornia.github.io//kornia-blog"">Blog</a> •
<a href=""https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-CnydWe5fmvkcktIeRFGCEQ"">Community</a>

[![PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/kornia)
[![Downloads](https://static.pepy.tech/badge/kornia)](https://pepy.tech/project/kornia)
[![Slack](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu"
numba,"*****
Numba
*****

.. image:: https://badges.gitter.im/numba/numba.svg
   :target: https://gitter.im/numba/numba?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge
   :alt: Gitter

.. image:: https://img.shields.io/badge/discuss-on%20discourse-blue
   :target: https://numba.discourse.group/
   :alt: Discourse

.. image:: https://zenodo.org/badge/3659275.svg
   :target: https://zenodo.org/badge/latestdoi/3659275
   :alt: Zenodo DOI

.. image:: https://img.shields.io/pypi/v/numba.svg
   :target: https://pypi.python.org/pypi/numba/
   :alt: PyPI

.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.numba?branchName=main
    :target: https://dev.azure.com/numba/numba/_build/latest?definitionId=1?branchName=main
    :alt: Azure Pipelines

A Just-In-Time Compiler for Numerical Functions in Python
#########################################################

Numba is an open source, NumPy-aware optimizing compiler for Python sponsored
by Anaconda, Inc.  It uses the LLVM com"
rq,"RQ (_Redis Queue_) is a simple Python library for queueing jobs and processing
them in the background with workers.  It is backed by Redis and it is designed
to have a low barrier to entry.  It should be integrated in your web stack
easily.

RQ requires Redis >= 3.0.0.

[![Build status](https://github.com/rq/rq/workflows/Test%20rq/badge.svg)](https://github.com/rq/rq/actions?query=workflow%3A%22Test+rq%22)
[![PyPI](https://img.shields.io/pypi/pyversions/rq.svg)](https://pypi.python.org/pypi/rq)
[![Coverage](https://codecov.io/gh/rq/rq/branch/master/graph/badge.svg)](https://codecov.io/gh/rq/rq)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


Full documentation can be found [here][d].


## Support RQ

If you find RQ useful, please consider supporting this project via [Tidelift](https://tidelift.com/subscription/pkg/pypi-rq?utm_source=pypi-rq&utm_medium=referral&utm_campaign=readme).


## Getting started

First, run a Red"
musicbox,"# NetEase-MusicBox

**感谢为 MusicBox 的开发付出过努力的[每一个人](https://github.com/darknessomi/musicbox/graphs/contributors)！**

高品质网易云音乐命令行版本，简洁优雅，丝般顺滑，基于 Python 编写。

[![Software License](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)
[![versions](https://img.shields.io/pypi/v/NetEase-MusicBox.svg)](https://pypi.org/project/NetEase-MusicBox/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/NetEase-MusicBox.svg)](https://pypi.org/project/NetEase-MusicBox/)

## Demo

[![NetEase-MusicBox-GIF](https://qfile.aobeef.cn/3abba3b8a3994ee3d5cd.gif)](https://pypi.org/project/NetEase-MusicBox/)

## 功能特性

1. 320kbps 的高品质音乐
2. 歌曲，艺术家，专辑检索
3. 网易 22 个歌曲排行榜
4. 网易新碟推荐
5. 网易精选歌单
6. 网易主播电台
7. 私人歌单，每日推荐
8. 随心打碟
9. 本地收藏，随时加 ❤
10. 播放进度及播放模式显示
11. 现在播放及桌面歌词显示
12. 歌曲评论显示
13. 一键进入歌曲专辑
14. 定时退出
15. Vimer 式快捷键让操作丝般顺滑
16. 可使用数字快捷键
17. 可使用自定义全局快捷键
18. 对当前歌单列表进行本地模糊搜索

### 键盘快捷键

有 num + 字样的快捷键可以用数字修饰，按键顺序为先输入数字再键入被修饰的键，即 num + 后的快捷键。

| Key                                   | Effect       "
OpenLLM,"# 🦾 OpenLLM: Self-Hosting LLMs Made Easy

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202-green.svg)](https://github.com/bentoml/OpenLLM/blob/main/LICENSE)
[![Releases](https://img.shields.io/pypi/v/openllm.svg?logo=pypi&label=PyPI&logoColor=gold)](https://pypi.org/project/openllm)
[![CI](https://results.pre-commit.ci/badge/github/bentoml/OpenLLM/main.svg)](https://results.pre-commit.ci/latest/github/bentoml/OpenLLM/main)
[![X](https://badgen.net/badge/icon/@bentomlai/000000?icon=twitter&label=Follow)](https://twitter.com/bentomlai)
[![Community](https://badgen.net/badge/icon/Community/562f5d?icon=slack&label=Join)](https://l.bentoml.com/join-slack)

OpenLLM allows developers to run **any open-source LLMs** (Llama 3.1, Qwen2, Phi3 and [more](#supported-models)) or **custom models** as **OpenAI-compatible APIs** with a single command. It features a [built-in chat UI](#chat-ui), state-of-the-art inference backends, and a simplified workflow for creating enterprise"
modin,"<p align=""center""><a href=""https://modin.readthedocs.io""><img width=77% alt="""" src=""https://github.com/modin-project/modin/raw/7c009c747caa90554607e30b9ac2bd1b190b8c7d/docs/img/MODIN_ver2_hrz.png?raw=true""></a></p>
<h2 align=""center"">Scale your pandas workflows by changing one line of code</h2>

<div align=""center"">

| <h3>Dev Community & Support</h3> | <h3>Forums</h3> | <h3>Socials</h3> | <h3>Docs</h3> |
|:---: | :---: | :---: | :---: |
| [![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/modin-project/shared_invite/zt-yvk5hr3b-f08p_ulbuRWsAfg9rMY3uA) | [![Stack Overflow](https://img.shields.io/badge/-Stackoverflow-FE7A16?style=for-the-badge&logo=stack-overflow&logoColor=white)](https://stackoverflow.com/questions/tagged/modin) | <a href=""https://twitter.com/modin_project""><img alt=""Twitter Follow"" src=""https://img.shields.io/twitter/follow/modin_project?style=social"" height=28 align=""center""></a> | <a href=""ht"
Telethon,"Telethon
========
.. epigraph::

  ⭐️ Thanks **everyone** who has starred the project, it means a lot!

|logo| **Telethon** is an asyncio_ **Python 3**
MTProto_ library to interact with Telegram_'s API
as a user or through a bot account (bot API alternative).

.. important::

    If you have code using Telethon before its 1.0 version, you must
    read `Compatibility and Convenience`_ to learn how to migrate.
    As with any third-party library for Telegram, be careful not to
    break `Telegram's ToS`_ or `Telegram can ban the account`_.

What is this?
-------------

Telegram is a popular messaging application. This library is meant
to make it easy for you to write Python programs that can interact
with Telegram. Think of it as a wrapper that has already done the
heavy job for you, so you can focus on developing an application.


Installing
----------

.. code-block:: sh

  pip3 install telethon


Creating a client
-----------------

.. code-block:: python

    from telethon import Te"
gunicorn,"Gunicorn
--------

.. image:: https://img.shields.io/pypi/v/gunicorn.svg?style=flat
    :alt: PyPI version
    :target: https://pypi.python.org/pypi/gunicorn

.. image:: https://img.shields.io/pypi/pyversions/gunicorn.svg
    :alt: Supported Python versions
    :target: https://pypi.python.org/pypi/gunicorn

.. image:: https://github.com/benoitc/gunicorn/actions/workflows/tox.yml/badge.svg
    :alt: Build Status
    :target: https://github.com/benoitc/gunicorn/actions/workflows/tox.yml

.. image:: https://github.com/benoitc/gunicorn/actions/workflows/lint.yml/badge.svg
    :alt: Lint Status
    :target: https://github.com/benoitc/gunicorn/actions/workflows/lint.yml

Gunicorn 'Green Unicorn' is a Python WSGI HTTP Server for UNIX. It's a pre-fork
worker model ported from Ruby's Unicorn_ project. The Gunicorn server is broadly
compatible with various web frameworks, simply implemented, light on server
resource usage, and fairly speedy.

Feel free to join us in `#gunicorn`_ on `Libera.chat"
sonnet,"![Sonnet](https://sonnet.dev/images/sonnet_logo.png)

# Sonnet

[**Documentation**](https://sonnet.readthedocs.io/) | [**Examples**](#examples)

Sonnet is a library built on top of [TensorFlow 2](https://www.tensorflow.org/)
designed to provide simple, composable abstractions for machine learning
research.

# Introduction

Sonnet has been designed and built by researchers at DeepMind. It can be used to
construct neural networks for many different purposes (un/supervised learning,
reinforcement learning, ...). We find it is a successful abstraction for our
organization, you might too!

More specifically, Sonnet provides a simple but powerful programming model
centered around a single concept: `snt.Module`. Modules can hold references to
parameters, other modules and methods that apply some function on the user
input. Sonnet ships with many predefined modules (e.g. `snt.Linear`,
`snt.Conv2D`, `snt.BatchNorm`) and some predefined networks of modules (e.g.
`snt.nets.MLP`) but users are als"
termtosvg,"**Note: As of June 2020 I do not have time to maintain termtosvg anymore and this repository is now read-only.**

# termtosvg
termtosvg is a Unix terminal recorder written in Python that renders your command
line sessions as standalone SVG animations.

![Example](./docs/examples/awesome_window_frame_powershell.svg)

* [Gallery of examples](https://nbedos.github.io/termtosvg/pages/examples.html)
* [Gallery of templates](https://nbedos.github.io/termtosvg/pages/templates.html)

## Features
* Produce lightweight and clean looking animations or still frames embeddable on a project page
* Custom color themes, terminal UI and animation controls via user-defined [SVG templates](man/termtosvg-templates.md)
* Rendering of recordings in asciicast format made with asciinema
    
## Installation
termtosvg is compatible with Linux, macOS and BSD OSes, requires Python >= 3.5 and can be installed as follows using pip:
```shell
# Create virtualenv named '.venv'
python3 -m venv .venv
# Activate virtual"
Sublist3r,"## About Sublist3r 

Sublist3r is a python tool designed to enumerate subdomains of websites using OSINT. It helps penetration testers and bug hunters collect and gather subdomains for the domain they are targeting. Sublist3r enumerates subdomains using many search engines such as Google, Yahoo, Bing, Baidu and Ask. Sublist3r also enumerates subdomains using Netcraft, Virustotal, ThreatCrowd, DNSdumpster and ReverseDNS.

[subbrute](https://github.com/TheRook/subbrute) was integrated with Sublist3r to increase the possibility of finding more subdomains using bruteforce with an improved wordlist. The credit goes to TheRook who is the author of subbrute.

## Screenshots

![Sublist3r](http://www.secgeek.net/images/Sublist3r.png ""Sublist3r in action"")


## Installation

```
git clone https://github.com/aboul3la/Sublist3r.git
```

## Recommended Python Version:

Sublist3r currently supports **Python 2** and **Python 3**.

* The recommended version for Python 2 is **2.7.x**
* The recommended "
ddddocr,"

# DdddOcr 带带弟弟OCR通用验证码离线本地识别SDK免费开源版

DdddOcr，其由 [本作者](https://github.com/sml2h3) 与 [kerlomz](https://github.com/kerlomz) 共同合作完成，通过大批量生成随机数据后进行深度网络训练，本身并非针对任何一家验证码厂商而制作，本库使用效果完全靠玄学，可能可以识别，可能不能识别。

DdddOcr、最简依赖的理念，尽量减少用户的配置和使用成本，希望给每一位测试者带来舒适的体验

项目地址： [点我传送](https://github.com/sml2h3/ddddocr) 

### 自营GPT聚合平台： [点我传送](https://juxiangyun.com/)

<!-- PROJECT SHIELDS -->

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]

<!-- PROJECT LOGO -->
<br />

<p align=""center"">
  <a href=""https://github.com/shaojintian/Best_README_template/"">
    <img src=""https://cdn.wenanzhe.com/img/logo.png!/crop/700x500a400a500"" alt=""Logo"">
  </a>
  <p align=""center"">
    一个容易使用的通用验证码识别python库
    <br />
    <a href=""https://github.com/shaojintian/Best_README_template""><strong>探索本项目的文档 »</strong></a>
    <br />
    <br />
    ·
    <a href=""ht"
mlcourse.ai,"<div align=""center"">

![ODS stickers](https://github.com/Yorko/mlcourse.ai/blob/main/img/ods_stickers.jpg)

**[mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course**

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-green)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
[![Slack](https://img.shields.io/badge/slack-ods.ai-orange)](https://opendatascience.slack.com/archives/C91N8TL83/p1567408586359500)
[![Donate](https://img.shields.io/badge/support-patreon-red)](https://www.patreon.com/ods_mlcourse)
[![Donate](https://img.shields.io/badge/support-ko--fi-red)](https://ko-fi.com/mlcourse_ai)

</div>

[mlcourse.ai](https://mlcourse.ai) is an open Machine Learning course by [OpenDataScience (ods.ai)](https://ods.ai/), led by [Yury Kashnitsky (yorko)](https://yorko.github.io/). Having both a Ph.D. degree in applied math and a Kaggle Competitions Master tier, Yury aimed at designing an ML course with a perfect balance between theory and "
robotframework,"Robot Framework
===============

.. contents::
   :local:

Introduction
------------

`Robot Framework <http://robotframework.org>`_ |r| is a generic open source
automation framework for acceptance testing, acceptance test driven
development (ATDD), and robotic process automation (RPA). It has simple plain
text syntax and it can be extended easily with generic and custom libraries.

Robot Framework is operating system and application independent. It is
implemented using `Python <http://python.org>`_ which is also the primary
language to extend it. The framework has a rich ecosystem around it consisting
of various generic libraries and tools that are developed as separate projects.
For more information about Robot Framework and the ecosystem, see
http://robotframework.org.

Robot Framework project is hosted on GitHub_ where you can find source code,
an issue tracker, and some further documentation. Downloads are hosted on PyPI_.

Robot Framework development is sponsored by non-profit `R"
tpot,"Master status: [![Master Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=master)](https://travis-ci.com/EpistasisLab/tpot)
[![Master Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/master?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=master)
[![Master Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=master)](https://coveralls.io/github/EpistasisLab/tpot?branch=master)

Development status: [![Development Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=development)](https://travis-ci.com/EpistasisLab/tpot/branches)
[![Development Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/development?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=development)
[![Development Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=development)](https://cov"
dbt-core,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/dbt-labs/dbt-core/fa1ea14ddfb1d5ae319d5141844910dd53ab2834/etc/dbt-core.svg"" alt=""dbt logo"" width=""750""/>
</p>
<p align=""center"">
  <a href=""https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml"">
    <img src=""https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml/badge.svg?event=push"" alt=""CI Badge""/>
  </a>
</p>

**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.

![architecture](https://github.com/dbt-labs/dbt-core/blob/202cb7e51e218c7b29eb3b11ad058bd56b7739de/etc/dbt-transform.png)

## Understanding dbt

Analysts using dbt can transform their data by simply writing select statements, while dbt handles turning these statements into tables and views in a data warehouse.

These select statements, or ""models"", form a dbt project. Models frequently build on top of one another – dbt mak"
undetected-chromedriver,"# undetected_chromedriver #

https://github.com/ultrafunkamsterdam/undetected-chromedriver


Optimized Selenium Chromedriver patch which does not trigger anti-bot services like Distill Network / Imperva / DataDome / Botprotect.io
Automatically downloads the driver binary and patches it.

* Tested until current chrome beta versions
* Works also on Brave Browser and many other Chromium based browsers, but you need to know what you're doing and needs some tweaking.
* Python 3.6++**


## Installation ##

```
pip install undetected-chromedriver
```
or , if you're feeling adventurous, install directly via github

```
pip install git+https://www.github.com/ultrafunkamsterdam/undetected-chromedriver@master     # replace @master with @branchname for other branches
```


- - -
## Message for all ##
I will be putting limits on the issue tracker. It has beeen abused too long.  
any good news?  
Yes, i've opened [Undetected-Discussions](https://github.com/ultrafunkamste"
qutebrowser,"// SPDX-License-Identifier: GPL-3.0-or-later

// If you are reading this in plaintext or on PyPi:
//
// A rendered version is available at:
// https://github.com/qutebrowser/qutebrowser/blob/main/README.asciidoc

qutebrowser
===========

// QUTE_WEB_HIDE
image:qutebrowser/icons/qutebrowser-64x64.png[qutebrowser logo] *A keyboard-driven, vim-like browser based on Python and Qt.*

image:https://github.com/qutebrowser/qutebrowser/workflows/CI/badge.svg[""Build Status"", link=""https://github.com/qutebrowser/qutebrowser/actions?query=workflow%3ACI""]
image:https://codecov.io/github/qutebrowser/qutebrowser/coverage.svg?branch=main[""coverage badge"",link=""https://codecov.io/github/qutebrowser/qutebrowser?branch=main""]

link:https://www.qutebrowser.org[website] | link:https://blog.qutebrowser.org[blog] | https://github.com/qutebrowser/qutebrowser/blob/main/doc/faq.asciidoc[FAQ] | https://www.qutebrowser.org/doc/contributing.html[contributing] | link:https://github.com/qutebrowser/qutebrowser/relea"
tflearn,"[![Build Status](https://travis-ci.org/tflearn/tflearn.svg?branch=master)](https://travis-ci.org/tflearn/tflearn)
[![PyPI version](https://badge.fury.io/py/tflearn.svg)](https://badge.fury.io/py/tflearn)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Join the chat at https://gitter.im/einsteinsci/betterbeginnings](https://badges.gitter.im/tflearn/tflearn.svg)](https://gitter.im/tflearn/tflearn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

# TFLearn: Deep learning library featuring a higher-level API for TensorFlow.

TFlearn is a modular and transparent deep learning library built on top of Tensorflow.  It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.

TFLearn features include:

- Easy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.
- Fast prototyping t"
Python,"
# 欢迎关注我的微信公众号【智能制造社区】

## 左手代码，右手制造，分享智能制造相关技术和业务，包括 Python, C#, 数据库，工业大数据、物联网技术及MES/ERP/SAP等系统。

## 可以通过微信公众号加我好友

![二维码](qrcode.jpg)

# 内容列表

## [Python微信公众号开发](https://github.com/injetlee/Python/tree/master/wechat)

- ### Python 微信公众号开发—小白篇(一)

- ### Python 公众号开发—颜值检测

## [Python 爬虫入门合集](https://github.com/injetlee/Python/tree/master/%E7%88%AC%E8%99%AB%E9%9B%86%E5%90%88)

- ### Python 爬虫入门(一)——爬取糗事百科

- ### Python 爬虫入门(二)——爬取妹子图

- ### Python 爬虫——Python 岗位分析报告

- ### Python 爬虫利器——Selenium介绍

- ### Python 爬虫—— 抖音 App 视频抓包爬取

## [Python 黑魔法](https://github.com/injetlee/Python/tree/master/Python%20%E9%BB%91%E9%AD%94%E6%B3%95)

- ### Python 远程关机

## SQL 数据库

- [1 小时 SQL 极速入门（一）](https://mp.weixin.qq.com/s/Lx4B349OlD49ihJPnB6YiA)
- [1 小时 SQL 极速入门（二）](https://mp.weixin.qq.com/s/D-CEtGYomne5kV_Ji4lodA)
- [1 小时 SQL 极速入门（三）](https://mp.weixin.qq.com/s/7aJqrhCNcvnt2gO3p5P50Q)
- [SQL 高级查询——（层次化查询，递归）](https://mp.weixin.qq.com/s/R9Yldd-5AK4ObRA9Lfbz-Q)
- [GROUP BY高级查询,ROLLUP，CUBE，GROUPPING详解]("
Chinese-BERT-wwm,"# [Chinese-LLaMA-Alpaca-2 v1.0版本](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)已正式发布！

[**中文说明**](https://github.com/ymcui/Chinese-BERT-wwm/) | [**English**](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md)

<p align=""center"">
    <br>
    <img src=""./pics/banner.png"" width=""500""/>
    <br>
</p>
<p align=""center"">
    <a href=""https://github.com/ymcui/Chinese-BERT-wwm/blob/master/LICENSE"">
        <img alt=""GitHub"" src=""https://img.shields.io/github/license/ymcui/Chinese-BERT-wwm.svg?color=blue&style=flat-square"">
    </a>
</p>

在自然语言处理领域中，预训练语言模型（Pre-trained Language Models）已成为非常重要的基础技术。为了进一步促进中文信息处理的研究发展，我们发布了基于全词掩码（Whole Word Masking）技术的中文预训练模型BERT-wwm，以及与此技术密切相关的模型：BERT-wwm-ext，RoBERTa-wwm-ext，RoBERTa-wwm-ext-large, RBT3, RBTL3等。  

- **[Pre-Training with Whole Word Masking for Chinese BERT](https://ieeexplore.ieee.org/document/9599397)**  
- *Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang*
- Published in *IEEE/ACM Transactions on Audio, Speech, and"
Osintgram,"# Osintgram 🔎📸

[![version-1.3](https://img.shields.io/badge/version-1.3-green)](https://github.com/Datalux/Osintgram/releases/tag/1.3)
[![GPLv3](https://img.shields.io/badge/license-GPLv3-blue)](https://img.shields.io/badge/license-GPLv3-blue)
[![Python3](https://img.shields.io/badge/language-Python3-red)](https://img.shields.io/badge/language-Python3-red)
[![Telegram](https://img.shields.io/badge/Telegram-Channel-blue.svg)](https://t.me/osintgram)
[![Docker](https://img.shields.io/badge/Docker-Supported-blue)](https://img.shields.io/badge/Docker-Supported-blue)

Osintgram is an **OSINT** tool on Instagram to collect, analyze, and run reconnaissance.

<p align=""center"">
<img align=""center"" src="".img/carbon.png"" width=""900"">
</p>

Disclaimer: **FOR EDUCATIONAL PURPOSE ONLY! The contributors do not assume any responsibility for the use of this tool.**

Warning: It is advisable to **not** use your own/primary account when using this tool.

## Tools and Commands 🧰

Osintgram offers an int"
yolov10,"# [YOLOv10: Real-Time End-to-End Object Detection](https://arxiv.org/abs/2405.14458)


Official PyTorch implementation of **YOLOv10**.

<p align=""center"">
  <img src=""figures/latency.svg"" width=48%>
  <img src=""figures/params.svg"" width=48%> <br>
  Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs.
</p>

[YOLOv10: Real-Time End-to-End Object Detection](https://arxiv.org/abs/2405.14458).\
Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding\
[![arXiv](https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg)](https://arxiv.org/abs/2405.14458) <a href=""https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb#scrollTo=SaKTSzSWnG7s""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingf"
falcon,".. image:: https://raw.githubusercontent.com/falconry/falcon/master/logo/banner.jpg
   :align: center
   :alt: Falcon logo
   :target: https://falconframework.org/
   :width: 100 %

|Build Status| |Docs| |codecov.io|

The Falcon Web Framework
========================

`Falcon <https://falconframework.org>`__ is a minimalist ASGI/WSGI framework for
building mission-critical REST APIs and microservices, with a focus on
reliability, correctness, and performance at scale.

When it comes to building HTTP APIs, other frameworks weigh you down with tons
of dependencies and unnecessary abstractions. Falcon cuts to the chase with a
clean design that embraces HTTP and the REST architectural style.

Falcon apps work with any `WSGI <https://www.python.org/dev/peps/pep-3333/>`_
or `ASGI <https://asgi.readthedocs.io/en/latest/>`_ server, and run like a
champ under CPython 3.8+ and PyPy 3.8+.

Quick Links
-----------

* `Read the docs <https://falcon.readthedocs.io/en/stable>`_
  (`FAQ <https://falco"
pifuhd,"# [PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (CVPR 2020)](https://shunsukesaito.github.io/PIFuHD/)

[![report](https://img.shields.io/badge/arxiv-report-red)](https://arxiv.org/pdf/2004.00452.pdf) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing)

News:
* \[2020/06/15\] Demo with Google Colab (incl. visualization) is available! Please check out [#pifuhd on Twitter](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) for many results tested by users!

This repository contains a pytorch implementation of ""Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization"".

![Teaser Image](https://shunsukesaito.github.io/PIFuHD/resources/images/pifuhd.gif)

This codebase provides: 
- test code
- visualization code

See our [blog post](https://ai.facebook.com/blog/facebook-research-at-cvp"
docker-android,"
<p align=""center"">
  <img id=""header"" src=""./images/logo_docker-android.png"" />
</p>

[![Paypal Donate](https://img.shields.io/badge/paypal-donate-blue.svg)](http://paypal.me/budtmo) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![codecov](https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg)](https://codecov.io/gh/budtmo/docker-android) [![Join the chat at https://gitter.im/budtmo/docker-android](https://badges.gitter.im/budtmo/docker-android.svg)](https://gitter.im/budtmo/docker-android?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![GitHub release](https://img.shields.io/github/release/budtmo/docker-android.svg)](https://github.com/budtmo/docker-android/releases)

Docker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).

Advantages of using this"
StreamDiffusion,"# StreamDiffusion

[English](./README.md) | [日本語](./README-ja.md) | [한국어](./README-ko.md)

<p align=""center"">
  <img src=""./assets/demo_07.gif"" width=90%>
  <img src=""./assets/demo_09.gif"" width=90%>
</p>

# StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation

**Authors:** [Akio Kodaira](https://www.linkedin.com/in/akio-kodaira-1a7b98252/), [Chenfeng Xu](https://www.chenfengx.com/), Toshiki Hazama, [Takanori Yoshimoto](https://twitter.com/__ramu0e__), [Kohei Ohno](https://www.linkedin.com/in/kohei--ohno/), [Shogo Mitsuhori](https://me.ddpn.world/), [Soichi Sugano](https://twitter.com/toni_nimono), [Hanying Cho](https://twitter.com/hanyingcl), [Zhijian Liu](https://zhijianliu.com/), [Kurt Keutzer](https://scholar.google.com/citations?hl=en&user=ID9QePIAAAAJ)

StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation technique"
pip,"pip - The Python Package Installer
==================================

.. |pypi-version| image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.org/project/pip/
   :alt: PyPI

.. |python-versions| image:: https://img.shields.io/pypi/pyversions/pip
   :target: https://pypi.org/project/pip
   :alt: PyPI - Python Version

.. |docs-badge| image:: https://readthedocs.org/projects/pip/badge/?version=latest
   :target: https://pip.pypa.io/en/latest
   :alt: Documentation

|pypi-version| |python-versions| |docs-badge|

pip is the `package installer`_ for Python. You can use pip to install packages from the `Python Package Index`_ and other indexes.

Please take a look at our documentation for how to install and use pip:

* `Installation`_
* `Usage`_

We release updates regularly, with a new version every 3 months. Find more details in our documentation:

* `Release notes`_
* `Release process`_

If you find bugs, need help, or want to talk to the developers, please use our maili"
LaZagne,"
__The LaZagne Project !!!__
==

Description
----
The __LaZagne project__ is an open source application used to __retrieve lots of passwords__ stored on a local computer. 
Each software stores its passwords using different techniques (plaintext, APIs, custom algorithms, databases, etc.). This tool has been developed for the purpose of finding these passwords for the most commonly-used software. 

<p align=""center""><img src=""https://user-images.githubusercontent.com/10668373/43320585-3e34c124-91a9-11e8-9ebc-d8eabafd8ac5.png"" alt=""The LaZagne project""></p>

This project has been added to [pupy](https://github.com/n1nj4sec/pupy/) as a post-exploitation module. Python code will be interpreted in memory without touching the disk and it works on Windows and Linux host.

Standalones
----
Standalones are now available here: https://github.com/AlessandroZ/LaZagne/releases/

Installation
----
```
pip install -r requirements.txt
```

Usage
----
* Launch all modules
```
laZagne.exe all
```

* Laun"
sqlalchemy,"SQLAlchemy
==========

|PyPI| |Python| |Downloads|

.. |PyPI| image:: https://img.shields.io/pypi/v/sqlalchemy
    :target: https://pypi.org/project/sqlalchemy
    :alt: PyPI

.. |Python| image:: https://img.shields.io/pypi/pyversions/sqlalchemy
    :target: https://pypi.org/project/sqlalchemy
    :alt: PyPI - Python Version

.. |Downloads| image:: https://static.pepy.tech/badge/sqlalchemy/month
    :target: https://pepy.tech/project/sqlalchemy
    :alt: PyPI - Downloads


The Python SQL Toolkit and Object Relational Mapper

Introduction
-------------

SQLAlchemy is the Python SQL toolkit and Object Relational Mapper
that gives application developers the full power and
flexibility of SQL. SQLAlchemy provides a full suite
of well known enterprise-level persistence patterns,
designed for efficient and high-performing database
access, adapted into a simple and Pythonic domain
language.

Major SQLAlchemy features include:

* An industrial strength ORM, built
  from the core on the identity"
PySyft,"<div align=""left""> <a href=""https://pypi.org/project/syft/""><img src=""https://static.pepy.tech/badge/pysyft"" /></a> <a href=""https://pypi.org/project/syft/""><img src=""https://badge.fury.io/py/syft.svg"" /></a> <a href=""https://hub.docker.com/u/openmined""><img src=""https://img.shields.io/badge/docker-images-blue?logo=docker"" /></a> <a href=""https://github.com/OpenMined/PySyft/actions/workflows/nightlies.yml""><img src=""https://github.com/OpenMined/PySyft/actions/workflows/nightlies.yml/badge.svg?branch=dev"" /></a> <a href=""https://join.slack.com/t/openmined/shared_invite/zt-2hxwk07i9-HO7u5C7XOgou4Z62VU78zA/""><img src=""https://img.shields.io/badge/chat-on%20slack-purple?logo=slack"" /></a> <a href=""https://docs.openmined.org/en/latest/index.html""><img src=""https://img.shields.io/badge/read-docs-yellow?logo=mdbook"" /></a>
<br /><br /></div>

<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/Syft-Logo-Light.svg"">
  <img alt=""Syft Logo"" src=""docs/img/Syft-Logo.svg"" widt"
segmentation_models.pytorch,"<div align=""center"">
 
![logo](https://i.ibb.co/dc1XdhT/Segmentation-Models-V2-Side-1-1.png)  
**Python library with Neural Networks for Image  
Segmentation based on [PyTorch](https://pytorch.org/).**  

[![Generic badge](https://img.shields.io/badge/License-MIT-<COLOR>.svg?style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/blob/main/LICENSE) 
[![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/qubvel/segmentation_models.pytorch/tests.yml?branch=main&style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/actions/workflows/tests.yml) 
[![Read the Docs](https://img.shields.io/readthedocs/smp?style=for-the-badge&logo=readthedocs&logoColor=white)](https://smp.readthedocs.io/en/latest/) 
<br>
[![PyPI](https://img.shields.io/pypi/v/segmentation-models-pytorch?color=blue&style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/segmentation-models-pytorch/) 
[![PyPI - Downloads](https://img.shi"
doccano,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png"">
</div>

# doccano

[![Codacy Badge](https://app.codacy.com/project/badge/Grade/35ac8625a2bc4eddbff23dbc61bc6abb)](https://www.codacy.com/gh/doccano/doccano/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=doccano/doccano&amp;utm_campaign=Badge_Grade)
[![doccano CI](https://github.com/doccano/doccano/actions/workflows/ci.yml/badge.svg)](https://github.com/doccano/doccano/actions/workflows/ci.yml)

doccano is an open-source text annotation tool for humans. It provides annotation features for text classification, sequence labeling, and sequence to sequence tasks. You can create labeled data for sentiment analysis, named entity recognition, text summarization, and so on. Just create a project, upload data, and start annotating. You can build a dataset in hours.

## Demo

Try the [annotation demo](http://doccano.herokuapp.com).

![Demo image](https:"
cleanlab,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/cleanlab_logo_open_source.png"" width=60% height=60%>
</p>

<div align=""center"">
<a href=""https://pypi.org/pypi/cleanlab/"" target=""_blank""><img src=""https://img.shields.io/pypi/v/cleanlab.svg"" alt=""pypi_versions""></a>
<a href=""https://pypi.org/pypi/cleanlab/"" target=""_blank""><img src=""https://img.shields.io/badge/python-3.8%2B-blue"" alt=""py_versions""></a>
<a href=""https://app.codecov.io/gh/cleanlab/cleanlab"" target=""_blank""><img src=""https://codecov.io/gh/cleanlab/cleanlab/branch/master/graph/badge.svg"" alt=""coverage""></a>
<a href=""https://github.com/cleanlab/cleanlab/stargazers/"" target=""_blank""><img src=""https://img.shields.io/github/stars/cleanlab/cleanlab?style=social&maxAge=2592000"" alt=""Github Stars""></a>
<a href=""https://cleanlab.ai/slack"" target=""_blank""><img src=""https://img.shields.io/static/v1?logo=slack&style=flat&color=white&label=slack&message=join"" alt=""Slack Community""></a>
<"
django-allauth,"
==========================
Welcome to django-allauth!
==========================

.. image:: https://codeberg.org/allauth/allauth.org/raw/commit/da3b56390e1b18eaec09b05cd89dfa7812212dfc/content/news/2024/04/website-redesign/logo-light.png
   :target: https://allauth.org
   :align: right
   :alt: django-allauth logo
   :width: 250px


.. |ci| image:: https://img.shields.io/github/actions/workflow/status/pennersr/django-allauth/ci.yml.svg
   :target: https://github.com/pennersr/django-allauth/actions
.. |pypi| image:: https://img.shields.io/pypi/v/django-allauth
   :target: https://pypi.python.org/pypi/django-allauth
.. |cov| image:: https://img.shields.io/coverallsCoverage/github/pennersr/django-allauth
   :alt: Coverage Status
   :target: https://coveralls.io/r/pennersr/django-allauth
.. |btc| image:: https://img.shields.io/badge/bitcoin-donate-yellow
   :target: https://blockchain.info/address/1AJXuBMPHkaDCNX2rwAy34bGgs7hmrePEr
.. |liberapay| image:: https://img.shields.io/liberapay/"
datasette,"<img src=""https://datasette.io/static/datasette-logo.svg"" alt=""Datasette"">

[![PyPI](https://img.shields.io/pypi/v/datasette.svg)](https://pypi.org/project/datasette/)
[![Changelog](https://img.shields.io/github/v/release/simonw/datasette?label=changelog)](https://docs.datasette.io/en/latest/changelog.html)
[![Python 3.x](https://img.shields.io/pypi/pyversions/datasette.svg?logo=python&logoColor=white)](https://pypi.org/project/datasette/)
[![Tests](https://github.com/simonw/datasette/workflows/Test/badge.svg)](https://github.com/simonw/datasette/actions?query=workflow%3ATest)
[![Documentation Status](https://readthedocs.org/projects/datasette/badge/?version=latest)](https://docs.datasette.io/en/latest/?badge=latest)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/datasette/blob/main/LICENSE)
[![docker: datasette](https://img.shields.io/badge/docker-datasette-blue)](https://hub.docker.com/r/datasetteproject/datasette)
[![discord](https"
PythonPark,"这里是学习 Python 的乐园，**保姆级教程**：AI实验室、宝藏视频、数据结构、学习指南、机器学习实战、深度学习实战、Python基础、网络爬虫、大厂面经、程序人生、资源分享。**我会逐渐完善它，持续输出中！**

原创文章每周最少两篇，**后续最新文章**会在[【公众号】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)首发，视频[【B站】](https://space.bilibili.com/331507846)首发，大家可以加我[【微信】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)进**交流群**，技术交流或提意见都可以，欢迎**Star**！

<!--!* [In English](https://github.com/Jack-Cherish/PythonPark/blob/master/README_en.md ""In English"")<br>-->

[【思维导图】](#思维导图)见文末~

<p align=""center"">
    <a href=""https://github.com/Jack-Cherish/PythonPark"" target=""_blank"">
        <!--<img src=""http://photos.cuijiahua.com/github/logo.png"" width=""200"" height=""200""/>-->
        <!--<img src=""https://ftp.bmp.ovh/imgs/2021/01/cb3d04cb065fcdad.png"" width=""200"" height=""200""/>-->
        <img src=""https://raw.githubusercontent.com/Jack-Cherish/PythonPark/master/images/logo.png"" width=""200"" height=""200""/>
    </a>
</p>

<p align=""center"">
  <a href=""https://cuijiahua.com/wp-content/uploads/"
cython,"Welcome to Cython!
==================

Cython is a Python compiler that makes writing C extensions for
Python as easy as Python itself.  Cython is based on Pyrex,
but supports more cutting edge functionality and optimizations.

Cython translates Python code to C/C++ code, but additionally supports calling
C functions and declaring C types on variables and class attributes.
This allows the compiler to generate very efficient C code from Cython code.

This makes Cython the ideal language for wrapping external C libraries, and
for fast C modules that speed up the execution of Python code.

* Official website: https://cython.org/
* Documentation: https://docs.cython.org/
* Github repository: https://github.com/cython/cython
* Wiki: https://github.com/cython/cython/wiki

Cython has `about 30 million downloads <https://pypistats.org/packages/cython>`_
per month on PyPI.  You can **support the Cython project** via
`Github Sponsors <https://github.com/users/scoder/sponsorship>`_ or
`Tidelift <"
ChatRWKV,"# ChatRWKV (pronounced as ""RwaKuv"" (rʌkuv in IPA), from 4 major params: R W K V)

RWKV homepage: https://www.rwkv.com

## Please check https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py first

ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :)

Our latest version is **RWKV-6** https://arxiv.org/abs/2404.05892 (Preview models: https://huggingface.co/BlinkDL/temp )

**RWKV-6 3B** Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1

**RWKV-6 7B** Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2

![RWKV-v5-benchmark-1](RWKV-v5-benchmark-1.png)

**RWKV-LM main repo**: https://github.com/BlinkDL/RWKV-LM (explanation, fine-tuning, training, etc.)

Chat Demo for developers: https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py

## RWKV Discord: https://discord.gg/bDSBUMe"
whoogle-search,"![Whoogle Search](docs/banner.png)

[![Latest Release](https://img.shields.io/github/v/release/benbusby/whoogle-search)](https://github.com/benbusby/shoogle/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![tests](https://github.com/benbusby/whoogle-search/actions/workflows/tests.yml/badge.svg)](https://github.com/benbusby/whoogle-search/actions/workflows/tests.yml)
[![buildx](https://github.com/benbusby/whoogle-search/actions/workflows/buildx.yml/badge.svg)](https://github.com/benbusby/whoogle-search/actions/workflows/buildx.yml)
[![codebeat badge](https://codebeat.co/badges/e96cada2-fb6f-4528-8285-7d72abd74e8d)](https://codebeat.co/projects/github-com-benbusby-shoogle-master)
[![Docker Pulls](https://img.shields.io/docker/pulls/benbusby/whoogle-search)](https://hub.docker.com/r/benbusby/whoogle-search)

<table>
  <tr>
    <td><a href=""https://sr.ht/~benbusby/whoogle-search"">SourceHut</a></td>
    <td><a href=""http"
kohya_ss,"# Kohya's GUI

This repository primarily provides a Gradio GUI for [Kohya's Stable Diffusion trainers](https://github.com/kohya-ss/sd-scripts). However, support for Linux OS is also offered through community contributions. macOS support is not optimal at the moment but might work if the conditions are favorable.

The GUI allows you to set the training parameters and generate and run the required CLI commands to train the model.

## Table of Contents

- [Kohya's GUI](#kohyas-gui)
  - [Table of Contents](#table-of-contents)
  - [🦒 Colab](#-colab)
  - [Installation](#installation)
    - [Windows](#windows)
      - [Windows Pre-requirements](#windows-pre-requirements)
      - [Setup Windows](#setup-windows)
      - [Optional: CUDNN 8.9.6.50](#optional-cudnn-89650)
    - [Linux and macOS](#linux-and-macos)
      - [Linux Pre-requirements](#linux-pre-requirements)
      - [Setup Linux](#setup-linux)
      - [Install Location](#install-location)
    - [Runpod](#runpod)
      - [Manual install"
shell_gpt,"# ShellGPT
A command-line productivity tool powered by AI large language models (LLM). This command-line tool offers streamlined generation of **shell commands, code snippets, documentation**, eliminating the need for external resources (like Google search). Supports Linux, macOS, Windows and compatible with all major Shells like PowerShell, CMD, Bash, Zsh, etc.

https://github.com/TheR1D/shell_gpt/assets/16740832/9197283c-db6a-4b46-bfea-3eb776dd9093

## Installation
```shell
pip install shell-gpt
```
By default, ShellGPT uses OpenAI's API and GPT-4 model. You'll need an API key, you can generate one [here](https://beta.openai.com/account/api-keys). You will be prompted for your key which will then be stored in `~/.config/shell_gpt/.sgptrc`. OpenAI API is not free of charge, please refer to the [OpenAI pricing](https://openai.com/pricing) for more information.

> [!TIP]
> Alternatively, you can use locally hosted open source models which are available for free. To use local models, you"
trl,"<div style=""text-align: center"">
<img src=""https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_banner_dark.png"">
</div>

# TRL - Transformer Reinforcement Learning

<h3 align=""center"">
    <p>Full stack library to post-train large language models.</p>
</h3>

<p align=""center"">
    <a href=""https://github.com/huggingface/trl/blob/main/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/github/license/huggingface/trl.svg?color=blue"">
    </a>
    <a href=""https://huggingface.co/docs/trl/index"">
        <img alt=""Documentation"" src=""https://img.shields.io/website/http/huggingface.co/docs/trl/index.svg?down_color=red&down_message=offline&up_message=online"">
    </a>
    <a href=""https://github.com/huggingface/trl/releases"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/huggingface/trl.svg"">
    </a>
</p>


## What is it?

TRL is a library to post-train LLMs and diffusion models with methods such as Supervis"
coursera-dl,"# Coursera Downloader

[![Build Status](https://travis-ci.org/coursera-dl/coursera-dl.svg?branch=master)](https://travis-ci.org/coursera-dl/coursera-dl)
[![Build status](https://ci.appveyor.com/api/projects/status/3hru0ycv5fbny5k8/branch/master?svg=true)](https://ci.appveyor.com/project/balta2ar/coursera-dl/branch/master)
[![Coverage Status](https://coveralls.io/repos/coursera-dl/coursera-dl/badge.svg)](https://coveralls.io/r/coursera-dl/coursera-dl)
[![Latest version on PyPI](https://img.shields.io/pypi/v/coursera-dl.svg)](https://pypi.python.org/pypi/coursera-dl)
[![Code Climate](https://codeclimate.com/github/coursera-dl/coursera-dl/badges/gpa.svg)](https://codeclimate.com/github/coursera-dl/coursera-dl)

<!-- TOC -->

- [Coursera Downloader](#coursera-downloader)
- [Introduction](#introduction)
- [Features](#features)
- [Disclaimer](#disclaimer)
- [Installation instructions](#installation-instructions)
    - [Recommended installation method for all Operating Systems](#recommended-i"
YOLOX,"<div align=""center""><img src=""assets/logo.png"" width=""350""></div>
<img src=""assets/demo.png"" >

## Introduction
YOLOX is an anchor-free version of YOLO, with a simpler design but better performance! It aims to bridge the gap between research and industrial communities.
For more details, please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).

This repo is an implementation of PyTorch version YOLOX, there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).

<img src=""assets/git_fig.png"" width=""1000"" >

## Updates!!
* 【2023/02/28】 We support assignment visualization tool, see doc [here](./docs/assignment_visualization.md).
* 【2022/04/14】 We support jit compile op.
* 【2021/08/19】 We optimize the training process with **2x** faster training and **~1%** higher performance! See [notes](docs/updates_note.md) for more details.
* 【2021/08/05】 We release [MegEngine version YOLOX](https://github.com/MegEngine/YOLOX).
* 【2021/07/28】 We fix the fatal error of "
serverless-application-model,"# AWS SAM transform

[![Tests](https://github.com/aws/serverless-application-model/actions/workflows/build.yml/badge.svg)](https://github.com/aws/serverless-application-model/actions/workflows/build.yml)
[![Update schema](https://github.com/aws/serverless-application-model/actions/workflows/schema.yml/badge.svg)](https://github.com/aws/serverless-application-model/actions/workflows/schema.yml)
[![PyPI](https://img.shields.io/pypi/v/aws-sam-translator?label=PyPI)](https://pypi.org/project/aws-sam-translator/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aws-sam-translator?label=Python)](https://pypi.org/project/aws-sam-translator/)
[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/aws/serverless-application-model.git)

The [AWS Serverless Application Model](https://aws.amazon.com/serverless/sam/) (AWS SAM) transform is a [AWS CloudFormation macro](https://docs.aws.amazon.com/AWS"
wait-for-it,"# wait-for-it

`wait-for-it.sh` is a pure bash script that will wait on the availability of a
host and TCP port.  It is useful for synchronizing the spin-up of
interdependent services, such as linked docker containers.  Since it is a pure
bash script, it does not have any external dependencies.

## Usage

```text
wait-for-it.sh host:port [-s] [-t timeout] [-- command args]
-h HOST | --host=HOST       Host or IP under test
-p PORT | --port=PORT       TCP port under test
                            Alternatively, you specify the host and port as host:port
-s | --strict               Only execute subcommand if the test succeeds
-q | --quiet                Don't output any status messages
-t TIMEOUT | --timeout=TIMEOUT
                            Timeout in seconds, zero for no timeout
-- COMMAND ARGS             Execute command with args after the test finishes
```

## Examples

For example, let's test to see if we can access port 80 on `www.google.com`,
and if it is available, echo the m"
nerfstudio,"<p align=""center"">
    <!-- community badges -->
    <a href=""https://discord.gg/uMbNqcraFc""><img src=""https://dcbadge.vercel.app/api/server/uMbNqcraFc?style=plastic""/></a>
    <!-- doc badges -->
    <a href='https://docs.nerf.studio/'>
        <img src='https://readthedocs.com/projects/plenoptix-nerfstudio/badge/?version=latest' alt='Documentation Status' /></a>
    <!-- pi package badge -->
    <a href=""https://badge.fury.io/py/nerfstudio""><img src=""https://badge.fury.io/py/nerfstudio.svg"" alt=""PyPI version""></a>
    <!-- code check badges -->
    <a href='https://github.com/nerfstudio-project/nerfstudio/actions/workflows/core_code_checks.yml'>
        <img src='https://github.com/nerfstudio-project/nerfstudio/actions/workflows/core_code_checks.yml/badge.svg' alt='Test Status' /></a>
    <!-- license badge -->
    <a href=""https://github.com/nerfstudio-project/nerfstudio/blob/master/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/badge/License-Apache_2.0-blue.svg""><"
Douyin-Bot,"# 如何在抖音上找到漂亮小姐姐----抖音机器人

[![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badge/) [![MIT Licence](https://badges.frapsoft.com/os/mit/mit.svg?v=103)](https://opensource.org/licenses/mit-license.php)      

最近沉迷于抖音无法自拔，常常花好几个小时在抖音**漂亮小姐姐**身上。

本着**高效、直接**地找到漂亮小姐姐的核心思想，我用 Python + ADB 做了一个 Python 抖音机器人 Douyin-Bot。

<img src=""./screenshot/demo.gif"" title=""Logo""  width=""300""> <img src=""./screenshot/auto_reply.gif"" title=""Logo""  width=""300"">
    
##  特性

- [x] **自动翻页**
- [x] **颜值检测**
- [x] **人脸识别**
- [x] **自动点赞**
- [x] **自动关注**
- [x] 随机防 Ban
- [x] **自动评论**

## 原理

- 打开《抖音短视频》APP，进入主界面
- 获取手机截图，并对截图进行压缩 (Size < 1MB)；
- 请求 [人脸识别 API](http://ai.qq.com/)；
- 解析返回的人脸 Json 信息，对人脸检测切割；
- 当颜值大于门限值 `BEAUTY_THRESHOLD`时，点赞并关注；
- 下一页，返回第一步；

## 使用教程

- Python版本：3.0及以上
- 相关软件工具安装和使用步骤请参考 [wechat_jump_game](https://github.com/wangshub/wechat_jump_game) 和 [Android 操作步骤](https://github.com/wangshub/wechat_jump_game/wiki/Android-%E5%92%8"
maskrcnn-benchmark,"# Faster R-CNN and Mask R-CNN in PyTorch 1.0

**maskrcnn-benchmark has been deprecated. Please see [detectron2](https://github.com/facebookresearch/detectron2), which includes implementations for all models in maskrcnn-benchmark**

This project aims at providing the necessary building blocks for easily
creating detection and segmentation models using PyTorch 1.0.

![alt text](demo/demo_e2e_mask_rcnn_X_101_32x8d_FPN_1x.png ""from http://cocodataset.org/#explore?id=345434"")

## Highlights
- **PyTorch 1.0:** RPN, Faster R-CNN and Mask R-CNN implementations that matches or exceeds Detectron accuracies
- **Very fast**: up to **2x** faster than [Detectron](https://github.com/facebookresearch/Detectron) and **30%** faster than [mmdetection](https://github.com/open-mmlab/mmdetection) during training. See [MODEL_ZOO.md](MODEL_ZOO.md) for more details.
- **Memory efficient:** uses roughly 500MB less GPU memory than mmdetection during training
- **Multi-GPU training and inference**
- **Mixed preci"
portia,"Portia
======

Portia is a tool that allows you to visually scrape websites without any programming knowledge required. With Portia you can annotate a web page to identify the data you wish to extract, and Portia will understand based on these annotations how to scrape data from similar pages.

# Running Portia

The easiest way to run Portia is using [Docker]:

You can run Portia using Docker & official Portia-image by running:

    docker run -v ~/portia_projects:/app/data/projects:rw -p 9001:9001 scrapinghub/portia

You can also set up a local instance with [Docker-compose] by cloning this repo & running from the root of the folder:

    docker-compose up

For more detailed instructions, and alternatives to using Docker, see the [Installation] docs.

# Documentation

Documentation can be found from [Read the docs]. Source files can be found in the ``docs`` directory.

[Docker]: https://www.docker.com/
[Docker-compose]:https://docs.docker.com/compose
[Installation]: http://portia.read"
python-prompt-toolkit,"Python Prompt Toolkit
=====================

|AppVeyor|  |PyPI|  |RTD|  |License|  |Codecov|

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/logo_400px.png

``prompt_toolkit`` *is a library for building powerful interactive command line applications in Python.*

Read the `documentation on readthedocs
<http://python-prompt-toolkit.readthedocs.io/en/stable/>`_.


Gallery
*******

`ptpython <http://github.com/prompt-toolkit/ptpython/>`_ is an interactive
Python Shell, build on top of ``prompt_toolkit``.

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/ptpython.png

`More examples <https://python-prompt-toolkit.readthedocs.io/en/stable/pages/gallery.html>`_


prompt_toolkit features
***********************

``prompt_toolkit`` could be a replacement for `GNU readline
<https://tiswww.case.edu/php/chet/readline/rltop.html>`_, but it can be much
more than that.

Some features:

- **Pure Python**.
- Syntax hi"
youtube-dl-gui,"[![Donations Badge](https://yourdonation.rocks/images/badge.svg)](https://mrs0m30n3.github.io/youtube-dl-gui/donate.html)

# youtube-dlG
A cross platform front-end GUI of the popular [youtube-dl](https://rg3.github.io/youtube-dl/) media downloader written in wxPython. [Supported sites](https://rg3.github.io/youtube-dl/supportedsites.html)

## Screenshots
![youtube-dl-gui main window](https://raw.githubusercontent.com/MrS0m30n3/youtube-dl-gui/gh-pages/images/ydlg_ui.gif)

## Requirements
* [Python 2.7.3+](https://www.python.org/downloads)
* [wxPython 3](https://wxpython.org/download.php)
* [TwoDict](https://pypi.python.org/pypi/twodict)
* [GNU gettext](https://www.gnu.org/software/gettext/) (to build the package)
* [FFmpeg](https://ffmpeg.org/download.html) (optional, to post process video files)

## Downloads
* [Source (.zip)](https://github.com/MrS0m30n3/youtube-dl-gui/archive/0.4.zip)
* [Source (.tar.gz)](https://github.com/MrS0m30n3/youtube-dl-gui/archive/0.4.tar.gz)
* [PyPi](https:"
promptflow,"# Prompt flow

[![Python package](https://img.shields.io/pypi/v/promptflow)](https://pypi.org/project/promptflow/)
[![Python](https://img.shields.io/pypi/pyversions/promptflow.svg?maxAge=2592000)](https://pypi.python.org/pypi/promptflow/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/promptflow)](https://pypi.org/project/promptflow/)
[![CLI](https://img.shields.io/badge/CLI-reference-blue)](https://microsoft.github.io/promptflow/reference/pf-command-reference.html)
[![vsc extension](https://img.shields.io/visual-studio-marketplace/i/prompt-flow.prompt-flow?logo=Visual%20Studio&label=Extension%20)](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow)

[![Doc](https://img.shields.io/badge/Doc-online-green)](https://microsoft.github.io/promptflow/index.html)
[![Issue](https://img.shields.io/github/issues/microsoft/promptflow)](https://github.com/microsoft/promptflow/issues/new/choose)
[![Discussions](https://img.shields.io/github/discussions/microsoft/promptf"
altair,"# Vega-Altair <a href=""https://altair-viz.github.io/""><img align=""right"" src=""https://altair-viz.github.io/_static/altair-logo-light.png"" height=""50""></img></a>

[![github actions](https://github.com/vega/altair/workflows/build/badge.svg)](https://github.com/vega/altair/actions?query=workflow%3Abuild)
[![typedlib_mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://www.mypy-lang.org)
[![JOSS Paper](https://joss.theoj.org/papers/10.21105/joss.01057/status.svg)](https://joss.theoj.org/papers/10.21105/joss.01057)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/altair)](https://pypi.org/project/altair)

**Vega-Altair** is a declarative statistical visualization library for Python. With Vega-Altair, you can spend more time understanding your data and its meaning. Vega-Altair's
API is simple, friendly and consistent and built on top of the powerful
[Vega-Lite](https://github.com/vega/vega-lite) JSON specification. This elegant
simplicity produces beautiful and effective visual"
hallo,"<h1 align='center'>Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation</h1>

<div align='center'>
    <a href='https://github.com/xumingw' target='_blank'>Mingwang Xu</a><sup>1*</sup>&emsp;
    <a href='https://github.com/crystallee-ai' target='_blank'>Hui Li</a><sup>1*</sup>&emsp;
    <a href='https://github.com/subazinga' target='_blank'>Qingkun Su</a><sup>1*</sup>&emsp;
    <a href='https://github.com/NinoNeumann' target='_blank'>Hanlin Shang</a><sup>1</sup>&emsp;
    <a href='https://github.com/AricGamma' target='_blank'>Liwei Zhang</a><sup>1</sup>&emsp;
    <a href='https://github.com/cnexah' target='_blank'>Ce Liu</a><sup>3</sup>&emsp;
</div>
<div align='center'>
    <a href='https://jingdongwang2017.github.io/' target='_blank'>Jingdong Wang</a><sup>2</sup>&emsp;
    <a href='https://yoyo000.github.io/' target='_blank'>Yao Yao</a><sup>4</sup>&emsp;
    <a href='https://sites.google.com/site/zhusiyucs/home' target='_blank'>Siyu Zhu</a><sup>1</sup>&emsp;
"
fuzzywuzzy,"
## This project has been renamed and moved to https://github.com/seatgeek/thefuzz


**[TheFuzz](https://github.com/seatgeek/thefuzz)** version 0.19.0 correlates with this project's 0.18.0 version with `thefuzz` replacing all instances of this project's name.

PRs and issues here will need to be resubmitted to **[TheFuzz](https://github.com/seatgeek/thefuzz)**"
WizardLM,"## WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions

<p style=""font-size:50px;"" align=""center"">
🏠 <a href=""https://wizardlm.github.io/"" target=""_blank"">Home Page</a> </p>
<p align=""center"">
    
<p align=""center"">
🤗 <a href=""https://huggingface.co/WizardLMTeam"" target=""_blank"">HF Repo</a> • 🐦 <a href=""https://twitter.com/WizardLM_AI"" target=""_blank"">Twitter</a> • 📃 <a href=""https://arxiv.org/abs/2304.12244"" target=""_blank"">[WizardLM] @ICLR2024</a>  • 📃 <a href=""https://arxiv.org/abs/2306.08568"" target=""_blank"">[WizardCoder] @ICLR2024</a>    • 📃 <a href=""https://arxiv.org/abs/2308.09583"" target=""_blank"">[WizardMath]</a> <br>
</p>
<p align=""center"">
    👋 Join our <a href=""https://discord.gg/VZjjHtWrKs"" target=""_blank"">Discord</a>
</p>

<p align=""center"" width=""100%"">
<a ><img src=""imgs/WizardLM.png"" alt=""WizardLM"" style=""width: 20%; min-width: 300px; display: block; margin: auto;""></a>
</p>

[![Code License](https://img.shields.io/badge/Code%20Licens"
Keras-GAN,"<p align=""center"">
    <img src=""assets/keras_gan.png"" width=""480""\>
</p>

**This repository has gone stale as I unfortunately do not have the time to maintain it anymore. If you would like to continue the development of it as a collaborator send me an email at eriklindernoren@gmail.com.**

## Keras-GAN
Collection of Keras implementations of Generative Adversarial Networks (GANs) suggested in research papers. These models are in some cases simplified versions of the ones ultimately described in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GAN varieties to implement are very welcomed.

<b>See also:</b> [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)

## Table of Contents
  * [Installation](#installation)
  * [Implementations](#implementations)
    + [Auxiliary Classifier GAN](#ac-gan)
    + [Adversarial Autoencoder](#adversarial-autoencoder)
    + [Bidirectional"
cupy,"<div align=""center""><img src=""https://raw.githubusercontent.com/cupy/cupy/main/docs/image/cupy_logo_1000px.png"" width=""400""/></div>

# CuPy : NumPy & SciPy for GPU

[![pypi](https://img.shields.io/pypi/v/cupy)](https://pypi.python.org/pypi/cupy)
[![Conda](https://img.shields.io/badge/conda--forge-cupy-blue)](https://anaconda.org/conda-forge/cupy)
[![GitHub license](https://img.shields.io/github/license/cupy/cupy)](https://github.com/cupy/cupy)
[![Matrix](https://img.shields.io/matrix/cupy_community:gitter.im?server_fqdn=matrix.org)](https://gitter.im/cupy/community)
[![Twitter](https://img.shields.io/twitter/follow/CuPy_Team?label=%40CuPy_Team)](https://twitter.com/CuPy_Team)
[![Medium](https://img.shields.io/badge/Medium-CuPy-teal)](https://medium.com/cupy-team)

[**Website**](https://cupy.dev/)
| [**Install**](https://docs.cupy.dev/en/stable/install.html)
| [**Tutorial**](https://docs.cupy.dev/en/stable/user_guide/basic.html)
| [**Examples**](https://github.com/cupy/cupy/tree/main/ex"
FlexiGen,"# FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU [[paper](https://arxiv.org/abs/2303.06865)]

FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows **high-throughput** generation by IO-efficient offloading, compression, and **large effective batch sizes**.

## Motivation

In recent years, large language models (LLMs) have shown great performance across a 
wide range of tasks. Increasingly, LLMs have been applied not only to interactive 
applications (such as chat), but also to many ""back-of-house"" tasks.
These tasks include benchmarking, information extraction, data wrangling, and form processing.

One key characteristic of these applications is that they are **throughput-oriented**: they require
running LLM inferences over millions of tokens in batches, e.g., all the private documents in a company's
corpus, or all the tasks in the [HELM](https://crfm.stanford.edu/helm/latest/) "
autokeras,"<p align=""center"">
  <img width=""500"" alt=""logo"" src=""https://autokeras.com/img/row_red.svg""/>
</p>

[![](https://github.com/keras-team/autokeras/workflows/Tests/badge.svg?branch=master)](https://github.com/keras-team/autokeras/actions?query=workflow%3ATests+branch%3Amaster)
[![codecov](https://codecov.io/gh/keras-team/autokeras/branch/master/graph/badge.svg)](https://codecov.io/gh/keras-team/autokeras)
[![PyPI version](https://badge.fury.io/py/autokeras.svg)](https://badge.fury.io/py/autokeras)
[![Python](https://img.shields.io/badge/python-v3.8.0+-success.svg)](https://www.python.org/downloads/)
[![Tensorflow](https://img.shields.io/badge/tensorflow-v2.8.0+-success.svg)](https://www.tensorflow.org/versions)
[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/keras-team/autokeras/issues)

Official Website: [autokeras.com](https://autokeras.com)

##
AutoKeras: An AutoML system based on Keras.
It is developed by <a"
chisel,"<a href=""https://opensource.facebook.com/support-ukraine"">
  <img src=""https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB"" alt=""Support Ukraine - Help Provide Humanitarian Aid to Ukraine."" />
</a>

# Chisel
`Chisel` is a collection of `LLDB` commands to assist in the debugging of iOS apps.

[[Installation](#installation) &bull; [Commands](#commands) &bull; [Custom Commands](#custom-commands) &bull; [Development Workflow](#development-workflow) [Contributing](#contributing) &bull; [License](#license)]

For a comprehensive overview of LLDB, and how Chisel complements it, read Ari Grant's [Dancing in the Debugger — A Waltz with LLDB](http://www.objc.io/issue-19/lldb-debugging.html) in issue 19 of [objc.io](http://www.objc.io/).

## Installation

```shell
brew update
brew install chisel
```

if `.lldbinit` file doesn't exist you can create it & open it by tapping on the terminal

 ```shell
 touch .lldbinit
 open .lldbinit
```

Then add the following line to yo"
TextBlob,"
TextBlob: Simplified Text Processing
====================================

.. image:: https://badgen.net/pypi/v/TextBlob
    :target: https://pypi.org/project/textblob/
    :alt: Latest version

.. image:: https://github.com/sloria/TextBlob/actions/workflows/build-release.yml/badge.svg
    :target: https://github.com/sloria/TextBlob/actions/workflows/build-release.yml
    :alt: Build status


Homepage: `https://textblob.readthedocs.io/ <https://textblob.readthedocs.io/>`_

`TextBlob` is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, and more.


.. code-block:: python

    from textblob import TextBlob

    text = """"""
    The titular threat of The Blob has always struck me as the ultimate movie
    monster: an insatiably hungry, amoeba-like mass able to penetrate
    virtually any safeguard, capable of--as a d"
petals,"<p align=""center"">
    <img src=""https://i.imgur.com/7eR7Pan.png"" width=""400""><br>
    Run large language models at home, BitTorrent-style.<br>
    Fine-tuning and inference <a href=""https://github.com/bigscience-workshop/petals#benchmarks"">up to 10x faster</a> than offloading
    <br><br>
    <a href=""https://pypi.org/project/petals/""><img src=""https://img.shields.io/pypi/v/petals.svg?color=green""></a>
    <a href=""https://discord.gg/tfHfe8B34k""><img src=""https://img.shields.io/discord/865254854262652969?label=discord&logo=discord&logoColor=white""></a>
    <br>
</p>

Generate text with distributed **Llama 3.1** (up to 405B), **Mixtral** (8x22B), **Falcon** (40B+) or **BLOOM** (176B) and fine‑tune them for your own tasks &mdash; right from your desktop computer or Google Colab:

```python
from transformers import AutoTokenizer
from petals import AutoDistributedModelForCausalLM

# Choose any model available at https://health.petals.dev
model_name = ""meta-llama/Meta-Llama-3.1-405B-Instru"
minbpe,"# minbpe

Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is ""byte-level"" because it runs on UTF-8 encoded strings.

This algorithm was popularized for LLMs by the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the associated GPT-2 [code release](https://github.com/openai/gpt-2) from OpenAI. [Sennrich et al. 2015](https://arxiv.org/abs/1508.07909) is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.

There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:

1. [minbpe/base.py](minbpe/base.py):"
claude-engineer,"# 🤖 Claude Engineer

Claude Engineer is an advanced interactive command-line interface (CLI) that harnesses the power of Anthropic's Claude 3 and Claude 3.5 models to assist with a wide range of software development tasks. This tool seamlessly combines the capabilities of state-of-the-art large language models with practical file system operations, web search functionality, intelligent code analysis, and execution capabilities.

## NEW

TTS using 11labs WebSockets and audio streaming.
Type
```
11labs on
```
to use TTS and 11labs off to return to regualr mode.

Voice mode 🗣️: Now you can talk to the Engineer directly without even touching your keyboard.

Type
```
voice
```
to enter voice mode.

Say ""exit voice mode"" to return to regular text.

If you want to use your voice and 11 labs at the same time, first activate 11labs then type voice to use your voice. 

Prompt caching. Make sure you udpate your Anthropic python package before running the script.
```
pip install --upgrade anthropi"
paramiko,"|version| |python| |license| |ci| |coverage|

.. |version| image:: https://img.shields.io/pypi/v/paramiko
    :target: https://pypi.org/project/paramiko/
    :alt: PyPI - Package Version
.. |python| image:: https://img.shields.io/pypi/pyversions/paramiko
    :target: https://pypi.org/project/paramiko/
    :alt: PyPI - Python Version
.. |license| image:: https://img.shields.io/pypi/l/paramiko
    :target: https://github.com/paramiko/paramiko/blob/main/LICENSE
    :alt: PyPI - License
.. |ci| image:: https://img.shields.io/circleci/build/github/paramiko/paramiko/main
    :target: https://app.circleci.com/pipelines/github/paramiko/paramiko
    :alt: CircleCI
.. |coverage| image:: https://img.shields.io/codecov/c/gh/paramiko/paramiko
    :target: https://app.codecov.io/gh/paramiko/paramiko
    :alt: Codecov

Welcome to Paramiko!
====================

Paramiko is a pure-Python [#]_ (3.6+) implementation of the SSHv2 protocol
[#]_, providing both client and server functionality. It provides "
fluentui-emoji,"# Fluent Emoji

Fluent Emoji are a collection of familiar, friendly, and modern emoji from Microsoft.

![Fluent Emoji](art/readme_banner.webp)

## Contact

Please feel free to [open a GitHub issue](https://github.com/microsoft/fluentui-emoji/issues/new) and assign to the following points of contact with questions or requests.

- Jason Custer([@jasoncuster](https://github.com/jasoncuster)) / Spencer Nelson([@spencer-nelson](https://github.com/spencer-nelson)) - Design

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.
"
akshare,"**欢迎加入专注于财经数据和量化投资的知识社区，请点击[了解更多](https://akshare.akfamily.xyz/learn.html)**

**相关视频教程已经发布：《AKShare-初阶-使用教学》、《AKShare-初阶-实战应用》、《AKShare-源码解析》、《开源项目巡礼》**，详情请访问[课程](https://app3rqjh1z21630.h5.xiaoeknow.com)查看更多课程信息！

**AKQuant 量化教程请访问：[利用 PyBroker 进行量化投资](https://akquant.akfamily.xyz/)**

**本次发布 [AKTools](https://github.com/akfamily/aktools) 作为 AKShare 的 HTTP API 版本，
突破 Python 语言的限制，欢迎各位小伙伴试用并提出更好的意见或建议！
点击 [AKTools](https://github.com/akfamily/aktools) 查看使用指南。另外提供 [awesome-data](https://github.com/akfamily/awesome-data) 方便各位小伙伴查询各种数据源。**

![AKShare Logo](https://github.com/akfamily/akshare/blob/main/assets/images/akshare_logo.jpg)

[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/akshare.svg)](https://pypi.org/project/akshare/)
[![PyPI](https://img.shields.io/pypi/v/akshare.svg)](https://pypi.org/project/akshare/)
[![Downloads](https://pepy.tech/badge/akshare)](https://pepy.tech/project/akshare)
[![Documentation Status](https://readthedocs.org/projects/akshare/badge/?ver"
pretrained-models.pytorch,"# Pretrained models for Pytorch (Work in progress)

The goal of this repo is:

- to help to reproduce research papers results (transfer learning setups for instance),
- to access pretrained ConvNets with a unique interface/API inspired by torchvision.

<a href=""https://travis-ci.org/Cadene/pretrained-models.pytorch""><img src=""https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master""/></a>

News:
- 27/10/2018: Fix compatibility issues, Add tests, Add travis
- 04/06/2018: [PolyNet](https://github.com/CUHK-MMLAB/polynet) and [PNASNet-5-Large](https://arxiv.org/abs/1712.00559) thanks to [Alex Parinov](https://github.com/creafz)
- 16/04/2018: [SE-ResNet* and SE-ResNeXt*](https://github.com/hujie-frank/SENet) thanks to [Alex Parinov](https://github.com/creafz)
- 09/04/2018: [SENet154](https://github.com/hujie-frank/SENet) thanks to [Alex Parinov](https://github.com/creafz)
- 22/03/2018: CaffeResNet101 (good for localization with FasterRCNN)
- 21/03/2018: NASNet Mobile tha"
Tkinter-Designer,"<a href=""https://www.ultrahuman.com/ring/buy/?referral=1pauci"" target=""_blank""><img src=""https://github.com/user-attachments/assets/3a3cf7e8-ebf2-48df-9e6d-5db67756e976"" alt=""Ultrahuman Ring Air Discount Code 2024""></a>

<p align=""center"">
  <img width=""200"" src=""https://user-images.githubusercontent.com/42001064/120057695-b1f6c680-c062-11eb-96d5-2c43d05f9018.png"" alt=""logo"">
  <h1 align=""center"" style=""margin: 0 auto 0 auto;"">Tkinter Designer</h1>
  <h4 align=""center"" style=""margin: 0 auto 0 auto;"">Drag & Drop GUI Creator</h4>


<p align=""center"">
  <img src=""https://img.shields.io/github/last-commit/ParthJadhav/Tkinter-Designer"">
  <img src=""https://img.shields.io/github/contributors/ParthJadhav/Tkinter-Designer"">
  <img src=""https://img.shields.io/github/issues/ParthJadhav/Tkinter-Designer?label=issues"">
  <img src=""https://img.shields.io/github/stars/ParthJadhav/Tkinter-Designer"">
</p>

<p align=""center"">
<a href=""https://www.producthunt.com/posts/tkinter-designer?utm_source=badge-"
awesome-scala,"<!--- This file is automatically generated. Do not edit directly. -->
Awesome Scala [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
=============

A community driven list of useful Scala libraries, frameworks and software. This is not a catalog of all the libraries, just a starting point for your explorations. Inspired by [awesome-python](https://github.com/vinta/awesome-python). Other amazingly awesome lists can be found in the [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) list.

Also awesome is [Scaladex](https://index.scala-lang.org/), the searchable, tagged, and centralized index of Scala libraries.

Projects with over 500 stargazers are in bold.

## Contributing

Your contributions are always welcome! Please submit a pull request or create an issue to add a new framework, library or software to the list. Do not submit a project that hasn’t been updat"
OpenChatKit,"# OpenChatKit

OpenChatKit provides a powerful, open-source base to create both specialized and general purpose models for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between [Together](https://www.together.xyz/), [LAION](https://laion.ai), and [Ontocord.ai](https://ontocord.ai). 

In this repo, you'll find code for:
- Training GPT-NeoXT-Chat-Base-20B, a 20B parameter chat model (see [docs/GPT-NeoXT-Chat-Base-20B.md](docs/GPT-NeoXT-Chat-Base-20B.md))
- Fine-tuning Llama-2-7B-32K-beta, a 7B parameter long context model
- Training Pythia-Chat-Base-7B, a 7B parameter chat model
- Testing inference using either of the chat models
- Augmenting the model with additional context from a retrieval index

# Contents

- [Getting Started](#getting-started)
  * ["
Pytorch-UNet,"# U-Net: Semantic segmentation with PyTorch
<a href=""#""><img src=""https://img.shields.io/github/actions/workflow/status/milesial/PyTorch-UNet/main.yml?logo=github&style=for-the-badge"" /></a>
<a href=""https://hub.docker.com/r/milesial/unet""><img src=""https://img.shields.io/badge/docker%20image-available-blue?logo=Docker&style=for-the-badge"" /></a>
<a href=""https://pytorch.org/""><img src=""https://img.shields.io/badge/PyTorch-v1.13+-red.svg?logo=PyTorch&style=for-the-badge"" /></a>
<a href=""#""><img src=""https://img.shields.io/badge/python-v3.6+-blue.svg?logo=python&style=for-the-badge"" /></a>

![input and output for a random image in the test dataset](https://i.imgur.com/GD8FcB7.png)


Customized implementation of the [U-Net](https://arxiv.org/abs/1505.04597) in PyTorch for Kaggle's [Carvana Image Masking Challenge](https://www.kaggle.com/c/carvana-image-masking-challenge) from high definition images.

- [Quick start](#quick-start)
  - [Without Docker](#without-docker)
  - [With Docker](#w"
boto3,"===============================
Boto3 - The AWS SDK for Python
===============================

|Version| |Python| |License|

Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for
Python, which allows Python developers to write software that makes use
of services like Amazon S3 and Amazon EC2. You can find the latest, most
up to date, documentation at our `doc site`_, including a list of
services that are supported.

Boto3 is maintained and published by `Amazon Web Services`_.

Boto (pronounced boh-toh) was named after the fresh water dolphin native to the Amazon river. The name was chosen by the author of the original Boto library, Mitch Garnaat, as a reference to the company.

Notices
-------

On 2023-12-13, support for Python 3.7 ended for Boto3. This follows the
Python Software Foundation `end of support <https://peps.python.org/pep-0537/#lifespan>`__
for the runtime which occurred on 2023-06-27.
For more information, see this `blog post <https://aws.amazon.com/"
Machine-Learning,"# Machine-Learning
* [In English](https://github.com/Jack-Cherish/Machine-Learning/blob/master/README-eng.md ""悬停显示"")<br>

原创文章每周最少两篇，**后续最新文章**会在[【公众号】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)首发，视频[【B站】](https://space.bilibili.com/331507846)首发，大家可以加我[【微信】](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)进**交流群**，技术交流或提意见都可以，欢迎**Star**！

<p align=""center"">
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/weChat-微信群-blue.svg"" alt=""微信群""></a>
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Jack%20Cui-lightgrey.svg"" alt=""公众号""></a>
  <a href=""https://space.bilibili.com/331507846""><img src=""https://img.shields.io/badge/bilibili-哔哩哔哩-critical"" alt=""B站""></a>
  <a href=""https://www.zhihu.com/people/Jack--Cui"" target=""_blank""><img src=""https://img.shields.io/badge/zhihu-知乎-informational"
http-prompt,"HTTP Prompt
===========

|PyPI| |Docs| |Build| |Coverage| |Discord|

HTTP Prompt is an interactive command-line HTTP client featuring autocomplete
and syntax highlighting, built on HTTPie_ and prompt_toolkit_.

|Asciinema|


Links
-----

* Home: https://http-prompt.com
* Documentation: https://docs.http-prompt.com
* Code: https://github.com/httpie/http-prompt
* Chat: https://httpie.io/chat


.. |PyPI| image:: https://img.shields.io/pypi/v/http-prompt.svg
    :target: https://pypi.python.org/pypi/http-prompt

.. |Docs| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat
    :target: http://docs.http-prompt.com/en/latest/?badge=latest

.. |Build| image:: https://github.com/httpie/http-prompt/workflows/Build/badge.svg
    :target: https://github.com/httpie/http-prompt/actions

.. |Coverage| image:: https://coveralls.io/repos/github/eliangcs/http-prompt/badge.svg?branch=master
    :target: https://coveralls.io/github/eliangcs/http-prompt?branch=master

.. |Discord| "
icecream,"<h1 align=""center"">
  <img src=""logo.svg"" width=""220px"" height=""370px"" alt=""icecream"">
</h1>

<p align=""center"">
  <a href=""https://pypi.python.org/pypi/icecream""><img src=""https://badge.fury.io/py/icecream.svg""></a>
  <a href=""https://github.com/gruns/icecream/actions/workflows/ci.yml""><img src=""https://github.com/gruns/icecream/actions/workflows/ci.yml/badge.svg""></a>
  <a href=""http://unlicense.org/""><img src=""https://img.shields.io/pypi/l/icecream.svg""></a>
  <a href=""https://pypi.python.org/pypi/icecream""><img src=""https://img.shields.io/pypi/pyversions/icecream.svg""></a>
</p>


### IceCream — Never use print() to debug again

Do you ever use `print()` or `log()` to debug your code? Of course you
do. IceCream, or `ic` for short, makes print debugging a little sweeter.

`ic()` is like `print()`, but better:

  1. It prints both expressions/variable names and their values.
  2. It's 60% faster to type.
  3. Data structures are pretty printed.
  4. Output is syntax highlighted.
  5. "
byob,"![Banner](https://github.com/malwaredllc/byob/blob/master/byob/static/byob_logo_black.svg)

[![license](https://img.shields.io/badge/license-GPL-brightgreen.svg)](https://github.com/malwaredllc/byob/blob/master/LICENSE)
[![version](https://img.shields.io/badge/version-2.0-blue.svg)](https://github.com/malwaredllc/byob)
[![Coverage Status](https://coveralls.io/repos/github/malwaredllc/byob/badge.svg)](https://coveralls.io/github/malwaredllc/byob)
<img alt=""Discord"" src=""https://img.shields.io/discord/709150520446550097""/>
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=BYOB%20(Post-Exploitation%20Framework)&url=https://github.com/malwaredllc/byob&via=malwaredllc&hashtags=byob,python,security,github)


**Questions?** Check out the [docs](https://github.com/malwaredllc/byob/wiki) or join our [Discord support server](https://discord.gg/8FsSrw7)

__Disclaimer__: This project should be used for authorized testing or educat"
wandb,"<p align=""center"">
  <img src=""./assets/logo-dark.svg#gh-dark-mode-only"" width=""600"" alt=""Weights & Biases"" />
  <img src=""./assets/logo-light.svg#gh-light-mode-only"" width=""600"" alt=""Weights & Biases"" />
</p>

<p align=""center"">
<a href=""https://pypi.python.org/pypi/wandb""><img src=""https://img.shields.io/pypi/v/wandb"" /></a>
<a href=""https://anaconda.org/conda-forge/wandb""><img src=""https://img.shields.io/conda/vn/conda-forge/wandb"" /></a>
<a href=""https://circleci.com/gh/wandb/wandb""><img src=""https://img.shields.io/circleci/build/github/wandb/wandb/main"" /></a>
<a href=""https://codecov.io/gh/wandb/wandb""><img src=""https://img.shields.io/codecov/c/gh/wandb/wandb"" /></a>
</p>
<p align='center'>
<a href=""https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" /></a>
</p>

Use W&B to build better models faster. Track and visualize all the pieces of your machin"
nicegui,"<a href=""https://nicegui.io/#about"">
  <img src=""https://raw.githubusercontent.com/zauberzeug/nicegui/main/screenshot.png""
    width=""200"" align=""right"" alt=""Try online!"" />
</a>

# NiceGUI

NiceGUI is an easy-to-use, Python-based UI framework, which shows up in your web browser.
You can create buttons, dialogs, Markdown, 3D scenes, plots and much more.

It is great for micro web apps, dashboards, robotics projects, smart home solutions and similar use cases.
You can also use it in development, for example when tweaking/configuring a machine learning algorithm or tuning motor controllers.

NiceGUI is available as [PyPI package](https://pypi.org/project/nicegui/), [Docker image](https://hub.docker.com/r/zauberzeug/nicegui) and on [conda-forge](https://anaconda.org/conda-forge/nicegui) as well as [GitHub](https://github.com/zauberzeug/nicegui).

[![PyPI](https://img.shields.io/pypi/v/nicegui?color=dark-green)](https://pypi.org/project/nicegui/)
[![PyPI downloads](https://img.shields.io/p"
sshuttle,"sshuttle: where transparent proxy meets VPN meets ssh
=====================================================

As far as I know, sshuttle is the only program that solves the following
common case:

- Your client machine (or router) is Linux, FreeBSD, or MacOS.

- You have access to a remote network via ssh.

- You don't necessarily have admin access on the remote network.

- The remote network has no VPN, or only stupid/complex VPN
  protocols (IPsec, PPTP, etc). Or maybe you *are* the
  admin and you just got frustrated with the awful state of
  VPN tools.

- You don't want to create an ssh port forward for every
  single host/port on the remote network.

- You hate openssh's port forwarding because it's randomly
  slow and/or stupid.

- You can't use openssh's PermitTunnel feature because
  it's disabled by default on openssh servers; plus it does
  TCP-over-TCP, which has terrible performance (see below).


Obtaining sshuttle
------------------

- From PyPI::

      pip install sshutt"
yolov9,"# YOLOv9

Implementation of paper - [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)

[![arxiv.org](http://img.shields.io/badge/cs.CV-arXiv%3A2402.13616-B31B1B.svg)](https://arxiv.org/abs/2402.13616)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/kadirnar/Yolov9)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/merve/yolov9)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb)
[![OpenCV](https://img.shields.io/badge/OpenCV-BlogPost-black?logo=opencv&labelColor=blue&color=black)](https://learnopencv.com/yolov9-advancing-the-yolo-legacy/)

<div align=""center"">
    <a href=""./"">
        <img src=""./figure/performance.png"" w"
hamulete,"
<!--
</a><img align=""right"" src=""https://fastly.jsdelivr.net/gh/hoochanlon/w3-goto-world/W3UnitTest/mof2.PNG"" width=""250 "" height=""250"" /></a><a><img align=""right"" src=""https://fastly.jsdelivr.net/gh/hoochanlon/w3-goto-world/W3UnitTest/mof1.PNG"" width=""250 "" height=""250"" />

[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE) [![996.icu](https://img.shields.io/badge/link-996.icu-red.svg)](https://996.icu) 

-->

<!--![冲出你的窗口](https://fastly.jsdelivr.net/gh/hoochanlon/w3-goto-world/W3UnitTest/ccndck.png)-->

<h1 align=""center""> 免责声明 </h1>

<p align=""center""> <a href=""https://github.com/hoochanlon/hamulete/blob/master/README_JP.md""> 日本語 </a> | <a href=""https://github.com/hoochanlon/hamulete/blob/master/README.md""> 中文 </a>  </p>

该库并不提供任何资料，所有内容均为搬运。仅面向海外华人及社科研究者，切勿用于其他用途！大陆用户请自觉关闭，并在24小时之内删掉与本项目相关的一切内容。否则出现一切问题，项目作者概不负责！<br>

# ***城中村哈姆雷特（《信息网络社科整合资源库》）***

<a href=""https://ndltd.ncl.edu.tw"" target=""_blank"">
<img "
Reinforcement-learning-with-tensorflow,"<p align=""center"">
    <a href=""https://www.youtube.com/watch?v=pieI7rOXELI&list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba"" target=""_blank"">
    <img width=""60%"" src=""https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/RL_cover.jpg"" style=""max-width:100%;"">
    </a>
</p>


<br>

# Reinforcement Learning Methods and Tutorials

In these tutorials for reinforcement learning, it covers from the basic RL algorithms to advanced algorithms developed recent years.

**If you speak Chinese, visit [莫烦 Python](https://mofanpy.com) or my [Youtube channel](https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg) for more.**

**As many requests about making these tutorials available in English, please find them in this playlist:** ([https://www.youtube.com/playlist?list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba](https://www.youtube.com/playlist?list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba))

# Table of Contents

* Tutorials
    * [Simple entry example](contents/1_command_line_reinforcement"
hacktricks,"# HackTricks

<figure><img src="".gitbook/assets/hacktricks.gif"" alt=""""><figcaption></figcaption></figure>

_Hacktricks logos & motion design by_ [_@ppiernacho_](https://www.instagram.com/ppieranacho/)_._

{% hint style=""success"" %}
**Welcome to the wiki where you will find each hacking trick/technique/whatever I have learnt from CTFs, real life apps, reading researches, and news.**
{% endhint %}

To get started follow this page where you will find the **typical flow** that **you should follow when pentesting** one or more **machines:**

{% content-ref url=""generic-methodologies-and-resources/pentesting-methodology.md"" %}
[pentesting-methodology.md](generic-methodologies-and-resources/pentesting-methodology.md)
{% endcontent-ref %}

## Corporate Sponsors

### [STM Cyber](https://www.stmcyber.com)

<figure><img src="".gitbook/assets/stm (1).png"" alt=""""><figcaption></figcaption></figure>

[**STM Cyber**](https://www.stmcyber.com) is a great cybersecurity company whose slogan is **HACK THE "
30-seconds-of-python,"> **IMPORTANT NOTICE:**
>
> As of May, 2023, all 30-seconds content repositories have been merged into [30-seconds-of-code](https://github.com/30-seconds/30-seconds-of-code).
>
> Please watch, star and follow relevant activity there.

[![Logo](/logo.png)](https://30secondsofcode.org/python/p/1)

# 30 seconds of code

> Short Python code snippets for all your development needs

* Visit [our website](https://30secondsofcode.org) to view our snippet collection.
* Use the [Search page](https://30secondsofcode.org/search) to find snippets that suit your needs. You can search by name, tag, language or using a snippet's description. Just start typing a term and see what comes up.
* Browse the [Python Snippet collection](https://30secondsofcode.org/python/p/1) to see all the snippets in this project or click individual tags at the top of the same page to narrow down your search to a specific tag.
* Click on each snippet card to view the whole snippet, including code, explanation and examples.
"
Mailpile,"# Welcome to Mailpile! #

**IMPORTANT NOTE**

Development on this codebase has halted, until the
[Python3 rewrite](https://community.mailpile.is/t/a-very-uninformative-progress-update-mailpile-2/785)
has completed.

Apologies to those who have unanswered, out-standing pull requests and
issues. 😢 Your efforts are appreciated!

If you rely on this code and have your own branch which you actively
maintain, let us know: we would be happy to link to it.

If you need to run Mailpile v1 to access legacy data, consider using
our [legacy Docker images](https://github.com/mailpile/Mailpile-v1-Docker).


------------------------------------------------------------------------

## Introduction (Obsolete) ##

Mailpile (<https://www.mailpile.is/>) is a modern, fast web-mail client
with user-friendly encryption and privacy features. The development of
Mailpile is funded by
[a large community of backers](https://www.mailpile.is/#community)
and all code related to the project is and will be released un"
text-generation-inference,"<div align=""center"">

<a href=""https://www.youtube.com/watch?v=jlMAX2Oaht0"">
  <img width=560 width=315 alt=""Making TGI deployment optimal"" src=""https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png"">
</a>

# Text Generation Inference

<a href=""https://github.com/huggingface/text-generation-inference"">
  <img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social"">
</a>
<a href=""https://huggingface.github.io/text-generation-inference"">
  <img alt=""Swagger API documentation"" src=""https://img.shields.io/badge/API-Swagger-informational"">
</a>

A Rust, Python and gRPC server for text generation inference. Used in production at [Hugging Face](https://huggingface.co)
to power Hugging Chat, the Inference API and Inference Endpoint.

</div>

## Table of contents

  - [Get Started](#get-started)
    - [Docker](#docker)
    - [API documentation](#api-documentation)
    - [Using a private or gated model](#using-a-"
stable-baselines3,"<!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) -->
![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)
[![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master)
[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


# Stable Baselines3

<img src=""docs/\_static/img/logo.png"" align=""right"" width=""40%""/>

Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).

You can read a detailed presentation"
pydub,"# Pydub [![Build Status](https://travis-ci.org/jiaaro/pydub.svg?branch=master)](https://travis-ci.org/jiaaro/pydub) [![Build status](https://ci.appveyor.com/api/projects/status/gy1ucp9o5khq7fqi/branch/master?svg=true)](https://ci.appveyor.com/project/jiaaro/pydub/branch/master)

Pydub lets you do stuff to audio in a way that isn't stupid.

**Stuff you might be looking for**:
 - [Installing Pydub](https://github.com/jiaaro/pydub#installation)
 - [API Documentation](https://github.com/jiaaro/pydub/blob/master/API.markdown)
 - [Dependencies](https://github.com/jiaaro/pydub#dependencies)
 - [Playback](https://github.com/jiaaro/pydub#playback)
 - [Setting up ffmpeg](https://github.com/jiaaro/pydub#getting-ffmpeg-set-up)
 - [Questions/Bugs](https://github.com/jiaaro/pydub#bugs--questions)
 

##  Quickstart

Open a WAV file

```python
from pydub import AudioSegment

song = AudioSegment.from_wav(""never_gonna_give_you_up.wav"")
```

...or a mp3

```python
song = AudioSegment.from_mp3(""never_gonn"
nougat,"<div align=""center"">
<h1>Nougat: Neural Optical Understanding for Academic Documents</h1>

[![Paper](https://img.shields.io/badge/Paper-arxiv.2308.13418-white)](https://arxiv.org/abs/2308.13418)
[![GitHub](https://img.shields.io/github/license/facebookresearch/nougat)](https://github.com/facebookresearch/nougat)
[![PyPI](https://img.shields.io/pypi/v/nougat-ocr?logo=pypi)](https://pypi.org/project/nougat-ocr)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Hugging Face Spaces](https://img.shields.io/badge/🤗%20Hugging%20Face-Community%20Space-blue)](https://huggingface.co/spaces/ysharma/nougat)

</div>

This is the official repository for Nougat, the academic document PDF parser that understands LaTeX math and tables.

Project page: https://facebookresearch.github.io/nougat/

## Install

From pip:
``"
Douyin_TikTok_Download_API,"<div align=""center"">
<a href=""https://douyin.wtf/"" alt=""logo"" ><img src=""https://raw.githubusercontent.com/Evil0ctal/Douyin_TikTok_Download_API/main/logo/logo192.png"" width=""120""/></a>
</div>
<h1 align=""center"">Douyin_TikTok_Download_API(抖音/TikTok API)</h1>

<div align=""center"">

[English](./README.en.md) | [简体中文](./README.md)

🚀「Douyin_TikTok_Download_API」是一个开箱即用的高性能异步[抖音](https://www.douyin.com)|[TikTok](https://www.tiktok.com)|[Bilibili](https://www.bilibili.com)数据爬取工具，支持API调用，在线批量解析及下载。

[![GitHub license](https://img.shields.io/github/license/Evil0ctal/Douyin_TikTok_Download_API?style=flat-square)](LICENSE)
[![Release Version](https://img.shields.io/github/v/release/Evil0ctal/Douyin_TikTok_Download_API?style=flat-square)](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/releases/latest)
[![GitHub Star](https://img.shields.io/github/stars/Evil0ctal/Douyin_TikTok_Download_API?style=flat-square)](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/stargazers)
[![GitHub For"
attention-is-all-you-need-pytorch,"# Attention is all you need: A Pytorch Implementation

This is a PyTorch implementation of the Transformer model in ""[Attention is All You Need](https://arxiv.org/abs/1706.03762)"" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). 


A novel sequence to sequence framework utilizes the **self-attention mechanism**, instead of Convolution operation or Recurrent structure, and achieve the state-of-the-art performance on **WMT 2014 English-to-German translation task**. (2017/06/12)

> The official Tensorflow Implementation can be found in: [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py).

> To learn more about self-attention mechanism, you could read ""[A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)"".

<p align=""center"">
<img src=""http://imgur.com/1krF2R6.png"" width=""250"">
</p>


The project support t"
txtai,"<p align=""center"">
    <img src=""https://raw.githubusercontent.com/neuml/txtai/master/logo.png""/>
</p>

<p align=""center"">
    <b>All-in-one embeddings database</b>
</p>

<p align=""center"">
    <a href=""https://github.com/neuml/txtai/releases"">
        <img src=""https://img.shields.io/github/release/neuml/txtai.svg?style=flat&color=success"" alt=""Version""/>
    </a>
    <a href=""https://github.com/neuml/txtai"">
        <img src=""https://img.shields.io/github/last-commit/neuml/txtai.svg?style=flat&color=blue"" alt=""GitHub last commit""/>
    </a>
    <a href=""https://github.com/neuml/txtai/issues"">
        <img src=""https://img.shields.io/github/issues/neuml/txtai.svg?style=flat&color=success"" alt=""GitHub issues""/>
    </a>
    <a href=""https://join.slack.com/t/txtai/shared_invite/zt-1cagya4yf-DQeuZbd~aMwH5pckBU4vPg"">
        <img src=""https://img.shields.io/badge/slack-join-blue?style=flat&logo=slack&logocolor=white"" alt=""Join Slack""/>
    </a>
    <a href=""https://github.com/neuml/txtai/"
hello-git,"# Hello Git & GitHub La queria tanto

[![Git](https://img.shields.io/badge/Git-2.37+-f14e32?style=for-the-badge&logo=git&logoColor=white&labelColor=101010)](https://git-scm.com/)
[![GitHub](https://img.shields.io/badge/GitHub-Web-blue?style=for-the-badge&logo=github&logoColor=white&labelColor=101010)](https://github.com/)

## Curso completo de 5 horas y 45 lecciones para aprender a trabajar con Git & GitHub desde cero y para principiantes

![](./Media/header.jpg)

### Proyecto realizado durante emisiones en directo desde [Twitch](https://twitch.tv/mouredev)

> ##### Si consideras útil el curso, apóyalo haciendo ""★ Star"" en el repositorio. ¡Gracias!

## Lo que aprenderás

- Git desde su historia y fundamentos
- Conceptos principales y flujo de trabajo
- Manejo de terminal
- Instalación y configuración
- Más de 25 comandos de Git
- GitHub desde cero
- Configuración y autenticación
- Integración de Git con GitHub
- Flujo colaborativo
- Herramientas destacadas
- Ejemplos prácticos

Y mucho"
pattern,"Pattern
=======

[![Build Status](http://img.shields.io/travis/clips/pattern/master.svg?style=flat)](https://travis-ci.org/clips/pattern/branches)
[![Coverage](https://img.shields.io/coveralls/clips/pattern/master.svg?style=flat)](https://coveralls.io/github/clips/pattern?branch=master)
[![PyPi version](http://img.shields.io/pypi/v/pattern.svg?style=flat)](https://pypi.python.org/pypi/pattern)
[![License](https://img.shields.io/badge/License-BSD%203--Clause-green.svg?style=flat)](https://github.com/clips/pattern/blob/master/LICENSE.txt)

Pattern is a web mining module for Python. It has tools for:

 * Data Mining: web services (Google, Twitter, Wikipedia), web crawler, HTML DOM parser
 * Natural Language Processing: part-of-speech taggers, n-gram search, sentiment analysis, WordNet
 * Machine Learning: vector space model, clustering, classification (KNN, SVM, Perceptron)
 * Network Analysis: graph centrality and visualization.

It is well documented, thoroughly tested with 350+ unit te"
pytorch3d,"<img src=""https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/.github/pytorch3dlogo.png"" width=""900""/>

[![CircleCI](https://circleci.com/gh/facebookresearch/pytorch3d.svg?style=svg)](https://circleci.com/gh/facebookresearch/pytorch3d)
[![Anaconda-Server Badge](https://anaconda.org/pytorch3d/pytorch3d/badges/version.svg)](https://anaconda.org/pytorch3d/pytorch3d)

# Introduction

PyTorch3D provides efficient, reusable components for 3D Computer Vision research with [PyTorch](https://pytorch.org).

Key features include:

- Data structure for storing and manipulating triangle meshes
- Efficient operations on triangle meshes (projective transformations, graph convolution, sampling, loss functions)
- A differentiable mesh renderer
- Implicitron, see [its README](projects/implicitron_trainer), a framework for new-view synthesis via implicit representations. ([blog post](https://ai.facebook.com/blog/implicitron-a-new-modular-extensible-framework-for-neural-implicit-representati"
so-vits-svc-fork,"# SoftVC VITS Singing Voice Conversion Fork

[简体中文](README_zh_CN.md)

<p align=""center"">
  <a href=""https://github.com/voicepaw/so-vits-svc-fork/actions/workflows/ci.yml?query=branch%3Amain"">
    <img src=""https://img.shields.io/github/actions/workflow/status/voicepaw/so-vits-svc-fork/ci.yml?branch=main&label=CI&logo=github&style=flat-square"" alt=""CI Status"" >
  </a>
  <a href=""https://so-vits-svc-fork.readthedocs.io"">
    <img src=""https://img.shields.io/readthedocs/so-vits-svc-fork.svg?logo=read-the-docs&logoColor=fff&style=flat-square"" alt=""Documentation Status"">
  </a>
  <a href=""https://codecov.io/gh/voicepaw/so-vits-svc-fork"">
    <img src=""https://img.shields.io/codecov/c/github/voicepaw/so-vits-svc-fork.svg?logo=codecov&logoColor=fff&style=flat-square"" alt=""Test coverage percentage"">
  </a>
</p>
<p align=""center"">
  <a href=""https://python-poetry.org/"">
    <img src=""https://img.shields.io/badge/packaging-poetry-299bd7?style=flat-square&logo=data:image/png;base64,iVBORw0KGgoAAA"
arrow,"Arrow: Better dates & times for Python
======================================

.. start-inclusion-marker-do-not-remove

.. image:: https://github.com/arrow-py/arrow/workflows/tests/badge.svg?branch=master
   :alt: Build Status
   :target: https://github.com/arrow-py/arrow/actions?query=workflow%3Atests+branch%3Amaster

.. image:: https://codecov.io/gh/arrow-py/arrow/branch/master/graph/badge.svg
   :alt: Coverage
   :target: https://codecov.io/gh/arrow-py/arrow

.. image:: https://img.shields.io/pypi/v/arrow.svg
   :alt: PyPI Version
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/pypi/pyversions/arrow.svg
   :alt: Supported Python Versions
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/pypi/l/arrow.svg
   :alt: License
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :alt: Code Style: Black
   :target: https://github.com/psf/black


**Arrow** is "
awesome-math,"# Awesome Math [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome mathematics resources.

All resources are freely available except those with a 💲 icon.

# Contents

<!-- START_TOC -->

* [Contents](#contents)
* [General Resources](#general-resources)
    * [Learning Platforms](#learning-platforms)
    * [Learn to Learn](#learn-to-learn)
    * [Youtube Series](#youtube-series)
    * [Tools](#tools)
    * [Questions and Answers](#questions-and-answers)
    * [Encyclopedia](#encyclopedia)
    * [Books](#books)
    * [Magazines](#magazines)
    * [Blogs](#blogs)
    * [Meetings and Conferences](#meetings-and-conferences)
    * [Misc](#misc)
* [Branches of Mathematics](#branches-of-mathematics)
    * [Foundations of Mathematics](#foundations-of-mathematics)
        * [Transition To Pure Rigour Math](#transition-to-pure-rigour-math)
        * [Set Theory](#set-"
pymc,".. image:: https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg
    :height: 100px
    :alt: PyMC logo
    :align: center

|Build Status| |Coverage| |NumFOCUS_badge| |Binder| |Dockerhub| |DOIzenodo| |Conda Downloads|

PyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling
focusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI)
algorithms. Its flexibility and extensibility make it applicable to a
large suite of problems.

Check out the `PyMC overview <https://docs.pymc.io/en/latest/learn/core_notebooks/pymc_overview.html>`__,  or
one of `the many examples <https://www.pymc.io/projects/examples/en/latest/gallery.html>`__!
For questions on PyMC, head on over to our `PyMC Discourse <https://discourse.pymc.io/>`__ forum.

Features
========

-  Intuitive model specification syntax, for example, ``x ~ N(0,1)``
   translates to ``x = Normal('x',0,1)``
-  **Powerful sampling algorithms**, such as the `No U-Turn
   Sampler <ht"
hydra,"<p align=""center""><img src=""https://raw.githubusercontent.com/facebookresearch/hydra/main/website/static/img/Hydra-Readme-logo2.svg"" alt=""logo"" width=""70%"" /></p>

<p align=""center"">
  <a href=""https://pypi.org/project/hydra-core/"">
    <img src=""https://img.shields.io/pypi/v/hydra-core"" alt=""PyPI"" />
  </a>
  <a href=""https://circleci.com/gh/facebookresearch/hydra"">
    <img src=""https://img.shields.io/circleci/build/github/facebookresearch/hydra?token=af199cd2deca9e70e53776f9ded96284b10687e9"" alt=""CircleCI"" />
  </a>
  <a href=""#"">
    <img src=""https://img.shields.io/pypi/l/hydra-core"" alt=""PyPI - License"" />
  </a>
  <a href=""#"">
    <img src=""https://img.shields.io/pypi/pyversions/hydra-core"" alt=""PyPI - Python Version"" />
  </a>
  <a href=""https://www.pepy.tech/projects/hydra-core?versions=0.11.*&versions=1.0.*&versions=1.1.*&versions=1.2.*&versions=1.3.*&versions=1.4.*"">
    <img src=""https://pepy.tech/badge/hydra-core/month"" alt=""Downloads"" />
  </a>
  <a href=""https://github.c"
self-operating-computer,"<h1 align=""center"">Self-Operating Computer Framework</h1>

<p align=""center"">
  <strong>A framework to enable multimodal models to operate a computer.</strong>
</p>
<p align=""center"">
  Using the same inputs and outputs as a human operator, the model views the screen and decides on a series of mouse and keyboard actions to reach an objective. 
</p>

<div align=""center"">
  <img src=""https://github.com/OthersideAI/self-operating-computer/blob/main/readme/self-operating-computer.png"" width=""750""  style=""margin: 10px;""/>
</div>

<!--
:rotating_light: **OUTAGE NOTIFICATION: gpt-4o**
**This model is currently experiencing an outage so the self-operating computer may not work as expected.**
-->


## Key Features
- **Compatibility**: Designed for various multimodal models.
- **Integration**: Currently integrated with **GPT-4o, Gemini Pro Vision, Claude 3 and LLaVa.**
- **Future Plans**: Support for additional models.

## Ongoing Development
At [HyperwriteAI](https://www.hyperwriteai.com/), we "
opendrop,"# OpenDrop: an Open Source AirDrop Implementation

[![Release](https://img.shields.io/pypi/v/opendrop?color=%23EC6500&label=release)](https://pypi.org/project/opendrop/)
[![Language grade](https://img.shields.io/lgtm/grade/python/github/seemoo-lab/opendrop?label=code%20quality)](https://lgtm.com/projects/g/seemoo-lab/opendrop/context:python)

*OpenDrop* is a command-line tool that allows sharing files between devices directly over Wi-Fi. Its unique feature is that it is protocol-compatible with Apple AirDrop which allows to share files with Apple devices running iOS and macOS. 
~~Currently (and probably also for the foreseeable future), OpenDrop only supports sending to Apple devices that are discoverable by *everybody* as the default *contacts only* mode requires [Apple-signed certificates](https://www.apple.com/certificateauthority/pdf/Apple_AAI_CPS_v6.1.pdf).~~
We support contacts-only devices by using extracted AirDrop credentials (keys and certificates) from macOS via our [keychai"
bisheng,"<img src=""https://dataelem.com/bs/face.png"" alt=""Bisheng banner"">

<p align=""center"">
    <a href=""https://dataelem.feishu.cn/wiki/ZxW6wZyAJicX4WkG0NqcWsbynde""><img src=""https://img.shields.io/badge/docs-Wiki-brightgreen""></a>
    <img src=""https://img.shields.io/github/license/dataelement/bisheng"" alt=""license""/>
    <img src=""https://img.shields.io/docker/pulls/dataelement/bisheng-frontend"" alt=""docker-pull-count"" />
    <a href=""""><img src=""https://img.shields.io/github/last-commit/dataelement/bisheng""></a>
    <a href=""https://star-history.com/#dataelement/bisheng&Timeline""><img src=""https://img.shields.io/github/stars/dataelement/bisheng?color=yellow""></a> 
</p>
<p align=""center"">
  <a href=""./README_CN.md"">简体中文</a> |
  <a href=""./README.md"">English</a> |
  <a href=""./README_JPN.md"">日本語</a>
</p>

<p align=""center"">
  <a href=""https://trendshift.io/repositories/717"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/717"" alt=""dataelement%2Fbisheng | Trendshift"" "
data-science-from-scratch,"Data Science from Scratch
=========================

Here's all the code and examples from the second edition of my book _Data Science from Scratch_. They require at least Python 3.6.

(If you're looking for the code and examples from the first edition, that's in the `first-edition` folder.)

If you want to use the code, you should be able to clone the repo and just do things like

```
In [1]: from scratch.linear_algebra import dot

In [2]: dot([1, 2, 3], [4, 5, 6])
Out[2]: 32
```

and so on and so forth.

Two notes:

1. In order to use the library like this, you need to be in the root directory (that is, the directory that contains the `scratch` folder). If you are in the `scratch` directory itself, the imports won't work.

2. It's possible that it will just work. It's also possible that you may need to add the root directory to your `PYTHONPATH`, if you are on Linux or OSX this is as simple as 

```
export PYTHONPATH=/path/to/where/you/cloned/this/repo
```

(substituting in the real "
speechbrain,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/speechbrain/speechbrain/develop/docs/images/speechbrain-logo.svg"" alt=""SpeechBrain Logo""/>
</p>

[![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&size=40&duration=7000&pause=1000&random=false&width=1200&height=100&lines=Simplify+Conversational+AI+Development)](https://git.io/typing-svg)


| 📘 [Tutorials](https://speechbrain.readthedocs.io) | 🌐 [Website](https://speechbrain.github.io/) | 📚 [Documentation](https://speechbrain.readthedocs.io/en/latest/index.html) | 🤝 [Contributing](https://speechbrain.readthedocs.io/en/latest/contributing.html) | 🤗 [HuggingFace](https://huggingface.co/speechbrain) | ▶️ [YouTube](https://www.youtube.com/@SpeechBrainProject) | 🐦 [X](https://twitter.com/SpeechBrain1) |

![GitHub Repo stars](https://img.shields.io/github/stars/speechbrain/speechbrain?style=social) *Please, help our community project. Star on GitHub!*

**Exciting News (January, 2024):** Discover what is new in "
Book4_Power-of-Matrix,"《统计至简》五折入口：
https://zhuanlan.zhihu.com/p/634253719
<br>
《数学要素》五折入口：
https://zhuanlan.zhihu.com/p/620243026
<br>
《矩阵力量》五折入口：
https://zhuanlan.zhihu.com/p/634253719

看个人情况，开源资源，永久有效哈。

纠错多的同学会得到赠书，以示感谢。
"
PaddleSeg,"简体中文 | [English](README_EN.md)

<div align=""center"">

<p align=""center"">
  <img src=""./docs/images/paddleseg_logo.png"" align=""middle"" width = ""500"" />
</p>

**飞桨高性能图像分割开发套件，端到端完成从训练到部署的全流程图像分割应用。**


[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)
[![Version](https://img.shields.io/github/release/PaddlePaddle/PaddleSeg.svg)](https://github.com/PaddlePaddle/PaddleSeg/releases)
![python version](https://img.shields.io/badge/python-3.6+-orange.svg)
![support os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-yellow.svg)
![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleSeg?color=ccf)
</div>

<div align=""center"">
<img src=""https://github.com/shiyutang/files/blob/9590ea6bfc36139982ce75b00d3b9f26713934dd/teasor.gif""  width = ""800"" />  
</div>

## <img src=""./docs/images/seg_news_icon.png"" width=""20""/> 最新动态
- [2024-06-27] **💥 飞桨低代码开发工具 PaddleX 3.0 重磅更新！**
  - 丰富的模型产线：精选 68 个优质飞桨模型，涵盖图像分类、目标检测、图像分割、OCR、文本图像版面分析、时序分析等任务场景；
  - 低代码开发范式：支持单模型"
chinese-dos-games,"# 🎮 中文 DOS 游戏

网址： https://dos.lol


中文 DOS 游戏合集，目前共有 1898 款游戏。

## 下载游戏文件

在根目录下运行 Python 3 脚本

``` python
python download_data.py
```

若下载出错请参见 [Issue #26](https://github.com/rwv/chinese-dos-games/issues/26)

## 游戏列表

参见 https://dos.lol/games

## IPFS

IPNS Hash: [`k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2`](https://ipfs.io/ipns/k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2)

## 网站源代码

请参见 [rwv/chinese-dos-games-web: 🌐 Source code of https://dos.zczc.cz](https://github.com/rwv/chinese-dos-games-web)

## 版权问题

本人明白此项目存在版权上的侵权，如版权方介意的话，请联系 [chinese.dos.games@outlook.com](mailto:chinese.dos.games@outlook.com)，本人将立刻删除有关文件。

## Contributing

欢迎提 [Issue](https://github.com/rwv/chinese-dos-games/issues) 和 [Pull request](https://github.com/rwv/chinese-dos-games/pulls) 来增加新的游戏!

PR 具体参见 [CONTRIBUTING.md](https://github.com/rwv/chinese-dos-games/blob/master/CONTRIBUTING.md)

## Credits

* [dreamlayers/em-dosbox: An Emscripten port of DOSBox](https://github.com/dreamla"
vid2vid,"<img src='imgs/teaser.gif' align=""right"" width=360>

<br><br><br><br>

# vid2vid
### [Project](https://tcwang0509.github.io/vid2vid/) | [YouTube(short)](https://youtu.be/5zlcXTCpQqM) | [YouTube(full)](https://youtu.be/GrP_aOSXt5U) | [arXiv](https://arxiv.org/abs/1808.06601) | [Paper(full)](https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf)

Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). <br><br>
[Video-to-Video Synthesis](https://tcwang0509.github.io/vid2vid/)  
 [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyul"
ImageAI,"# ImageAI (v3.0.3)



[![Build Status](https://travis-ci.com/OlafenwaMoses/ImageAI.svg?branch=master)](https://travis-ci.com/OlafenwaMoses/ImageAI)  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/OlafenwaMoses/ImageAI/blob/master/LICENSE) [![PyPI version](https://badge.fury.io/py/imageai.svg)](https://badge.fury.io/py/imageai)   [![Downloads](https://pepy.tech/badge/imageai/month)](https://pepy.tech/project/imageai) [![Downloads](https://pepy.tech/badge/imageai/week)](https://pepy.tech/project/imageai)

An open-source python library built to empower developers to build applications and systems with self-contained Deep Learning and Computer Vision capabilities using simple and few lines of code.
 
 If you will like to sponsor this project, kindly visit the <strong>[Github sponsor page](https://github.com/sponsors/OlafenwaMoses)</strong>.
 
 
## ---------------------------------------------------
## Introducing Jarvis and TheiaEngine.

We the cr"
google-images-download,"Google Images Download
######################

Python Script for 'searching' and 'downloading' hundreds of Google images to the local hard disk!

Documentation
=============

* `Documentation Homepage <https://google-images-download.readthedocs.io/en/latest/index.html>`__
* `Installation <https://google-images-download.readthedocs.io/en/latest/installation.html>`__
* `Input arguments <https://google-images-download.readthedocs.io/en/latest/arguments.html>`__
* `Examples and Code Samples <https://google-images-download.readthedocs.io/en/latest/examples.html#>`__


Disclaimer
==========

This program lets you download tons of images from Google.
Please do not download or use any image that violates its copyright terms.
Google Images is a search engine that merely indexes images and allows you to find them.
It does NOT produce its own images and, as such, it doesn't own copyright on any of them.
The original creators of the images own the copyrights.

Images published in the United States"
instaloader,".. image:: https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png

.. badges-start

|pypi| |pyversion| |license| |aur| |contributors| |downloads|

.. |pypi| image:: https://img.shields.io/pypi/v/instaloader.svg
   :alt: Instaloader PyPI Project Page
   :target: https://pypi.org/project/instaloader/

.. |license| image:: https://img.shields.io/github/license/instaloader/instaloader.svg
   :alt: MIT License
   :target: https://github.com/instaloader/instaloader/blob/master/LICENSE

.. |pyversion| image:: https://img.shields.io/pypi/pyversions/instaloader.svg
   :alt: Supported Python Versions

.. |contributors| image:: https://img.shields.io/github/contributors/instaloader/instaloader.svg
   :alt: Contributor Count
   :target: https://github.com/instaloader/instaloader/graphs/contributors

.. |aur| image:: https://img.shields.io/aur/version/instaloader.svg
   :alt: Arch User Repository Package
   :target: https://aur.archlinux.org/packages/instaloader/

.."
flasky,"Flasky
======

This repository contains the source code examples for the second edition of my O'Reilly book [Flask Web Development](http://www.flaskbook.com).

The commits and tags in this repository were carefully created to match the sequence in which concepts are presented in the book. Please read the section titled ""How to Work with the Example Code"" in the book's preface for instructions.

For Readers of the First Edition of the Book
--------------------------------------------

The code examples for the first edition of the book were moved to a different repository: [https://github.com/miguelgrinberg/flasky-first-edition](https://github.com/miguelgrinberg/flasky-first-edition).
"
RobustVideoMatting,"# Robust Video Matting (RVM)

![Teaser](/documentation/image/teaser.gif)

<p align=""center"">English | <a href=""README_zh_Hans.md"">中文</a></p>

Official repository for the paper [Robust High-Resolution Video Matting with Temporal Guidance](https://peterl1n.github.io/RobustVideoMatting/). RVM is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network to process videos with temporal memory. RVM can perform matting in real-time on any videos without additional inputs. It achieves **4K 76FPS** and **HD 104FPS** on an Nvidia GTX 1080 Ti GPU. The project was developed at [ByteDance Inc.](https://www.bytedance.com/)

<br>

## News

* [Nov 03 2021] Fixed a bug in [train.py](https://github.com/PeterL1n/RobustVideoMatting/commit/48effc91576a9e0e7a8519f3da687c0d3522045f).
* [Sep 16 2021] Code is re-released under GPL-3.0 license.
* [Aug 25 2021] Source code and pretrained models are published."
U-2-Net,"<p align=""center"">
  <img width=""320"" height=""320"" src=""figures/U2Net_Logo.png"">
  
  <h1 align=""center"">U<sup>2</sup>-Net: U Square Net</h1>
    
</p>

This is the official repo for our paper **U<sup>2</sup>-Net(U square net)** published in Pattern Recognition 2020:

## [U<sup>2</sup>-Net: Going Deeper with Nested U-Structure for Salient Object Detection](https://arxiv.org/pdf/2005.09007.pdf)
[Xuebin Qin](https://xuebinqin.github.io/), [Zichen Zhang](https://webdocs.cs.ualberta.ca/~zichen2/), [Chenyang Huang](https://chenyangh.com/), [Masood Dehghan](https://sites.google.com/view/masooddehghan), [Osmar R. Zaiane](http://webdocs.cs.ualberta.ca/~zaiane/) and [Martin Jagersand](https://webdocs.cs.ualberta.ca/~jag/)


__Contact__: xuebin[at]ualberta[dot]ca

## Updates !!!

** (2022-Aug.-24) ** We are glad to announce that our U<sup>2</sup>-Net published in Pattern Recognition has been awarded the 2020 Pattern Recognition BEST PAPER AWARD !!!
![u2net-best-paper](figures/u2net-best-paper.jp"
pyro,"<!--
Copyright Contributors to the Pyro project.

SPDX-License-Identifier: Apache-2.0
-->

<div align=""center"">
  <a href=""http://pyro.ai""> <img width=""220px"" height=""220px"" src=""docs/source/_static/img/pyro_logo_with_text.png""></a>
</div>

-----------------------------------------

[![Build Status](https://github.com/pyro-ppl/pyro/workflows/CI/badge.svg)](https://github.com/pyro-ppl/pyro/actions)
[![Coverage Status](https://coveralls.io/repos/github/pyro-ppl/pyro/badge.svg?branch=dev)](https://coveralls.io/github/pyro-ppl/pyro?branch=dev)
[![Latest Version](https://badge.fury.io/py/pyro-ppl.svg)](https://pypi.python.org/pypi/pyro-ppl)
[![Documentation Status](https://readthedocs.org/projects/pyro-ppl/badge/?version=dev)](http://pyro-ppl.readthedocs.io/en/stable/?badge=dev)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3056/badge)](https://bestpractices.coreinfrastructure.org/projects/3056)

[Getting Started](http://pyro.ai/examples) |
[Documentation](htt"
awesome-honeypots,"# Awesome Honeypots [![Awesome Honeypots](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome honeypots, plus related components and much more, divided into categories such as Web, services, and others, with a focus on free and open source projects.

There is no pre-established order of items in each category, the order is for contribution. If you want to contribute, please read the [guide](CONTRIBUTING.md).

Discover more awesome lists at [sindresorhus/awesome](https://github.com/sindresorhus/awesome).

# Contents

- [Awesome Honeypots ![Awesome Honeypots](https://github.com/sindresorhus/awesome)](#awesome-honeypots-)
- [Contents](#contents)
  - [Related Lists](#related-lists)
  - [Honeypots](#honeypots)
  - [Honeyd Tools](#honeyd-tools)
  - [Network and Artifact Analysis](#network-and-artifact-analysis)
  - [Data Tools](#data-tools)
  - [Guides](#guides)

## Related "
pyod,"Python Outlier Detection (PyOD)
===============================

**Deployment & Documentation & Stats & License**

|badge_pypi| |badge_anaconda| |badge_docs| |badge_stars| |badge_forks| |badge_downloads| |badge_testing| |badge_coverage| |badge_maintainability| |badge_license| |badge_benchmark|

.. |badge_pypi| image:: https://img.shields.io/pypi/v/pyod.svg?color=brightgreen
   :target: https://pypi.org/project/pyod/
   :alt: PyPI version

.. |badge_anaconda| image:: https://anaconda.org/conda-forge/pyod/badges/version.svg
   :target: https://anaconda.org/conda-forge/pyod
   :alt: Anaconda version

.. |badge_docs| image:: https://readthedocs.org/projects/pyod/badge/?version=latest
   :target: https://pyod.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation status

.. |badge_stars| image:: https://img.shields.io/github/stars/yzhao062/pyod.svg
   :target: https://github.com/yzhao062/pyod/stargazers
   :alt: GitHub stars

.. |badge_forks| image:: https://img.shields.io/github/for"
FreeAskInternet,"# FreeAskInternet

## 🎉🎉🎉 Yeah we have a logo now! 🎉🎉🎉

![lgoo](./doc/logo-20240412.png)

> Running www.perplexity.ai like app complete FREE, LOCAL, PRIVATE and NO GPU NEED on any computer
> [!IMPORTANT]  
> **If you are unable to use this project normally, it is most likely due to issues with your internet connection or your IP, you need free internet connection to use this project normally. 如果您无法正常使用此项目，很可能是由于您的 IP 存在问题，或者你不能自由访问互联网。**

## What is FreeAskInternet

FreeAskInternet is a completely free, private and locally running search aggregator & answer generate using LLM, Without GPU needed. The user can ask a question and the system will use searxng to make a multi engine search and combine the search result to the ChatGPT3.5 LLM and generate the answer based on search results. All process running locally and  No GPU or OpenAI or Google API keys are needed.

## Features

- 🈚️ Completely FREE (no need for any API keys)
- 💻 Completely LOCAL (no GPU need, any computer can run )
- 🔐 "
LibreTranslate,"# LibreTranslate

[Try it online!](https://libretranslate.com) | [API Docs](https://libretranslate.com/docs) | [Community Forum](https://community.libretranslate.com/)

[![Python versions](https://img.shields.io/pypi/pyversions/libretranslate)](https://pypi.org/project/libretranslate) [![Run tests](https://github.com/LibreTranslate/LibreTranslate/workflows/Run%20tests/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions?query=workflow%3A%22Run+tests%22) [![Build and Publish Docker Image](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml) [![Publish package](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml) [![Awesome Humane Tech](https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/"
supervisor,"Supervisor
==========

Supervisor is a client/server system that allows its users to
control a number of processes on UNIX-like operating systems.

Supported Platforms
-------------------

Supervisor has been tested and is known to run on Linux (Ubuntu), Mac OS X
(10.4, 10.5, 10.6), and Solaris (10 for Intel) and FreeBSD 6.1.  It will
likely work fine on most UNIX systems.

Supervisor will not run at all under any version of Windows.

Supervisor is intended to work on Python 3 version 3.4 or later
and on Python 2 version 2.7.

Documentation
-------------

You can view the current Supervisor documentation online `in HTML format
<http://supervisord.org/>`_ .  This is where you should go for detailed
installation and configuration documentation.

Reporting Bugs and Viewing the Source Repository
------------------------------------------------

Please report bugs in the `GitHub issue tracker
<https://github.com/Supervisor/supervisor/issues>`_.

You can view the source repository for superv"
WARP-Clash-API,"# WARP Clash API

![GitHub License](https://img.shields.io/github/license/vvbbnn00/WARP-Clash-API)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/67ca8d105fb947eca6204230ba3ac09b)](https://app.codacy.com/gh/vvbbnn00/WARP-Clash-API/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)
![GitHub Repo stars](https://img.shields.io/github/stars/vvbbnn00/WARP-Clash-API?style=flat)

中文 | [English](./README_en.md)

> **Warning**
>
> 本项目是完全非商业项目，仅供学习交流使用，请勿用于非法用途，否则后果自负。

## 🤔 这是什么？

该项目可以让你通过订阅的方式使用`WARP+`，支持`Clash`、`Shadowrocket`等客户端。项目内置了
刷取`WARP+`流量的功能，可以让你的`WARP+`流量不再受限制（每`18`秒可获得`1GB`流量），同时，
配备了`IP`选优功能。支持`Docker compose`
一键部署，无需额外操作，即可享受你自己的`WARP+`私
有高速节点！

## 💡 特色功能

- 💻 支持`Clash`、`Surge`、`Shadowrocket`等客户端
- 🔑 支持设置您自己的`LicenseKey`
- 🌏 支持`IP`选优
- 🐋 支持`Docker compose`一键部署
- 📕 全自动刷取`WARP+`流量，请求经过代理，防封`IP`
- ❓ 每次更新订阅随机节点，让你体验抽卡的乐趣

## 🚀 快速上手

### 1. 安装`Docker`和`Docker compose`

- `Docker`
  安装教程：[https://docs.docker.com/engine/install/](https://doc"
microk8s,"<img src=""docs/images/MicroK8s-logo-RGB-2022.png"" width=""400px;"" />

[![](https://github.com/canonical/microk8s/actions/workflows/build-snap.yml/badge.svg)](https://github.com/canonical/microk8s/actions/workflows/build-snap.yml)
[![](https://snapcraft.io/microk8s/badge.svg)](https://snapcraft.io/microk8s)
![](https://img.shields.io/badge/Kubernetes-1.30-326de6.svg)

<img src=""/docs/images/certified_kubernetes_color-222x300.png"" align=""right"" width=""200px"">

## The smallest, fastest Kubernetes

Single-package fully conformant lightweight Kubernetes that works on [42
flavours of Linux](https://snapcraft.io/microk8s). Perfect for:

- Developer workstations
- IoT
- Edge
- CI/CD

 > Canonical might have assembled the easiest way to provision a single node Kubernetes cluster - [Kelsey Hightower](https://twitter.com/kelseyhightower/status/1120834594138406912)

## Why MicroK8s?

- **Small**. Developers want the smallest K8s for laptop and workstation
  development.  MicroK8s provides a standal"
xformers,"<img src=""./docs/assets/logo.png"" width=800>

![Install with conda](https://anaconda.org/xformers/xformers/badges/installer/conda.svg)
![Downloads](https://anaconda.org/xformers/xformers/badges/downloads.svg)
![License](https://anaconda.org/xformers/xformers/badges/license.svg)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/xformers/blob/main/docs/source/xformers_mingpt.ipynb)
<br/><!--
![PyPI](https://img.shields.io/pypi/v/xformers)
![PyPI - License](https://img.shields.io/pypi/l/xformers)
[![Documentation Status](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)
-->
[![CircleCI](https://circleci.com/gh/facebookresearch/xformers.svg?style=shield)](https://app.circleci.com/pipelines/github/facebookresearch/xformers/)
[![Codecov](https://codecov.io/gh/facebookresearch/xformers/"
uvicorn,"<p align=""center"">
  <img width=""320"" height=""320"" src=""https://raw.githubusercontent.com/tomchristie/uvicorn/master/docs/uvicorn.png"" alt='uvicorn'>
</p>

<p align=""center"">
<em>An ASGI web server, for Python.</em>
</p>

---

[![Build Status](https://github.com/encode/uvicorn/workflows/Test%20Suite/badge.svg)](https://github.com/encode/uvicorn/actions)
[![Package version](https://badge.fury.io/py/uvicorn.svg)](https://pypi.python.org/pypi/uvicorn)
[![Supported Python Version](https://img.shields.io/pypi/pyversions/uvicorn.svg?color=%2334D058)](https://pypi.org/project/uvicorn)

**Documentation**: [https://www.uvicorn.org](https://www.uvicorn.org)

---

Uvicorn is an ASGI web server implementation for Python.

Until recently Python has lacked a minimal low-level server/application interface for
async frameworks. The [ASGI specification][asgi] fills this gap, and means we're now able to
start building a common set of tooling usable across all async frameworks.

Uvicorn supports HTTP/1.1"
bottle,".. image:: http://bottlepy.org/docs/dev/_static/logo_nav.png
  :target: http://bottlepy.org/
  :alt: Bottle Logo
  :align: right

.. image:: https://github.com/bottlepy/bottle/workflows/Tests/badge.svg
    :target: https://github.com/bottlepy/bottle/workflows/Tests
    :alt: Tests Status

.. image:: https://img.shields.io/pypi/v/bottle.svg
    :target: https://pypi.python.org/pypi/bottle/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/l/bottle.svg
    :target: https://pypi.python.org/pypi/bottle/
    :alt: License

.. _Python: https://python.org/
.. _mako: https://www.makotemplates.org/
.. _cheetah: https://www.cheetahtemplate.org/
.. _jinja2: https://jinja.palletsprojects.com/

.. _WSGI: https://peps.python.org/pep-3333/
.. _gunicorn: https://gunicorn.org/
.. _paste: https://pythonpaste.readthedocs.io/
.. _cheroot: https://cheroot.cherrypy.dev/

============================
Bottle: Python Web Framework
============================

Bottle is a fast, simple and lightw"
einops,"
<!--
<a href='http://arogozhnikov.github.io/images/einops/einops_video.mp4' >
<div align=""center"">
  <img src=""http://arogozhnikov.github.io/images/einops/einops_video.gif"" alt=""einops package examples"" />
  <br>
  <small><a href='http://arogozhnikov.github.io/images/einops/einops_video.mp4'>This video in high quality (mp4)</a></small>
  <br><br>
</div>
</a>
-->

<!-- this link magically rendered as video on github readme, unfortunately not in docs -->

https://user-images.githubusercontent.com/6318811/177030658-66f0eb5d-e136-44d8-99c9-86ae298ead5b.mp4




# einops 
[![Run tests](https://github.com/arogozhnikov/einops/actions/workflows/run_tests.yml/badge.svg)](https://github.com/arogozhnikov/einops/actions/workflows/run_tests.yml)
[![PyPI version](https://badge.fury.io/py/einops.svg)](https://badge.fury.io/py/einops)
[![Documentation](https://img.shields.io/badge/documentation-link-blue.svg)](https://einops.rocks/)
![Supported python versions](https://raw.githubusercontent.com/arogoz"
pupy,"# Pupy

[![Build Status](https://api.travis-ci.org/n1nj4sec/pupy.svg?branch=unstable)](https://travis-ci.org/n1nj4sec/pupy)

## Installation

Installation instructions are on the wiki, in addition to all other documentation. For maximum compatibility, it is recommended to use Docker Compose.

[Refer to the wiki](https://github.com/n1nj4sec/pupy/wiki/Installation)

## Description

Pupy is a cross-platform, multi function RAT and post-exploitation tool mainly written in python. It features an all-in-memory execution guideline and leaves a very low footprint. Pupy can communicate using multiple transports, migrate into processes using reflective injection, and load remote python code, python packages and python C-extensions from memory.

## Features

- Windows payload can load the entire Python interpreter from memory using a reflective DLL.
	- Pupy does not touch the disk.

- Can be packed into a single .py file and run without any dependencies other than the python standard library on a"
optimate,"# OptiMate

**[Legacy]**

This repository is now in a legacy phase and is no longer actively maintained. Although the source code is still available in the Git history, there will be no additional updates or official support.

**[About Nebuly]**

Our team is fully committed on creating the best user-experience platform for LLMs so that companies can understand user behavior at scale when interacting with their LLM-based products. 
- To learn more on how to get started, visit our [official documentation](https://docs.nebuly.com/welcome/overview)
- If you need enterprise support, please contact us [here](https://www.nebuly.com/nebuly-book-a-demo)

**[About optimate]**

We have open-sourced a couple of internal projects to the community, but we are not currently maintaining them. Optimate is a collection of libraries designed to help you optimize your AI models. It is an open-source project developed by Nebuly AI but is **not actively maintained**.

The tools available to assist you in yo"
CrackMapExec,"# No Longer Maintained

This project is no longer mantained due to the existence of a hostile fork.

# CrackMapExec

<p align=""center"">
  <img src=""https://cloud.githubusercontent.com/assets/5151193/17577511/d312ceb4-5f3b-11e6-8de5-8822246289fd.jpg"" alt=""cme""/>
</p>

You are on the **latest up-to-date** repository of the project CrackMapExec ! 🎉

- 🚧 If you want to report a problem, open un [Issue](https://github.com/mpgn/CrackMapExec/issues) 
- 🔀 If you want to contribute, open a [Pull Request](https://github.com/mpgn/CrackMapExec/pulls)
- 💬 If you want to discuss, open a [Discussion](https://github.com/mpgn/CrackMapExec/discussions)

# Acknowledgments
**(These are the people who did the hard stuff)**

This project was originally inspired by:
- [CredCrack](https://github.com/gojhonny/CredCrack)
- [smbexec](https://github.com/pentestgeek/smbexec)
- [smbmap](https://github.com/ShawnDEvans/smbmap)

Unintentional contributors:

- The [Empire](https://github.com/PowerShellEmpire/Empire) pr"
ansible-for-devops,"# Ansible for DevOps Examples

[![CI](https://github.com/geerlingguy/ansible-for-devops/workflows/CI/badge.svg?event=push)](https://github.com/geerlingguy/ansible-for-devops/actions?query=workflow%3ACI) [![Molecule CI](https://github.com/geerlingguy/ansible-for-devops/workflows/Molecule%20CI/badge.svg?event=push)](https://github.com/geerlingguy/ansible-for-devops/actions?query=workflow%3A%22Molecule+CI%22)

This repository contains Ansible examples developed to support different sections of [Ansible for DevOps](https://www.ansiblefordevops.com/), a book on [Ansible](http://www.ansible.com/) by [Jeff Geerling](https://www.jeffgeerling.com/).

Many examples use Vagrant, VirtualBox, and Ansible to boot and configure VMs on your local workstation.

Not all playbooks follow all of Ansible's best practices, as they illustrate particular Ansible features in an instructive manner.

## Manuscript

The book's manuscript is released under the CC BY-SA license, and is publicly available in a separ"
composio,"<p align=""center"">
  <a href=""https://x.com/GanatraSoham/?utm_campaign=github-readme"" target=""_blank"">
    <img src=""./python/docs/imgs/follow_x.png"" width=""100%"" alt=""Follow me"" />
  </a>
  <br /> <br />
</p>
<p align=""center"">
  <a href=""https://app.composio.dev/?utm_campaign=github-readme"" target=""_blank"">
    <img src=""./python/docs/imgs/try_hosted.png"" width=""100%"" alt=""Sign up"" />
  </a>

  <br /> <br />
</p>
<p>
  <a href=""https://github.com/composiohq/composio/blob/master/README.md"">EN</a> | <a href=""https://github.com/composiohq/composio/blob/master/README-CN.md"">CN</a> | <a href=""https://github.com/composiohq/composio/blob/master/README-JP.md"">JP</a>
</p>

<p align=""center"">
  <a href=""https://composio.dev//#gh-dark-mode-only"">
    <img src=""./python/docs/imgs/composio_white_font.svg"" width=""318px"" alt=""Composio logo"" />
  </a>
  <a href=""https://composio.dev//#gh-light-mode-only"">
    <img src=""./python/docs/imgs/composio_black_font.svg"" width=""318px"" alt=""Composio Logo"" />
"
speech_recognition,"SpeechRecognition
=================

.. image:: https://img.shields.io/pypi/v/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/status/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: Development Status

.. image:: https://img.shields.io/pypi/pyversions/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: Supported Python Versions

.. image:: https://img.shields.io/pypi/l/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: License

.. image:: https://api.travis-ci.org/Uberi/speech_recognition.svg?branch=master
    :target: https://travis-ci.org/Uberi/speech_recognition
    :alt: Continuous Integration Test Results

Library for performing speech recognition, with support for several engines and APIs, online and offline.

**UPDATE 2022-02-09**:"
xonsh,"xonsh
=====

.. class:: center

    **xonsh** is a Python-powered shell. Full-featured and cross-platform. The language is a superset of Python 3.6+ with additional shell primitives.  Xonsh word was made from *conch* (🐚, *@*) and indicates belonging to the command shells world.


.. list-table::
   :widths: 1 1

   *  -  **Xonsh is the Shell**
      -  **Xonsh is Python**

   *  -  .. code-block:: shell

            cd $HOME

            id $(whoami)

            cat /etc/passwd | grep root > ~/root.txt

            $PROMPT = '@ '


      -  .. code-block:: python

            2 + 2

            var = ""hello"".upper()

            import json; json.loads('{""a"":1}')

            [i for i in range(0,10)]

   *  -  **Xonsh is the Shell in Python**
      -  **Xonsh is Python in the Shell**

   *  -  .. code-block:: python

            len($(curl -L https://xon.sh))

            $PATH.append('/tmp')

            p'/etc/passwd'.read_text().find('root')

            xontrib load dalias
       "
pywal,"<h3 align=""center""><img src=""https://i.imgur.com/5WgMACe.gif"" width=""200px""></h3>
<p align=""center"">Generate and change color-schemes on the fly.</p>

<p align=""center"">
<a href=""https://travis-ci.org/dylanaraps/pywal""><img src=""https://travis-ci.org/dylanaraps/pywal.svg?branch=master""></a>
<a href=""./LICENSE.md""><img src=""https://img.shields.io/badge/license-MIT-blue.svg""></a>
<a href=""https://pypi.python.org/pypi/pywal/""><img src=""https://img.shields.io/pypi/v/pywal.svg""></a>
<a href=""https://www.patreon.com/dyla""><img src=""https://img.shields.io/badge/donate-patreon-yellow.svg""></a>
<a href=""https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=V7QNJNKS3WYVS""><img src=""https://img.shields.io/badge/donate-paypal-green.svg""></a>
</p>

<img src=""https://i.imgur.com/HhK3LDv.jpg"" alt=""img"" align=""right"" width=""400px"">

Pywal is a tool that generates a color palette from the dominant colors in an image. It then applies the colors system-wide and on-the-fly in all of your fa"
apex,"# Introduction

This repository holds NVIDIA-maintained utilities to streamline mixed precision and distributed training in Pytorch.
Some of the code here will be included in upstream Pytorch eventually.
The intent of Apex is to make up-to-date utilities available to users as quickly as possible.

## Full API Documentation: [https://nvidia.github.io/apex](https://nvidia.github.io/apex)

## [GTC 2019](https://github.com/mcarilli/mixed_precision_references/tree/master/GTC_2019) and [Pytorch DevCon 2019](https://github.com/mcarilli/mixed_precision_references/tree/master/Pytorch_Devcon_2019) Slides

# Contents

## 1. Amp:  Automatic Mixed Precision

**Deprecated. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)**

`apex.amp` is a tool to enable mixed precision training by changing only 3 lines of your script.
Users can easily experiment with different pure and mixed precision training modes by supplying
different flags to `amp.initialize`.

[Webinar introducing Amp](https://info"
espnet,"<div align=""left""><img src=""doc/image/espnet_logo1.png"" width=""550""/></div>

# ESPnet: end-to-end speech processing toolkit

|system/pytorch ver.|1.13.1|2.0.1|2.1.2|2.2.2|2.3.1|2.4.0|
| :---- | :---: | :---: | :---: | :---: | :---: | :---: |
|ubuntu/python3.10/pip||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query"
ml-ferret,"<!-- # Project Name

This software project accompanies the research paper, [Paper title](https://arxiv.org).

Brief description of the project.

## Documentation

## Getting Started  -->

# <img src=""figs/ferret_icon.png"" alt=""Alt text for the image"" width=""40"" height=""45""> Ferret: Refer and Ground Anything Anywhere at Any Granularity

*An End-to-End MLLM that Accept Any-Form Referring and Ground Anything in Response.* [[Paper](https://arxiv.org/abs/2310.07704)]

[Haoxuan You*](https://hxyou.github.io/), [Haotian Zhang*](https://haotian-zhang.github.io/), [Zhe Gan](https://zhegan27.github.io/), [Xianzhi Du](https://scholar.google.com/citations?user=l1hP40AAAAAJ&hl=en), [Bowen Zhang](https://zbwglory.github.io/), [Zirui Wang](https://www.cs.cmu.edu/~ziruiw/), [Liangliang Cao](http://llcao.net/), [Shih-Fu Chang](https://www.ee.columbia.edu/~sfchang/), [Yinfei Yang](https://sites.google.com/site/yinfeiyang/) 
[*: equal contribution]


## Overview

<p align=""center"">
    <img src=""figs/fer"
weiboSpider,"[![Build Status](https://github.com/dataabc/weiboSpider/workflows/Python%20application/badge.svg)](https://badge.fury.io/py/weibo-spider)
[![Python](https://img.shields.io/pypi/pyversions/weibo-spider)](https://badge.fury.io/py/weibo-spider)
[![PyPI](https://badge.fury.io/py/weibo-spider.svg)](https://badge.fury.io/py/weibo-spider)

# Weibo Spider

本程序可以连续爬取**一个**或**多个**新浪微博用户（如[胡歌](https://weibo.cn/u/1223178222)、[迪丽热巴](https://weibo.cn/u/1669879400)、[郭碧婷](https://weibo.cn/u/1729370543)）的数据，并将结果信息写入**文件**或**数据库**。写入信息几乎包括用户微博的所有数据，包括**用户信息**和**微博信息**两大类。因为内容太多，这里不再赘述，详细内容见[获取到的字段](#获取到的字段)。如果只需要用户信息，可以通过设置实现只爬取微博用户信息的功能。本程序需设置cookie来获取微博访问权限，后面会讲解[如何获取cookie](#如何获取cookie)。如果不想设置cookie，可以使用[免cookie版](https://github.com/dataabc/weibo-crawler)，二者功能类似。

爬取结果可写入文件和数据库，具体的写入文件类型如下：

- **txt文件**（默认）
- **csv文件**（默认）
- **json文件**（可选）
- **MySQL数据库**（可选）
- **MongoDB数据库**（可选）
- **SQLite数据库**（可选）

同时支持下载微博中的图片和视频，具体的可下载文件如下：

- **原创**微博中的原始**图片**（可选）
- **转发**微博中的原始**图片**（可选）
- **原创**微博中的**视频**（可选）
"
outlines,"<div align=""center"" style=""margin-bottom: 1em;"">

<img src=""./docs/assets/images/logo.png"" alt=""Outlines Logo"" width=500></img>

[![.txt Twitter][dottxt-twitter-badge]][dottxt-twitter]

[![Documentation][documentation-badge]][documentation]
[![Contributors][contributors-badge]][contributors]
[![Downloads][downloads-badge]][pypistats]
[![Discord][discord-badge]][discord]


*Robust (structured) text generation.*

Made with ❤👷️ by the team at [.txt](https://dottxt.co).

</div>


``` bash
pip install outlines
```

First time here? Go to our [setup guide](https://dottxt-ai.github.io/outlines/welcome)

## Features

- [x] 🤖 [Multiple model integrations](https://dottxt-ai.github.io/outlines/installation): OpenAI, transformers, llama.cpp, exllama2, mamba
- [x] 🖍️ Simple and powerful prompting primitives based on the [Jinja templating engine](https://jinja.palletsprojects.com/)
- [x] 🚄 [Multiple choices](#multiple-choices), [type constraints](#type-constraint) and dynamic stopping
- [x] ⚡ Fast ["
machine_learning_examples,"machine_learning_examples
=========================

A collection of machine learning examples and tutorials.

Find associated tutorials at https://lazyprogrammer.me

Find associated courses at https://deeplearningcourses.com

Please note that not all code from all courses will be found in this repository. Some newer code examples (e.g. most of Tensorflow 2.0) were done in Google Colab. Therefore, you should check the instructions given in the lectures for the course you are taking.


How to I find the code for a particular course?
===============================================

The code for each course is separated by folder. You can determine which folder corresponds with which course by watching the ""Where to get the code"" lecture inside the course (usually Lecture 2 or 3).

Remember: one folder = one course.


Why you should not fork this repo
=================================

I've noticed that many people have out-of-date forks. Thus, I recommend not forking this repository if y"
python-for-android,"# python-for-android

python-for-android (p4a) is a development tool that packages Python apps into
binaries that can run on Android devices.

It can generate: 

* [Android Package](https://en.wikipedia.org/wiki/Apk_(file_format)) (APK)
  files, ready to install locally on a device, especially for testing. This format
  is used by many [app stores](https://en.wikipedia.org/wiki/List_of_Android_app_stores)
  but not [Google Play Store](https://play.google.com/store/). 
* [Android App Bundle](https://developer.android.com/guide/app-bundle/faq) 
  (AAB) files which can be shared on [Google Play Store](https://play.google.com/store/).
* [Android Archive](https://developer.android.com/studio/projects/android-library)
  (AAR) files which can be used as a re-usable bundle of resources for other 
  projects.
 
It supports multiple CPU architectures.

It supports apps developed with [Kivy framework](http://kivy.org), but was
built to be flexible about the backend libraries (through ""bootstraps"""
Monocraft,"# Monocraft

[![Github all releases](https://img.shields.io/github/downloads/IdreesInc/Monocraft/total.svg)](https://GitHub.com/IdreesInc/Monocraft/releases/)
![](https://img.shields.io/github/license/IdreesInc/Monocraft)
[![](https://img.shields.io/github/v/release/IdreesInc/Monocraft)](https://GitHub.com/IdreesInc/Monocraft/releases/)

## [`Download it here!`](https://github.com/IdreesInc/Monocraft/releases)
<br/>

![](images/preview.png)


The monospaced font for developers who like Minecraft a bit _too_ much.

If you'd like to see a vectorized version of this font, try [Miracode](https://github.com/IdreesInc/Miracode)!

*Notice: This project is not affiliated with Minecraft or Mojang in any way and is exclusively a fan project. This font emulates the typeface of the font used in the Minecraft UI, but it does not include any assets or font files from the original game.*

## Features

- Minecraft!
  - The characters in this font were based around the [typeface](https://github.com/Idr"
vaex,"[![Supported Python Versions](https://img.shields.io/pypi/pyversions/vaex-core)](https://pypi.org/project/vaex-core/)
[![Documentation](https://readthedocs.org/projects/vaex/badge/?version=latest)](https://docs.vaex.io)
[![Slack](https://img.shields.io/badge/slack-chat-green.svg)](https://join.slack.com/t/vaexio/shared_invite/zt-shhxzf5i-Cf5n2LtkoYgUjOjbB3bGQQ)

# What is Vaex?

Vaex is a high performance Python library for lazy **Out-of-Core DataFrames**
(similar to Pandas), to visualize and explore big tabular datasets. It
calculates *statistics* such as mean, sum, count, standard deviation etc, on an
*N-dimensional grid* for more than **a billion** (`10^9`) samples/rows **per
second**. Visualization is done using **histograms**, **density plots** and **3d
volume rendering**, allowing interactive exploration of big data. Vaex uses
memory mapping, zero memory copy policy and lazy computations for best
performance (no memory wasted).

# Installing
With pip:
```
$ pip install vaex
```
O"
anomaly-detection-resources,"Anomaly Detection Learning Resources
====================================

.. image:: https://img.shields.io/github/stars/yzhao062/anomaly-detection-resources.svg
   :target: https://github.com/yzhao062/anomaly-detection-resources/stargazers
   :alt: GitHub stars


.. image:: https://img.shields.io/github/forks/yzhao062/anomaly-detection-resources.svg?color=blue
   :target: https://github.com/yzhao062/anomaly-detection-resources/network
   :alt: GitHub forks


.. image:: https://img.shields.io/github/license/yzhao062/anomaly-detection-resources.svg?color=blue
   :target: https://github.com/yzhao062/anomaly-detection-resources/blob/master/LICENSE
   :alt: License


.. image:: https://awesome.re/badge-flat2.svg
   :target: https://awesome.re/badge-flat2.svg
   :alt: Awesome


.. image:: https://img.shields.io/badge/ADBench-benchmark_results-pink
   :target: https://github.com/Minqi824/ADBench
   :alt: Benchmark


----

`Outlier Detection <https://en.wikipedia.org/wiki/Anomaly_detection>`"
gixy,"GIXY
====
[![Mozilla Public License 2.0](https://img.shields.io/github/license/yandex/gixy.svg?style=flat-square)](https://github.com/yandex/gixy/blob/master/LICENSE)
[![Build Status](https://img.shields.io/travis/yandex/gixy.svg?style=flat-square)](https://travis-ci.org/yandex/gixy)
[![Your feedback is greatly appreciated](https://img.shields.io/maintenance/yes/2019.svg?style=flat-square)](https://github.com/yandex/gixy/issues/new)
[![GitHub issues](https://img.shields.io/github/issues/yandex/gixy.svg?style=flat-square)](https://github.com/yandex/gixy/issues)
[![GitHub pull requests](https://img.shields.io/github/issues-pr/yandex/gixy.svg?style=flat-square)](https://github.com/yandex/gixy/pulls)

# Overview
<img align=""right"" width=""192"" height=""192"" src=""/docs/logo.png"">

Gixy is a tool to analyze Nginx configuration.
The main goal of Gixy is to prevent security misconfiguration and automate flaw detection.

Currently supported Python versions are 2.7, 3.5, 3.6 and 3.7.

Disclaimer: "
OctoPrint,"<p align=""center""><img src=""https://octoprint.org/assets/img/logo.png"" alt=""OctoPrint's logo"" /></p>

<h1 align=""center"">OctoPrint</h1>

<p align=""center"">
  <img src=""https://img.shields.io/github/v/release/OctoPrint/OctoPrint?logo=github&logoColor=white"" alt=""GitHub release""/>
  <img src=""https://img.shields.io/pypi/v/OctoPrint?logo=python&logoColor=white"" alt=""PyPI""/>
  <img src=""https://img.shields.io/github/actions/workflow/status/OctoPrint/OctoPrint/build.yml?branch=master"" alt=""Build status""/>
  <a href=""https://community.octoprint.org""><img src=""https://img.shields.io/discourse/users?label=forum&logo=discourse&logoColor=white&server=https%3A%2F%2Fcommunity.octoprint.org"" alt=""Community Forum""/></a>
  <a href=""https://discord.octoprint.org""><img src=""https://img.shields.io/discord/704958479194128507?label=discord&logo=discord&logoColor=white"" alt=""Discord""/></a>
  <a href=""https://octoprint.org/conduct/""><img src=""https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopte"
ImageBind,"# ImageBind: One Embedding Space To Bind Them All

**[FAIR, Meta AI](https://ai.facebook.com/research/)** 

Rohit Girdhar*,
Alaaeldin El-Nouby*,
Zhuang Liu,
Mannat Singh,
Kalyan Vasudev Alwala,
Armand Joulin,
Ishan Misra*

To appear at CVPR 2023 (*Highlighted paper*)

[[`Paper`](https://facebookresearch.github.io/ImageBind/paper)] [[`Blog`](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/)] [[`Demo`](https://imagebind.metademolab.com/)] [[`Supplementary Video`](https://dl.fbaipublicfiles.com/imagebind/imagebind_video.mp4)] [[`BibTex`](#citing-imagebind)]

PyTorch implementation and pretrained models for ImageBind. For details, see the paper: **[ImageBind: One Embedding Space To Bind Them All](https://facebookresearch.github.io/ImageBind/paper)**.

ImageBind learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. It enables novel emergent applications ‘out-of-the-box’ including cross-modal retrieval, composing modali"
spyder,"![Spyder — The Scientific Python Development Environment](https://raw.githubusercontent.com/spyder-ide/spyder/master/branding/logo/spyder_readme_banner.png)

*Copyright © 2009– [Spyder Project Contributors](https://github.com/spyder-ide/spyder/graphs/contributors)* and others (see AUTHORS.txt)

*Some source files and icons may be under other authorship/licenses; see
[NOTICE.txt](https://github.com/spyder-ide/spyder/blob/master/NOTICE.txt).*

## Project status

[![license](https://img.shields.io/pypi/l/spyder.svg)](./LICENSE.txt)
[![pypi version](https://img.shields.io/pypi/v/spyder.svg)](https://pypi.org/project/spyder/)
[![conda version](https://img.shields.io/conda/vn/conda-forge/spyder.svg)](https://anaconda.org/conda-forge/spyder)
[![download count](https://img.shields.io/conda/dn/conda-forge/spyder.svg)](https://anaconda.org/conda-forge/spyder)
[![OpenCollective Backers](https://opencollective.com/spyder/backers/badge.svg?color=blue)](#backers)
[![OpenCollective Sponsors](https://"
LMFlow,"<p align=""center"" width=""50%"">
<img src=""assets/logo.png"" alt=""LMFlow"" style=""width: 50%; min-width: 200px; display: block; margin: auto; background-color: transparent;"">
</p>

# LMFlow

<h4 align=""center"">
    <p>
        <b>English</b> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_zh-hans.md"">简体中文</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_es.md"">Español</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_jp.md"">日本語</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_ko.md"">한국어</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_hindi.md"">हिंदी</a>
    <p>
</h4>

[![Website](https://img.shields.io/badge/Website-Demo-20B2AA.svg)](https://lmflow.com)
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/OptimalScale/LMFlow/blob/main/LICENSE)
[![Python 3.9+](ht"
cookiecutter-data-science,"# Cookiecutter Data Science

_A logical, reasonably standardized but flexible project structure for doing and sharing data science work._

**Cookiecutter Data Science (CCDS)** is a tool for setting up a data science project template that incorporates best practices. To learn more about CCDS's philosophy, visit the [project homepage](https://cookiecutter-data-science.drivendata.org/).

> ℹ️ Cookiecutter Data Science v2 has changed from v1. It now requires installing the new cookiecutter-data-science Python package, which extends the functionality of the [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/README.html) templating utility. Use the provided `ccds` command-line program instead of `cookiecutter`.

## Installation

Cookiecutter Data Science v2 requires Python 3.8+. Since this is a cross-project utility application, we recommend installing it with [pipx](https://pypa.github.io/pipx/). Installation command options:

```bash
# With pipx from PyPI (recommended)
pipx insta"
gpt-neo,"# GPT Neo

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5297715.svg)](https://doi.org/10.5281/zenodo.5297715) [![arXiv](https://img.shields.io/badge/arXiv-2101.00027-f9f107.svg)](https://arxiv.org/abs/2101.00027)

**As of August, 2021 code is no longer maintained. It is preserved here in archival form for people who wish to continue to use it.*

🎉 1T or bust my dudes 🎉

An implementation of model & data parallel [GPT3](https://arxiv.org/abs/2005.14165)-like models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.

**If you're just here to play with our pre-trained models, we strongly recommend you try out the [HuggingFace Transformer integration](https://huggingface.co/EleutherAI).**

Training and inference is officially supported on TPU and should work on GPU as well. This repository will be (mostly) archived as we move focus to our GPU-specific repo, [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/).

In addition to the functionality offered by GPT-3, "
DAIN,"# DAIN (Depth-Aware Video Frame Interpolation)
[Project](https://sites.google.com/view/wenbobao/dain) **|** [Paper](http://arxiv.org/abs/1904.00830)

[Wenbo Bao](https://sites.google.com/view/wenbobao/home),
[Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), 
[Chao Ma](https://sites.google.com/site/chaoma99/),
Xiaoyun Zhang, 
Zhiyong Gao, 
and [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)

IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CVPR 2019

This work is developed based on our TPAMI work [MEMC-Net](https://github.com/baowenbo/MEMC-Net), where we propose the adaptive warping layer. Please also consider referring to it.

### Table of Contents
1. [Introduction](#introduction)
1. [Citation](#citation)
1. [Requirements and Dependencies](#requirements-and-dependencies)
1. [Installation](#installation)
1. [Testing Pre-trained Models](#testing-pre-trained-models)
1. [Downloading Results](#downloading-results)
1. [Slow-motion Generation](#slow-"
git-filter-repo,"git filter-repo is a versatile tool for rewriting history, which includes
[capabilities I have not found anywhere
else](#design-rationale-behind-filter-repo).  It roughly falls into the
same space of tool as [git
filter-branch](https://git-scm.com/docs/git-filter-branch) but without the
capitulation-inducing poor
[performance](https://public-inbox.org/git/CABPp-BGOz8nks0+Tdw5GyGqxeYR-3FF6FT5JcgVqZDYVRQ6qog@mail.gmail.com/),
with far more capabilities, and with a design that scales usability-wise
beyond trivial rewriting cases.  [git filter-repo is now recommended by the
git project](https://git-scm.com/docs/git-filter-branch#_warning) instead
of git filter-branch.

While most users will probably just use filter-repo as a simple command
line tool (and likely only use a few of its flags), at its core filter-repo
contains a library for creating history rewriting tools.  As such, users
with specialized needs can leverage it to quickly create [entirely new
history rewriting tools](contrib/f"
OneForAll,"# OneForAll

[![Build Status](https://travis-ci.org/shmilylty/OneForAll.svg?branch=master)](https://travis-ci.org/shmilylty/OneForAll)
[![codecov](https://codecov.io/gh/shmilylty/OneForAll/branch/master/graph/badge.svg)](https://codecov.io/gh/shmilylty/OneForAll)
[![Maintainability](https://api.codeclimate.com/v1/badges/1287668a6b4c72af683e/maintainability)](https://codeclimate.com/github/shmilylty/OneForAll/maintainability)
[![License](https://img.shields.io/github/license/shmilylty/OneForAll)](https://github.com/shmilylty/OneForAll/tree/master/LICENSE)
[![python](https://img.shields.io/badge/python-3.6+-blue)](https://github.com/shmilylty/OneForAll/tree/master/)
[![python](https://img.shields.io/badge/release-v0.4.5-brightgreen)](https://github.com/shmilylty/OneForAll/releases)

👊**OneForAll是一款功能强大的子域收集工具**  📝[English Document](https://github.com/shmilylty/OneForAll/tree/master/docs/en-us/README.md)

![Example](./docs/usage_example.svg)

## 🚀上手指南

📢 请务必花一点时间阅读此文档，有助于你快速熟悉OneForAll！

"
stable-dreamfusion,"# Stable-Dreamfusion

A pytorch implementation of the text-to-3D model **Dreamfusion**, powered by the [Stable Diffusion](https://github.com/CompVis/stable-diffusion) text-to-2D model.

**ADVERTISEMENT: Please check out [threestudio](https://github.com/threestudio-project/threestudio) for recent improvements and better implementation in 3D content generation!**

**NEWS (2023.6.12)**:

* Support of [Perp-Neg](https://perp-neg.github.io/) to alleviate multi-head problem in Text-to-3D.
* Support of Perp-Neg for both [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and [DeepFloyd-IF](https://github.com/deep-floyd/IF).

https://user-images.githubusercontent.com/25863658/236712982-9f93bd32-83bf-423a-bb7c-f73df7ece2e3.mp4

https://user-images.githubusercontent.com/25863658/232403162-51b69000-a242-4b8c-9cd9-4242b09863fa.mp4

### [Update Logs](assets/update_logs.md)

### Colab notebooks:
* Instant-NGP backbone (`-O`): [![Instant-NGP Backbone](https://colab.research.google.com/ass"
demucs,"# Demucs Music Source Separation

[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)
![tests badge](https://github.com/facebookresearch/demucs/workflows/tests/badge.svg)
![linter badge](https://github.com/facebookresearch/demucs/workflows/linter/badge.svg)


**Important:** As I am no longer working at Meta, **this repository is not maintained anymore**.
I've created a fork at [github.com/adefossez/demucs](https://github.com/adefossez/demucs). Note that this project is not actively maintained anymore
and only important bug fixes will be processed on the new repo. Please do not open issues for feature request or if Demucs doesn't work perfectly for your use case :)

This is the 4th release of Demucs (v4), featuring Hybrid Transformer based source separation.
**For the classic Hybrid Demucs (v3):** [Go this commit][demucs_v3].
If you are experiencing issues and want the old Demucs back, please f"
Airtest,"# Airtest &middot; [![Build status](https://travis-ci.org/AirtestProject/Airtest.svg?branch=master)](https://travis-ci.org/AirtestProject/Airtest)

**Cross-Platform UI Automation Framework for Games and Apps**

**跨平台的UI自动化框架，适用于游戏和App** （[中文版点这里](./README_zh.md)）


![image](./demo.gif)


## Features

*   **Write Once, Run Anywhere:** Airtest provides cross-platform APIs, including app installation, simulated input, assertion and so forth. Airtest uses image recognition technology to locate UI elements so that you can automate games and apps without injecting any code. 

*   **Fully Scalable:** Airtest cases can be easily run on large device farms, using commandline or python API. HTML reports with detailed info and screen recording allow you to quickly locate failure points. NetEase builds [Airlab](https://airlab.163.com/) on top of the Airtest Project.

*   **AirtestIDE:** AirtestIDE is an out of the box GUI tool that helps to create and run cases in a user-friendly way. AirtestIDE su"
sigma,"# Sigma - Generic Signature Format for SIEM Systems

<a href=""https://sigmahq.io/"">
<p align=""center"">
<br />
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""./images/sigma_logo_dark.png"">
  <img width=""454"" alt=""Sigma Logo"" src=""./images/sigma_logo_light.png"">
</picture>
</p>
</a>
<br />

<p align=""center"">
<a href=""https://github.com/SigmaHQ/sigma/actions?query=branch%3Amaster""><img src=""https://github.com/SigmaHQ/sigma/actions/workflows/sigma-test.yml/badge.svg?branch=master"" alt=""Sigma Build Status""></a> <a href=""https://sigmahq.io/""><img src=""https://cdn.jsdelivr.net/gh/SigmaHQ/sigmahq.github.io@master/images/Sigma%20Official%20Badge.svg"" alt=""Sigma Official Badge""></a> <img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/SigmaHQ/sigma"">
<img alt=""GitHub all releases"" src=""https://img.shields.io/github/downloads/SigmaHq/Sigma/total"">
<br />
<a href=""https://opensourcesecurityindex.io/"" target=""_blank"" rel=""noopener"">
<img style=""width: 170px;"" src"
conan,"<picture>
  <!-- These are also used for https://github.com/conan-io/.github/blob/main/profile/README.md -->
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/conan-io/conan/develop2/.github/conan2-logo-for-dark.svg"">
  <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/conan-io/conan/develop2/.github/conan2-logo-for-light.svg"">
  <img alt=""JFrog | Conan 2.0 Logo"" src=""https://raw.githubusercontent.com/conan-io/conan/develop2/.github/conan2-logo-with-bg.svg"">
</picture>

# Conan

Decentralized, open-source (MIT), C/C++ package manager.

- Homepage: https://conan.io/
- Github: https://github.com/conan-io/conan
- Docs: https://docs.conan.io
- Slack: https://cpplang.slack.com (#conan channel. Please, click [here](https://join.slack.com/t/cpplang/shared_invite/zt-1snzdn6rp-rOUxF3166oz1_11Tr5H~xg) to get an invitation)
- Twitter: https://twitter.com/conan_io


Conan is a package manager for C and C++ developers:

- "
fiftyone,"<div align=""center"">
<p align=""center"">

<!-- prettier-ignore -->
<img src=""https://user-images.githubusercontent.com/25985824/106288517-2422e000-6216-11eb-871d-26ad2e7b1e59.png"" height=""55px""> &nbsp;
<img src=""https://user-images.githubusercontent.com/25985824/106288518-24bb7680-6216-11eb-8f10-60052c519586.png"" height=""50px"">

**The open-source tool for building high-quality datasets and computer vision
models**

---

<!-- prettier-ignore -->
<a href=""https://voxel51.com/fiftyone"">Website</a> •
<a href=""https://voxel51.com/docs/fiftyone"">Docs</a> •
<a href=""https://colab.research.google.com/github/voxel51/fiftyone-examples/blob/master/examples/quickstart.ipynb"">Try it Now</a> •
<a href=""https://voxel51.com/docs/fiftyone/tutorials/index.html"">Tutorials</a> •
<a href=""https://github.com/voxel51/fiftyone-examples"">Examples</a> •
<a href=""https://voxel51.com/blog/"">Blog</a> •
<a href=""https://slack.voxel51.com"">Community</a>

[![PyPI python](https://img.shields.io/pypi/pyversions/fiftyone"
CodeGeeX,"<img src=""resources/logo/codegeex_logo.png"">

<p align=""center"">
    🏠 <a href=""https://codegeex.cn"" target=""_blank"">Homepage</a> | 📖 <a href=""https://models.aminer.cn/codegeex/blog/"" target=""_blank"">Blog</a> | 🪧 <a href=""https://models.aminer.cn/codegeex/playground"" target=""_blank"">DEMO</a> | 🤖 <a href=""https://codegeex.cn/download/request"" target=""_blank"">Download Model</a> | 📄 <a href=""https://arxiv.org/abs/2303.17568"" target=""_blank"">Paper</a> | 🌐 <a href=""README_zh.md"" target=""_blank"">中文</a>
</p>
<p align=""center"">
    🛠 <a href=""https://marketplace.visualstudio.com/items?itemName=aminer.codegeex"" target=""_blank"">VS Code</a>, <a href=""https://plugins.jetbrains.com/plugin/20587-codegeex"" target=""_blank"">Jetbrains</a>, <a href=""https://plugins.jetbrains.com/plugin/20587-codegeex"" target=""_blank"">Cloud Studio</a> supported | 👋 Join our <a href=""https://discord.gg/8gjHdkmAN6"" target=""_blank"">Discord</a>, <a href=""https://join.slack.com/t/codegeexworkspace/shared_invite/zt-1s118ffrp-mp"
healthchecks,"# Healthchecks

[![Tests](https://github.com/healthchecks/healthchecks/actions/workflows/tests.yml/badge.svg)](https://github.com/healthchecks/healthchecks/actions/workflows/tests.yml)
[![Coverage Status](https://coveralls.io/repos/healthchecks/healthchecks/badge.svg?branch=master&service=github)](https://coveralls.io/github/healthchecks/healthchecks?branch=master)

Healthchecks is a cron job monitoring service. It listens for HTTP requests
and email messages (""pings"") from your cron jobs and scheduled tasks (""checks"").
When a ping does not arrive on time, Healthchecks sends out alerts.

Healthchecks comes with a web dashboard, API, 25+ integrations for
delivering notifications, monthly email reports, WebAuthn 2FA support,
team management features: projects, team members, read-only access.

The building blocks are:

* Python 3.10+
* Django 5.1
* PostgreSQL or MySQL

Healthchecks is licensed under the BSD 3-clause license.

Healthchecks is available as a hosted service
at [https://healt"
FastUI,"# FastUI

Find the documentation [here](https://docs.pydantic.dev/fastui/).
Join the discussion in the #fastui slack channel [here](https://pydanticlogfire.slack.com/archives/C0720M7D31S)

[![CI](https://github.com/pydantic/FastUI/actions/workflows/ci.yml/badge.svg)](https://github.com/pydantic/FastUI/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![pypi](https://img.shields.io/pypi/v/fastui.svg)](https://pypi.python.org/pypi/fastui)
[![versions](https://img.shields.io/pypi/pyversions/fastui.svg)](https://github.com/pydantic/FastUI)
[![license](https://img.shields.io/github/license/pydantic/FastUI.svg)](https://github.com/pydantic/FastUI/blob/main/LICENSE)

**Please note:** FastUI is still an active work in progress, do not expect it to be complete.

## The Principle (short version)

You can see a simple demo of an application built with FastUI [here](https://fastui-demo.onrender.com).

FastUI is a new way to build web application user interfaces defined by declarative Python"
QUANTAXIS,"# QUANTAXIS 2.0.0

[![Github workers](https://img.shields.io/github/watchers/quantaxis/quantaxis.svg?style=social&label=Watchers&)](https://github.com/quantaxis/quantaxis/watchers)
[![GitHub stars](https://img.shields.io/github/stars/quantaxis/quantaxis.svg?style=social&label=Star&)](https://github.com/quantaxis/quantaxis/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/quantaxis/quantaxis.svg?style=social&label=Fork&)](https://github.com/quantaxis/quantaxis/fork)

[点击右上角Star和Watch来跟踪项目进展! 点击Fork来创建属于你的QUANTAXIS!]

![QUANTAXIS_LOGO_LAST_small.jpg](http://picx.gulizhu.com/Fn0TPEcwu_uhraf58_93Ul5yfvAz)

![gvp](http://picx.gulizhu.com/gvp.jpg)


更多文档在[QABook Release](https://github.com/QUANTAXIS/QUANTAXIS/releases/download/latest/quantaxis.pdf)

Quantitative Financial FrameWork

本项目分为几个大块:


1. QASU/ QAFetch 支持多市场数据存储/ 自动运维/ 数据获取(mongodb/ clickhouse)

2. QAUtil 支持交易时间, 交易日历, 时间向前向后推算, 市场识别, dataframe 数据转换等

3. QIFI/ QAMarket 一套统一的多市场 多语言账户体系
    - qifiaccount qifi 的标准账户体系,"
py-faster-rcnn,"# py-faster-rcnn has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron), which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

### Disclaimer

The official Faster R-CNN code (written in MATLAB) is available [here](https://github.com/ShaoqingRen/faster_rcnn).
If your goal is to reproduce the results in our NIPS 2015 paper, please use the [official code](https://github.com/ShaoqingRen/faster_rcnn).

This repository contains a Python *reimplementation* of the MATLAB code.
This Python implementation is built on a fork of [Fast R-CNN](https://github.com/rbgirshick/fast-rcnn).
There are slight differences between the two implementations.
In particular, this Python port
 - is ~10% slower at test-time, because some operations execute on the CPU in Python layers (e.g., 220ms / image vs. 200ms / image for VGG16)
 - gives similar, but not exactly the same, mAP as the MATLAB version
 - is *not compatible* with models trained using "
ctf-wiki,"# CTF Wiki

[![Discord](https://dcbadge.vercel.app/api/server/ekv7WDa9pq)](https://discord.gg/ekv7WDa9pq)

[中文](./README-zh_CN.md)  [English](./README.md)

Welcome to **CTF Wiki**！

**CTF** (Capture The Flag) started from **DEFCON CTF**, a competitive game among computer security enthusiasts, originally hosted in 1996.

**CTF** covers a wide range of fields. Along with the evolving security technology, the difficulty of **CTF** challenges is getting harder and harder. As a result, the learning curve for beginners is getting steeper. Most online information is scattered and trivial. Beginners often don't know how to systematically learn **CTF**, which requires a lot of work and effort.

In order to let those people who are interested in **CTF**s start easily, in October 2016, **CTF Wiki** was established on Github. Along with gradually improved content over time, **CTF Wiki** has received lots of appreciation from security enthusiasts, many of those are guys that we think we would never"
server,"<!--
# Copyright 2018-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# P"
homelab,"# Khue's Homelab

**[Features](#features) • [Get Started](#get-started) • [Documentation](https://homelab.khuedoan.com)**

[![tag](https://img.shields.io/github/v/tag/khuedoan/homelab?style=flat-square&logo=semver&logoColor=white)](https://github.com/khuedoan/homelab/tags)
[![document](https://img.shields.io/website?label=document&logo=gitbook&logoColor=white&style=flat-square&url=https%3A%2F%2Fhomelab.khuedoan.com)](https://homelab.khuedoan.com)
[![license](https://img.shields.io/github/license/khuedoan/homelab?style=flat-square&logo=gnu&logoColor=white)](https://www.gnu.org/licenses/gpl-3.0.html)
[![stars](https://img.shields.io/github/stars/khuedoan/homelab?logo=github&logoColor=white&color=gold&style=flat-square)](https://github.com/khuedoan/homelab)

This project utilizes [Infrastructure as Code](https://en.wikipedia.org/wiki/Infrastructure_as_code) and [GitOps](https://www.weave.works/technologies/gitops) to automate provisioning, operating, and updating self-hosted services in m"
pypdf,"[![PyPI version](https://badge.fury.io/py/pypdf.svg)](https://badge.fury.io/py/pypdf)
[![Python Support](https://img.shields.io/pypi/pyversions/pypdf.svg)](https://pypi.org/project/pypdf/)
[![](https://img.shields.io/badge/-documentation-green)](https://pypdf.readthedocs.io/en/stable/)
[![GitHub last commit](https://img.shields.io/github/last-commit/py-pdf/pypdf)](https://github.com/py-pdf/pypdf)
[![codecov](https://codecov.io/gh/py-pdf/pypdf/branch/main/graph/badge.svg?token=id42cGNZ5Z)](https://codecov.io/gh/py-pdf/pypdf)

# pypdf

pypdf is a free and open-source pure-python PDF library capable of splitting,
[merging](https://pypdf.readthedocs.io/en/stable/user/merging-pdfs.html),
[cropping, and transforming](https://pypdf.readthedocs.io/en/stable/user/cropping-and-transforming.html)
the pages of PDF files. It can also add
custom data, viewing options, and
[passwords](https://pypdf.readthedocs.io/en/stable/user/encryption-decryption.html)
to PDF files. pypdf can
[retrieve text](https"
iOS-DeviceSupport,"# iOS-DeviceSupport

This repository holds the device support files for the iOS, and I will update it regularly.

## Usage

See docs: [https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/](https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/)

Below command will try to unzip all new device support files to `/Applications/Xcode.app`.

```sh
sudo ./deploy.py
```

You can use `-t` if your Xcode is not in `/Applications/` or has different name.

```sh
sudo ./deploy.py -t /Applications/Xcode\ 9.app
```

```sh
./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this help message and exit
  -t TARGET   The path for Xcode
```

## Supported versions

1. iOS8
   * 8.0 `2017/04/07`
   * 8.1 `2017/04/07`
   * 8.2 `2017/04/07`
   * 8.3 `2017/04/07`
   * 8.4 `2017/04/07`
2. iOS9
   * 9.0 `2017/04/07`
   * 9.1 `2017/04/07`
   * 9.2 `2017/04/07`
   * 9.3 `2017/04/07`
3. iOS10
   * 10.0 (14A345) `2017/04/07`
   * 10.0 `2"
yewtube,"
![](https://img.shields.io/pypi/v/yewtube.svg)  ![](https://img.shields.io/pypi/wheel/yewtube.svg)

## STOP GENOCIDE OF INNOCENT PEOPLE

![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/1280px-Flag_of_Palestine.svg.png)

<pre>
                      _         _          
                     | |       | |         
  _   _  _____      _| |_ _   _| |__   ___ 
 | | | |/ _ \ \ /\ / / __| | | | '_ \ / _ \
 | |_| |  __/\ V  V /| |_| |_| | |_) |  __/
  \__, |\___| \_/\_/  \__|\__,_|_.__/ \___|
   __/ |                                   
  |___/


</pre>

yewtube, forked from mps-youtube , is a Terminal based YouTube player and downloader. No Youtube API key required. <br>
Visit [this](./COLLABORATORS.md) page if you want to support maintainers of this project.

Installation
-----------
# Stable Version

### Using pip
1. Install using `pip install yewtube`
2. Run using, `yt`. Enjoy! 

### Using pipx (Recommended)
1.  Install **_pipx_** using `pip install pipx"
EdgeGPT,"> # This project has been archived. Due to personal circumstances, I lack the time to maintain this repository.

<div align=""center"">
  <img src=""https://socialify.git.ci/acheong08/EdgeGPT/image?font=Inter&language=1&logo=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2F9%2F9c%2FBing_Fluent_Logo.svg&owner=1&pattern=Floating%20Cogs&theme=Auto"" alt=""EdgeGPT"" width=""640"" height=""320"" />

# Edge GPT

_The reverse engineering the chat feature of the new version of Bing_

<a>English</a> -
<a href=""./README_zh-cn.md"">简体中文</a> -
<a href=""./README_zh-tw.md"">繁體中文</a> -
<a href=""./README_es.md"">Español</a> -
<a href=""./README_ja.md"">日本語</a>

</div>

<p align=""center"">
  <a href=""https://github.com/acheong08/EdgeGPT"">
    <img alt=""PyPI version"" src=""https://img.shields.io/pypi/v/EdgeGPT"">
  </a>
  <img alt=""Python version"" src=""https://img.shields.io/badge/python-3.8+-blue.svg"">
  <img alt=""Total downloads"" src=""https://static.pepy.tech/badge/edgegpt"">

</p>

<details open>

<summary>

"
metaflow,"![Metaflow_Logo_Horizontal_FullColor_Ribbon_Dark_RGB](https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png)

# Metaflow

Metaflow is a human-friendly library that helps scientists and engineers build and manage real-life data science projects. Metaflow was [originally developed at Netflix](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) to boost productivity of data scientists who work on a wide variety of projects from classical statistics to state-of-the-art deep learning.

For more information, see [Metaflow's website](https://metaflow.org) and [documentation](https://docs.metaflow.org).

## From prototype to production (and back)

Metaflow provides a simple, friendly API that covers foundational needs of ML, AI, and data science projects:
<img src=""./docs/prototype-to-prod.png"" width=""800px"">

1. [Rapid local prototyping](https://docs.metaflow.org/metaflow/basics), [support for "
graphene,"# ![Graphene Logo](http://graphene-python.org/favicon.png) [Graphene](http://graphene-python.org)  [![PyPI version](https://badge.fury.io/py/graphene.svg)](https://badge.fury.io/py/graphene) [![Coverage Status](https://coveralls.io/repos/graphql-python/graphene/badge.svg?branch=master&service=github)](https://coveralls.io/github/graphql-python/graphene?branch=master) [![](https://dcbadge.vercel.app/api/server/T6Gp6NFYHe?style=flat)](https://discord.gg/T6Gp6NFYHe)

[💬 Join the community on Discord](https://discord.gg/T6Gp6NFYHe)

**We are looking for contributors**! Please check the current issues to see how you can help ❤️

## Introduction

[Graphene](http://graphene-python.org) is an opinionated Python library for building GraphQL schemas/types fast and easily.

- **Easy to use:** Graphene helps you use GraphQL in Python without effort.
- **Relay:** Graphene has builtin support for Relay.
- **Data agnostic:** Graphene supports any kind of data source: SQL (Django, SQLAlchemy), Mongo, "
deeplake,"<img src=""https://static.scarf.sh/a.png?x-pxid=bc3c57b0-9a65-49fe-b8ea-f711c4d35b82"" /><p align=""center"">
     <img src=""https://i.postimg.cc/rsjcWc3S/deeplake-logo.png"" width=""400""/>
</h1>

</br>

<h1 align=""center"">Deep Lake: Database for AI</h1>

<p align=""center"">
    <a href=""https://github.com/activeloopai/deeplake/actions/workflows/test-pr-on-label.yml""><img src=""https://github.com/activeloopai/deeplake/actions/workflows/test-push.yml/badge.svg"" alt=""PyPI version"" height=""18""></a>
    <a href=""https://pypi.org/project/deeplake/""><img src=""https://badge.fury.io/py/deeplake.svg"" alt=""PyPI version"" height=""18""></a>
    <a href='https://docs.deeplake.ai/en/latest/?badge=latest'>
     <img src='https://readthedocs.org/projects/deep-lake/badge/?version=latest' alt='Documentation Status' />
     </a>
    <a href=""https://pepy.tech/project/deeplake""><img src=""https://static.pepy.tech/badge/deeplake"" alt=""PyPI version"" height=""18""></a>
     <a href=""https://github.com/activeloopai/deepla"
trape,"trape (stable) v2.0
========

People tracker on the Internet: Learn to track the world, to avoid being traced.

---
Trape is an **OSINT** analysis and research tool, which allows people to track and execute intelligent **social engineering** attacks in real time. It was created with the aim of teaching the world how large Internet companies could obtain **confidential information** such as the status of sessions of their websites or services and control their users through their browser, without their knowledge, but It evolves with the aim of helping **government** organizations, companies and **researchers** to track the cybercriminals.

![--trape header](https://i.imgur.com/2ycpXEj.png)


At the beginning of the year 2018 was presented at **BlackHat Arsenal in Singapore**: https://www.blackhat.com/asia-18/arsenal.html#jose-pino and in multiple security events worldwide.

Some benefits
-----------
* **LOCATOR OPTIMIZATION:** Trace the path between you and the target you're tracking. E"
django-debug-toolbar,"=====================================
Django Debug Toolbar |latest-version|
=====================================

|jazzband| |build-status| |coverage| |docs| |python-support| |django-support|

.. |latest-version| image:: https://img.shields.io/pypi/v/django-debug-toolbar.svg
   :target: https://pypi.org/project/django-debug-toolbar/
   :alt: Latest version on PyPI

.. |jazzband| image:: https://jazzband.co/static/img/badge.svg
   :target: https://jazzband.co/
   :alt: Jazzband

.. |build-status| image:: https://github.com/jazzband/django-debug-toolbar/workflows/Test/badge.svg
   :target: https://github.com/jazzband/django-debug-toolbar/actions
   :alt: Build Status

.. |coverage| image:: https://img.shields.io/badge/Coverage-94%25-green
   :target: https://github.com/jazzband/django-debug-toolbar/actions/workflows/test.yml?query=branch%3Amain
   :alt: Test coverage status

.. |docs| image:: https://img.shields.io/readthedocs/django-debug-toolbar/latest.svg
   :target: https://readthed"
trax,"# Trax &mdash; Deep Learning with Clear Code and Speed

![train tracks](https://images.pexels.com/photos/461772/pexels-photo-461772.jpeg?dl&fit=crop&crop=entropy&w=32&h=21)
[![PyPI
version](https://badge.fury.io/py/trax.svg)](https://badge.fury.io/py/trax)
[![GitHub
Issues](https://img.shields.io/github/issues/google/trax.svg)](https://github.com/google/trax/issues)
![GitHub Build](https://github.com/google/trax/actions/workflows/build.yaml/badge.svg)
[![Contributions
welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)
[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/trax-ml/community)

[Trax](https://trax-ml.readthedocs.io/en/latest/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https://resear"
XAgent,"<div align= ""center"">
    <h1> <img src=""assets/readme/xagent_logo.png"" height=40 align=""texttop"">XAgent</h1>
</div>

<div align=""center"">

[![Twitter](https://img.shields.io/twitter/follow/XAgent?style=social)](https://twitter.com/XAgentTeam) [![Discord](https://img.shields.io/badge/XAgent-Discord-purple?style=flat)](https://discord.gg/zncs5aQkWZ) [![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://opensource.org/license/apache-2-0/) ![Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)

</div>

<p align=""center"">
    <a>English</a> •
    <a href=""README_ZH.md"">中文</a> •
    <a href=""README_JA.md"">日本語</a>
</p>

<p align=""center"">
  <a href=""#quickstart"">Tutorial</a> •
  <a href=""https://www.youtube.com/watch?v=QGkpd-tsFPA"">Demo</a> •
  <a href=""https://blog.x-agent.net/blog/xagent/"">Blog</a> •
  <a href=""https://xagent-doc.readthedocs.io/en/latest/"">Documentation</a> •
  <a href=""#Citation"">Citation</a>
</p>


"
gitfiti,"[![Build Status](https://travis-ci.org/gelstudios/gitfiti.svg?branch=master)](https://travis-ci.org/gelstudios/gitfiti)

**gitfiti** _noun_ : Carefully crafted graffiti in a github commit history calendar.  

An example of gitfiti in the wild:  
![screenshot of gitfiti](https://raw.github.com/gelstudios/gitfiti/master/gitfiti-screenshot.png ""screenshot"")

`gitfiti.py` is a tool to decorate your github account's commit history calendar by (blatantly) abusing git's ability to accept commits _in the past_.

How? `gitfiti.py` generates a script (powershell or bash) that makes commits with the GIT_AUTHOR_DATE and GIT_COMMITTER_DATE environment variables set for each targeted pixel.

Since this is likely to clobber repo's history, it is highly recommend that you create a _new_ github repo when using gitfiti. Also, the generated script assumes you are using public-key authentication with git.

### Pixel Art

![pixel art examples](https://raw.github.com/gelstudios/gitfiti/master/pixels-large.p"
pyTelegramBotAPI,"
[![PyPi Package Version](https://img.shields.io/pypi/v/pyTelegramBotAPI.svg)](https://pypi.python.org/pypi/pyTelegramBotAPI)
[![Supported Python versions](https://img.shields.io/pypi/pyversions/pyTelegramBotAPI.svg)](https://pypi.python.org/pypi/pyTelegramBotAPI)
[![Documentation Status](https://readthedocs.org/projects/pytba/badge/?version=latest)](https://pytba.readthedocs.io/en/latest/?badge=latest)
[![PyPi downloads](https://img.shields.io/pypi/dm/pyTelegramBotAPI.svg)](https://pypi.org/project/pyTelegramBotAPI/)
[![PyPi status](https://img.shields.io/pypi/status/pytelegrambotapi.svg?style=flat-square)](https://pypi.python.org/pypi/pytelegrambotapi)

# <p align=""center"">pyTelegramBotAPI

<p align=""center"">A simple, but extensible Python implementation for the <a href=""https://core.telegram.org/bots/api"">Telegram Bot API</a>.</p>
<p align=""center"">Both synchronous and asynchronous.</p>

## <p align=""center"">Supported Bot API version: <a href=""https://core.telegram.org/bots/api#sept"
mopidy,"******
Mopidy
******

`Mopidy`_ is an extensible music server written in Python.

Mopidy plays music from local disk, Spotify, SoundCloud, Google Play Music, and
more. You edit the playlist from any phone, tablet, or computer using a variety
of MPD and web clients.

**Stream music from the cloud**

Vanilla Mopidy only plays music from files and radio streams.  Through
`extensions`_, Mopidy can play music from cloud services like Spotify,
SoundCloud, and Google Play Music.
With Mopidy's extension support, backends for new music sources can be easily
added.

**Mopidy is just a server**

Mopidy is a Python application that runs in a terminal or in the background on
Linux computers or Macs that have network connectivity and audio output.
Out of the box, Mopidy is an HTTP server. If you install the `Mopidy-MPD`_
extension, it becomes an MPD server too. Many additional frontends for
controlling Mopidy are available as extensions.

**Pick your favorite client**

You and the people around you "
catboost,"<img src=http://storage.mds.yandex.net/get-devtools-opensource/250854/catboost-logo.png width=300/>

[Website](https://catboost.ai) |
[Documentation](https://catboost.ai/docs/) |
[Tutorials](https://catboost.ai/docs/concepts/tutorials.html) |
[Installation](https://catboost.ai/docs/concepts/installation.html) |
[Release Notes](https://github.com/catboost/catboost/releases)

[![GitHub license](https://img.shields.io/github/license/catboost/catboost.svg)](https://github.com/catboost/catboost/blob/master/LICENSE)
[![PyPI version](https://badge.fury.io/py/catboost.svg)](https://badge.fury.io/py/catboost)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/catboost.svg)](https://anaconda.org/conda-forge/catboost)
[![GitHub issues](https://img.shields.io/github/issues/catboost/catboost.svg)](https://github.com/catboost/catboost/issues)
[![Telegram](https://img.shields.io/badge/chat-on%20Telegram-2ba2d9.svg)](https://t.me/catboost_en)
[![Twitter](https://img.shields.io/badge/@CatBoo"
imagen-pytorch,"<img src=""./imagen.png"" width=""450px""></img>

## Imagen - Pytorch

Implementation of <a href=""https://gweb-research-imagen.appspot.com/"">Imagen</a>, Google's Text-to-Image Neural Network that beats DALL-E2, in Pytorch. It is the new SOTA for text-to-image synthesis.

Architecturally, it is actually much simpler than DALL-E2. It consists of a cascading DDPM conditioned on text embeddings from a large pretrained T5 model (attention network). It also contains dynamic clipping for improved classifier free guidance, noise level conditioning, and a memory efficient unet design.

It appears neither CLIP nor prior network is needed after all. And so research continues.

<a href=""https://www.youtube.com/watch?v=xqDeAz0U-R4"">AI Coffee Break with Letitia</a> | <a href=""https://www.assemblyai.com/blog/how-imagen-actually-works/"">Assembly AI</a> | <a href=""https://www.youtube.com/watch?v=af6WPqvzjjk"">Yannic Kilcher</a>

Please join <a href=""https://discord.gg/xBPBXfcFHd""><img alt=""Join us on Discor"
mmsegmentation,"<div align=""center"">
  <img src=""resources/mmseg-logo.png"" width=""600""/>
  <div>&nbsp;</div>
  <div align=""center"">
    <b><font size=""5"">OpenMMLab website</font></b>
    <sup>
      <a href=""https://openmmlab.com"">
        <i><font size=""4"">HOT</font></i>
      </a>
    </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <b><font size=""5"">OpenMMLab platform</font></b>
    <sup>
      <a href=""https://platform.openmmlab.com"">
        <i><font size=""4"">TRY IT OUT</font></i>
      </a>
    </sup>
  </div>
  <div>&nbsp;</div>

[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mmsegmentation)](https://pypi.org/project/mmsegmentation/)
[![PyPI](https://img.shields.io/pypi/v/mmsegmentation)](https://pypi.org/project/mmsegmentation)
[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmsegmentation.readthedocs.io/en/latest/)
[![badge](https://github.com/open-mmlab/mmsegmentation/workflows/build/badge.svg)](https://github.com/open-mmlab/mmsegmentation/actions)
[![codecov](https"
pysc2,"<div align=""center"">
  <a href=""https://www.youtube.com/watch?v=-fKUyT14G-8""
     target=""_blank"">
    <img src=""http://img.youtube.com/vi/-fKUyT14G-8/0.jpg""
         alt=""DeepMind open source PySC2 toolset for Starcraft II""
         width=""240"" height=""180"" border=""10"" />
  </a>
  <a href=""https://www.youtube.com/watch?v=6L448yg0Sm0""
     target=""_blank"">
    <img src=""http://img.youtube.com/vi/6L448yg0Sm0/0.jpg""
         alt=""StarCraft II 'mini games' for AI research""
         width=""240"" height=""180"" border=""10"" />
  </a>
  <a href=""https://www.youtube.com/watch?v=WEOzide5XFc""
     target=""_blank"">
    <img src=""http://img.youtube.com/vi/WEOzide5XFc/0.jpg""
         alt=""Trained and untrained agents play StarCraft II 'mini-game'""
         width=""240"" height=""180"" border=""10"" />
  </a>
</div>

# PySC2 - StarCraft II Learning Environment

[PySC2](https://github.com/deepmind/pysc2) is [DeepMind](http://deepmind.com)'s
Python component of the StarCraft II Learning Environment (SC2LE). It"
denoising-diffusion-pytorch,"<img src=""./images/denoising-diffusion.png"" width=""500px""></img>

## Denoising Diffusion Probabilistic Model, in Pytorch

Implementation of <a href=""https://arxiv.org/abs/2006.11239"">Denoising Diffusion Probabilistic Model</a> in Pytorch. It is a new approach to generative modeling that may <a href=""https://ajolicoeur.wordpress.com/the-new-contender-to-gans-score-matching-with-langevin-sampling/"">have the potential</a> to rival GANs. It uses denoising score matching to estimate the gradient of the data distribution, followed by Langevin sampling to sample from the true distribution.

This implementation was inspired by the official Tensorflow version <a href=""https://github.com/hojonathanho/diffusion"">here</a>

Youtube AI Educators - <a href=""https://www.youtube.com/watch?v=W-O7AZNzbzQ"">Yannic Kilcher</a> | <a href=""https://www.youtube.com/watch?v=344w5h24-h8"">AI Coffeebreak with Letitia</a> | <a href=""https://www.youtube.com/watch?v=HoKDTa5jHvg"">Outlier</a>

<a href=""https://github.co"
readthedocs.org,"Welcome to Read the Docs
========================

|build-status| |docs| |coverage|

Purpose
-------

`Read the Docs`_ hosts documentation for the open source community.
It supports many documentation tools
(e.g. Sphinx_ docs written with reStructuredText_, MkDocs_ docs written with markdown_, among others),
and can pull Git_ repositories.
Then we build documentation and host it for you.
Think of it as *Continuous Documentation*, or Docs as Code.

.. _Read the docs: https://readthedocs.org/
.. _Sphinx: http://www.sphinx-doc.org/
.. _reStructuredText: http://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html
.. _Git: http://git-scm.com/
.. _MkDocs: https://www.mkdocs.org/
.. _markdown: https://daringfireball.net/projects/markdown/

Documentation for Read the Docs
-------------------------------

You will find complete documentation for setting up your project at `the Read the Docs site`_.

.. _the Read the Docs site: https://docs.readthedocs.io/

Get in touch
------------

"
elastalert,"**ElastAlert is no longer maintained. Please use [ElastAlert2](https://github.com/jertel/elastalert2) instead.**


[![Build Status](https://travis-ci.org/Yelp/elastalert.svg)](https://travis-ci.org/Yelp/elastalert)
[![Join the chat at https://gitter.im/Yelp/elastalert](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Yelp/elastalert?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

## ElastAlert - [Read the Docs](http://elastalert.readthedocs.org).
### Easy & Flexible Alerting With Elasticsearch

ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.

ElastAlert works with all versions of Elasticsearch.

At Yelp, we use Elasticsearch, Logstash and Kibana for managing our ever increasing amount of data and logs.
Kibana is great for visualizing and querying data, but we quickly realized that it needed a companion tool for alerting
on inconsistencies in our data. Out of this need, "
hummingbot,"![Hummingbot](https://i.ibb.co/X5zNkKw/blacklogo-with-text.png)

----
[![License](https://img.shields.io/badge/License-Apache%202.0-informational.svg)](https://github.com/hummingbot/hummingbot/blob/master/LICENSE)
[![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/_hummingbot?style=social&label=_hummingbot)](https://twitter.com/_hummingbot)
[![Youtube](https://img.shields.io/youtube/channel/subscribers/UCxzzdEnDRbylLMWmaMjywOA)](https://www.youtube.com/@hummingbot)
[![Discord](https://img.shields.io/discord/530578568154054663?logo=discord&logoColor=white&style=flat-square)](https://discord.gg/hummingbot)

Hummingbot is an open source  framework that helps you build automated trading strategies, or **bots** that run on cryptocurrency exchanges.

This code is free and publicly available under the Apache 2.0 open source license!

## Why Hummingbot?

* **Both CEX and DEX connectors**: Hummingbot supports connectors to centralized exchanges like Binance and KuCoin, as we"
pysheeet,"
.. raw:: html

    <h1 align=""center"">
    <br>
      <a href=""https://www.pythonsheets.com""><img src=""docs/_static/logo.svg"" alt=""pysheeet"" width=200""></a>
    </h1>
    <p align=""center"">
      <a href=""https://github.com/crazyguitar/pysheeet/actions"">
        <img src=""https://github.com/crazyguitar/pysheeet/actions/workflows/pythonpackage.yml/badge.svg"" alt=""Build Status"">
      </a>
      <a href=""https://coveralls.io/github/crazyguitar/pysheeet?branch=master"">
        <img src=""https://coveralls.io/repos/github/crazyguitar/pysheeet/badge.svg?branch=master"" alt=""Coverage"">
      </a>
      <a href=""https://raw.githubusercontent.com/crazyguitar/pysheeet/master/LICENSE"">
        <img src=""https://img.shields.io/badge/License-MIT-blue.svg"" alt=""License MIT"">
      </a>
    </p>

Introduction
=============

Pysheeet was created with intention of collecting python code snippets for
reducing coding hours and making life easier and faster. Any contributions are welcome.
Please feel free"
python-small-examples,"
<div align=""center"">
<img src=""https://img.shields.io/badge/-Python-brightgreen"">
<img src=""https://img.shields.io/badge/-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-yellowgreen"">
<img src=""https://img.shields.io/badge/-%E7%AE%97%E6%B3%95-yellow"">
<img src=""https://img.shields.io/badge/-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lightgrey"">
<a href=""https://static01.imgkr.com/temp/c6e10a16c4764dcdb32587760f6769ec.png"" width=""28%""><img src=""https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Python%E5%B0%8F%E4%BE%8B%E5%AD%90-orange""></a>
</div>
<br>

<!-- <div align=""center"">
<img src=""https://static01.imgkr.com/temp/f379139a2c5d463799c35c1aa68911d7.png"" width=""18%""/>
</div> -->
</div>

## 介绍

告别枯燥，告别枯燥，致力于打造 Python 经典小例子、小案例。 

## License

允许按照要求转载，但禁止用于任何商用目的。如果转载本库小例子、小案例，请备注下方链接：

[Python小例子所有汇总](https://ai-jupyter.com/python-small-examples/)

### 更多教程

[AI消息](https://ai-jupyter.com/)

[AI新闻报道](https://ai-jupyter.com/ai-news-all/)

[AI大模型](https://ai-jupyter.com/ai-llm/)

[AI工具集](https://a"
AlphaPose,"
<div align=""center"">
    <img src=""docs/logo.jpg"", width=""400"">
</div>


## News!
- Nov 2022: [**AlphaPose paper**](http://arxiv.org/abs/2211.03375) is released! Checkout the paper for more details about this project.
- Sep 2022: [**Jittor** version](https://github.com/tycoer/AlphaPose_jittor) of AlphaPose is released! It achieves 1.45x speed up with resnet50 backbone on the training stage.
- July 2022: [**v0.6.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! [HybrIK](https://github.com/Jeff-sjtu/HybrIK) for 3D pose and shape estimation is supported!
- Jan 2022: [**v0.5.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Stronger whole body(face,hand,foot) keypoints! More models are availabel. Checkout [docs/MODEL_ZOO.md](docs/MODEL_ZOO.md)
- Aug 2020: [**v0.4.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Stronger tracking! Include whole body(face,hand,foot) keypoints! [Colab](https://colab.resea"
darts,"# Time Series Made Easy in Python

![darts](https://github.com/unit8co/darts/raw/master/static/images/darts-logo-trim.png ""darts"")

---
[![PyPI version](https://badge.fury.io/py/u8darts.svg)](https://badge.fury.io/py/darts)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/u8darts-all.svg)](https://anaconda.org/conda-forge/u8darts-all)
![Supported versions](https://img.shields.io/badge/python-3.8+-blue.svg)
[![Docker Image Version (latest by date)](https://img.shields.io/docker/v/unit8/darts?label=docker&sort=date)](https://hub.docker.com/r/unit8/darts)
![GitHub Release Date](https://img.shields.io/github/release-date/unit8co/darts)
![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/unit8co/darts/release.yml?branch=master)
[![Downloads](https://pepy.tech/badge/darts)](https://pepy.tech/project/darts)
[![Downloads](https://pepy.tech/badge/u8darts)](https://pepy.tech/project/u8darts)
[![codecov](https://codecov.io/gh/unit8co/darts/branch/master/gr"
docopt,"``docopt`` creates *beautiful* command-line interfaces
======================================================================

.. image:: https://travis-ci.org/docopt/docopt.svg?branch=master
    :target: https://travis-ci.org/docopt/docopt

.. image:: https://img.shields.io/pypi/v/docopt.svg
    :target: https://pypi.python.org/pypi/docopt

Video introduction to **docopt**: `PyCon UK 2012: Create *beautiful*
command-line interfaces with Python <http://youtu.be/pXhcPJK5cMc>`_

    New in version 0.6.1:

    - Fix issue `#85 <https://github.com/docopt/docopt/issues/85>`_
      which caused improper handling of ``[options]`` shortcut
      if it was present several times.

    New in version 0.6.0:

    - New argument ``options_first``, disallows interspersing options
      and arguments.  If you supply ``options_first=True`` to
      ``docopt``, it will interpret all arguments as positional
      arguments after first positional argument.

    - If option with argument could be repeated"
docker-stacks,"# Jupyter Docker Stacks

[![GitHub actions badge](https://github.com/jupyter/docker-stacks/actions/workflows/docker.yml/badge.svg)
](https://github.com/jupyter/docker-stacks/actions/workflows/docker.yml?query=branch%3Amain ""Docker images build status"")
[![Read the Docs badge](https://img.shields.io/readthedocs/jupyter-docker-stacks.svg)](https://jupyter-docker-stacks.readthedocs.io/en/latest/ ""Documentation build status"")
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/jupyter/docker-stacks/main.svg)](https://results.pre-commit.ci/latest/github/jupyter/docker-stacks/main ""pre-commit.ci build status"")
[![Discourse badge](https://img.shields.io/discourse/users.svg?color=%23f37626&server=https%3A%2F%2Fdiscourse.jupyter.org)](https://discourse.jupyter.org/ ""Jupyter Discourse Forum"")
[![Binder badge](https://static.mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyter/docker-stacks/main?urlpath=lab/tree/README.ipynb ""Launch a quay.io/jupyter/base-notebook cont"
imaginAIry,"# ImaginAIry 🤖🧠

[![Downloads](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rOvQNs0Cmn_yU1bKWjCOHzGVDgZkaTtO?usp=sharing)
[![Downloads](https://pepy.tech/badge/imaginairy)](https://pepy.tech/project/imaginairy)
[![image](https://img.shields.io/pypi/v/imaginairy.svg)](https://pypi.org/project/imaginairy/)
[![image](https://img.shields.io/badge/license-MIT-green)](https://github.com/brycedrennan/imaginAIry/blob/master/LICENSE/)
[![Discord](https://flat.badgen.net/discord/members/FdD7ut3YjW)](https://discord.gg/FdD7ut3YjW)

AI imagined images. Pythonic generation of stable diffusion images **and videos** *!.

""just works"" on Linux and macOS(M1) (and sometimes windows).


```bash
# on macOS, make sure rust is installed first
# be sure to use Python 3.10, Python 3.11 is not supported at the moment
>> pip install imaginairy
>> imagine ""a scenic landscape"" ""a photo of a dog"" ""photo of a fruit bowl"" ""portrait photo of a freckled woman"" ""a "
binance-trade-bot,"# binance-trade-bot
> Automated cryptocurrency trading bot

![github](https://img.shields.io/github/workflow/status/edeng23/binance-trade-bot/binance-trade-bot)
![docker](https://img.shields.io/docker/pulls/edeng23/binance-trade-bot)
[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy?template=https://github.com/edeng23/binance-trade-bot)

[![Deploy to DO](https://mp-assets1.sfo2.digitaloceanspaces.com/deploy-to-do/do-btn-blue.svg)](https://cloud.digitalocean.com/apps/new?repo=https://github.com/coinbookbrasil/binance-trade-bot/tree/master&refcode=a076ff7a9a6a)


## Follow me on Twitter :)

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%400xedeng)](https://twitter.com/0xedeng)

## Why?

This project was inspired by the observation that all cryptocurrencies pretty much behave in the same way. When one spikes, they all spike, and when one takes a dive, they all do. _Pretty much_. Moreover, all co"
EfficientNet-PyTorch,"# EfficientNet PyTorch

### Quickstart

Install with `pip install efficientnet_pytorch` and load a pretrained EfficientNet with:
```python
from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0')
```

### Updates

#### Update (April 2, 2021)

The [EfficientNetV2 paper](https://arxiv.org/abs/2104.00298) has been released! I am working on implementing it as you read this :) 

About EfficientNetV2:
> EfficientNetV2 is a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. 

Here is a comparison: 
> <img src=""https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnetv2-image.png"" width=""100%"
sygil-webui,"# <center>Web-based UI for Stable Diffusion</center>

## Created by [Sygil.Dev](https://github.com/sygil-dev)

## Join us at Sygil.Dev's Discord Server [![Generic badge](https://flat.badgen.net/discord/members/ttM8Tm6wge?icon=discord)](https://discord.gg/ttM8Tm6wge)

## Installation instructions for:

- **[Windows](https://sygil-dev.github.io/sygil-webui/docs/Installation/windows-installation)**
- **[Linux](https://sygil-dev.github.io/sygil-webui/docs/Installation/linux-installation)**

### Want to ask a question or request a feature?

Come to our [Discord Server](https://discord.gg/gyXNe4NySY) or use [Discussions](https://github.com/sygil-dev/sygil-webui/discussions).

## Documentation

[Documentation is located here](https://sygil-dev.github.io/sygil-webui/)

## Want to contribute?

Check the [Contribution Guide](CONTRIBUTING.md)

[Sygil-Dev](https://github.com/Sygil-Dev) main devs:

* ![ZeroCool940711's avatar](https://avatars.githubusercontent.com/u/5977640"
pulse,"# PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
Code accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/pdf/2003.03808.pdf

## NOTE

We have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - **PULSE makes imaginary faces of people who do not exist, which should not be confused for real people.** It will **not** help identify or reconstruct the original image.

We also want to address concerns of bias in PULSE. **We have now included a new section in the [paper](https://arxiv.org/pdf/2003.03808.pdf) and an accompanying model card directly addressing this bias.**

---

![Transformation Preview](./readme_resources/014.jpeg)
![Transformation Preview](./readme_resources/034.jpeg)
![Transformation Preview](./readme_resources/094.jpeg)

Table of Contents
=================
- [PULSE: Self-Supervised Photo Upsampling via Lat"
cog,"# Cog: Containers for machine learning

Cog is an open-source tool that lets you package machine learning models in a standard, production-ready container.

You can deploy your packaged model to your own infrastructure, or to [Replicate](https://replicate.com/).

## Highlights

- 📦 **Docker containers without the pain.** Writing your own `Dockerfile` can be a bewildering process. With Cog, you define your environment with a [simple configuration file](#how-it-works) and it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.

- 🤬️ **No more CUDA hell.** Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.

- ✅ **Define the inputs and outputs for your model with standard Python.** Then, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.

- 🎁 **Automa"
PaddleGAN,"
English | [简体中文](./README_cn.md)

# PaddleGAN

PaddleGAN provides developers with high-performance implementation of classic and SOTA Generative Adversarial Networks, and supports developers to quickly build, train and deploy GANs for academic, entertainment and industrial usage.

GAN-Generative Adversarial Network, was praised by ""the Father of Convolutional Networks""  **Yann LeCun (Yang Likun)**  as **[One of the most interesting ideas in the field of computer science in the past decade]**. It's the one research area in deep learning that AI researchers are most concerned about.

<div align='center'>
  <img src='./docs/imgs/ppgan.jpg'>
</div>

[![License](https://img.shields.io/badge/license-Apache%202-red.svg)](LICENSE)![python version](https://img.shields.io/badge/python-3.6+-orange.svg)

## 🎪 Hot Activities

- 2021.4.15~4.22

  GAN 7 Days Course Camp: Baidu Senior Research Developers help you learn the basic and advanced GAN knowledge in 7 days!

  **Courses videos and related ma"
monoid,"<img alt=""Monoid Banner"" src=""https://github.com/andreaslarsen/monoid/raw/master/Utilities/Images/MonoidReadme.png"" />
<p align=""center"">
<a href=""#font_log""><img alt=""version"" src=""https://img.shields.io/github/tag/larsenwork/monoid.svg"" height=""20px""></a>  <a href=""#license""><img alt=""license"" src=""https://img.shields.io/badge/license-MIT%20%2B%20OFL-lightgrey.svg"" height=""20px""></a>  <a href=""http://twitter.com/larsenwork""><img alt=""twitter"" src=""https://img.shields.io/badge/updates-%40larsenwork-blue.svg"" height=""20px""/></a>
</p>
<p align=""center""><a href=""#guide"">Guide</a>       <a href=""#liga"">Ligature Support</a>       <a href=""#links"">Links</a>       <a href=""#font_log"">Log</a>       <a href=""#license"">License</a>
</p>

<a name=""guide""></a>

&nbsp;

# Guide

### Live Preview + Download

[larsenwork.com/monoid](http://larsenwork.com/monoid)

&nbsp;

### Install

Quit your editor/program. Unzip and open the folder.

**Mac + Linux (with font-viewer)**  
Select the .ttf files and d"
tensorboardX,"# tensorboardX

[![PyPI version](https://badge.fury.io/py/tensorboardX.svg)](https://badge.fury.io/py/tensorboardX)
[![Documentation Status](https://readthedocs.org/projects/tensorboardx/badge/?version=latest)](https://tensorboardx.readthedocs.io/en/latest/?badge=latest)
[![Coverage Status](https://codecov.io/gh/lanpa/tensorboardX/branch/master/graph/badge.svg)](https://codecov.io/gh/lanpa/tensorboardX/)

Write TensorBoard events with simple function call.

The current release (v2.5) is tested on anaconda3, with PyTorch 1.11.0 / torchvision 0.12 / tensorboard 2.9.0.

* Support `scalar`, `image`, `figure`, `histogram`, `audio`, `text`, `graph`, `onnx_graph`, `embedding`, `pr_curve`, `mesh`, `hyper-parameters`
  and `video` summaries.

* [FAQ](https://github.com/lanpa/tensorboardX/wiki)


## Install

`pip install tensorboardX`

or build from source:

`pip install 'git+https://github.com/lanpa/tensorboardX'`

You can optionally install [`crc32c`](https://github.com/ICRAR/crc32c) to speed "
PathPlanning,"Overview
------
This repository implements some common path planning algorithms used in robotics, including Search-based algorithms and Sampling-based algorithms. We designed animation for each algorithm to display the running process. The related papers are listed in [Papers](https://github.com/zhm-real/PathPlanning#papers).

Directory Structure
------
    .
    └── Search-based Planning
        ├── Breadth-First Searching (BFS)
        ├── Depth-First Searching (DFS)
        ├── Best-First Searching
        ├── Dijkstra's
        ├── A*
        ├── Bidirectional A*
        ├── Anytime Repairing A*
        ├── Learning Real-time A* (LRTA*)
        ├── Real-time Adaptive A* (RTAA*)
        ├── Lifelong Planning A* (LPA*)
        ├── Dynamic A* (D*)
        ├── D* Lite
        └── Anytime D*
    └── Sampling-based Planning
        ├── RRT
        ├── RRT-Connect
        ├── Extended-RRT
        ├── Dynamic-RRT
        ├── RRT*
        ├── Informed RRT*
        ├── RRT* Smart
        ├──"
reactpy,"# <img src=""https://raw.githubusercontent.com/reactive-python/reactpy/main/branding/svg/reactpy-logo-square.svg"" align=""left"" height=""45""/> ReactPy

<p>
    <a href=""https://github.com/reactive-python/reactpy/actions"">
        <img src=""https://github.com/reactive-python/reactpy/workflows/test/badge.svg?event=push"">
    </a>
    <a href=""https://pypi.org/project/reactpy/"">
        <img src=""https://img.shields.io/pypi/v/reactpy.svg?label=PyPI"">
    </a>
    <a href=""https://github.com/reactive-python/reactpy/blob/main/LICENSE"">
        <img src=""https://img.shields.io/badge/License-MIT-purple.svg"">
    </a>
    <a href=""https://reactpy.dev/"">
        <img src=""https://img.shields.io/website?down_message=offline&label=Docs&logo=read-the-docs&logoColor=white&up_message=online&url=https%3A%2F%2Freactpy.dev%2Fdocs%2Findex.html"">
    </a>
    <a href=""https://discord.gg/uNb5P4hA9X"">
        <img src=""https://img.shields.io/discord/1111078259854168116?label=Discord&logo=discord"">
    </a>
</"
paperless,"[ en | [de](README-de.md) | [el](README-el.md) ]

![Paperless](https://raw.githubusercontent.com/the-paperless-project/paperless/master/src/paperless/static/paperless/img/logo-dark.png)

> ## Important news about the future of this project
> 
> It's been more than 5 years since I started this project on a whim as an effort to try to get a handle on the massive amount of paper I was dealing with in relation to various visa applications (expat life is complicated!)  Since then, the project has *exploded* in popularity, so much so that it overwhelmed me and working on it stopped being ""fun"" and started becoming a serious source of stress.
> 
> In an effort to fix this, I created the Paperless GitHub [organisation](https://github.com/the-paperless-project), and brought on a few people to manage the issue and pull request load.  Unfortunately, that model has proven to be unworkable too.  With 23 pull requests waiting and 157 issues slowly filling up with confused/annoyed people wanting to g"
platformio-core,"PlatformIO Core
===============

.. image:: https://github.com/platformio/platformio-core/workflows/Core/badge.svg
    :target: https://docs.platformio.org/en/latest/core/index.html
    :alt:  CI Build for PlatformIO Core
.. image:: https://github.com/platformio/platformio-core/workflows/Docs/badge.svg
    :target: https://docs.platformio.org?utm_source=github&utm_medium=core
    :alt:  CI Build for Docs
.. image:: https://github.com/platformio/platformio-core/workflows/Examples/badge.svg
    :target: https://github.com/platformio/platformio-examples
    :alt:  CI Build for dev-platform examples
.. image:: https://github.com/platformio/platformio-core/workflows/Projects/badge.svg
    :target: https://docs.platformio.org/en/latest/tutorials/index.html#projects
    :alt:  CI Build for the Community Projects
.. image:: https://img.shields.io/pypi/v/platformio.svg
    :target: https://pypi.python.org/pypi/platformio/
    :alt: Latest Version
.. image:: https://img.shields.io/badge/Platform"
text_classification,"Text Classification
-------------------------------------------------------------------------
The purpose of this repository is to explore text classification methods in NLP with deep learning.

#### Update: 

Customize an NLP API in three minutes, for free: <a href='https://www.cluebenchmarks.com/clueai.html'>NLP API Demo</a>

Language Understanding Evaluation benchmark for Chinese(<a href='https://www.CLUEbenchmarks.com'>CLUE benchmark<a/>): run 10 tasks & 9 baselines with one line of code, performance comparision with details.

Releasing Pre-trained Model of <a href=""https://github.com/brightmart/albert_zh"">ALBERT_Chinese</a> Training with 30G+ Raw Chinese Corpus, xxlarge, xlarge and more, Target to match State of the Art performance in Chinese, 2019-Oct-7, During the National Day of China!
 
<a href='https://github.com/brightmart/nlp_chinese_corpus'>Large Amount of Chinese Corpus for NLP Available!</a>

Google's BERT achieved new state of art result on more than 10 tasks in NLP usi"
DrissionPage,"How to use: [Documents](https://DrissionPage.cn)

This project is mainly updated in gitee, and will be submitted to GitHub after producing a stable version.
Check out the latest developments at [gitee](https://gitee.com/g1879/DrissionPage).

# ✨️ Overview

DrissionPage is a python-based web page automation tool.
It can control the browser, send and receive data packets, and combine the two into one.
It can take into account the convenience of browser automation and the high efficiency of requests.
It is powerful and has countless built-in user-friendly designs and convenient functions.
Its syntax is concise and elegant, the amount of code is small, and it is friendly to novices.

Your star is the greatest support for me.💖

---

# ☕ Buy me coffee

If this project is helpful to you, why not buy the author a cup of coffee :)

![](https://drissionpage.cn/code2.jpg)

---

<a href=""https://hellogithub.com/repository/dad1ecb7fbd34898a3380f5f0948ceb6"" target=""_blank""><"
easytrader,"# easytrader

[![Package](https://img.shields.io/pypi/v/easytrader.svg)](https://pypi.python.org/pypi/easytrader)
[![Travis](https://img.shields.io/travis/shidenggui/easytrader.svg)](https://travis-ci.org/shidenggui/easytrader)
[![License](https://img.shields.io/github/license/shidenggui/easytrader.svg)](https://github.com/shidenggui/easytrader/blob/master/LICENSE)

* 进行自动的程序化股票交易
* 支持跟踪 `joinquant`, `ricequant` 的模拟交易
* 支持跟踪 雪球组合 调仓
* 支持通用的同花顺客户端模拟操作
* 实现自动登录
* 支持通过 webserver 远程操作客户端
* 支持命令行调用，方便其他语言适配
* 基于 Python3.6, Win。注: Linux 仅支持雪球


### 微信群以及公众号

欢迎大家扫码关注公众号「食灯鬼」，一起交流。进群可通过菜单加我好友，备注量化。

![公众号二维码](https://gitee.com/shidenggui/assets/raw/master/uPic/mp-qr.png)

若二维码因 Github 网络无法打开，请点击[公众号二维码](https://gitee.com/shidenggui/assets/raw/master/uPic/mp-qr.png)直接打开图片。

### Author

**easytrader** © [shidenggui](https://github.com/shidenggui), Released under the [MIT](./LICENSE) License.<br>

> Blog [@shidenggui](https://shidenggui.com) · Weibo [@食灯鬼](https://www.weibo.com/u/1651274491) · T"
pdm,"<div align=""center"">

# PDM

A modern Python package and dependency manager supporting the latest PEP standards.
[中文版本说明](README_zh.md)

![PDM logo](https://raw.githubusercontent.com/pdm-project/pdm/main/docs/assets/logo_big.png)

[![Docs](https://img.shields.io/badge/Docs-mkdocs-blue?style=for-the-badge)](https://pdm-project.org)
[![Twitter Follow](https://img.shields.io/twitter/follow/pdm_project?label=get%20updates&logo=twitter&style=for-the-badge)](https://twitter.com/pdm_project)
[![Discord](https://img.shields.io/discord/824472774965329931?label=discord&logo=discord&style=for-the-badge)](https://discord.gg/Phn8smztpv)

![Github Actions](https://github.com/pdm-project/pdm/workflows/Tests/badge.svg)
[![PyPI](https://img.shields.io/pypi/v/pdm?logo=python&logoColor=%23cccccc)](https://pypi.org/project/pdm)
[![codecov](https://codecov.io/gh/pdm-project/pdm/branch/main/graph/badge.svg?token=erZTquL5n0)](https://codecov.io/gh/pdm-project/pdm)
[![Packaging status](https://repology.org/ba"
Bert-VITS2,"<div align=""center"">

<img alt=""LOGO"" src=""https://avatars.githubusercontent.com/u/122017386"" width=""256"" height=""256"" />

# Bert-VITS2

VITS2 Backbone with multilingual bert

For quick guide, please refer to `webui_preprocess.py`.

简易教程请参见 `webui_preprocess.py`。

## 【项目推介】
# FishAudio下的全新自回归TTS [Fish-Speech](https://github.com/fishaudio/fish-speech)现已可用，效果为目前开源SOTA水准，且在持续维护，推荐使用该项目作为BV2/GSV的替代。本项目短期内不再进行维护。
## Demo Video: https://www.bilibili.com/video/BV18E421371Q
## Tech slides Video: https://www.bilibili.com/video/BV1zJ4m1K7cj
## 请注意，本项目核心思路来源于[anyvoiceai/MassTTS](https://github.com/anyvoiceai/MassTTS) 一个非常好的tts项目
## MassTTS的演示demo为[ai版峰哥锐评峰哥本人,并找回了在金三角失落的腰子](https://www.bilibili.com/video/BV1w24y1c7z9)

[//]: # (## 本项目与[PlayVoice/vits_chinese]&#40;https://github.com/PlayVoice/vits_chinese&#41; 没有任何关系)

[//]: # ()
[//]: # (本仓库来源于之前朋友分享了ai峰哥的视频，本人被其中的效果惊艳，在自己尝试MassTTS以后发现fs在音质方面与vits有一定差距，并且training的pipeline比vits更复杂，因此按照其思路将bert)

## 成熟的旅行者/开拓者/舰长/博士/sensei/猎魔人/喵喵露/V应当参阅代码自己学习如何训练。
"
BayesianOptimization,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/bayesian-optimization/BayesianOptimization/master/docsrc/static/func.png""><br><br>
</div>

# Bayesian Optimization

![tests](https://github.com/bayesian-optimization/BayesianOptimization/actions/workflows/run_tests.yml/badge.svg)
[![docs - stable](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fbayesian-optimization%2FBayesianOptimization%2Fgh-pages%2Fversions.json&query=%24%5B%3F(%40.aliases%20%26%26%20%40.aliases.indexOf('stable')%20%3E%20-1)%5D.version&prefix=stable%20(v&suffix=)&label=docs)](https://bayesian-optimization.github.io/BayesianOptimization/)
[![Codecov](https://codecov.io/github/bayesian-optimization/BayesianOptimization/badge.svg?branch=master&service=github)](https://codecov.io/github/bayesian-optimization/BayesianOptimization?branch=master)
[![Pypi](https://img.shields.io/pypi/v/bayesian-optimization.svg)](https://pypi.python.org/pypi/bayesian-optimization)
![Py"
pytorch-cnn-visualizations,"# Convolutional Neural Network Visualizations 

This repository contains a number of convolutional neural network visualization techniques implemented in PyTorch.

**Note**: I removed cv2 dependencies and moved the repository towards PIL. A few things might be broken (although I tested all methods), I would appreciate if you could create an issue if something does not work.

**Note**: The code in this repository was tested with torch version 0.4.1 and some of the functions may not work as intended in later versions. Although it shouldn't be too much of an effort to make it work, I have no plans at the moment to make the code in this repository compatible with the latest version because I'm still using 0.4.1.

## Implemented Techniques

* [Gradient visualization with vanilla backpropagation](#gradient-visualization)
* [Gradient visualization with guided backpropagation](#gradient-visualization) [1]
* [Gradient visualization with saliency maps](#gradient-visualization) [4]
* [Gradient-we"
tianshou,"<div align=""center"">
  <a href=""http://tianshou.readthedocs.io""><img width=""300px"" height=""auto"" src=""https://github.com/thu-ml/tianshou/raw/master/docs/_static/images/tianshou-logo.png""></a>
</div>

---

[![PyPI](https://img.shields.io/pypi/v/tianshou)](https://pypi.org/project/tianshou/) [![Conda](https://img.shields.io/conda/vn/conda-forge/tianshou)](https://github.com/conda-forge/tianshou-feedstock) [![Read the Docs](https://readthedocs.org/projects/tianshou/badge/?version=master)](https://tianshou.org/en/master/) [![Pytest](https://github.com/thu-ml/tianshou/actions/workflows/pytest.yml/badge.svg)](https://github.com/thu-ml/tianshou/actions) [![codecov](https://img.shields.io/codecov/c/gh/thu-ml/tianshou)](https://codecov.io/gh/thu-ml/tianshou) [![GitHub issues](https://img.shields.io/github/issues/thu-ml/tianshou)](https://github.com/thu-ml/tianshou/issues) [![GitHub stars](https://img.shields.io/github/stars/thu-ml/tianshou)](https://github.com/thu-ml/tianshou/stargazers) [![Git"
bypy,"bypy - Python client for Baidu Yun (Personal Cloud Storage) 百度云/百度网盘Python客户端
====================================================================================

[![alt text](https://img.shields.io/pypi/v/bypy.svg ""PyPi Version"")](https://pypi.python.org/pypi/bypy)
[![alt text](https://img.shields.io/pypi/dm/bypy.svg ""PyPi Downloads"")](https://pypi.python.org/pypi/bypy)
[![alt text](https://travis-ci.org/houtianze/bypy.svg ""Build status"")](https://travis-ci.org/houtianze/bypy)
[![Coverage Status](https://coveralls.io/repos/houtianze/bypy/badge.svg?branch=master&service=github)](https://coveralls.io/github/houtianze/bypy?branch=master)
[![Code Climate](https://codeclimate.com/github/houtianze/bypy/badges/gpa.svg)](https://codeclimate.com/github/houtianze/bypy)
[![Join the chat at https://gitter.im/houtianze/bypy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/houtianze/bypy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

极简说明
-------

- 安装: `p"
visidata,"# VisiData v3.0

[![Tests](https://github.com/saulpw/visidata/workflows/visidata-ci-build/badge.svg)](https://github.com/saulpw/visidata/actions/workflows/main.yml)
[![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/saulpw/visidata)

[![discord](https://img.shields.io/discord/880915750007750737?label=discord)](https://visidata.org/chat)
[![mastodon @visidata@fosstodon.org][2.1]][2]
[![twitter @VisiData][1.1]][1]

A terminal interface for exploring and arranging tabular data.

![Frequency table](http://visidata.org/freq-move-row.gif)

VisiData supports tsv, csv, sqlite, json, xlsx (Excel), hdf5, and [many other formats](https://visidata.org/formats).

## Platform requirements

- Linux, OS/X, or Windows (with WSL)
- Python 3.8+
- additional Python modules are required for certain formats and sources

## Install

To install the latest release from PyPi:

    pip3 install visidata

To install the cutting edg"
jc,"[![Tests](https://github.com/kellyjonbrazil/jc/workflows/Tests/badge.svg?branch=master)](https://github.com/kellyjonbrazil/jc/actions)
[![Pypi](https://img.shields.io/pypi/v/jc.svg)](https://pypi.org/project/jc/)

> Check out the `jc` Python [package documentation](https://github.com/kellyjonbrazil/jc/tree/master/docs) for developers

> Try the `jc` [web demo](https://jc-web.onrender.com/) and [REST API](https://github.com/kellyjonbrazil/jc-restapi)

> `jc` is available as an
[Ansible filter plugin](https://docs.ansible.com/ansible/latest/collections/community/general/jc_filter.html#ansible-collections-community-general-jc-filter)
in the `community.general` collection. See this
[blog post](https://blog.kellybrazil.com/2020/08/30/parsing-command-output-in-ansible-with-jc/)
for an example.

# JC
JSON Convert

`jc` JSONifies the output of many CLI tools, file-types, and common strings
for easier parsing in scripts. See the [**Parsers**](#parsers) section for
supported commands, file-types"
InfoSpider,"<p align=""center"">
    <img src=""https://i.loli.net/2020/10/20/SKOdFZpVYo4LvgT.png"" alt=""InfoSpider logo""/>
</p>

***

<p align=""center"">
    <a>
        <img alt=""GitHub stars"" src=""https://img.shields.io/github/stars/kangvcar/infospider?style=social"">
    </a>
    <a>
        <img src=""https://img.shields.io/badge/python-v3-blue?style=flat-square"" alt=""UW2eVx.png"" />
    </a>
    <a>
        <img src=""https://img.shields.io/badge/platform-Windows-blue?style=flat-square"" alt=""UW2eVx.png"" />
    </a>
    <a>
        <img src=""https://img.shields.io/website?up_message=%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3&url=https%3A%2F%2Finfospider.vercel.app%2F"" alt=""UW2eVx.png"" />
    </a>
    <a>
    <img alt=""GitHub repo size"" src=""https://img.shields.io/github/repo-size/kangvcar/infospider?style=flat-square"">
    </a>
    <a>
    <img alt=""GitHub repo size"" src=""https://img.shields.io/badge/license-GPL-blue?style=flat-square"">
    </a>
</p>
<p align=""center"">一个神奇的工具箱，拿回你的个人信息。</p>
<p align=""center"
llama-cpp-python,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/abetlen/llama-cpp-python/main/docs/icon.svg"" style=""height: 5rem; width: 5rem"">
</p>

#  Python Bindings for [`llama.cpp`](https://github.com/ggerganov/llama.cpp)

[![Documentation Status](https://readthedocs.org/projects/llama-cpp-python/badge/?version=latest)](https://llama-cpp-python.readthedocs.io/en/latest/?badge=latest)
[![Tests](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml)
[![PyPI](https://img.shields.io/pypi/v/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)
[![PyPI - License](https://img.shields.io/pypi/l/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-cpp-python)](https://pypi.or"
dream-textures,"![Dream Textures, subtitle: Stable Diffusion built-in to Blender](docs/assets/banner.png)

[![Latest Release](https://flat.badgen.net/github/release/carson-katri/dream-textures)](https://github.com/carson-katri/dream-textures/releases/latest)
[![Join the Discord](https://flat.badgen.net/badge/icon/discord?icon=discord&label)](https://discord.gg/EmDJ8CaWZ7)
[![Total Downloads](https://img.shields.io/github/downloads/carson-katri/dream-textures/total?style=flat-square)](https://github.com/carson-katri/dream-textures/releases/latest)
[![Buy on Blender Market](https://flat.badgen.net/badge/buy/blender%20market/orange)](https://www.blendermarket.com/products/dream-textures)

* Create textures, concept art, background assets, and more with a simple text prompt
* Use the 'Seamless' option to create textures that tile perfectly with no visible seam
* Texture entire scenes with 'Project Dream Texture' and depth to image
* Re-style animations with the Cycles render pass
* Run the models on your "
jukebox,"**Status:** Archive (code is provided as-is, no updates expected)

# Jukebox
Code for ""Jukebox: A Generative Model for Music""

[Paper](https://arxiv.org/abs/2005.00341) 
[Blog](https://openai.com/blog/jukebox) 
[Explorer](http://jukebox.openai.com/) 
[Colab](https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb) 

# Install
Install the conda package manager from https://docs.conda.io/en/latest/miniconda.html    
    
``` 
# Required: Sampling
conda create --name jukebox python=3.7.5
conda activate jukebox
conda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3
conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch
git clone https://github.com/openai/jukebox.git
cd jukebox
pip install -r requirements.txt
pip install -e .

# Required: Training
conda install av=7.0.01 -c conda-forge 
pip install ./tensorboardX
 
# Optional: Apex for faster training with fused_adam
conda install pytorch=1.1 torchvision=0"
sktime,"<a href=""https://www.sktime.net""><img src=""https://github.com/sktime/sktime/blob/main/docs/source/images/sktime-logo.svg"" width=""175"" align=""right"" /></a>

# Welcome to sktime

> A unified interface for machine learning with time series

:rocket: **Version 0.33.1 out now!** [Check out the release notes here](https://www.sktime.net/en/latest/changelog.html).

sktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes time series classification, regression, clustering, annotation, and forecasting. It comes with [time series algorithms](https://www.sktime.net/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune and validate time series models.

[scikit-learn]: https://scikit-learn.org/stable/

|  | **[Documentation](https://www.sktime.net/en/stable/users.html)** · **[Tutorials](https://www.sktime.net/en/stable/examples.html)** · **[Release Notes](https://www.skti"
ASRT_SpeechRecognition,"![](assets/asrt_title_header.png)

[![GPL-3.0 Licensed](https://img.shields.io/badge/License-GPL3.0-blue.svg?style=flat)](https://opensource.org/licenses/GPL-3.0) 
[![Stars](https://img.shields.io/github/stars/nl8590687/ASRT_SpeechRecognition)](https://github.com/nl8590687/ASRT_SpeechRecognition) 
[![TensorFlow Version](https://img.shields.io/badge/Tensorflow-2.5+-blue.svg)](https://www.tensorflow.org/) 
[![Python Version](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/) 
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5808434.svg)](https://doi.org/10.5281/zenodo.5808434)

ASRT是一个基于深度学习的中文语音识别系统，如果您觉得喜欢，请点一个 **""Star""** 吧~

**ReadMe Language** | 中文版 | [English](https://github.com/nl8590687/ASRT_SpeechRecognition/blob/master/README_EN.md) |

[**ASRT项目主页**](https://asrt.ailemon.net/) | 
[**发布版下载**](https://wiki.ailemon.net/docs/asrt-doc/download) | 
[**查看本项目的Wiki文档**](https://wiki.ailemon.net/docs/asrt-doc) | 
[**实用效果体验Demo**](https://asrt.ai"
WSABuilds,"## ""Microsoft is ending support for the Windows Subsystem for Android™️ (WSA). As a result, the Amazon Appstore on Windows and all applications and games dependent on WSA will no longer be supported beginning March 5, 2025."" 
###### (Source: [GitHub](https://github.com/microsoft/WSA/discussions/536) and [Microsoft Learn](https://learn.microsoft.com/en-us/windows/android/wsa/))
---
### WSABuilds has entered LTS (Long Term Support) for WSA version 2311.40000.5.0, where the Magisk version, KernelSU version and GApps version will be kept up to date via new releases.
### This repo will not be archived and support will still be given to any users installing WSA Builds from this repo. Thank you all for using this repository and supporting my work, its been a pleasure serving this community. 

---

## Next LTS Release Date:
### WSABuilds LTS 5 (v2407.40000.0.0): 
``TBD``
### WSABuilds LTS 4 (v2407.40000.0.0): 
~~``Monday 15th July 2024``~~  **Available Now (via the Pre-release buttons in [Down"
waydroid,"# Waydroid

Waydroid uses a container-based approach to boot a full Android system on a
regular GNU/Linux system like Ubuntu.

## Overview

Waydroid uses Linux namespaces (user, pid, uts, net, mount, ipc) to run a
full Android system in a container and provide Android applications on
any GNU/Linux-based platform.

The Android system inside the container has direct access to any needed hardware.

The Android runtime environment ships with a minimal customized Android system
image based on [LineageOS](https://lineageos.org/). The image is currently based
on Android 11.

## Documentation

Our documentation site can be found at [docs.waydro.id](https://docs.waydro.id)

## Reporting bugs

If you have found an issue with Waydroid, please [file a bug](https://github.com/Waydroid/waydroid/issues/new).

## Get in Touch

If you want to get in contact with the developers please feel free to join the
*Waydroid* groups in [Matrix](https://matrix.to/#/#waydroid:matrix.org) or [Telegram](https://t.me"
mage-ai,"<h1 align=""center"">
  <a
    target=""_blank""
    href=""https://mage.ai""
  >
    <img
      align=""center""
      alt=""Mage""
      src=""https://github.com/mage-ai/assets/blob/main/mascots/mascots-shorter.jpeg?raw=true""
      style=""width:100%;""
    />
  </a>
</h1>
<p align=""center"">
  🧙 A modern replacement for Airflow.
</p>

<p align=""center"">
  <a
    href=""https://docs.mage.ai""
    target=""_blank""
  ><b>Documentation</b></a>&nbsp;&nbsp;&nbsp;🌪️&nbsp;&nbsp;&nbsp;
  <a
    href=""https://youtu.be/GswOdShLGmg""
    target=""_blank""
  ><b>Get a 5 min overview</b></a>&nbsp;&nbsp;&nbsp;🌊&nbsp;&nbsp;&nbsp;
  <a
    href=""https://demo.mage.ai""
    target=""_blank""
  ><b>Play with live tool</b></a>&nbsp;&nbsp;&nbsp;🔥&nbsp;&nbsp;&nbsp;
  <a
    href=""https://www.mage.ai/chat""
    target=""_blank""
  >
    <b>Get instant help</b>
  </a>
</p>
<div align=""center"">
  <a
    href=""https://pypi.org/project/mage-ai/""
    target=""_blank""
  >
    <img alt=""PyPi"" src=""https://img.shields.io/pypi/v/mage-ai?colo"
accelerate,"<!---
Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/huggingface/accelerate/main/docs/source/imgs/accelerate_logo.png"" width=""400""/>
    <br>
<p>

<p align=""center"">
    <!-- Uncomment when CircleCI is set up
    <a href=""https://circleci.com/gh/huggingface/accelerate""><img alt=""Build"" src=""https://img.shields.io/circleci/build/github/huggingface/transformers/master""></a>
    -->
"
lutris,"******
Lutris
******

|LiberaPayBadge|_ |PatreonBadge|_

Lutris helps you install and play video games from all eras and from most
gaming systems. By leveraging and combining existing emulators, engine
re-implementations and compatibility layers, it gives you a central interface
to launch all your games.

The client can connect with existing services like Humble Bundle, GOG and Steam
to make your game libraries easily available. Game downloads and installations
are automated and can be modified through user made scripts.

Running Lutris
==============

If you have not installed Lutris through your package manager and are using the
source package, it is recommended that you install lutris at least once, even an
older version to have all dependencies available.
Once all dependencies are satisfied, you can run lutris directly from the source
directory with `./bin/lutris`

If you need to run lutris through gdb to troubleshoot segmentation faults, you
can use the following command:

`gdb -e"
jupyterhub,"**[Technical Overview](#technical-overview)** |
**[Installation](#installation)** |
**[Configuration](#configuration)** |
**[Docker](#docker)** |
**[Contributing](#contributing)** |
**[License](#license)** |
**[Help and Resources](#help-and-resources)**

---

# [JupyterHub](https://github.com/jupyterhub/jupyterhub)

[![Latest PyPI version](https://img.shields.io/pypi/v/jupyterhub?logo=pypi)](https://pypi.python.org/pypi/jupyterhub)
[![Latest conda-forge version](https://img.shields.io/conda/vn/conda-forge/jupyterhub?logo=conda-forge)](https://anaconda.org/conda-forge/jupyterhub)
[![Documentation build status](https://img.shields.io/readthedocs/jupyterhub?logo=read-the-docs)](https://jupyterhub.readthedocs.org/en/latest/)
[![GitHub Workflow Status - Test](https://img.shields.io/github/workflow/status/jupyterhub/jupyterhub/Test?logo=github&label=tests)](https://github.com/jupyterhub/jupyterhub/actions)
[![Test coverage of code](https://codecov.io/gh/jupyterhub/jupyterhub/branch/main/grap"
autogluon,"

<div align=""center"">
<img src=""https://user-images.githubusercontent.com/16392542/77208906-224aa500-6aba-11ea-96bd-e81806074030.png"" width=""350"">

## Fast and Accurate ML in 3 Lines of Code

[![Latest Release](https://img.shields.io/github/v/release/autogluon/autogluon)](https://github.com/autogluon/autogluon/releases)
[![Conda Forge](https://img.shields.io/conda/vn/conda-forge/autogluon.svg)](https://anaconda.org/conda-forge/autogluon)
[![Python Versions](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue)](https://pypi.org/project/autogluon/)
[![Downloads](https://pepy.tech/badge/autogluon/month)](https://pepy.tech/project/autogluon)
[![GitHub license](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](./LICENSE)
[![Discord](https://img.shields.io/discord/1043248669505368144?logo=discord&style=flat)](https://discord.gg/wjUmjqAc2N)
[![Twitter](https://img.shields.io/twitter/follow/autogluon?style=social)](https://twitter.com/autogluon)
[![Cont"
sqlfluff,"![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)

# The SQL Linter for Humans

[![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
[![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)

[![Coveralls](https://img.shields.io/coverallsCoverage/github/sqlfluff/sqlfluff?logo=coveralls&style=flat-square)](https://coveralls.io/github/sqlfluff/sqlfluff?branch=main)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/sqlfluff/"
CogVideo,"# CogVideo & CogVideoX

[中文阅读](./README_zh.md)

[日本語で読む](./README_ja.md)

<div align=""center"">
<img src=resources/logo.svg width=""50%""/>
</div>
<p align=""center"">
Experience the CogVideoX-5B model online at <a href=""https://huggingface.co/spaces/THUDM/CogVideoX-5B"" target=""_blank""> 🤗 Huggingface Space</a> or <a href=""https://modelscope.cn/studios/ZhipuAI/CogVideoX-5b-demo"" target=""_blank""> 🤖 ModelScope Space</a>
</p>
<p align=""center"">
📚 View the <a href=""https://arxiv.org/abs/2408.06072"" target=""_blank"">paper</a> and <a href=""https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh"" target=""_blank"">user guide</a>
</p>
<p align=""center"">
    👋 Join our <a href=""resources/WECHAT.md"" target=""_blank"">WeChat</a> and <a href=""https://discord.gg/dCGfUsagrD"" target=""_blank"">Discord</a> 
</p>
<p align=""center"">
📍 Visit <a href=""https://chatglm.cn/video?lang=en?fr=osm_cogvideo"">QingYing</a> and <a href=""https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9"">API Platform</a>"
pip-tools,"[![jazzband-image]][jazzband]
[![pypi][pypi-image]][pypi]
[![pyversions][pyversions-image]][pyversions]
[![pre-commit][pre-commit-image]][pre-commit]
[![buildstatus-gha][buildstatus-gha-image]][buildstatus-gha]
[![codecov][codecov-image]][codecov]
[![Matrix Room Badge]][Matrix Room]
[![Matrix Space Badge]][Matrix Space]
[![discord-chat-image]][discord-chat]

# pip-tools = pip-compile + pip-sync

A set of command line tools to help you keep your `pip`-based packages fresh,
even when you've pinned them. You do pin them, right? (In building your Python application and its dependencies for production, you want to make sure that your builds are predictable and deterministic.)

[![pip-tools overview for phase II][pip-tools-overview]][pip-tools-overview]

## Installation

Similar to `pip`, `pip-tools` must be installed in each of your project's
[virtual environments](https://packaging.python.org/tutorials/installing-packages/#creating-virtual-environments):

```console
$ source /path/to/venv/"
google-api-python-client,"# Google API Client

[![PyPI version](https://badge.fury.io/py/google-api-python-client.svg)](https://badge.fury.io/py/google-api-python-client)

This is the [Google API Python client library](https://cloud.google.com/apis/docs/client-libraries-explained#google_api_client_libraries)
for Google's discovery based APIs. To get started, please see the
[docs folder](https://github.com/googleapis/google-api-python-client/blob/main/docs/README.md).

This library is considered complete and is in maintenance mode. This means
that we will address critical bugs and security issues but will not add any
new features.

This library is officially supported by Google.  However, the maintainers of
this repository recommend using [Cloud Client Libraries for Python](https://github.com/googleapis/google-cloud-python),
where possible, for new code development. For more information, please visit
[Client Libraries Explained](https://cloud.google.com/apis/docs/client-libraries-explained).

## Version 2.0 Rele"
TinyLlama,"<div align=""center"">

# TinyLlama-1.1B
English | [中文](README_zh-CN.md)

[Chat Demo](https://huggingface.co/spaces/TinyLlama/tinyllama-chat) | [Discord](https://discord.gg/74Wcx4j5Nb)
</div>

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of ""just"" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 

<div align=""center"">
  <img src="".github/TinyLlama_logo.png"" width=""300""/>
</div>

We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### News
- 2023-12-18： Add two notes [1](https://whimsical-aphid-86d.notion.site/Release-of-TinyLlama-1-5T-Checkpoints-Postponed-01b266998c1c4"
PaLM-rlhf-pytorch,"<img src=""./chatgpt.png"" width=""450px""></img>

*<a href=""https://openai.com/blog/chatgpt/"">official chatgpt blogpost</a>*

## PaLM + RLHF - Pytorch (wip)

Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I'll add retrieval functionality too, à la <a href=""https://github.com/lucidrains/RETRO-pytorch"">RETRO</a>

If you are interested in replicating something like ChatGPT out in the open, please consider joining <a href=""https://discord.gg/xBPBXfcFHd"">Laion <img alt=""Join us on Discord"" src=""https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white""></a>

Potential successor: <a href=""https://arxiv.org/abs/2305.18290"">Direct Preference Optimization</a> - all the code in this repo becomes ~ binary cross entropy loss, < 5 loc. So much for Reward models and PPO

## FAQ

- Does this contain a model for inference?

There is no trained model. This is just the ship and overall map. We still need millions "
faster-rcnn.pytorch,"# A *Faster* Pytorch Implementation of Faster R-CNN

## Write at the beginning

[05/29/2020] This repo was initaited about two years ago, developed as the first open-sourced object detection code which supports multi-gpu training. It has been integrating tremendous efforts from many people. However, we have seen many high-quality repos emerged in the last years, such as:

* [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)
* [detectron2](https://github.com/facebookresearch/detectron2)
* [mmdetection](https://github.com/open-mmlab/mmdetection)

**At this point, I think this repo is out-of-data in terms of the pipeline and coding style, and will not maintain actively. Though you can still use this repo as a playground, I highly recommend you move to the above repos to delve into west world of object detection!**

## Introduction

### :boom: Good news! This repo supports pytorch-1.0 now!!! We borrowed some code and techniques from [maskrcnn-benchmark](https://gi"
SpaceshipGenerator,"# Spaceship Generator

A Blender script to procedurally generate 3D spaceships from a random seed.

![Spaceship screenshots](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/spaceships_grid.jpg)

Usage
-----
* Install Blender 2.80 or greater: http://blender.org/download/
* Download newest `add_mesh_SpaceshipGenerator.zip` from the [Releases](https://github.com/a1studmuffin/SpaceshipGenerator/releases) section
* Under Edit > Preferences... > Add-ons > Install... open the downloaded ZIP file
* Under Edit > Preferences... > Add-ons enable the ""Add Mesh: Spaceship Generator"" script (search for ""spaceship"")
* Add a spaceship in the 3D View under Add > Mesh > Spaceship
* Expand the Spaceship tab that appears in the bottom left of the viewport to adjust procedural generation settings

How it works
------------

![Step-by-step animation](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/step-by-step-animation.gif)

Watch on"
PyMySQL,"[![Documentation Status](https://readthedocs.org/projects/pymysql/badge/?version=latest)](https://pymysql.readthedocs.io/)
[![codecov](https://codecov.io/gh/PyMySQL/PyMySQL/branch/main/graph/badge.svg?token=ppEuaNXBW4)](https://codecov.io/gh/PyMySQL/PyMySQL)

# PyMySQL

This package contains a pure-Python MySQL and MariaDB client library, based on [PEP
249](https://www.python.org/dev/peps/pep-0249/).

## Requirements

- Python -- one of the following:
  - [CPython](https://www.python.org/) : 3.7 and newer
  - [PyPy](https://pypy.org/) : Latest 3.x version
- MySQL Server -- one of the following:
  - [MySQL](https://www.mysql.com/) \>= 5.7
  - [MariaDB](https://mariadb.org/) \>= 10.4

## Installation

Package is uploaded on [PyPI](https://pypi.org/project/PyMySQL).

You can install it with pip:

    $ python3 -m pip install PyMySQL

To use ""sha256_password"" or ""caching_sha2_password"" for authenticate,
you need to install additional dependency:

    $ python3 -m pip install PyMySQL[rsa]

"
GLM-130B,"<img src=""resources/7D6433A42D189E2E6FBC62BE066BCE91.png"">

<p align=""center"">
   🌐 <a href=""http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/"" target=""_blank"">Blog</a> • ⏬ <a href=""https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform"" target=""_blank"">Download Model</a> • 🪧 <a href=""https://huggingface.co/spaces/THUDM/GLM-130B"" target=""_blank"">Demo</a> • ✉️ <a href=""mailto:glm-130b@googlegroups.com"">Email</a> • 📃 <a href=""https://arxiv.org/abs/2210.02414"" target=""_blank"">Paper [ICLR 2023]</a><br>
</p>

<p align=""center"">
   💬 <a href=""https://groups.google.com/g/glm-130b-forum"" target=""_blank"">Google Group</a> (Updates) or <a href=""https://github.com/THUDM/GLM-130B/blob/main/resources/WECHAT.md"" target=""_blank"">Wechat Group</a> or <a href=""https://join.slack.com/t/glm-130b/shared_invite/zt-1f2ih11xy-EAuDComTAr~XVB3MywE9Cg"" target=""_blank"">Slack channel</a> (Discussions)
</p>

# GLM-130B: An Open Bilingual Pre-Trained Model
"
IF,"[![License](https://img.shields.io/badge/Code_License-Modified_MIT-blue.svg)](LICENSE)
[![License](https://img.shields.io/badge/Weights_License-DeepFloyd_IF-orange.svg)](LICENSE-MODEL)
[![Downloads](https://pepy.tech/badge/deepfloyd_if)](https://pepy.tech/project/deepfloyd_if)
[![Discord](https://img.shields.io/badge/Discord-%237289DA.svg?logo=discord&logoColor=white)](https://discord.gg/umz62Mgr)
[![Twitter](https://img.shields.io/badge/Twitter-%231DA1F2.svg?logo=twitter&logoColor=white)](https://twitter.com/deepfloydai)
[![Linktree](https://img.shields.io/badge/Linktree-%2339E09B.svg?logo=linktree&logoColor=white)](http://linktr.ee/deepfloyd)

# IF by [DeepFloyd Lab](https://deepfloyd.ai) at [StabilityAI](https://stability.ai/)

<p align=""center"">
  <img src=""./pics/nabla.jpg"" width=""100%"">
</p>

We introduce DeepFloyd IF, a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. DeepFloyd IF is a modular composed of a fro"
BlenderGIS,"Blender GIS
==========
Blender minimum version required : v2.83

Note : Since 2022, the OpenTopography web service requires an API key. Please register to opentopography.org and request a key. This service is still free.


[Wiki](https://github.com/domlysz/BlenderGIS/wiki/Home) - [FAQ](https://github.com/domlysz/BlenderGIS/wiki/FAQ) - [Quick start guide](https://github.com/domlysz/BlenderGIS/wiki/Quick-start) - [Flowchart](https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/flowchart.jpg)
--------------------

## Functionalities overview

**GIS datafile import :** Import in Blender most commons GIS data format : Shapefile vector, raster image, geotiff DEM, OpenStreetMap xml.

There are a lot of possibilities to create a 3D terrain from geographic data with BlenderGIS, check the [Flowchart](https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/flowchart.jpg) to have an overview.

Exemple : import vector contour lines, create faces by triangulation and put a topographic raster "
CodeGeeX2,"![](resources/codegeex_logo.png)

<p align=""center"">
    🏠 <a href=""https://codegeex.cn"" target=""_blank"">主页</a>｜🛠 插件 <a href=""https://marketplace.visualstudio.com/items?itemName=aminer.codegeex"" target=""_blank"">VS Code</a>, <a href=""https://plugins.jetbrains.com/plugin/20587-codegeex"" target=""_blank"">Jetbrains</a>｜🤗 <a href=""https://huggingface.co/THUDM/codegeex2-6b"" target=""_blank"">模型下载</a>｜📄 <a href=""https://arxiv.org/abs/2303.17568"" target=""_blank"">论文</a>｜👋 加入<a href=""resources/wechat.md""target=""_blank"">微信开发者交流群</a>
</p>

Read this in [English](README_EN.md)<br>
[日本語](README_JA.md)で読む<br>
Lire en [Français](README_FR.md)

⭐️ 最新一代 [CodeGeeX4](https://github.com/THUDM/CodeGeeX4) 模型已经正式开源。
The newest [CodeGeeX4](https://github.com/THUDM/CodeGeeX4) has been released.

# CodeGeeX2: 更强大的多语言代码生成模型

CodeGeeX2 是多语言代码生成模型 [CodeGeeX](https://github.com/THUDM/CodeGeeX) ([KDD’23](https://arxiv.org/abs/2303.17568)) 的第二代模型。不同于一代 CodeGeeX（完全在国产华为昇腾芯片平台训练） ，CodeGeeX2 是基于 [ChatGLM2](https://github.co"
stable-diffusion-webui-forge,"# Stable Diffusion WebUI Forge

Stable Diffusion WebUI Forge is a platform on top of [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) (based on [Gradio](https://www.gradio.app/) <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>) to make development easier, optimize resource management, speed up inference, and study experimental features.

The name ""Forge"" is inspired from ""Minecraft Forge"". This project is aimed at becoming SD WebUI's Forge.

Forge is currently based on SD-WebUI 1.10.1 at [this commit](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/82a973c04367123ae98bd9abdf80d9eda9b910e2). (Because original SD-WebUI is almost static now, Forge will sync with original WebUI every 90 days, or when important fixes.)

# News

2024 Sep 7: New sampler `Flux Realistic` is available now! Recommended scheduler is ""simple"".

# Quick List

[Gradio 4 UI Must Read (TLDR: You need to "
instructor,"# Instructor: Structured LLM Outputs

Instructor is a Python library that makes it a breeze to work with structured outputs from large language models (LLMs). Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses. Get ready to supercharge your LLM workflows!

[![Twitter Follow](https://img.shields.io/twitter/follow/jxnlco?style=social)](https://twitter.com/jxnlco)
[![Discord](https://img.shields.io/discord/1192334452110659664?label=discord)](https://discord.gg/bD9YE9JArw)
[![Downloads](https://img.shields.io/pypi/dm/instructor.svg)](https://pypi.python.org/pypi/instructor)

## Want your logo on our website?

If your company use instructor a lot, we'd love to have your logo on our website! Please fill out [this form](https://q7gjsgfstrp.typeform.com/to/wluQlVVQ)

## Key Features

- **Response Models**: Specify Pydantic models to define the structure of your LLM outputs
- **Retry Management**: Easily conf"
UFO,"<h1 align=""center"">
    <b>UFO</b> <img src=""./assets/ufo_blue.png"" alt=""UFO Image"" width=""40"">: A <b>U</b>I-<b>Fo</b>cused Agent for Windows OS Interaction
</h1>


<div align=""center"">

[![arxiv](https://img.shields.io/badge/Paper-arXiv:202402.07939-b31b1b.svg)](https://arxiv.org/abs/2402.07939)&ensp;
![Python Version](https://img.shields.io/badge/Python-3776AB?&logo=python&logoColor=white-blue&label=3.10%20%7C%203.11)&ensp;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&ensp;
[![Documentation](https://img.shields.io/badge/Documentation-%230ABAB5?style=flat&logo=readthedocs&logoColor=black)](https://microsoft.github.io/UFO/)&ensp;
[![YouTube](https://img.shields.io/badge/YouTube-white?logo=youtube&logoColor=%23FF0000)](https://www.youtube.com/watch?v=QT_OhygMVXU)&ensp;
<!-- [![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/UFO_Agent)](https://twitter.com/intent/follow?screen_name=UFO_Agent) -->
<!-- ![Wel"
axolotl,"# Axolotl

![tests](https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests.yml/badge.svg)
![tests-nightly](https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests-nightly.yml/badge.svg)
![multigpu-semi-weekly tests](https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/multi-gpu-e2e.yml/badge.svg)

Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.

Features:
- Train various Huggingface models such as llama, pythia, falcon, mpt
- Supports fullfinetune, lora, qlora, relora, and gptq
- Customize configurations using a simple yaml file or CLI overwrite
- Load different dataset formats, use custom formats, or bring your own tokenized datasets
- Integrated with xformer, flash attention, [liger kernel](https://github.com/linkedin/Liger-Kernel), rope scaling, and multipacking
- Works with single GPU or multiple GPUs via FSDP or Deepspeed
- Easily run with Docker locally"
moto,"# Moto - Mock AWS Services

[![Join the chat at https://gitter.im/awsmoto/Lobby](https://badges.gitter.im/awsmoto/Lobby.svg)](https://gitter.im/awsmoto/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

[![Build Status](https://github.com/getmoto/moto/workflows/TestNDeploy/badge.svg)](https://github.com/getmoto/moto/actions)
[![Coverage Status](https://codecov.io/gh/getmoto/moto/branch/master/graph/badge.svg)](https://codecov.io/gh/getmoto/moto)
[![Docs](https://readthedocs.org/projects/pip/badge/?version=stable)](http://docs.getmoto.org)
[![PyPI](https://img.shields.io/pypi/v/moto.svg)](https://pypi.org/project/moto/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/moto.svg)](#)
[![PyPI - Downloads](https://img.shields.io/pypi/dw/moto.svg)](https://pypistats.org/packages/moto)
[![Code style: Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/"
SPADE,"[![License CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC4.0-blue.svg)](https://raw.githubusercontent.com/nvlabs/SPADE/master/LICENSE.md)
![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)

# Semantic Image Synthesis with SPADE
![GauGAN demo](https://nvlabs.github.io/SPADE//images/ocean.gif)

# New implementation available at imaginaire repository

We have a reimplementation of the SPADE method that is more performant. It is avaiable at [Imaginaire](https://github.com/NVlabs/imaginaire)

### [Project page](https://nvlabs.github.io/SPADE/) |   [Paper](https://arxiv.org/abs/1903.07291) | [Online Interactive Demo of GauGAN](https://www.nvidia.com/en-us/research/ai-playground/) | [GTC 2019 demo](https://youtu.be/p5U4NgVGAwg) | [Youtube Demo of GauGAN](https://youtu.be/MXWm6w4E5q0)

Semantic Image Synthesis with Spatially-Adaptive Normalization.<br>
[Taesung Park](http://taesung.me/),  [Ming-Yu Liu](http://mingyuliu.net/), [Ting-Chun Wang](https://tcwang0509.github.i"
pwnagotchi,"<p align=""center"">
  <small>Join the project community on our server!</small>
  <br/><br/>
  <a href=""https://discord.gg/https://discord.gg/btZpkp45gQ"" target=""_blank"" title=""Join our community!"">
    <img src=""https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ""/>
  </a>
</p>
<hr/>

<p align=""center"">
    <a href=""https://github.com/evilsocket/pwnagotchi/releases/latest""><img alt=""Release"" src=""https://img.shields.io/github/release/evilsocket/pwnagotchi.svg?style=flat-square""></a>
    <a href=""https://github.com/evilsocket/pwnagotchi/blob/master/LICENSE.md""><img alt=""Software License"" src=""https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square""></a>
    <a href=""https://github.com/evilsocket/pwnagotchi/graphs/contributors""><img alt=""Contributors"" src=""https://img.shields.io/github/contributors/evilsocket/pwnagotchi""/></a>
    <a href=""https://twitter.com/intent/follow?screen_name=pwnagotchi""><img src=""https://img.shields.io/twitter/follow/pwnagotch"
VALL-E-X,"# VALL-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning 🔊
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/qCBRmAnTxg)
<br>
English | [中文](README-ZH.md)
<br>
An open source implementation of Microsoft's [VALL-E X](https://arxiv.org/pdf/2303.03926) zero-shot TTS model.<br>
**We release our trained model to the public for research or application usage.**

![vallex-framework](/images/vallex_framework.jpg ""VALL-E X framework"")

VALL-E X is an amazing multilingual text-to-speech (TTS) model proposed by Microsoft. While Microsoft initially publish in their research paper, they did not release any code or pretrained models. Recognizing the potential and value of this technology, our team took on the challenge to reproduce the results and train our own model. We are glad to share our trained VALL-E X model with the community, allowing everyone to experience the power next-generation TTS! 🎧
<br>
<br>"
gitsome,"<p align=""center"">
  <img src=""http://i.imgur.com/0SXZ90y.gif"">
</p>
<p align=""center"">
  An <a href=""https://github.com/works-with/category/desktop-tools"">Official Integration</a> for GitHub and <a href=""#for-github-enterprise-users"">GitHub Enterprise</a>.
</p>

gitsome
=======

[![Build Status](https://travis-ci.org/donnemartin/gitsome.svg?branch=master)](https://travis-ci.org/donnemartin/gitsome) [![PyPI version](https://badge.fury.io/py/gitsome.svg)](http://badge.fury.io/py/gitsome) [![PyPI](https://img.shields.io/pypi/pyversions/gitsome.svg)](https://pypi.python.org/pypi/gitsome/) [![License](https://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)

## Why `gitsome`?

### The Git Command Line

Although the standard Git command line is a great tool to manage your Git-powered repos, it can be **tough to remember the usage** of:

* 150+ porcelain and plumbing commands
* Countless command-specific options
* Resources such as tags and branches
"
auto-sklearn,"# auto-sklearn

**auto-sklearn** is an automated machine learning toolkit and a drop-in replacement for a [scikit-learn](https://scikit-learn.org) estimator.

Find the documentation **[here](https://automl.github.io/auto-sklearn/)**. Quick links:
  * [Installation Guide](https://automl.github.io/auto-sklearn/master/installation.html)
  * [Releases](https://automl.github.io/auto-sklearn/master/releases.html)
  * [Manual](https://automl.github.io/auto-sklearn/master/manual.html)
  * [Examples](https://automl.github.io/auto-sklearn/master/examples/index.html)
  * [API](https://automl.github.io/auto-sklearn/master/api.html)

## auto-sklearn in one image

![image](doc/images/askl_pipeline.png)

## auto-sklearn in four lines of code

```python
import autosklearn.classification
cls = autosklearn.classification.AutoSklearnClassifier()
cls.fit(X_train, y_train)
predictions = cls.predict(X_test)
```

## Relevant publications

If you use auto-sklearn in scientific publications, we would appreciat"
TikTokDownloader,"<div align=""center"">
<img src=""https://github.com/JoeanAmier/TikTokDownloader/blob/master/static/images/TikTokDownloader.png"" alt=""TikTokDownloader"" height=""256"" width=""256""><br>
<h1>TikTokDownloader</h1>
<img alt=""GitHub"" src=""https://img.shields.io/github/license/JoeanAmier/TikTokDownloader?style=for-the-badge&color=ff6348"">
<img alt=""GitHub forks"" src=""https://img.shields.io/github/forks/JoeanAmier/TikTokDownloader?style=for-the-badge&color=ffa502"">
<img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/JoeanAmier/TikTokDownloader?style=for-the-badge&color=ffee6f"">
<img alt=""GitHub code size in bytes"" src=""https://img.shields.io/github/languages/code-size/JoeanAmier/TikTokDownloader?style=for-the-badge&color=13c2c2"">
<br>
<img alt=""Static Badge"" src=""https://img.shields.io/badge/Python-3.12-3498db?style=for-the-badge&logo=python&labelColor=fffa65"">
<img alt=""GitHub release (with filter)"" src=""https://img.shields.io/github/v/release/JoeanAmier/TikTokDownloader?style=fo"
TrumpScript,"# Final Update
It's been a while since we made any updates to TrumpScript, and we just wanted to make it official that our development on this project has stopped and that we will no longer be accepting issues or pull requests on this repo.

Frankly, this joke isn't funny anymore. Rather than spend your time beating the ""Trump is ridiculous"" meme to death, please actually do something instead and donate to:
* [American Civil Liberties Union](https://www.aclu.org)
* [National Resources Defense Council](https://www.nrdc.org)
* [Planned Parenthood](https://www.plannedparenthood.org)

# TrumpScript <img src=""https://raw.github.com/samshadwell/TrumpScript/master/TrumpScript.jpg"" width=""50px"" height=""50px"" />
Make Python great again

## Mission
TrumpScript is a language based upon the illustrious Donald Trump. As the undeniably best US President, we found that the current field of programming languages does not include any that Trump's glorious golden combover would approve of.

TrumpScript "
DeepCTR,"# DeepCTR

[![Python Versions](https://img.shields.io/pypi/pyversions/deepctr.svg)](https://pypi.org/project/deepctr)
[![TensorFlow Versions](https://img.shields.io/badge/TensorFlow-1.4+/2.0+-blue.svg)](https://pypi.org/project/deepctr)
[![Downloads](https://pepy.tech/badge/deepctr)](https://pepy.tech/project/deepctr)
[![PyPI Version](https://img.shields.io/pypi/v/deepctr.svg)](https://pypi.org/project/deepctr)
[![GitHub Issues](https://img.shields.io/github/issues/shenweichen/deepctr.svg
)](https://github.com/shenweichen/deepctr/issues)
<!-- [![Activity](https://img.shields.io/github/last-commit/shenweichen/deepctr.svg)](https://github.com/shenweichen/DeepCTR/commits/master) -->


[![Documentation Status](https://readthedocs.org/projects/deepctr-doc/badge/?version=latest)](https://deepctr-doc.readthedocs.io/)
![CI status](https://github.com/shenweichen/deepctr/workflows/CI/badge.svg)
[![codecov](https://codecov.io/gh/shenweichen/DeepCTR/branch/master/graph/badge.svg)](https://codecov."
python-dotenv,"# python-dotenv

[![Build Status][build_status_badge]][build_status_link]
[![PyPI version][pypi_badge]][pypi_link]

Python-dotenv reads key-value pairs from a `.env` file and can set them as environment
variables. It helps in the development of applications following the
[12-factor](https://12factor.net/) principles.

- [Getting Started](#getting-started)
- [Other Use Cases](#other-use-cases)
  * [Load configuration without altering the environment](#load-configuration-without-altering-the-environment)
  * [Parse configuration as a stream](#parse-configuration-as-a-stream)
  * [Load .env files in IPython](#load-env-files-in-ipython)
- [Command-line Interface](#command-line-interface)
- [File format](#file-format)
  * [Multiline values](#multiline-values)
  * [Variable expansion](#variable-expansion)
- [Related Projects](#related-projects)
- [Acknowledgements](#acknowledgements)

## Getting Started

```shell
pip install python-dotenv
```

If your application takes its configuration from"
hypothesis,"==========
Hypothesis
==========

Hypothesis is a family of testing libraries which let you write tests parametrized
by a source of examples. A Hypothesis implementation then generates simple and
comprehensible examples that make your tests fail.
This simplifies writing your tests and makes them more powerful at the same time,
by letting software automate the boring bits and do them to a higher standard than a human would,
freeing you to focus on the higher level test logic.

This sort of testing is often called ""property-based testing"",
and the most widely known implementation of the concept is the Haskell
library `QuickCheck <https://hackage.haskell.org/package/QuickCheck>`_,
but Hypothesis differs significantly from QuickCheck and is designed to fit
idiomatically and easily into existing styles of testing that you are used to,
with absolutely no familiarity with Haskell or functional programming needed.

`Hypothesis for Python <hypothesis-python>`_ is the original implementation,
an"
ultisnips,"![Build Status](https://github.com/SirVer/ultisnips/actions/workflows/main.yml/badge.svg)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/SirVer/ultisnips?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

UltiSnips
=========

UltiSnips is the ultimate solution for snippets in Vim. It has many features,
speed being one of them.

![GIF Demo](https://raw.github.com/SirVer/ultisnips/master/doc/demo.gif)

In this demo I am editing a python file. I first expand the `#!` snippet, then
the `class` snippet. The completion menu comes from
[YouCompleteMe](https://github.com/Valloric/YouCompleteMe), UltiSnips also
integrates with [deoplete](https://github.com/Shougo/deoplete.nvim),
[vim-easycomplete](https://github.com/jayli/vim-easycomplete) and more. I can
jump through placeholders and add text while the snippet inserts text in other
places automatically: when I add `Animal` as a base class, `__init__` gets
updated to call the base class constructor. When I add ar"
ajenti,"[![Logo](docs/img/Logo.png)](https://ajenti.org/)

Ajenti is a Linux & BSD modular server admin panel. Ajenti 2 provides a new interface and a better architecture, developed with [Python3](https://www.python.org/) and [AngularJS](https://angularjs.org/).

<p align=""center"">
    <a href=""https://crowdin.net/project/ajenti"">
        <img src=""https://badges.crowdin.net/ajenti/localized.svg"" alt=""Badge Crowdin"" />
    </a>
    <a href=""https://github.com/ajenti/ajenti/graphs/contributors"">
        <img src=""https://img.shields.io/github/contributors/ajenti/ajenti?label=Contributors"" alt=""Badge Contributors"" />
    </a>
    <a href=""https://raw.githubusercontent.com/ajenti/ajenti/master/LICENSE""> 
        <img src=""https://img.shields.io/github/license/ajenti/ajenti?label=License"" alt=""Badge License"" />
    </a>
</p>

----

# Feature highlights

* **Easy installation** : Ajenti 2 can be easy installed [with pip and the provided script](https://docs.ajenti.org/en/latest/man/install.html#ins"
angr,"# angr

[![Latest Release](https://img.shields.io/pypi/v/angr.svg)](https://pypi.python.org/pypi/angr/)
[![Python Version](https://img.shields.io/pypi/pyversions/angr)](https://pypi.python.org/pypi/angr/)
[![PyPI Statistics](https://img.shields.io/pypi/dm/angr.svg)](https://pypistats.org/packages/angr)
[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/angr/angr/blob/master/LICENSE)

angr is a platform-agnostic binary analysis framework.
It is brought to you by [the Computer Security Lab at UC Santa Barbara](https://seclab.cs.ucsb.edu), [SEFCOM at Arizona State University](https://sefcom.asu.edu), their associated CTF team, [Shellphish](https://shellphish.net), the open source community, and **[@rhelmot](https://github.com/rhelmot)**.

## Project Links
Homepage: https://angr.io

Project repository: https://github.com/angr/angr

Documentation: https://docs.angr.io

API Documentation: https://api.angr.io/en/latest/

## What is angr?

angr is a suite of P"
fast-stable-diffusion,"# Shoutout to <a href=""https://www.scenario.com"" target=""_blank""><img src='https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/.github/Scenario.png' width=""170"" height=""40"" style=""vertical-align: middle; margin-bottom: 8px; background-color: black""></a> and <a href=""https://www.paperspace.com"" target=""_blank""><img src='https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/.github/Paperspace.png' width=""170"" height=""40"" style=""vertical-align: middle; margin-bottom: 8px; background-color: black""></a> for sponsoring the project
 
# fast-stable-diffusion Notebooks, A1111 + ComfyUI + DreamBooth
Paperspace adaptations AUTOMATIC1111 Webui, ComfyUI and Dreambooth.
 
<center><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n"
Machine-Learning-Collection,"<p align=""center""><img width=""100%"" src=""ML/others/logo/torch_and_tf.svg"" /></p>

--------------------------------------------------------------------------------


[![Build Status](https://travis-ci.com/aladdinpersson/Machine-Learning-Collection.svg?branch=master)](https://travis-ci.com/aladdinpersson/Machine-Learning-Collection) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[logo]: https://github.com/AladdinPerzon/Machine-Learning-Collection/blob/master/ML/others/logo/youtube_logo.png

# Machine Learning Collection
In this repository you will find tutorials and projects related to Machine Learning. I try to make the code as clear as possible, and the goal is be to used as a learning resource and a way to lookup problems to solve specific problems. For most I have also done video explanations on YouTube if you want a walkthrough for the code. If you got any questions or suggestions for future videos I prefer if you ask it "
universe,"**This repository has been deprecated in favor of the Retro (https://github.com/openai/retro) library. See our Retro Contest (https://blog.openai.com/retro-contest) blog post for detalis.**

universe
***************

`Universe <https://openai.com/blog/universe/>`_ is a software
platform for measuring and training an AI's general intelligence
across the world's supply of games, websites and other
applications. This is the ``universe`` open-source library, which
provides a simple `Gym <https://github.com/openai/gym>`__
interface to each Universe environment.

Universe allows anyone to train and evaluate AI agents on an extremely
wide range of real-time, complex environments.

Universe makes it possible for any existing program to become an
OpenAI Gym environment, without needing special access to the
program's internals, source code, or APIs. It does this by packaging
the program into a Docker container, and presenting the AI with the
same interface a human uses: sending keyboard and mou"
PyTorch_Tutorial,"﻿# Pytorch模型训练实用教程
<img src=""./Data/cover.png"" alt=""Image text"" style=""zoom:33%;"" />

---

📢：《PyTorch实用教程》（第二版）已开源，欢迎阅读：https://tingsongyu.github.io/PyTorch-Tutorial-2nd/

📢：《PyTorch实用教程》（第二版）已开源，欢迎阅读：https://tingsongyu.github.io/PyTorch-Tutorial-2nd/

📢：《PyTorch实用教程》（第二版）已开源，欢迎阅读：https://tingsongyu.github.io/PyTorch-Tutorial-2nd/

第二版新增丰富的**深度学习应用案例**和**推理部署框架**，包括CV、NLP和LLM的十多个实战项目，以及ONNX和TensorRT的教程。

# 1.简介

本代码为教程——《Pytorch模型训练实用教程》中配套代码；<br/>
《Pytorch模型训练实用教程》可通过如下方式获取：<br/>

1. https://github.com/tensor-yu/PyTorch_Tutorial/tree/master/Data<br/>
2. QQ群： 四群：854620826  <br/>


# 2.环境配置
代码在以下两种环境测试过：<br/>
1. win10 64位 + python3.5 + pytorch==0.4.0 <br/>
2. mac + python3.6 + pytorch==0.4.1/ pytorch==1.0.0 <br/>

**第一步 安装各依赖包：**<br/>
pip install -r requirements.txt

**第二步 手动安装pytorch及torchvision：**<br/>
均选择无gpu版本进行安装，进入官网选择相应的指令进行安装
https://pytorch.org/get-started/locally/


# 3.问题反馈
若发现任何问题和改进意见，请您随时联系我。<br/>
联系方式：yts3221@126.com<br/>
读者qq群：

​	一群：671103375 (已满)  <br/>

​	二群：773031536"
GPT2-Chinese,"# GPT2-Chinese

## Description

- Chinese version of GPT2 training code, using BERT tokenizer or BPE tokenizer. It is based on the extremely awesome repository from HuggingFace team [Transformers](https://github.com/huggingface/transformers). Can write poems, news, novels, or train general language models. Support char level, word level and BPE level. Support large training corpus.
- 中文的GPT2训练代码，使用BERT的Tokenizer或Sentencepiece的BPE model（感谢[kangzhonghua](https://github.com/kangzhonghua)的贡献，实现BPE模式需要略微修改train.py的代码）。可以写诗，新闻，小说，或是训练通用语言模型。支持字为单位或是分词模式或是BPE模式（需要略微修改train.py的代码）。支持大语料训练。

## UPDATE 04.11.2024

- 非常感谢各位对本项目的关注。ChatGPT发布以来本项目也重新引起了一些注意。项目本身是我自学Pytorch的练手项目，我也无意做长期的维护更新。如果大家对大模型LLM感兴趣的话，可以邮件我(ned1991@gmail.com)加群沟通，或是在Issue中进行讨论。

## UPDATE 02.06.2021

- 本项目新增了[通用中文GPT-2预训练模型](https://github.com/Morizeyao/GPT2-Chinese#%E6%A8%A1%E5%9E%8B%E5%88%86%E4%BA%AB)、[通用中文GPT-2预训练小模型](https://github.com/Morizeyao/GPT2-Chinese#%E6%A8%A1%E5%9E%8B%E5%88%86%E4%BA%AB)、[中文歌词GPT-2预训练模型](https://g"
fuzzDicts,"# fuzzDicts
Web Pentesting Fuzz 字典,一个就够了。

## log 

不定期更新，使用前建议git pull一下，同步更新。


  **分享字典建议直接提交PR** 

20210608:

* 在rcePayloads字典下添加了一个[Remote Code Execution ( Unix and Windows )](https://ansar0047.medium.com/remote-code-execution-unix-and-windows-4ed3367158b3)中提到的所有Payload。

20201202:

* 在目录字典下更新了一个[Se7en](https://github.com/r00tSe7en)师傅给的admin目录变种。

20200510:

* 用户名字典下新增了一个百家姓top3000的拼音，去重后188条，Attack!!!.


20200420:

* 合并一个由[lanyi1998](https://github.com/lanyi1998)提交的pr，测试常用手机号码top300+，放在用户名字典里面，瓶颈测试时可以试试；添加一份团队Child师傅提供的某集团的弱口令字典。

20200410:

* 新增centOS和AIX主机的/etc/目录的文件列表，放在ssrfDict目录，实战中遇到的，aix和其他系统区别还是蛮大的，作用自己琢磨。

20200406:

* 合并一个由[lewiswu1209](https://github.com/lewiswu1209)提交的pr，密码top19576。


20200221:

* 更新由[makoto56](https://github.com/makoto56)师傅加强后的webshell密码字典,离职学习中，毕业前不会有太多的web测试任务（也不想再继续打web了），字典更新频率会降低很多，如果有小伙伴想一起维护可以联系我啊。

20200211:

* 新增一个lot字典，数据来源于tg群里别人发的50w互联网lot设备弱口令，由[sunu11](https://github.com/sunu11)师傅提取，在此基础上添加了国内的数据。遇到不知名的设备时一阵爆怼咯，擅用字典，事半功倍。

20200115:

* "
diff-match-patch,"The Diff Match and Patch libraries offer robust algorithms to perform the
operations required for synchronizing plain text.

1. Diff:
   * Compare two blocks of plain text and efficiently return a list of differences.
   * [Diff Demo](https://neil.fraser.name/software/diff_match_patch/demos/diff.html)
2. Match:
   * Given a search string, find its best fuzzy match in a block of plain text. Weighted for both accuracy and location.
   * [Match Demo](https://neil.fraser.name/software/diff_match_patch/demos/match.html)
3. Patch:
   * Apply a list of patches onto plain text. Use best-effort to apply patch even when the underlying text doesn't match.
   * [Patch Demo](https://neil.fraser.name/software/diff_match_patch/demos/patch.html)

Originally built in 2006 to power Google Docs, this library is now available in C++, C#, Dart, Java, JavaScript, Lua, Objective C, and Python.

### Reference

* [API](https://github.com/google/diff-match-patch/wiki/API) - Common API across all languages.
* [L"
pwndbg,"![repository-open-graph](https://github.com/pwndbg/pwndbg/assets/150354584/77b2e438-898f-416f-a989-4bef30759627)
# pwndbg

[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://choosealicense.com/licenses/mit/)
[![Unit tests](https://github.com/pwndbg/pwndbg/actions/workflows/tests.yml/badge.svg?branch=dev&event=push)](https://github.com/pwndbg/pwndbg/actions/workflows/tests.yml)
[![codecov.io](https://codecov.io/github/pwndbg/pwndbg/graph/badge.svg?token=i1cBPFVCav)](https://codecov.io/github/pwndbg/pwndbg?branch=dev)
[![Discord](https://img.shields.io/discord/843809097920413717?label=Discord&style=plastic)](https://discord.gg/x47DssnGwm)

`pwndbg` (/paʊnˈdiˌbʌɡ/) is a GDB plug-in that makes debugging with GDB suck less, with a focus on features needed by low-level software developers, hardware hackers, reverse-engineers and exploit developers.

It has a boatload of features, see [FEATURES.md](FEATURES.md) and [CHEATSHEET](https://drive.googl"
FlareSolverr,"# FlareSolverr

[![Latest release](https://img.shields.io/github/v/release/FlareSolverr/FlareSolverr)](https://github.com/FlareSolverr/FlareSolverr/releases)
[![Docker Pulls](https://img.shields.io/docker/pulls/flaresolverr/flaresolverr)](https://hub.docker.com/r/flaresolverr/flaresolverr/)
[![GitHub issues](https://img.shields.io/github/issues/FlareSolverr/FlareSolverr)](https://github.com/FlareSolverr/FlareSolverr/issues)
[![GitHub pull requests](https://img.shields.io/github/issues-pr/FlareSolverr/FlareSolverr)](https://github.com/FlareSolverr/FlareSolverr/pulls)
[![Donate PayPal](https://img.shields.io/badge/Donate-PayPal-yellow.svg)](https://www.paypal.com/paypalme/diegoheras0xff)
[![Donate Bitcoin](https://img.shields.io/badge/Donate-Bitcoin-f7931a.svg)](https://www.blockchain.com/btc/address/13Hcv77AdnFWEUZ9qUpoPBttQsUT7q9TTh)
[![Donate Ethereum](https://img.shields.io/badge/Donate-Ethereum-8c8c8c.svg)](https://www.blockchain.com/eth/address/0x0D1549BbB00926BF3D92c1A8A58"
objection,"# 📱objection - Runtime Mobile Exploration

`objection` is a runtime mobile exploration toolkit, powered by [Frida](https://www.frida.re/), built to help you assess the security posture of your mobile applications, without needing a jailbreak.

[![Twitter](https://img.shields.io/badge/twitter-%40leonjza-blue.svg)](https://twitter.com/leonjza)
[![PyPi](https://badge.fury.io/py/objection.svg)](https://pypi.python.org/pypi/objection)
[![Black Hat Arsenal](https://raw.githubusercontent.com/toolswatch/badges/master/arsenal/europe/2017.svg?sanitize=true)](https://www.blackhat.com/eu-17/arsenal-overview.html)
[![Black Hat Arsenal](https://raw.githubusercontent.com/toolswatch/badges/master/arsenal/usa/2019.svg?sanitize=true)](https://www.blackhat.com/us-19/arsenal-overview.html)

<img align=""right"" src=""./images/objection.png"" height=""220"" alt=""objection"">

- Supports both iOS and Android.
- Inspect and interact with container file systems.
- Bypass SSL pinning.
- Dump keychains.
- Perform memo"
umap,".. -*- mode: rst -*-

.. image:: doc/logo_large.png
  :width: 600
  :alt: UMAP logo
  :align: center

|pypi_version|_ |pypi_downloads|_

|conda_version|_ |conda_downloads|_

|License|_ |build_status|_ |Coverage|_

|Docs|_ |joss_paper|_

.. |pypi_version| image:: https://img.shields.io/pypi/v/umap-learn.svg
.. _pypi_version: https://pypi.python.org/pypi/umap-learn/

.. |pypi_downloads| image:: https://pepy.tech/badge/umap-learn/month
.. _pypi_downloads: https://pepy.tech/project/umap-learn

.. |conda_version| image:: https://anaconda.org/conda-forge/umap-learn/badges/version.svg
.. _conda_version: https://anaconda.org/conda-forge/umap-learn

.. |conda_downloads| image:: https://anaconda.org/conda-forge/umap-learn/badges/downloads.svg
.. _conda_downloads: https://anaconda.org/conda-forge/umap-learn

.. |License| image:: https://img.shields.io/pypi/l/umap-learn.svg
.. _License: https://github.com/lmcinnes/umap/blob/master/LICENSE.txt

.. |build_status| image:: https://dev.azure.com/TutteI"
FastSAM,"![](assets/logo.png)

# Fast Segment Anything

[[`📕Paper`](https://arxiv.org/pdf/2306.12156.pdf)] [[`🤗HuggingFace Demo`](https://huggingface.co/spaces/An-619/FastSAM)] [[`Colab demo`](https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9?usp=sharing)] [[`Replicate demo & API`](https://replicate.com/casia-iva-lab/fastsam)] [~~[`OpenXLab Demo`](https://openxlab.org.cn/apps/detail/zxair/FastSAM)~~] [[`Model Zoo`](#model-checkpoints)] [[`BibTeX`](#citing-fastsam)] [[`Video Demo`](https://youtu.be/yHNPyqazYYU)]

![FastSAM Speed](assets/head_fig.png)

The **Fast Segment Anything Model(FastSAM)** is a CNN Segment Anything Model trained using only 2% of the SA-1B dataset published by SAM authors. FastSAM achieves comparable performance with
the SAM method at **50× higher run-time speed**.

![FastSAM design](assets/Overview.png)

**🍇 Updates**
- **`2024/6/25`** The edge jaggies issue has been slightly improved [#231](https://github.com/CASIA-IVA-Lab/FastSAM/pul"
electrum,"# Electrum - Lightweight Bitcoin client

```
Licence: MIT Licence
Author: Thomas Voegtlin
Language: Python (>= 3.8)
Homepage: https://electrum.org/
```

[![Build Status](https://api.cirrus-ci.com/github/spesmilo/electrum.svg?branch=master)](https://cirrus-ci.com/github/spesmilo/electrum)
[![Test coverage statistics](https://coveralls.io/repos/github/spesmilo/electrum/badge.svg?branch=master)](https://coveralls.io/github/spesmilo/electrum?branch=master)
[![Help translate Electrum online](https://d322cqt584bo4o.cloudfront.net/electrum/localized.svg)](https://crowdin.com/project/electrum)


## Getting started

_(If you've come here looking to simply run Electrum,
[you may download it here](https://electrum.org/#download).)_

Electrum itself is pure Python, and so are most of the required dependencies,
but not everything. The following sections describe how to run from source, but here
is a TL;DR:

```
$ sudo apt-get install libsecp256k1-dev
$ python3 -m pip install --user "".[gui,crypto]""
"
holehe,"# **Holehe OSINT - Email to Registered Accounts**
👋 Hi there! For any professional inquiries or collaborations, please reach out to me at:
megadose@protonmail.com

📧 Preferably, use your professional email for correspondence. Let's keep it short and sweet, and all in English!

![](https://files.catbox.moe/5we2ya.png)
![PyPI](https://img.shields.io/pypi/v/holehe) ![PyPI - Week](https://img.shields.io/pypi/dw/holehe) ![PyPI - Downloads](https://static.pepy.tech/badge/holehe) ![PyPI - License](https://img.shields.io/pypi/l/holehe)

# [Holehe Online Version](https://osint.industries/)

## **Summary**

*Efficiently finding registered accounts from emails.*

Holehe checks if an email is attached to an account on sites like twitter, instagram, imgur and more than 120 others.

+ Retrieves information using the forgotten password function.
+ **[Does not alert the target email.](https://github.com/megadose/holehe/issues/12)**
+ Runs on [Python 3](https://www.python.org/downloads/release/python-3"
edx-platform,"Open edX Platform
#################
| |License: AGPL v3| |Status| |Python CI|

.. |License: AGPL v3| image:: https://img.shields.io/badge/License-AGPL_v3-blue.svg
  :target: https://www.gnu.org/licenses/agpl-3.0

.. |Python CI| image:: https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml/badge.svg
  :target: https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml

.. |Status| image:: https://img.shields.io/badge/status-maintained-31c653

Purpose
*******
The `Open edX Platform <https://openedx.org>`_ is a service-oriented platform for authoring and
delivering online learning at any scale.  The platform is written in
Python and JavaScript and makes extensive use of the Django
framework. At the highest level, the platform is composed of a
monolith, some independently deployable applications (IDAs), and
micro-frontends (MFEs) based on the ReactJS.

This repository hosts the monolith at the center of the Open edX
platform.  Functionally, the edx-platform"
cheatsheets,"# Cheatsheets for Matplotlib users

## Cheatsheets
Cheatsheet [(download pdf)](https://matplotlib.org/cheatsheets/cheatsheets.pdf) | |
:------------------------------------------------------------------------------:|:----------------------------------------------------------:
![](https://matplotlib.org/cheatsheets/cheatsheets-1.png)                       | ![](https://matplotlib.org/cheatsheets/cheatsheets-2.png)

## Handouts

Beginner handout [(download pdf)](https://matplotlib.org/cheatsheets/handout-beginner.pdf) | Intermediate handout [(download pdf)](https://matplotlib.org/cheatsheets/handout-intermediate.pdf) | Tips handout [(download pdf)](https://matplotlib.org/cheatsheets/handout-tips.pdf)
:-----------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------:
![](https://ma"
real-url,"# Real-Url

## 说明

没想到还有这么多朋友发 issue 和邮件咨询问题，感谢大家的支持🎈！因为有时很忙，回复和提交代码的周期会有点长，抱歉哦😋

这个仓库存放的是：获取一些直播平台真实流媒体地址（直播源）和弹幕的 Python 代码实现。获取的地址经测试，均可在 PotPlayer、VLC、DPlayer(flv.js + hls.js)等播放器中播放。

>  🤘👌🤙🙏🐉👉 ：如果该项目能帮助到您，欢迎 star 和 pr；或在您的项目中标注 Real-Url 为参考来源。

目前已实现：

 **59** 个直播平台的直播源获取：斗鱼直播、虎牙直播、哔哩哔哩直播、战旗直播、网易 CC 直播、火猫直播、企鹅电竞、YY 直播、一直播、快手直播、花椒直播、映客直播、西瓜直播、触手直播（已倒闭）、NOW 直播、抖音直播，爱奇艺直播、酷狗直播、龙珠直播、PPS 奇秀直播、六间房、17 直播、来疯直播、优酷轮播台、网易 LOOK 直播、千帆直播、陌陌直播、小米直播、迅雷直播、京东直播、企鹅体育、人人直播、棉花糖直播、九秀直播、羚萌直播、95秀、新浪疯播、红人直播、艾米直播、KK直播、酷我聚星、乐嗨直播、秀色直播、星光直播、我秀直播、热猫直播、艺气山直播、AcFun 直播、猫耳FM、畅秀阁、Twitch、TikTok、央视频、PP体育、zhibotv、腾讯体育直播、爱奇艺体育直播、liveU、bigolive、咪咕视频体育。

 **18** 个直播平台的弹幕获取：斗鱼直播、虎牙直播、哔哩哔哩直播、快手直播、火猫直播、企鹅电竞、花椒直播、映客直播、网易 CC 直播、酷狗直播、龙珠直播、PPS 奇秀、搜狐千帆、战旗直播、来疯直播、网易 LOOK 直播、AcFun 直播、艺气山直播。

## 运行

1. 项目使用了很简单的 Python 代码，仅在 Python 3 环境运行测试。
2. 具体所需模块请查看 requirements.txt
3. 获取斗鱼和爱奇艺的直播源，需 JavaScript 环境，可使用 node.js。爱奇艺直播里有个参数是加盐的 MD5，由仓库中的 iqiyi.js 生成。
4. 每个平台的直播源和弹幕获取功能相互独立，以后再整合。弹幕食用：python main.py

## 反馈

有直播平台失效或新增其他平台解析的，可发 [i"
TensorLayer,"<a href=""https://tensorlayer.readthedocs.io/"">
    <div align=""center"">
        <img src=""img/tl_transparent_logo.png"" width=""50%"" height=""30%""/>
    </div>
</a>

<!--- [![PyPI Version](https://badge.fury.io/py/tensorlayer.svg)](https://badge.fury.io/py/tensorlayer) --->
<!--- ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tensorlayer.svg)) --->

![GitHub last commit (branch)](https://img.shields.io/github/last-commit/tensorlayer/tensorlayer/master.svg)
[![Supported TF Version](https://img.shields.io/badge/TensorFlow-2.0.0%2B-brightgreen.svg)](https://github.com/tensorflow/tensorflow/releases)
[![Documentation Status](https://readthedocs.org/projects/tensorlayer/badge/)](https://tensorlayer.readthedocs.io/)
[![Build Status](https://travis-ci.org/tensorlayer/tensorlayer.svg?branch=master)](https://travis-ci.org/tensorlayer/tensorlayer)
[![Downloads](http://pepy.tech/badge/tensorlayer)](http://pepy.tech/project/tensorlayer)
[![Downloads](https://pepy.tech/badge/tensorlay"
bilingual_book_maker,"**[中文](./README-CN.md) | English**
[![litellm](https://img.shields.io/badge/%20%F0%9F%9A%85%20liteLLM-OpenAI%7CAzure%7CAnthropic%7CPalm%7CCohere%7CReplicate%7CHugging%20Face-blue?color=green)](https://github.com/BerriAI/litellm)

# bilingual_book_maker
The bilingual_book_maker is an AI translation tool that uses ChatGPT to assist users in creating multi-language versions of epub/txt/srt files and books. This tool is exclusively designed for translating epub books that have entered the public domain and is not intended for copyrighted works. Before using this tool, please review the project's **[disclaimer](./disclaimer.md)**.

![image](https://user-images.githubusercontent.com/15976103/222317531-a05317c5-4eee-49de-95cd-04063d9539d9.png)

## Supported Models
gpt-4, gpt-3.5-turbo, claude-2, palm, llama-2, azure-openai, command-nightly, gemini
For using Non-OpenAI models, use class `liteLLM()` - liteLLM supports all models above.
Find more info here for using liteLLM: https://github.com/B"
PyTorch-YOLOv3,"# PyTorch YOLO
A minimal PyTorch implementation of YOLOv3, with support for training, inference and evaluation.

YOLOv4 and YOLOv7 weights are also compatible with this implementation.

[![CI](https://github.com/eriklindernoren/PyTorch-YOLOv3/actions/workflows/main.yml/badge.svg)](https://github.com/eriklindernoren/PyTorch-YOLOv3/actions/workflows/main.yml) [![PyPI pyversions](https://img.shields.io/pypi/pyversions/pytorchyolo.svg)](https://pypi.python.org/pypi/pytorchyolo/) [![PyPI license](https://img.shields.io/pypi/l/pytorchyolo.svg)](LICENSE)

## Installation
### Installing from source

For normal training and evaluation we recommend installing the package from source using a poetry virtual environment.

```bash
git clone https://github.com/eriklindernoren/PyTorch-YOLOv3
cd PyTorch-YOLOv3/
pip3 install poetry --user
poetry install
```

You need to join the virtual environment by running `poetry shell` in this directory before running any of the following commands without the `poet"
generative-models,"# Generative Models
Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.
Also present here are RBM and Helmholtz Machine.

## Note:
Generated samples will be stored in `GAN/{gan_model}/out` (or `VAE/{vae_model}/out`, etc) directory during training.

## What's in it?

#### Generative Adversarial Nets (GAN)
  1. [Vanilla GAN](https://arxiv.org/abs/1406.2661)
  2. [Conditional GAN](https://arxiv.org/abs/1411.1784)
  3. [InfoGAN](https://arxiv.org/abs/1606.03657)
  4. [Wasserstein GAN](https://arxiv.org/abs/1701.07875)
  5. [Mode Regularized GAN](https://arxiv.org/abs/1612.02136)
  6. [Coupled GAN](https://arxiv.org/abs/1606.07536)
  7. [Auxiliary Classifier GAN](https://arxiv.org/abs/1610.09585)
  8. [Least Squares GAN](https://arxiv.org/abs/1611.04076v2)
  9. [Boundary Seeking GAN](https://arxiv.org/abs/1702.08431)
  10. [Energy Based GAN](https://arxiv.org/abs/1609.03126)
  11. [f-GAN](https://arxiv.org/abs/1606.00709)
  12. [Generative Adversarial Parallelization]("
deep-learning-models,"# Trained image classification models for Keras

**THIS REPOSITORY IS DEPRECATED. USE THE MODULE `keras.applications` INSTEAD.**

Pull requests will not be reviewed nor merged. Direct any PRs to `keras.applications`. Issues are not monitored either.

----

This repository contains code for the following Keras models:

- VGG16
- VGG19
- ResNet50
- Inception v3
- CRNN for music tagging

All architectures are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image dimension ordering set in your Keras configuration file at `~/.keras/keras.json`. For instance, if you have set `image_dim_ordering=tf`, then any model loaded from this repository will get built according to the TensorFlow dimension ordering convention, ""Width-Height-Depth"".

Pre-trained weights can be automatically loaded upon instantiation (`weights='imagenet'` argument in model constructor for all image models, `weights='msd'` for the music tagging model). Weights are"
Anti-Anti-Spider,"## 基于CNN的验证码图片识别
### 简介
	本项目采用alexnet模型和letnet模型，可根据实际需要选择(在train_model.py中的train函数修改即可)95.5%
### 作者有话说
	不知不觉这个git库伴随我从16到到20年，带给我自己最棒的一段人生旅程，
	整理了这份文档，希望任何想学习图片识别，玩玩卷积神经网络的同学可以最便捷的上手体验。
	请谨慎使用技术，仅支持学习，不支持任何黑灰产相关
	可参看：https://www.urlteam.cn/?p=1893 https://www.urlteam.cn/?p=1406
	原先的Anti-Anti-Spider 全部内容移动到 原Anti-Anti-Spider 目录下
	有何疑问可邮件 543429245@qq.com 咨询
	模型文件下载 如果出现无法解压，可以使用：
	https://www.urlteam.cn/%E5%8F%AF%E7%94%A8%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%8E%E8%AE%AD%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B.zip

#### Alexnet 模型结构

![](src/READMEIMG2.PNG)

根据验证码的复杂度不同，训练的时间也会有较大的不同
![](src/READMEIMG1.PNG)

###  使用方法
	1.开始训练样本前，修改conf/config.json
	2.将预处理过的数据集分成验证集和训练集，放到sample目录下
	3.运行train_model.py开始训练，训练完成的模型保存至model_result中
	4.将训练好的模型放置model_result，运行cnn_models/recognition.py，选定验证码，即可看到模型效果
### 环境配置
TensorFlow CPU版本安装：`pip install tensorflow==1.9.0`
TensorFlow GPU版本安装：`pip install tensorflow-gpu==1.9.0`
GUP版本的安装比较麻烦，需要安装CUDA和cuDNN才能使tensorflow调动GPU
下图为TensorFlow，Python，CUDA与cuDNN之间的版本对应关系：
"
clone-voice,"[English README](./README_EN.md)  / [捐助项目](https://github.com/jianchang512/pyvideotrans/issues/80) / [Discord](https://discord.gg/7ZWbwKGMcx)

# CV声音克隆工具

> 本项目所用模型为[coqui.ai](https://coqui.ai/)出品的xtts_v2，模型开源协议为[Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt),使用本项目请遵循该协议，协议全文见 https://coqui.ai/cpml.txt


 这是一个声音克隆工具，可使用任何人类音色，将一段文字合成为使用该音色说话的声音，或者将一个声音使用该音色转换为另一个声音。
 
 使用非常简单，没有N卡GPU也可以使用，下载预编译版本，双击 app.exe 打开一个web界面，鼠标点点就能用。
 
 支持 **中、英、日、韩、法、德、意等16种语言**，可在线从麦克风录制声音。
 
 为保证合成效果，建议录制时长5秒到20秒，发音清晰准确，不要存在背景噪声。
 
 英文效果很棒，中文效果还凑合。


> **[赞助商]**
> 
> [![](https://github.com/user-attachments/assets/e3e2e6f9-e2e4-44e4-860b-9d1ce5b53d4f)](https://302.ai/)
>  [302.AI](https://302.ai)是一个汇集全球顶级品牌的AI超市，按需付费，零月费，零门槛使用各种类型AI。
> 
> 功能全面: 将最好用的AI集成到在平台之上，包括不限于AI聊天，图片生成，图片处理，视频生成，全方位覆盖。
> 
> 简单易用: 提供机器人，工具和API多种使用方法，可以满足从小白到开发者多种角色的需求。
> 
> 按需付费零门槛: 不提供月付套餐，对产品不设任何门槛，按需付费，全部开放。充值余额永久有效。
> 
> 管理者和使用者分离： 管理者一键分享，使用者无需登录。


# 视频演示


https://github.com/jianchang512/clone-voice/assets/3378335/"
starcoder,"# 💫 StarCoder

[Paper](https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view) | [Model](https://huggingface.co/bigcode/starcoder) | [Playground](https://huggingface.co/spaces/bigcode/bigcode-playground) | [VSCode](https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode) | [Chat](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)

# What is this about?
💫 StarCoder is a language model (LM) trained on source code and natural language text. Its training data incorporates more that 80 different programming languages as well as text extracted from GitHub issues and commits and from notebooks. This repository showcases how we get an overview of this LM's capabilities.

# News

* **May 9, 2023:** We've fine-tuned StarCoder to act as a helpful coding assistant 💬! Check out the `chat/` directory for the training code and play with the model [here](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground).

# Disclaimer

Before you "
EmotiVoice,"<div align=""center"">
<a href=""https://trendshift.io/repositories/4833"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/4833"" alt=""netease-youdao%2FEmotiVoice | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

<font size=4> README: EN | <a href=""./README.zh.md"">中文</a>  </font>
    <h1>EmotiVoice 😊: a Multi-Voice and Prompt-Controlled TTS Engine</h1>
</div>

<div align=""center"">
    <a href=""./README.zh.md""><img src=""https://img.shields.io/badge/README-中文版本-red""></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache--2.0-yellow""></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href=""https://twitter.com/YDopensource""><img src=""https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}""></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
</div>
<br>

**EmotiVoice** is a powerful and modern open-source text-to-speech engine that is available to you at no cost. EmotiVoice speaks bot"
CenterNet,"# Objects as Points
Object detection, 3D detection, and pose estimation using center point detection:
![](readme/fig2.png)
> [**Objects as Points**](http://arxiv.org/abs/1904.07850),            
> Xingyi Zhou, Dequan Wang, Philipp Kr&auml;henb&uuml;hl,        
> *arXiv technical report ([arXiv 1904.07850](http://arxiv.org/abs/1904.07850))*         


Contact: [zhouxy2017@gmail.com](mailto:zhouxy2017@gmail.com). Any questions or discussions are welcomed! 

## Updates

 - (June, 2020) We released a state-of-the-art Lidar-based 3D detection and tracking framework [CenterPoint](https://github.com/tianweiy/CenterPoint).
 - (April, 2020) We released a state-of-the-art (multi-category-/ pose-/ 3d-) tracking extension [CenterTrack](https://github.com/xingyizhou/CenterTrack).

## Abstract 

Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, ineffici"
programming-talks,"# Programming Talks

I watch a lot of talks that I love to share with my friends, fellows and coworkers.
As I consider all GitHubbers my friends (oh yeah!), I decided it's time to share the
list.

There are talks on programming language specifics as well as a more general section I call ""theory"".
But don't expect to always get theoretical computer science for every talk there;
most of them are on the architecture and design of software.

I welcome every contribution to the list; for guidelines look [below](#contributing).

**Disclaimer:** I did not give any of the talks on the list and am responsible neither
for their content nor for their presentation. All links below will direct you to
external sites (mostly YouTube, really), be aware of that. If you are one of the people
responsible for the talks or the platform presenting it and want it removed,
tell me and I'll sort it out with you.

**[A]** after a talk name denotes a talk that *someone* thought could be listened to as audio, wit"
stanza,"<div align=""center""><img src=""https://github.com/stanfordnlp/stanza/raw/dev/images/stanza-logo.png"" height=""100px""/></div>

<h2 align=""center"">Stanza: A Python NLP Library for Many Human Languages</h2>

<div align=""center"">
    <a href=""https://github.com/stanfordnlp/stanza/actions"">
       <img alt=""Run Tests"" src=""https://github.com/stanfordnlp/stanza/actions/workflows/stanza-tests.yaml/badge.svg"">
    </a>
    <a href=""https://pypi.org/project/stanza/"">
        <img alt=""PyPI Version"" src=""https://img.shields.io/pypi/v/stanza?color=blue"">
    </a>
    <a href=""https://anaconda.org/stanfordnlp/stanza"">
        <img alt=""Conda Versions"" src=""https://img.shields.io/conda/vn/stanfordnlp/stanza?color=blue&label=conda"">
    </a>
    <a href=""https://pypi.org/project/stanza/"">
        <img alt=""Python Versions"" src=""https://img.shields.io/pypi/pyversions/stanza?colorB=blue"">
    </a>
</div>

The Stanford NLP Group's official Python NLP library. It contains support for running various accur"
Omost,"# Omost

Omost is a project to convert LLM's coding capability to image generation (or more accurately, image composing) capability. 

The name `Omost` (pronunciation: almost) has two meanings: 1) everytime after you use Omost, your image is almost there; 2) the `O` mean ""omni"" (multi-modal) and `most` means we want to get the most out of it.

Omost provides LLMs models that will write codes to compose image visual contents with Omost's virtual `Canvas` agent. This `Canvas` can be rendered by specific implementations of image generators to actually generate images.

Currently, we provide 3 pretrained LLM models based on variations of Llama3 and Phi3 (see also the model notes at the end of this page).

All models are trained with mixed data of (1) ground-truth annotations of several datasets including Open-Images, (2) extracted data by automatically annotating images, (3) reinforcement from DPO (Direct Preference Optimization, ""whether the codes can be compiled by python 3.10 or not"" as"
featuretools,"<p align=""center"">
<img width=50% src=""https://www.featuretools.com/wp-content/uploads/2017/12/FeatureLabs-Logo-Tangerine-800.png"" alt=""Featuretools"" />
</p>
<p align=""center"">
<i>""One of the holy grails of machine learning is to automate more and more of the feature engineering process.""</i> ― Pedro Domingos, <a href=""https://bit.ly/things_to_know_ml"">A Few Useful Things to Know about Machine Learning</a>
</p>

<p align=""center"">
    <a href=""https://github.com/alteryx/featuretools/actions/workflows/tests_with_latest_deps.yaml"" alt=""Tests"" target=""_blank"">
        <img src=""https://github.com/alteryx/featuretools/actions/workflows/tests_with_latest_deps.yaml/badge.svg?branch=main"" alt=""Tests"" />
    </a>
    <a href=""https://codecov.io/gh/alteryx/featuretools"">
        <img src=""https://codecov.io/gh/alteryx/featuretools/branch/main/graph/badge.svg""/>
    </a>
    <a href='https://featuretools.alteryx.com/en/stable/?badge=stable'>
        <img src='https://readthedocs.com/projects/fea"
WeixinBot,"# WeixinBot [![star this repo](http://github-svg-buttons.herokuapp.com/star.svg?user=Urinx&repo=WeixinBot&style=flat&background=1081C1)](http://github.com/Urinx/WeixinBot) [![fork this repo](http://github-svg-buttons.herokuapp.com/fork.svg?user=Urinx&repo=WeixinBot&style=flat&background=1081C1)](http://github.com/Urinx/WeixinBot/fork) ![python](https://img.shields.io/badge/python-2.7%20&%203.6-ff69b4.svg)

网页版微信API，包含终端版微信及微信机器人

## Contents
* [Demo](#Demo)
* [Web Weixin Pipeline](#Web-Weixin-Pipeline)
* [Web Weixin API](#Web-Weixin-API)
* [Discussion Group](#Discussion-Group)
* [Recent Update](#Recent-Update)

## <a name=""Demo"">Demo</a>
为了确保能正常运行示例脚本，请安装所需的第三方包。

```
pip install -r requirements.txt
```

注：下面演示的图片与功能可能不是最新的，具体请看源码。

<div align=center>
<img src=""imgs/1.png"" width=""500"" height=""550""/>
</div>

按照操作指示在手机微信上扫描二维码然后登录，你可以选择是否开启自动回复模式。

![2](imgs/2.png)

开启自动回复模式后，如果接收到的是文字消息就会自动回复，包括群消息。

![3](imgs/3.png)

名片，链接，动画表情和地址位置消息。

![4](imgs/4.png)

![5](imgs/5.png)

网页版上有的功能目前基本上"
hyperopt,"
# Hyperopt: Distributed Hyperparameter Optimization

<p align=""center"">
<img src=""https://i.postimg.cc/TPmffWrp/hyperopt-new.png"" />
</p>

[![build](https://github.com/hyperopt/hyperopt/actions/workflows/build.yml/badge.svg)](https://github.com/hyperopt/hyperopt/actions/workflows/build.yml)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/hyperopt/hyperopt/master.svg)](https://results.pre-commit.ci/latest/github/hyperopt/hyperopt/master)
[![PyPI version](https://badge.fury.io/py/hyperopt.svg)](https://badge.fury.io/py/hyperopt)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/hyperopt/badges/version.svg)](https://anaconda.org/conda-forge/hyperopt)

[Hyperopt](https://github.com/hyperopt/hyperopt) is a Python library for serial and parallel optimization over awkward
search spaces, which may include real-valued, discrete, and conditional
dimensions.

## Getting started

Install hyperopt from PyPI

```bash
pip install hyperopt
```

to run your first example

"
volatility,"============================================================================
Volatility Framework - Volatile memory extraction utility framework
============================================================================

The Volatility Framework is a completely open collection of tools,
implemented in Python under the GNU General Public License, for the
extraction of digital artifacts from volatile memory (RAM) samples.
The extraction techniques are performed completely independent of the
system being investigated but offer visibilty into the runtime state
of the system. The framework is intended to introduce people to the
techniques and complexities associated with extracting digital artifacts
from volatile memory samples and provide a platform for further work into
this exciting area of research.

The Volatility distribution is available from: 
http://www.volatilityfoundation.org/#!releases/component_71401

Volatility should run on any platform that supports 
Python (http://www.pyt"
lbry-sdk,"# <img src=""https://raw.githubusercontent.com/lbryio/lbry-sdk/master/lbry.png"" alt=""LBRY"" width=""48"" height=""36"" /> LBRY SDK [![build](https://github.com/lbryio/lbry-sdk/actions/workflows/main.yml/badge.svg)](https://github.com/lbryio/lbry-sdk/actions/workflows/main.yml) [![coverage](https://coveralls.io/repos/github/lbryio/lbry-sdk/badge.svg)](https://coveralls.io/github/lbryio/lbry-sdk)

LBRY is a decentralized peer-to-peer protocol for publishing and accessing digital content. It utilizes the [LBRY blockchain](https://github.com/lbryio/lbrycrd) as a global namespace and database of digital content. Blockchain entries contain searchable content metadata, identities, rights and access rules. LBRY also provides a data network that consists of peers (seeders) uploading and downloading data from other peers, possibly in exchange for payments, as well as a distributed hash table used by peers to discover other peers.

LBRY SDK for Python is currently the most fully featured implementation"
aws-shell,"aws-shell - The interactive productivity booster for the AWS CLI
================================================================

.. image:: https://aws-developer-blog-media.s3-us-west-2.amazonaws.com/cli/Super-Charge-Your-AWS-Command-Line-Experience-with-aws-shell/aws-shell-final.gif


Installation
============

The aws-shell requires python and `pip`_ to install.
You can install the aws-shell using `pip`_::

    $ pip install aws-shell

If you are not installing into a virtualenv you can run::

    $ sudo pip install aws-shell

**Mac OS X (10.11 El Capitan) users**: There is a known issue with Apple and
its included python package dependencies (more info at
https://github.com/pypa/pip/issues/3165).
We are investigating ways to fix this issue but in the meantime,
to install the aws-shell, you can run:
``sudo pip install aws-shell --upgrade --ignore-installed six``

Once you've installed the aws-shell, you can now run::

    $ aws-shell

To exit the shell, press ``Ctrl-D``.

Upgrading"
mae,"## Masked Autoencoders: A PyTorch Implementation

<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png"" width=""480"">
</p>


This is a PyTorch/GPU re-implementation of the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377):
```
@Article{MaskedAutoencoders2021,
  author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll{\'a}r and Ross Girshick},
  journal = {arXiv:2111.06377},
  title   = {Masked Autoencoders Are Scalable Vision Learners},
  year    = {2021},
}
```

* The original implementation was in TensorFlow+TPU. This re-implementation is in PyTorch+GPU.

* This repo is a modification on the [DeiT repo](https://github.com/facebookresearch/deit). Installation and preparation follow that repo.

* This repo is based on [`timm==0.3.2`](https://github.com/rwightman/pytorch-image-models), for which a [fix](https://github.com/rwightman/pytorch-im"
awesome-cheatsheet,"<img src=""https://cdn.rawgit.com/detailyang/awesome-cheatsheet/master/awesome.svg"" alt=""awesome"" width=""120"" align=""right"" >

# Awesome Cheatsheet 

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) ![Branch master](https://img.shields.io/badge/branch-master-brightgreen.svg?style=flat-square) [![Build Status](https://api.travis-ci.org/detailyang/awesome-cheatsheet.svg)](https://travis-ci.org/detailyang/awesome-cheatsheet)    [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/detailyang/awesome-cheatsheet/master/LICENSE)
> List of useful cheatsheets

Inspired by [@sindresorhus](https://github.com/sindresorhus) [awesome](https://github.com/sindresorhus/awesome) and improved by these **[amazing contributors](https://github.com/detailyang/awesome-cheatsheet/graphs/contributors)**.

#### *If you see a link here is not fit, you can fi"
latexify_py,"# latexify

[![Python](https://img.shields.io/pypi/pyversions/latexify-py.svg)](https://pypi.org/project/latexify-py/)
[![PyPI Latest Release](https://img.shields.io/pypi/v/latexify-py.svg)](https://pypi.org/project/latexify-py/)
[![License](https://img.shields.io/pypi/l/latexify-py.svg)](https://github.com/google/latexify_py/blob/main/LICENSE)
[![Downloads](https://pepy.tech/badge/latexify-py/month)](https://pepy.tech/project/latexify-py)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)

`latexify` is a Python package to compile a fragment of Python source code to a
corresponding $\LaTeX$ expression:

![Example of latexify usage](https://raw.githubusercontent.com/google/latexify_py/main/example.jpg)

`latexify` provides the following functionalities:

* Libraries to compile Python sourc"
records,"# Records: SQL for Humans™

[![image](https://img.shields.io/pypi/v/records.svg)](https://pypi.python.org/pypi/records)

**Records is a very simple, but powerful, library for making raw SQL
queries to most relational databases.**

![image](https://farm1.staticflickr.com/569/33085227621_7e8da49b90_k_d.jpg)

Just write SQL. No bells, no whistles. This common task can be
surprisingly difficult with the standard tools available. This library
strives to make this workflow as simple as possible, while providing an
elegant interface to work with your query results.

*Database support includes RedShift, Postgres, MySQL, SQLite, Oracle,
and MS-SQL (drivers not included).*

## ☤ The Basics

We know how to write SQL, so let's send some to our database:

``` python
import records

db = records.Database('postgres://...')
rows = db.query('select * from active_users')    # or db.query_file('sqls/active-users.sql')
```

Grab one row at a time:

``` python
>>> rows[0]
<Record {""username"": ""model-t"", ""a"
Ultra-Light-Fast-Generic-Face-Detector-1MB,"[English](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB ) | [中文简体](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/README_CN.md)
# Ultra-Light-Fast-Generic-Face-Detector-1MB 
# Ultra-lightweight face detection model
![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/27.jpg)
This model is a lightweight facedetection model designed for edge computing devices.

- In terms of model size, the default FP32 precision (.pth) file size is **1.04~1.1MB**, and the inference framework int8 quantization size is about **300KB**.
- In terms of the calculation amount of the model, the input resolution of 320x240 is about **90~109 MFlops**.
- There are two versions of the model, version-slim (network backbone simplification,slightly faster) and version-RFB (with the modified RFB module, higher precision).
- Widerface training pre-training model with different input resolutions of 320x240 and 640"
keras-yolo3,"# keras-yolo3

[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](LICENSE)

## Introduction

A Keras implementation of YOLOv3 (Tensorflow backend) inspired by [allanzelener/YAD2K](https://github.com/allanzelener/YAD2K).


---

## Quick Start

1. Download YOLOv3 weights from [YOLO website](http://pjreddie.com/darknet/yolo/).
2. Convert the Darknet YOLO model to a Keras model.
3. Run YOLO detection.

```
wget https://pjreddie.com/media/files/yolov3.weights
python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5
python yolo_video.py [OPTIONS...] --image, for image detection mode, OR
python yolo_video.py [video_path] [output_path (optional)]
```

For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with `--model model_file` and `--anchors anchor_file`.

### Usage
Use --help to see usage of yolo_video.py:
```
usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]
                     [--classes CLASSES] [--gpu_num GPU_NUM] ["
MachineLearning_Python,"机器学习算法Python实现
=========

[![MIT license](https://img.shields.io/dub/l/vibe-d.svg)](https://github.com/lawlite19/MachineLearning_Python/blob/master/LICENSE)

## 目录
* [机器学习算法Python实现](#机器学习算法python实现)
	* [一、线性回归](#一线性回归)
		* [1、代价函数](#1代价函数)
		* [2、梯度下降算法](#2梯度下降算法)
		* [3、均值归一化](#3均值归一化)
		* [4、最终运行结果](#4最终运行结果)
		* [5、使用scikit-learn库中的线性模型实现](#5使用scikit-learn库中的线性模型实现)
	* [二、逻辑回归](#二逻辑回归)
		* [1、代价函数](#1代价函数)
		* [2、梯度](#2梯度)
		* [3、正则化](#3正则化)
		* [4、S型函数（即）](#4s型函数即)
		* [5、映射为多项式](#5映射为多项式)
		* [6、使用的优化方法](#6使用scipy的优化方法)
		* [7、运行结果](#7运行结果)
		* [8、使用scikit-learn库中的逻辑回归模型实现](#8使用scikit-learn库中的逻辑回归模型实现)
	* [逻辑回归_手写数字识别_OneVsAll](#逻辑回归_手写数字识别_onevsall)
		* [1、随机显示100个数字](#1随机显示100个数字)
		* [2、OneVsAll](#2onevsall)
		* [3、手写数字识别](#3手写数字识别)
		* [4、预测](#4预测)
		* [5、运行结果](#5运行结果)
		* [6、使用scikit-learn库中的逻辑回归模型实现](#6使用scikit-learn库中的逻辑回归模型实现)
	* [三、BP神经网络](#三bp神经网络)
		* [1、神经网络model](#1神经网络model)
		* [2、代价函数](#2代价函数)
		* [3、正则化](#3正则化)
		* [4、反向传播BP](#4反向传播bp)
		* [5、BP可以求梯度的原因](#5bp可以求梯"
bup,"bup: It backs things up
=======================

bup is a program that backs things up.  It's short for ""backup."" Can you
believe that nobody else has named an open source program ""bup"" after all
this time?  Me neither.

Despite its unassuming name, bup is pretty cool.  To give you an idea of
just how cool it is, I wrote you this poem:

                             Bup is teh awesome
                          What rhymes with awesome?
                            I guess maybe possum
                           But that's irrelevant.
			
Hmm.  Did that help?  Maybe prose is more useful after all.


Reasons bup is awesome
----------------------

bup has a few advantages over other backup software:

 - It uses a rolling checksum algorithm (similar to rsync) to split large
   files into chunks.  The most useful result of this is you can backup huge
   virtual machine (VM) disk images, databases, and XML files incrementally,
   even though they're typically all in one huge file, and not use "
GPTCache,"# GPTCache : A Library for Creating Semantic Cache for LLM Queries
Slash Your LLM API Costs by 10x 💰, Boost Speed by 100x ⚡ 

[![Release](https://img.shields.io/pypi/v/gptcache?label=Release&color&logo=Python)](https://pypi.org/project/gptcache/)
[![pip download](https://img.shields.io/pypi/dm/gptcache.svg?color=bright-green&logo=Pypi)](https://pypi.org/project/gptcache/)
[![Codecov](https://img.shields.io/codecov/c/github/zilliztech/GPTCache/dev?label=Codecov&logo=codecov&token=E30WxqBeJJ)](https://codecov.io/gh/zilliztech/GPTCache)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/license/mit/)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/zilliz_universe.svg?style=social&label=Follow%20%40Zilliz)](https://twitter.com/zilliz_universe)
[![Discord](https://img.shields.io/discord/1092648432495251507?label=Discord&logo=discord)](https://discord.gg/Q8C6WEjSWV)

🎉 GPTCache has been fully integrated with 🦜️🔗[LangChain](https://github.c"
linux-insides-zh,"# Linux 内核揭秘

一系列关于 Linux 内核和其内在机理的帖子。

**目的很简单** - 分享我对 Linux 内核机理的一些浅见，帮助读者理解 Linux 内核机理和其他底层内容。

**问题/建议**: 如有相关问题或建议，请提交 issue。一方面，对于英文原文问题，请在上游仓库 - [linux-insides](https://github.com/0xAX/linux-insides) 中提交 issue；另一方面，对于中文翻译问题，请在下游仓库 - [linux-insides-zh](https://github.com/hust-open-atom-club/linux-insides-zh) 中提交 issue。

## 贡献

如有相关问题或建议，请不吝指教，提交 issues 或者 PRs。对于 `linux-insides-zh` 翻译项目，请通过以下方法进行贡献：

- 英文翻译，目前只提供简体中文的译文；
- 同步未被翻译的英文原本，其实就是将上游英文同步到本项目中；
- 更新已经翻译的中文译文，其实就是查看上游英文的更新，检查是否需要对中文译文进行更新；
- 校对当前已经翻译过的中文译文，包括修改错别字，润色等工作；

目前本项目的**翻译进度**与**翻译认领规则**，请查看 [TRANSLATION_STATUS.md](TRANSLATION_STATUS.md)。

在开始翻译之前，请阅读 [CONTRIBUTING.md](CONTRIBUTING.md) 与 [TRANSLATION_NOTES.md](TRANSLATION_NOTES.md)。关于翻译约定的任何问题或建议，同样请提交 issue 讨论。

## 邮件列表

我们开源俱乐部内部有一个[ Google Group 邮件列表](https://groups.google.com/g/hust-os-kernel-patches)来学习和贡献 Linux 内核源码。

**加入邮件列表** 发送任意主题/内容的邮件到 hust-os-kernel-patches+subscribe@googlegroups.com。随后，你将获得一封确认邮件，并加入邮件列表。如果你有谷歌账号，你可以通过上述网址直接加入我们邮件列表。

## 中文维护者

[@m"
Movie_Data_Capture,"<h1 align=""center"">Movie Data Capture</h1>

## 网站
[www.mvdc.top](https://www.mvdc.top)

## 核心代码开源
"
WeasyPrint,"**The Awesome Document Factory**

WeasyPrint is a smart solution helping web developers to create PDF
documents. It turns simple HTML pages into gorgeous statistical reports,
invoices, tickets…

From a technical point of view, WeasyPrint is a visual rendering engine for
HTML and CSS that can export to PDF. It aims to support web standards for
printing. WeasyPrint is free software made available under a BSD license.

It is based on various libraries but *not* on a full rendering engine like
WebKit or Gecko. The CSS layout engine is written in Python, designed for
pagination, and meant to be easy to hack on.

* Free software: BSD license
* For Python 3.9+, tested on CPython and PyPy
* Documentation: https://doc.courtbouillon.org/weasyprint
* Examples: https://weasyprint.org/#samples
* Changelog: https://github.com/Kozea/WeasyPrint/releases
* Code, issues, tests: https://github.com/Kozea/WeasyPrint
* Code of conduct: https://www.courtbouillon.org/code-of-conduct
* Professional support: ht"
LWM,"# Large World Model (LWM)

[[Project]](https://largeworldmodel.github.io/)
[[Paper]](https://arxiv.org/abs/2402.08268)
[[Models]](https://huggingface.co/LargeWorldModel)

**Large World Model (LWM)** is a general-purpose large-context multimodal autoregressive model. It is trained on a large dataset of diverse long videos and books using RingAttention, and can perform language, image, and video understanding and generation.


## Approach

<div align=""center"">
  <img src=""./imgs/data.png""/>
</div>

Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video"
gcn,"# Graph Convolutional Networks

This is a TensorFlow implementation of Graph Convolutional Networks for the task of (semi-supervised) classification of nodes in a graph, as described in our paper:
 
Thomas N. Kipf, Max Welling, [Semi-Supervised Classification with Graph Convolutional Networks](http://arxiv.org/abs/1609.02907) (ICLR 2017)

For a high-level explanation, have a look at our blog post:

Thomas Kipf, [Graph Convolutional Networks](http://tkipf.github.io/graph-convolutional-networks/) (2016)

## Installation

```bash
python setup.py install
```

## Requirements
* tensorflow (>0.12)
* networkx

## Run the demo

```bash
cd gcn
python train.py
```

## Data

In order to use your own data, you have to provide 
* an N by N adjacency matrix (N is the number of nodes), 
* an N by D feature matrix (D is the number of features per node), and
* an N by E binary label matrix (E is the number of classes).

Have a look at the `load_data()` function in `utils.py` for an example.

In this ex"
exo,"<div align=""center"">

<picture>
  <source media=""(prefers-color-scheme: light)"" srcset=""/docs/exo-logo-black-bg.jpg"">
  <img alt=""exo logo"" src=""/docs/exo-logo-transparent.png"" width=""50%"" height=""50%"">
</picture>

exo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).


<h3>

[Discord](https://discord.gg/EUnjGpsmWw) | [Telegram](https://t.me/+Kh-KqHTzFYg3MGNk) | [X](https://x.com/exolabs)

</h3>

[![GitHub Repo stars](https://img.shields.io/github/stars/exo-explore/exo)](https://github.com/exo-explore/exo/stargazers)
[![Tests](https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)

</div>

---

Forget expensive NVIDIA GPUs, unify your existing de"
ipwndfu,"![](repo/ipwndfu.png)
# Open-source jailbreaking tool for many iOS devices


**Read [disclaimer](#disclaimer) before using this software.*


## checkm8

* permanent unpatchable bootrom exploit for hundreds of millions of iOS devices

* meant for researchers, this is not a jailbreak with Cydia yet

* allows dumping SecureROM, decrypting keybags for iOS firmware, and demoting device for JTAG

* current SoC support: s5l8947x, s5l8950x, s5l8955x, s5l8960x, t8002, t8004, t8010, t8011, t8015

* future SoC support: s5l8940x, s5l8942x, s5l8945x, s5l8747x, t7000, t7001, s7002, s8000, s8001, s8003, t8012

* full jailbreak with Cydia on latest iOS version is possible, but requires additional work


## Quick start guide for checkm8

1. Use a cable to connect device to your Mac. Hold buttons as needed to enter DFU Mode.

2. First run ```./ipwndfu -p``` to exploit the device. Repeat the process if it fails, it is not reliable.

3. Run ```./ipwndfu --dump-rom``` to get a dump of SecureROM.

4. Run ``"
Chinese-LLaMA-Alpaca-2,"# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)项目启动！

[**🇨🇳中文**](./README.md) | [**🌐English**](./README_EN.md) | [**📖文档/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki) | [**❓提问/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/issues) | [**💬讨论/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/discussions) | [**⚔️竞技场/Arena**](http://llm-arena.ymcui.com/)

<p align=""center"">
    <br>
    <img src=""./pics/banner.png"" width=""800""/>
    <br>
</p>
<p align=""center"">
    <img alt=""GitHub"" src=""https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca-2.svg?color=blue&style=flat-square"">
    <img alt=""GitHub release (latest by date)"" src=""https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca-2"">
    <img alt=""GitHub top language"" src=""https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca-2"">
    <a href=""https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca-2/dashboard?utm_source=gh&utm_medium=re"
yowsup,"# yowsup [![Build Status](https://travis-ci.org/tgalal/yowsup.svg?branch=master)](https://travis-ci.org/tgalal/yowsup)

<a href=""https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=Z9KKEUVYEY6BN"" target=""_blank""><img src=""https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif"" /></a>

**For private consultancy feel free to directly schedule it over [codementor](https://www.codementor.io/@tgalal).**

---

yowsup is a python library that enables building applications that can communicate with WhatsApp users.
The project started as the protocol engine behind [Wazapp for Meego](https://wiki.maemo.org/Wazapp) and
[OpenWA for BB10](https://www.lowyat.net/2013/5896/try-this-openwhatsapp-for-blackberry-10/). Now as a standalone
library it can be used to power any custom WhatsApp client.

```
updated: 2021-12-14
yowsup version: 3.3.0
yowsup-cli version: 3.2.1
requires:
- python>=2.7,<=3.7
- consonance==0.1.5
- python-axolotl==0.2.2
- protobuf>=3.6.0
- six==1.10
uses:
 - ar"
gspread,"# Google Spreadsheets Python API v4

![main workflow](https://img.shields.io/github/actions/workflow/status/burnash/gspread/main.yaml?logo=github)
![GitHub licence](https://img.shields.io/pypi/l/gspread?logo=github)
![GitHub downloads](https://img.shields.io/github/downloads-pre/burnash/gspread/latest/total?logo=github)
![documentation](https://img.shields.io/readthedocs/gspread?logo=readthedocs)
![PyPi download](https://img.shields.io/pypi/dm/gspread?logo=pypi)
![PyPi version](https://img.shields.io/pypi/v/gspread?logo=pypi)
![python version](https://img.shields.io/pypi/pyversions/gspread?style=pypi)

Simple interface for working with Google Sheets.

Features:

- Open a spreadsheet by **title**, **key** or **URL**.
- Read, write, and format cell ranges.
- Sharing and access control.
- Batching updates.

## Installation

```sh
pip install gspread
```

Requirements: Python 3.8+.

## Basic Usage

1. [Create credentials in Google API Console](http://gspread.readthedocs.org/en/latest/oauth"
librosa,"[![librosa logo](docs/img/librosa_logo_text.svg)](https://librosa.org/)

# librosa


A python package for music and audio analysis.  

[![PyPI](https://img.shields.io/pypi/v/librosa.svg)](https://pypi.python.org/pypi/librosa)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/librosa/badges/version.svg)](https://anaconda.org/conda-forge/librosa)
[![License](https://img.shields.io/pypi/l/librosa.svg)](https://github.com/librosa/librosa/blob/main/LICENSE.md)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.591533.svg)](https://doi.org/10.5281/zenodo.591533)

[![CI](https://github.com/librosa/librosa/actions/workflows/ci.yml/badge.svg)](https://github.com/librosa/librosa/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/librosa/librosa/branch/main/graph/badge.svg?token=ULWnUHaIJC)](https://codecov.io/gh/librosa/librosa)
[![Docs](https://github.com/librosa/librosa/actions/workflows/docs.yml/badge.svg)](https://librosa.org/doc/latest/index.html)

#  Table of Contents

-"
PentestGPT,"<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->
<a name=""readme-top""></a>

<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown ""reference style"" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]



<!-- PROJECT LOGO -->
<br />
<div align=""center"">
  <a href=""https://github.com/GreyDGL/PentestGPT"">
  </a>

<h3 align=""center"">PentestGPT</h3>

  <p align=""center"">
    A "
frappe,"<div align=""center"">
	<picture>
		<source media=""(prefers-color-scheme: dark)"" srcset="".github/frappe-framework-logo-dark.svg"">
		<img src="".github/frappe-framework-logo.svg"" height=""50"">
	</picture>
	<h3>
		a web framework with <a href=""https://www.youtube.com/watch?v=LOjk3m0wTwg"">""batteries included""</a>
	</h3>
	<h5>
		it's pronounced - <em>fra-pay</em>
	</h5>
</div>

<div align=""center"">
	<a target=""_blank"" href=""#LICENSE"" title=""License: MIT"">
		<img src=""https://img.shields.io/badge/License-MIT-success.svg"">
	</a>
	<a target=""_blank"" href=""https://www.python.org/downloads/"" title=""Python version"">
		<img src=""https://img.shields.io/badge/python-%3E=_3.10-success.svg"">
	</a>
	<a href=""https://frappeframework.com/docs"">
		<img src=""https://img.shields.io/badge/docs-%F0%9F%93%96-success.svg""/>
	</a>
	<a href=""https://github.com/frappe/frappe/actions/workflows/server-tests.yml"">
		<img src=""https://github.com/frappe/frappe/actions/workflows/server-tests.yml/badge.svg"">
	</a>
	<a href="
dowhy,"|BuildStatus|_ |PyPiVersion|_ |PythonSupport|_ |Downloads|_ |discord|_

.. |PyPiVersion| image:: https://img.shields.io/pypi/v/dowhy.svg
.. _PyPiVersion: https://pypi.org/project/dowhy/

.. |PythonSupport| image:: https://img.shields.io/pypi/pyversions/dowhy.svg
.. _PythonSupport: https://pypi.org/project/dowhy/

.. |BuildStatus| image:: https://github.com/py-why/dowhy/actions/workflows/ci.yml/badge.svg
.. _BuildStatus: https://github.com/py-why/dowhy/actions

.. |Downloads| image:: https://pepy.tech/badge/dowhy
.. _Downloads: https://pepy.tech/project/dowhy

.. |discord| image:: https://img.shields.io/discord/818456847551168542
.. _discord: https://discord.gg/cSBGb3vsZb

.. image:: dowhy-logo-large.png
  :width: 50%
  :align: center


`Checkout the documentation <https://py-why.github.io/dowhy/>`_
===============================================================

- The documentation, user guide, sample notebooks and other information are available at
    `https://py-why.github.io/dowhy "
face-alignment,"# Face Recognition

Detect facial landmarks from Python using the world's most accurate face alignment network, capable of detecting points in both 2D and 3D coordinates.

Build using [FAN](https://www.adrianbulat.com)'s state-of-the-art deep learning based face alignment method. 

<p align=""center""><img src=""docs/images/face-alignment-adrian.gif"" /></p>

**Note:** The lua version is available [here](https://github.com/1adrianb/2D-and-3D-face-alignment).

For numerical evaluations it is highly recommended to use the lua version which uses indentical models with the ones evaluated in the paper. More models will be added soon.

[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)  [![Test Face alignmnet](https://github.com/1adrianb/face-alignment/workflows/Test%20Face%20alignmnet/badge.svg)](https://github.com/1adrianb/face-alignment/actions?query=workflow%3A%22Test+Face+alignmnet%22) [![Anaconda-Server Badge](https://a"
dotbot,"# Dotbot [![Build Status](https://github.com/anishathalye/dotbot/workflows/CI/badge.svg)](https://github.com/anishathalye/dotbot/actions?query=workflow%3ACI) [![Coverage](https://codecov.io/gh/anishathalye/dotbot/branch/master/graph/badge.svg)](https://app.codecov.io/gh/anishathalye/dotbot) [![PyPI](https://img.shields.io/pypi/v/dotbot.svg)](https://pypi.org/pypi/dotbot/) [![Python 3.6+](https://img.shields.io/badge/python-3.6%2B-blue)](https://pypi.org/pypi/dotbot/)

Dotbot makes installing your dotfiles as easy as `git clone $url && cd dotfiles
&& ./install`, even on a freshly installed system!

- [Rationale](#rationale)
- [Getting Started](#getting-started)
- [Configuration](#configuration)
- [Directives](#directives) ([Link](#link), [Create](#create), [Shell](#shell), [Clean](#clean), [Defaults](#defaults))
- [Plugins](#plugins)
- [Command-line Arguments](#command-line-arguments)
- [Wiki][wiki]

---

## Rationale

Dotbot is a tool that bootstraps your dotfiles (it's a [Dot]files
[b"
SciencePlots,"Science Plots
=============

<p align=""left"">
    <table>
        <tr>
            <td style=""text-align: center;"">PyPI version</td>
            <td style=""text-align: center;"">
                <a href=""https://badge.fury.io/py/SciencePlots"">
                    <img src=""https://badge.fury.io/py/SciencePlots.svg"" alt=""PyPI version"" height=""18""/>
                </a>
            </td>
        </tr>
        <tr>
            <td style=""text-align: center;"">conda-forge version</td>
            <td style=""text-align: center;"">
                <a href=""https://anaconda.org/conda-forge/pvlib"">
                    <img src=""https://anaconda.org/conda-forge/scienceplots/badges/version.svg"" alt=""conda-forge version"" height=""18""/>
                </a>
            </td>
        </tr>
        <tr>
            <td style=""text-align: center;"">DOI</td>
            <td style=""text-align: center;"">
                <a href=""https://zenodo.org/badge/latestdoi/144605189"">
                    <img src=""htt"
checkov,"[![checkov](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/web/images/checkov_blue_logo.png)](#)
       
[![Maintained by Prisma Cloud](https://img.shields.io/badge/maintained_by-Prisma_Cloud-blue)](https://prismacloud.io/?utm_source=github&utm_medium=organic_oss&utm_campaign=checkov)
[![build status](https://github.com/bridgecrewio/checkov/workflows/build/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Abuild)
[![security status](https://github.com/bridgecrewio/checkov/workflows/security/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=event%3Apush+branch%3Amaster+workflow%3Asecurity)
[![code_coverage](https://raw.githubusercontent.com/bridgecrewio/checkov/main/coverage.svg?sanitize=true)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Acoverage)
[![docs](https://img.shields.io/badge/docs-passing-brightgreen)](https://www.checkov.io/1.Welcome/What%20is%20Checkov.html?utm_source=github&utm_medium=organic_os"
machine-learning-course,"

###################################################
A Machine Learning Course with Python
###################################################

.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat
    :target: https://github.com/pyairesearch/machine-learning-for-everybody/pulls
.. image:: https://badges.frapsoft.com/os/v2/open-source.png?v=103
    :target: https://github.com/ellerbrock/open-source-badge/
.. image:: https://img.shields.io/badge/Made%20with-Python-1f425f.svg
      :target: https://www.python.org/
.. image:: https://img.shields.io/github/contributors/machinelearningmindset/machine-learning-course.svg
      :target: https://github.com/machinelearningmindset/machine-learning-course/graphs/contributors
.. image:: https://img.shields.io/badge/book-pdf-blue.svg
   :target: https://machinelearningmindset.com/wp-content/uploads/2019/06/machine-learning-course.pdf
.. image:: https://img.shields.io/badge/official-documentation-green.svg
   :tar"
django-ninja,"<a href=""https://github.com/vitalik/django-ninja/issues/383""><img width=""814"" alt=""SCR-20230123-m1t"" src=""https://user-images.githubusercontent.com/95222/214056666-585c0479-c122-4cb3-add4-b8844088ccdd.png""></a>



<a href=""https://github.com/vitalik/django-ninja/issues/383"">^ Please read ^</a>




<p align=""center"">
  <a href=""https://django-ninja.dev/""><img src=""https://django-ninja.dev/img/logo-big.png""></a>
</p>
<p align=""center"">
    <em>Fast to learn, fast to code, fast to run</em>
</p>


![Test](https://github.com/vitalik/django-ninja/actions/workflows/test_full.yml/badge.svg)
![Coverage](https://img.shields.io/codecov/c/github/vitalik/django-ninja)
[![PyPI version](https://badge.fury.io/py/django-ninja.svg)](https://badge.fury.io/py/django-ninja)
[![Downloads](https://static.pepy.tech/personalized-badge/django-ninja?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=downloads/month)](https://pepy.tech/project/django-ninja)

# Django Ninja "
marshmallow,"********************************************
marshmallow: simplified object serialization
********************************************

|pypi| |build-status| |pre-commit| |docs|

.. |pypi| image:: https://badgen.net/pypi/v/marshmallow
    :target: https://pypi.org/project/marshmallow/
    :alt: Latest version

.. |build-status| image:: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml/badge.svg
    :target: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml
    :alt: Build status

.. |pre-commit| image:: https://results.pre-commit.ci/badge/github/marshmallow-code/marshmallow/dev.svg
   :target: https://results.pre-commit.ci/latest/github/marshmallow-code/marshmallow/dev
   :alt: pre-commit.ci status

.. |docs| image:: https://readthedocs.org/projects/marshmallow/badge/
   :target: https://marshmallow.readthedocs.io/
   :alt: Documentation

**marshmallow** is an ORM/ODM/framework-agnostic library for converting complex d"
BentoML,"<picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/bentoml/BentoML/assets/489344/d3e6c95d-d224-49a5-9cff-0789f094e127"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://github.com/bentoml/BentoML/assets/489344/de4da660-6aeb-4e5a-bf76-b7177435444d"">
    <img alt=""BentoML: Unified Model Serving Framework"" src=""https://github.com/bentoml/BentoML/assets/489344/de4da660-6aeb-4e5a-bf76-b7177435444d"" width=""370"" style=""max-width: 100%;"">
</picture>

## Unified Model Serving Framework

🍱 Build model inference APIs and multi-model serving systems with any open-source or custom AI models. 👉 [Join our Slack community!](https://l.bentoml.com/join-slack)

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202-green.svg)](https://github.com/bentoml/BentoML?tab=Apache-2.0-1-ov-file)
[![Releases](https://img.shields.io/github/v/release/bentoml/bentoml.svg)](https://github.com/bentoml/bentoml/releases)
[![CI](https://github.com/bentom"
fantasque-sans,"Fantasque Sans Mono
===================

A programming font, designed with functionality in mind, and with some
wibbly-wobbly handwriting-like fuzziness that makes it unassumingly cool.
[Download](https://github.com/belluzj/fantasque-sans/releases/latest) or 
see [installation instructions](#installation).


![](Specimen/urxvt13.png)

Previously known as *Cosmic Sans Neue Mono*. It
appeared that [similar names were already in use for other
fonts](https://github.com/belluzj/cosmic-sans-neue/issues/16), and that
people tended to extend their instinctive hatred of Comic Sans to this very
font of mine (which of course can only be *loved*). Why the previous name?
Here is my original explanation:

> The name comes from my realization that at some point it looked like the
> mutant child of Comic Sans and Helvetica Neue. Hopefully it is not the
> case any more.

Inspirational sources include Inconsolata and Monaco. I have also been using
Consolas a lot in my programming life, so it may have so"
multi-v2ray,"# multi-v2ray
V2ray/Xray多用户管理脚本，向导式管理[新增|删除|修改]传输协议  
![](https://img.shields.io/pypi/v/v2ray-util.svg) 
[![Downloads](https://pepy.tech/badge/v2ray-util)](https://pepy.tech/project/v2ray-util)
[![Downloads](https://pepy.tech/badge/v2ray-util/month)](https://pepy.tech/project/v2ray-util)
![](https://img.shields.io/docker/pulls/jrohy/v2ray.svg)
![](https://img.shields.io/github/license/Jrohy/multi-v2ray.svg)

## [中文](README.md)  [English](README_EN.md)

## 特色
- [x] 支持Xray管理, v2ray和xray相互独立, 不同命令(v2ray/xray)进入不同的core管理
- [x] 调用v2ray官方api进行流量统计
- [x] **多用户, 多端口管理**, 混合传输协议管理不再是梦
- [x] 首次安装时产生随机端口，默认配置mkcp + 随机一种 (srtp | wechat-video | utp | dtls | wireguard) header伪装;  
  安装完成显示配置信息;
- [x] 查看配置信息显示vmess/vless字符串(v2rayN的分享链接格式)
- [x] 生成**Telegram**的socks5/MTProto分享链接, 支持socks5 + tls组合
- [x] 支持http/2, 随机生成伪装h2 path
- [x] 开启关闭tcpFastOpen
- [x] 直接开启[CDN](https://github.com/Jrohy/multi-v2ray/wiki/CloudFlare-cdn%E4%BB%A3%E7%90%86v2ray%E6%B5%81%E9%87%8F)
- [x] 开启关闭动态端口
- [x] 定时更新v2ray(需手动开启)
- ["
KeymouseGo,"<div align=""center"">

# KeymouseGo

<br>
<img src=""Preview.png"" width=""50%"" height=""50%"" />

<div>
    <img alt=""platform"" src=""https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-blueviolet"">
</div>
<div>
    <img alt=""license"" src=""https://img.shields.io/github/license/taojy123/KeymouseGo"">
    <img alt=""language"" src=""https://img.shields.io/badge/python-%3E%3D%203.7-green"">
    <img alt=""stars"" src=""https://img.shields.io/github/stars/taojy123/KeymouseGo?style=social"">
</div>

<br>

[简体中文](README.md) | [English](README_en-US.md)

</div>

功能：记录用户的鼠标键盘操作，通过触发按钮自动执行之前记录的操作，可设定执行的次数，可以理解为 `精简绿色版` 的 `按键精灵`。

用途：在进行某些操作简单、单调重复的工作时，使用本软件就可以很省力了。自己只要做一遍，然后接下来就让电脑来做。


# 目录

+ [安装](#安装)
+ [使用方法](#使用方法)
  + [基本操作](#基本操作)
  + [提示](#提示)
  + [脚本语法说明](#脚本语法说明)
  + [自定义扩展](#自定义扩展)
+ [关于作者](#关于作者)
+ [开源贡献者](#开源贡献者)
+ [更新说明](#更新说明)

# 安装

该软件通过 `Python` 语言编写，已打包为可执行文件，未安装 `Python` 的用户可直接下载 [release](https://github.com/taojy123/KeymouseGo/releases) 版本 ，直接点击 `KeymouseGo` 运行

### "
financial-machine-learning,"[![Repo-Updater](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_status.yml/badge.svg)](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_status.yml)
[![Wiki-Generator](https://github.com/firmai/financial-machine-learning/actions/workflows/wiki_gen.yml/badge.svg)](https://github.com/firmai/financial-machine-learning/actions/workflows/wiki_gen.yml)
[![Repo-Search](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_search.yml/badge.svg)](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_search.yml)
[![Gitter](https://badges.gitter.im/financial-machine-learning/community.svg)](https://gitter.im/financial-machine-learning/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

# Future of the Community
## Two things:
1. We have decided to start a [Slack](https://join.slack.com/t/mlquant/shared_invite/zt-ztvxxxtz-8_LCEwi1Wvy4cvkVm~IcyQ) group ([invite](https://join.slack.com/"
sh,".. image:: https://raw.githubusercontent.com/amoffat/sh/master/images/logo-230.png
    :target: https://amoffat.github.com/sh
    :alt: Logo

**If you are migrating from 1.* to 2.*, please see MIGRATION.md**

|

.. image:: https://img.shields.io/pypi/v/sh.svg?style=flat-square
    :target: https://pypi.python.org/pypi/sh
    :alt: Version
.. image:: https://img.shields.io/pypi/dm/sh.svg?style=flat-square
    :target: https://pypi.python.org/pypi/sh
    :alt: Downloads Status
.. image:: https://img.shields.io/pypi/pyversions/sh.svg?style=flat-square
    :target: https://pypi.python.org/pypi/sh
    :alt: Python Versions
.. image:: https://img.shields.io/coveralls/amoffat/sh.svg?style=flat-square
    :target: https://coveralls.io/r/amoffat/sh?branch=master
    :alt: Coverage Status

|

sh is a full-fledged subprocess replacement for Python 3.8 - 3.11, and PyPy
that allows you to call *any* program as if it were a function:

.. code:: python

    from sh import ifconfig
    print(ifconfig("
FlagEmbedding,"<h1 align=""center"">FlagEmbedding</h1>
<p align=""center"">
    <a href=""https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d"">
        <img alt=""Build"" src=""https://img.shields.io/badge/BGE_series-🤗-yellow"">
    </a>
    <a href=""https://github.com/FlagOpen/FlagEmbedding"">
            <img alt=""Build"" src=""https://img.shields.io/badge/Contribution-Welcome-blue"">
    </a>
    <a href=""https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/badge/LICENSE-MIT-green"">
    </a>
    <a href=""https://huggingface.co/C-MTEB"">
        <img alt=""Build"" src=""https://img.shields.io/badge/C_MTEB-🤗-yellow"">
    </a>
    <a href=""https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding"">
        <img alt=""Build"" src=""https://img.shields.io/badge/FlagEmbedding-1.1-red"">
    </a>
</p>

<h4 align=""center"">
    <p>
        <a href=#news>News</a> |
        <a href=#installation>Installation</a> |
"
Dango-Translator,"# 团子翻译器 - 基于OCR的生肉翻译软件


[![最新版本](https://img.shields.io/badge/%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC-Ver5.2.2-ff69b4)](https://github.com/PantsuDango/Dango-Translator)
[![更新时间](https://img.shields.io/badge/%E6%9B%B4%E6%96%B0%E6%97%B6%E9%97%B4-2024--06--01-ff69b4)]()
[![操作系统](https://img.shields.io/badge/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-win7--11-ff69b4)]()
[![GitHubStars](https://img.shields.io/github/stars/PantsuDango/Dango-Translator)]()
[![GitHubForks](https://img.shields.io/github/forks/PantsuDango/Dango-Translator)]()
[![作者](https://img.shields.io/badge/QQ-%E8%83%96%E6%AC%A1%E5%9B%A2%E5%AD%90-ff69b4)](https://github.com/PantsuDango/ImageHub/blob/master/DangoTranslate/public/%E4%BD%9C%E8%80%85.png)
[![群号](https://img.shields.io/badge/%E6%9C%80%E6%96%B0%E4%BA%A4%E6%B5%81%E7%BE%A4-11%E7%BE%A4835628840-ff69b4)](https://github.com/PantsuDango/ImageHub/blob/master/DangoTranslate/public/qrcode_1717254152512.jpg)

  
## 简介

团子翻译器是一款生肉翻译软件，通过OCR识别屏幕特定范围内的文字，然后将识别到的文字调取各种翻译源，并实时输出翻译结果。

+"
autograd,"# Autograd  [![Checks status][checks-badge]][checks-url] [![Tests status][tests-badge]][tests-url] [![Publish status][publish-badge]][publish-url] [![asv][asv-badge]](#)

[publish-badge]: https://github.com/HIPS/autograd/actions/workflows/publish.yml/badge.svg
[checks-badge]: https://github.com/HIPS/autograd/actions/workflows/check.yml/badge.svg
[tests-badge]: https://github.com/HIPS/autograd/actions/workflows/test.yml/badge.svg
[asv-badge]: http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat
[publish-url]: https://github.com/HIPS/autograd/actions/workflows/publish.yml
[checks-url]: https://github.com/HIPS/autograd/actions/workflows/check.yml
[tests-url]: https://github.com/HIPS/autograd/actions/workflows/test.yml

Autograd can automatically differentiate native Python and Numpy code. It can
handle a large subset of Python's features, including loops, ifs, recursion and
closures, and it can even take derivatives of derivatives of derivatives. It
supports reverse-mode "
gh-proxy,"# gh-proxy

## 简介

github release、archive以及项目文件的加速项目，支持clone，有Cloudflare Workers无服务器版本以及Python版本

## 演示

[https://gh.api.99988866.xyz/](https://gh.api.99988866.xyz/)

演示站为公共服务，如有大规模使用需求请自行部署，演示站有点不堪重负

![imagea272c95887343279.png](https://img.maocdn.cn/img/2021/04/24/imagea272c95887343279.png)

当然也欢迎[捐赠](#捐赠)以支持作者

## python版本和cf worker版本差异

- python版本支持进行文件大小限制，超过设定返回原地址 [issue #8](https://github.com/hunshcn/gh-proxy/issues/8)

- python版本支持特定user/repo 封禁/白名单 以及passby [issue #41](https://github.com/hunshcn/gh-proxy/issues/41)

## 使用

直接在copy出来的url前加`https://gh.api.99988866.xyz/`即可

也可以直接访问，在input输入

***大量使用请自行部署，以上域名仅为演示使用。***

访问私有仓库可以通过

`git clone https://user:TOKEN@ghproxy.com/https://github.com/xxxx/xxxx` [#71](https://github.com/hunshcn/gh-proxy/issues/71)

以下都是合法输入（仅示例，文件不存在）：

- 分支源码：https://github.com/hunshcn/project/archive/master.zip

- release源码：https://github.com/hunshcn/project/archive/v0.1.0.tar.gz

- release文件：https://github.com/hunshcn/project/releases/download/v0.1.0/"
corenet,"# CoreNet: A library for training deep neural networks

CoreNet is a deep neural network toolkit that allows researchers and engineers to train standard and novel small and large-scale models for variety of tasks, including foundation models (e.g., CLIP and LLM), object classification, object detection, and semantic segmentation.

## Table of contents

   * [What's new?](#whats-new)
   * [Research efforts at Apple using CoreNet](#research-efforts-at-apple-using-corenet)
   * [Installation](#installation)
   * [Directory Structure](#directory-structure)
   * [Maintainers](#maintainers)
   * [Contributing to CoreNet](#contributing-to-corenet)
   * [License](#license)
   * [Relationship with CVNets](#relationship-with-cvnets)
   * [Citation](#citation)

## What's new?

   * ***April 2024***: Version 0.1.0 of the CoreNet library includes
      * OpenELM
      * CatLIP
      * MLX examples

## Research efforts at Apple using CoreNet

Below is the list of publications from Apple that uses Co"
pdfGPT,"# pdfGPT
## Demo
1. **Demo URL**: https://bhaskartripathi-pdfgpt-turbo.hf.space
2. **Demo Video**:
   
   [![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/LzPgmmqpBk8/0.jpg)](https://www.youtube.com/watch?v=LzPgmmqpBk8)
#### Version Updates (27 July, 2023):
1. Improved error handling
2. PDF GPT now supports Turbo models and GPT4 including 16K and 32K token model.
3. Pre-defined questions for auto-filling the input.
4. Implemented Chat History feature.
![image](https://github.com/bhaskatripathi/pdfGPT/assets/35177508/11549b24-9ed4-4dcb-a877-bad9c2266bf9)


### Note on model performance
```If you find the response for a specific question in the PDF is not good using Turbo models, then you need to understand that Turbo models such as gpt-3.5-turbo are chat completion models and will not give a good response in some cases where the embedding similarity is low. Despite the claim by OpenAI, the turbo model is not the best model for Q&A. In those specific cases, either use the good old text"
PyGithub,"# PyGitHub

[![PyPI](https://img.shields.io/pypi/v/PyGithub.svg)](https://pypi.python.org/pypi/PyGithub)
![CI](https://github.com/PyGithub/PyGithub/workflows/CI/badge.svg)
[![readthedocs](https://img.shields.io/badge/docs-stable-brightgreen.svg?style=flat)](https://pygithub.readthedocs.io/en/stable/?badge=stable)
[![License](https://img.shields.io/badge/license-LGPL-blue.svg)](https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License)
[![Slack](https://img.shields.io/badge/Slack%20channel-%20%20-blue.svg)](https://join.slack.com/t/pygithub-project/shared_invite/zt-duj89xtx-uKFZtgAg209o6Vweqm8xeQ)
[![Open Source Helpers](https://www.codetriage.com/pygithub/pygithub/badges/users.svg)](https://www.codetriage.com/pygithub/pygithub)
[![codecov](https://codecov.io/gh/PyGithub/PyGithub/branch/master/graph/badge.svg)](https://codecov.io/gh/PyGithub/PyGithub)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

PyGitHub is a Pyt"
3d-photo-inpainting,"# [CVPR 2020] 3D Photography using Context-aware Layered Depth Inpainting

[![Open 3DPhotoInpainting in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)

### [[Paper](https://arxiv.org/abs/2004.04727)] [[Project Website](https://shihmengli.github.io/3D-Photo-Inpainting/)] [[Google Colab](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)]

<p align='center'>
<img src='https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_teaser.jpg' width='900'/>
</p>

We propose a method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that iteratively synthesizes new local color-and"
nginx-book,".. nginx_book documentation master file, created by
   sphinx-quickstart on Wed Feb 29 17:58:19 2012.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Nginx开发从入门到精通
=============================


缘起
++++++

nginx由于出色的性能，在世界范围内受到了越来越多人的关注，在淘宝内部它更是被广泛的使用，众多的开发以及运维同学都迫切的想要了解nginx模块的开发和它的内部原理，但是国内却没有一本关于这方面的书，源于此我们决定自己来写一本。本书的作者为淘宝核心系统服务器平台组的成员，本书写作的思路是从模块开发逐渐过渡到nginx原理剖析。书籍的内容会定期在这里更新，欢迎大家提出宝贵意见，不管是本书的内容问题，还是字词错误，都欢迎大家提交issue(章节标题的左侧有评注按钮)，我们会及时的跟进。

.. topic:: 更新历史

    .. csv-table:: 
       :header: 日期, 描述
       :widths: 20, 160
       :quote: $
       :delim: |

       2012/03/01|创建目录大纲
       2012/03/28|增加了样章
       2012/05/25|更新样章
       2012/06/08|增加第5章
       2012/06/11|增加第4章
       2012/06/26|增加第6章(event module)
       2012/06/27|更新第5章部分内容
       2012/07/04|更新第6章event module部分内容
       2012/07/12|增加第12章（请求头读取，subrequest解析）
       2012/08/14|增加第2章(nginx基础架构及基础概念)
       2012/08/14|增加第2章(ngx_str_t数据结构介绍)
      "
howmanypeoplearearound,"
# howmanypeoplearearound 

Count the number of people around you :family_man_man_boy: by monitoring wifi signals :satellite:.

*howmanypeoplearearound* calculates the number of people in the vicinity
using the approximate number of smartphones as a proxy (since [~70% of people have smartphones nowadays](https://twitter.com/conradhackett/status/701798230619590656)). 
A cellphone is determined to be in proximity to the computer based on sniffing WiFi probe 
requests. Possible uses of *howmanypeoplearearound* include: monitoring foot traffic in your house
with Raspberry Pis, seeing if your roommates are home, etc.

Tested on Linux (Raspbian and Ubuntu) and Mac OS X.

### **It may be illegal** to monitor networks for MAC addresses, especially on networks that *you do not own*. Please check your country's laws (for US [Section 18 U.S. Code § 2511](https://www.law.cornell.edu/uscode/text/18/2511)) - [discussion](https://github.com/schollz/howmanypeoplearearound/issues/4).

Getting started
="
models,"# 欢迎使用飞桨产业级开源模型库

## 简介

飞桨的产业级模型库，包含大量经过产业实践长期打磨的主流模型以及在国际竞赛中的夺冠模型；提供面向语义理解、图像分类、目标检测、图像分割、文字识别、语音合成等场景的多个端到端开发套件，满足企业低成本开发和快速集成的需求。飞桨的模型库是围绕国内企业实际研发流程量身定制打造的产业级模型库，服务企业遍布能源、金融、工业、农业等多个领域。

## 近期更新

**`2022-11-29`**: 更新`release/2.4`分支，飞桨官方模型超过600个，生态模型超过260个（数量持续更新中）.

**`2022-5-17`**: 更新`release/2.3`分支，飞桨官方模型超过500个，生态模型超过170个.

**`2021-11-30`**: 更新`release/2.2`分支，系统的梳理了飞桨官方模型、学术模型和社区模型的清单，其中官方模型超过400个，生态模型超过100个

**`Note`**:`release/2.2`以后分支模型均基于动态图实现，目前`dev-static`分支中仍有一些静态图模型代码，有需要的开发者可以继续切换到`dev-static`分支使用.

## 主要内容
|  目录 |   说明 |
| --- | --- |
| [官方模型(official)](docs/official/README.md) |• 面向产业实践，数量超过600个<br />• [飞桨PP系列模型](docs/official/PP-Models.md)，效果与精度最佳平衡<br />• 支持使用动态图开发视觉、自然语言、语音和推荐等领域模型<br />• 飞桨官方实现并提供持续技术支持及答疑<br />• 与飞桨核心框架版本对齐，已经经过充分的测试保证 |
|[学术模型(research)](docs/research/README.md) |• 面向学术前沿，侧重对于问题的持续更新<br />• 主要由飞桨相关的学术生态合作伙伴贡献|
|[社区模型(community)](docs/community/README.md) | • 面向更多丰富场景，侧重对于学术论文的覆盖<br />• 主要由飞桨生态开发者贡献，持续更新中|

## 欢迎加入飞桨模型库技术交流群
- 如果你希望了解飞桨模型库最新进展，或者希"
asyncpg,"asyncpg -- A fast PostgreSQL Database Client Library for Python/asyncio
=======================================================================

.. image:: https://github.com/MagicStack/asyncpg/workflows/Tests/badge.svg
   :target: https://github.com/MagicStack/asyncpg/actions?query=workflow%3ATests+branch%3Amaster
   :alt: GitHub Actions status
.. image:: https://img.shields.io/pypi/v/asyncpg.svg
   :target: https://pypi.python.org/pypi/asyncpg

**asyncpg** is a database interface library designed specifically for
PostgreSQL and Python/asyncio.  asyncpg is an efficient, clean implementation
of PostgreSQL server binary protocol for use with Python's ``asyncio``
framework.  You can read more about asyncpg in an introductory
`blog post <http://magic.io/blog/asyncpg-1m-rows-from-postgres-to-python/>`_.

asyncpg requires Python 3.8 or later and is supported for PostgreSQL
versions 9.5 to 16.  Older PostgreSQL versions or other databases implementing
the PostgreSQL protocol *may* work, but "
EZFN-Lobbybot,"# Welcome 🥳

EZFN.DEV is a free Fortnite Lobbybot which allows you to see all Fortnite cosmetics.

## Get Help
[Join my Discord](https://ezfn.dev/discord)

<sub>If you see this on github.com, visit [my website](https://ezfn.dev)...</sub>
"
PythonSpiderNotes,"# [Python入门网络爬虫之精华版](https://github.com/lining0806/PythonSpiderNotes)

*** 

Python学习网络爬虫主要分3个大的版块：**抓取**，**分析**，**存储**  

另外，比较常用的爬虫框架[Scrapy](http://scrapy.org/)，这里最后也详细介绍一下。    

首先列举一下本人总结的相关文章，这些覆盖了入门网络爬虫需要的基本概念和技巧：[宁哥的小站-网络爬虫](http://www.lining0806.com/category/spider/)  
***

当我们在浏览器中输入一个url后回车，后台会发生什么？比如说你输入[http://www.lining0806.com/](http://www.lining0806.com/)，你就会看到宁哥的小站首页。

简单来说这段过程发生了以下四个步骤：

* 查找域名对应的IP地址。
* 向IP对应的服务器发送请求。
* 服务器响应请求，发回网页内容。
* 浏览器解析网页内容。

网络爬虫要做的，简单来说，就是实现浏览器的功能。通过指定url，直接返回给用户所需要的数据，而不需要一步步人工去操纵浏览器获取。

## 抓取  
这一步，你要明确要得到的内容是什么？是HTML源码，还是Json格式的字符串等。  

#### 1. 最基本的抓取  

抓取大多数情况属于get请求，即直接从对方服务器上获取数据。  

首先，Python中自带urllib及urllib2这两个模块，基本上能满足一般的页面抓取。另外，[requests](https://github.com/kennethreitz/requests)也是非常有用的包，与此类似的，还有[httplib2](https://github.com/jcgregorio/httplib2)等等。    

```
Requests：
	import requests
	response = requests.get(url)
	content = requests.get(url).content
	print ""response headers:"", response.headers
	print ""content:"", content
Urllib2：
	"
Gymnasium,"[![Python](https://img.shields.io/pypi/pyversions/gymnasium.svg)](https://badge.fury.io/py/gymnasium)
[![PyPI](https://badge.fury.io/py/gymnasium.svg)](https://badge.fury.io/py/gymnasium)
[![arXiv](https://img.shields.io/badge/arXiv-2407.17032-b31b1b.svg)](https://arxiv.org/abs/2407.17032)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

<p align=""center"">
    <img src=""https://raw.githubusercontent.com/Farama-Foundation/Gymnasium/main/gymnasium-text.png"" width=""500px""/>
</p>

Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. This is a fork of OpenAI's [Gym](https://github.com/open"
folium,"|PyPI| |Test| |Gitter| |DOI| |binder|

.. |PyPI| image:: https://img.shields.io/pypi/v/folium.svg
    :target: https://pypi.org/project/folium
    :alt: PyPI Package

.. |Test| image:: https://github.com/python-visualization/folium/actions/workflows/test_code.yml/badge.svg
    :target: https://github.com/python-visualization/folium/actions/workflows/test_code.yml
    :alt: Code tests

.. |Gitter| image:: https://badges.gitter.im/python-visualization/folium.svg
    :target: https://gitter.im/python-visualization/folium
    :alt: Gitter

.. |DOI| image:: https://zenodo.org/badge/18669/python-visualization/folium.svg
   :target: https://zenodo.org/badge/latestdoi/18669/python-visualization/folium
   :alt: DOI

.. |binder| image:: https://mybinder.org/badge_logo.svg
 :target: https://mybinder.org/v2/gh/python-visualization/folium/main?filepath=examples

folium
======

.. image:: https://github.com/python-visualization/folium/blob/main/docs/_static/folium_logo.png
   :height: 100px


Python"
gef,"<p align=""center"">
  <img src=""https://i.imgur.com/o0L8lPN.png"" alt=""logo""/>
</p>

<p align=""center"">
    <a href=""https://discord.gg/hSbqxxBgRX""><img alt=""Discord"" src=""https://img.shields.io/badge/Discord-BlahCats-yellow""></a>
  <a href=""https://hugsy.github.io/gef""><img alt=""Docs"" src=""https://img.shields.io/badge/Docs-gh--pages-brightgreen""></a>
  <a title=""Use the IDs: gef/gef-demo"" href=""https://demo.gef.blah.cat""><img alt=""Try GEF"" src=""https://img.shields.io/badge/Demo-Try%20GEF%20Live-blue""></a>
</p>

`GEF` (pronounced ʤɛf - ""Jeff"") is a set of commands for x86/64, ARM, MIPS, PowerPC and SPARC to
assist exploit developers and reverse-engineers when using old school GDB. It provides additional
features to GDB using the Python API to assist during the process of dynamic analysis and exploit
development. Application developers will also benefit from it, as GEF lifts a great part of regular
GDB obscurity, avoiding repeating traditional commands, or bringing out the relevant inform"
librephotos,"[![Discord](https://img.shields.io/discord/784619049208250388?style=plastic)][discord] [![Website](https://img.shields.io/website?down_color=lightgrey&down_message=offline&style=plastic&up_color=blue&up_message=online&url=https%3A%2F%2Flibrephotos.com)](https://librephotos.com/)
[![Read the docs](https://img.shields.io/static/v1?label=Read&message=the%20docs&color=blue&style=plastic)](https://docs.librephotos.com/) [![GitHub contributors](https://img.shields.io/github/contributors/librephotos/librephotos?style=plastic)](https://github.com/LibrePhotos/librephotos/graphs/contributors)
<a href=""https://hosted.weblate.org/engage/librephotos/"">
<img src=""https://hosted.weblate.org/widgets/librephotos/-/librephotos-frontend/svg-badge.svg"" alt=""Translation status"" />
</a>

# LibrePhotos

![](https://github.com/LibrePhotos/librephotos/blob/dev/screenshots/mockups_main_fhd.png?raw=true)
<sub>Mockup designed by rawpixel.com / Freepik</sub>

- **Stable** demo is available here:https://demo1.libre"
hug,"[![HUG](https://raw.github.com/hugapi/hug/develop/artwork/logo.png)](http://hug.rest)
===================

[![PyPI version](https://badge.fury.io/py/hug.svg)](http://badge.fury.io/py/hug)
[![Build Status](https://travis-ci.org/hugapi/hug.svg?branch=develop)](https://travis-ci.org/hugapi/hug)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/0h7ynsqrbaxs7hfm/branch/master?svg=true)](https://ci.appveyor.com/project/TimothyCrosley/hug)
[![Coverage Status](https://coveralls.io/repos/hugapi/hug/badge.svg?branch=develop&service=github)](https://coveralls.io/github/hugapi/hug?branch=master)
[![License](https://img.shields.io/github/license/mashape/apistatus.svg)](https://pypi.python.org/pypi/hug/)
[![Join the chat at https://gitter.im/timothycrosley/hug](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/timothycrosley/hug?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

_________________

[Read Latest Documentation](https://hugapi.githu"
dumb-init,"dumb-init
========

[![PyPI version](https://badge.fury.io/py/dumb-init.svg)](https://pypi.python.org/pypi/dumb-init)


**dumb-init** is a simple process supervisor and init system designed to run as
PID 1 inside minimal container environments (such as [Docker][docker]). It is
deployed as a small, statically-linked binary written in C.

Lightweight containers have popularized the idea of running a single process or
service without normal init systems like [systemd][systemd] or
[sysvinit][sysvinit]. However, omitting an init system often leads to incorrect
handling of processes and signals, and can result in problems such as
containers which can't be gracefully stopped, or leaking containers which
should have been destroyed.

`dumb-init` enables you to simply prefix your command with `dumb-init`. It acts
as PID 1 and immediately spawns your command as a child process, taking care to
properly handle and forward signals as they are received.


## Why you need an init system

Normally, whe"
gpt-neox,"[![GitHub issues](https://img.shields.io/github/issues/EleutherAI/gpt-neox)](https://github.com/EleutherAI/gpt-neox/issues)
[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Weights & Biases monitoring"" height=20>](https://wandb.ai/eleutherai/neox)

# GPT-NeoX

This repository records [EleutherAI](https://www.eleuther.ai)'s library for training large-scale language models on GPUs. Our current framework is based on NVIDIA's [Megatron Language Model](https://github.com/NVIDIA/Megatron-LM) and has been augmented with techniques from [DeepSpeed](https://www.deepspeed.ai) as well as some novel optimizations. We aim to make this repo a centralized and accessible place to gather techniques for training large-scale autoregressive language models, and accelerate research into large-scale training. This library is in widespread use in [academic, industry, and government labs](https://github.com/EleutherAI/gpt-neox#adoption-and-publications), including"
modelscope,"
<p align=""center"">
    <br>
    <img src=""https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif"" width=""400""/>
    <br>
<p>

<div align=""center"">

[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%2"
chainlit,"# Welcome to Chainlit by Literal AI 👋

[![](https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat)](https://discord.gg/k73SQ3FyUh)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&label=Follow%20%40chainlit_io)](https://twitter.com/chainlit_io)
![PyPI - Downloads](https://img.shields.io/pypi/dm/chainlit)
[![GitHub Contributors](https://img.shields.io/github/contributors/chainlit/chainlit)](https://github.com/chainlit/chainlit/graphs/contributors)
[![CI](https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg)](https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml)

**Build production-ready Conversational AI applications in minutes, not weeks ⚡️**

Chainlit is an open-source async Python framework which allows developers to build scalable Conversational AI or agentic applications.

- ✅ ChatGPT-like application
- ✅ Embedded Chatbot & Software Copilot
- ✅ Slack & Discord
- ✅ Custom frontend (build your own agenti"
gpt-migrate,"<div align=""center"">

# ◐ &nbsp; GPT-Migrate &nbsp; ◑

**Easily migrate your codebase from one framework or language to another.**

<p>
<a href=""https://github.com/0xpayne/gpt-migrate/blob/main/LICENSE""><img alt=""Github License"" src=""https://img.shields.io/badge/License-MIT-green.svg"" /></a>
<a href=""https://github.com/0xpayne/gpt-migrate""><img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/0xpayne/gpt-migrate?style=social"" /></a>
</p>

<br />

</div>

If you've ever faced the pain of migrating a codebase to a new framework or language, this project is for you.

https://user-images.githubusercontent.com/25165841/250232917-bcc99ce8-99b7-4e3d-a653-f89e163ed825.mp4

Migration is a costly, tedious, and non-trivial problem. Do not trust the current version blindly and please use responsibly. Please also be aware that costs can add up quickly as GPT-Migrate is designed to write (and potentially re-write) the entirety of a codebase.

However, with the collective brilliance o"
flask-restful,"# Flask-RESTful

[![Build Status](https://travis-ci.org/flask-restful/flask-restful.svg?branch=master)](http://travis-ci.org/flask-restful/flask-restful)
[![Coverage Status](http://img.shields.io/coveralls/flask-restful/flask-restful/master.svg)](https://coveralls.io/r/flask-restful/flask-restful)
[![PyPI Version](http://img.shields.io/pypi/v/Flask-RESTful.svg)](https://pypi.python.org/pypi/Flask-RESTful)

Flask-RESTful provides the building blocks for creating a great REST API.

## User Guide

You'll find the user guide and all documentation [here](https://flask-restful.readthedocs.io/)

"
BackgroundMattingV2,"# Real-Time High-Resolution Background Matting

![Teaser](https://github.com/PeterL1n/Matting-PyTorch/blob/master/images/teaser.gif?raw=true)

Official repository for the paper [Real-Time High-Resolution Background Matting](https://arxiv.org/abs/2012.07810). Our model requires capturing an additional background image and produces state-of-the-art matting results at 4K 30fps and HD 60fps on an Nvidia RTX 2080 TI GPU.

* [Visit project site](https://grail.cs.washington.edu/projects/background-matting-v2/)
* [Watch project video](https://www.youtube.com/watch?v=oMfPTeYDF9g)

**Disclaimer**: The video conversion script in this repo is not meant be real-time. Our research's main contribution is the neural architecture for high resolution refinement and the new matting datasets. The `inference_speed_test.py` script allows you to measure the tensor throughput of our model, which should achieve real-time. The `inference_video.py` script allows you to test your video on our model, but the video"
Depth-Anything,"<div align=""center"">
<h2>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</h2>

[**Lihe Yang**](https://liheyoung.github.io/)<sup>1</sup> · [**Bingyi Kang**](https://scholar.google.com/citations?user=NmHgX-wAAAAJ)<sup>2&dagger;</sup> · [**Zilong Huang**](http://speedinghzl.github.io/)<sup>2</sup> · [**Xiaogang Xu**](https://xiaogang00.github.io/)<sup>3,4</sup> · [**Jiashi Feng**](https://sites.google.com/site/jshfeng/)<sup>2</sup> · [**Hengshuang Zhao**](https://hszhao.github.io/)<sup>1*</sup>

<sup>1</sup>HKU&emsp;&emsp;&emsp;&emsp;<sup>2</sup>TikTok&emsp;&emsp;&emsp;&emsp;<sup>3</sup>CUHK&emsp;&emsp;&emsp;&emsp;<sup>4</sup>ZJU

&dagger;project lead&emsp;*corresponding author

**CVPR 2024**

<a href=""https://arxiv.org/abs/2401.10891""><img src='https://img.shields.io/badge/arXiv-Depth Anything-red' alt='Paper PDF'></a>
<a href='https://depth-anything.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything-green' alt='Project Page'></a>
<a href='h"
bertviz,"<h1 align=""center"">
    BertViz
</h1>
<h3 align=""center"">
 Visualize Attention in NLP Models
</h3>
<h3 align=""center"">
    <a href=""#-quick-tour"">Quick Tour</a> &bull;
    <a href=""#%EF%B8%8F-getting-started"">Getting Started</a> &bull;
    <a href=""https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing"">Colab Tutorial</a> &bull;
    <a href=""#-paper"">Paper</a>
</h3>

BertViz is an interactive tool for visualizing attention in [Transformer](https://jalammar.github.io/illustrated-transformer/) language models such as BERT, GPT2, or T5. It can be run inside a Jupyter or Colab
 notebook through a simple Python API that supports most [Huggingface models](https://huggingface.co/models). BertViz extends the
   [Tensor2Tensor visualization tool](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization)
    by [Llion Jones](https://medium.com/@llionj), providing multiple views that each offer a unique lens into the attention mechanism.

"
imbalanced-learn,".. -*- mode: rst -*-

.. _scikit-learn: http://scikit-learn.org/stable/

.. _scikit-learn-contrib: https://github.com/scikit-learn-contrib

|Azure|_ |Codecov|_ |CircleCI|_ |PythonVersion|_ |Pypi|_ |Gitter|_ |Black|_

.. |Azure| image:: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_apis/build/status/scikit-learn-contrib.imbalanced-learn?branchName=master
.. _Azure: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_build

.. |Codecov| image:: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn/branch/master/graph/badge.svg
.. _Codecov: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn

.. |CircleCI| image:: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn.svg?style=shield
.. _CircleCI: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn/tree/master

.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg
.. _PythonVersion: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg

.. |Pypi| image:"
docker-py,"# Docker SDK for Python

[![Build Status](https://github.com/docker/docker-py/actions/workflows/ci.yml/badge.svg)](https://github.com/docker/docker-py/actions/workflows/ci.yml)

A Python library for the Docker Engine API. It lets you do anything the `docker` command does, but from within Python apps – run containers, manage containers, manage Swarms, etc.

## Installation

The latest stable version [is available on PyPI](https://pypi.python.org/pypi/docker/). Install with pip:

    pip install docker

> Older versions (< 6.0) required installing `docker[tls]` for SSL/TLS support.
> This is no longer necessary and is a no-op, but is supported for backwards compatibility.

## Usage

Connect to Docker using the default socket or the configuration in your environment:

```python
import docker
client = docker.from_env()
```

You can run containers:

```python
>>> client.containers.run(""ubuntu:latest"", ""echo hello world"")
'hello world\n'
```

You can run containers in the background:

```pyt"
NLP_ability,"# 背景介绍

NLP日常工作经验和论文解析，包含：预训练模型，文本表征，文本相似度，文本分类，多模态，知识蒸馏，词向量。

我觉得NLP是一个值得深耕的领域，所以希望可以不停的提升自己核心竞争力和自己的段位！

微信公众号：DASOU

## 深度学习自然语言处理

### Transformer

1. [史上最全Transformer面试题](./深度学习自然语言处理/Transformer/史上最全Transformer面试题.md)
2. [答案解析(1)-史上最全Transformer面试题](./深度学习自然语言处理/Transformer/答案解析(1)—史上最全Transformer面试题：灵魂20问帮你彻底搞定Transformer.md) 
3. [Pytorch代码分析--如何让Bert在finetune小数据集时更“稳”一点](./深度学习自然语言处理/Bert/Pytorch代码分析-如何让Bert在finetune小数据集时更“稳”一点.md)
4. [解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数(附Pytorch代码详细解读)](https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/%E8%A7%A3%E5%86%B3%E8%80%81%E5%A4%A7%E9%9A%BE%E9%97%AE%E9%A2%98-%E5%A6%82%E4%BD%95%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%B8%A6%E4%BD%A0%E9%9A%8F%E5%BF%83%E6%89%80%E6%AC%B2%E9%87%8D%E6%96%B0%E5%88%9D%E5%A7%8B%E5%8C%96bert%E7%9A%84%E6%9F%90%E4%BA%9B%E5%8F%82%E6%95%B0(%E9%99%84Pytorch%E4%BB%A3%E7%A0%81).md)
5. [3分钟从零解读Transformer的Encoder](https:/"
AzurLaneAutoScript,"**| [English](README_en.md) | 简体中文 | [日本語](README_jp.md) |**

# AzurLaneAutoScript

#### Discord [![](https://img.shields.io/discord/720789890354249748?logo=discord&logoColor=ffffff&color=4e4c97)](https://discord.gg/AQN6GeJ) QQ群  ![](https://img.shields.io/badge/QQ%20Group-1087735381-4e4c97)
Azur Lane bot with GUI (Supports CN, EN, JP, TW, able to support other servers), designed for 24/7 running scenes, can take over almost all Azur Lane gameplay. Azur Lane, as a mobile game, has entered the late stage of its life cycle. During the period from now to the server down, please reduce the time spent on the Azur Lane and leave everything to Alas.

Alas is a free open source software, link: https://github.com/LmeSzinc/AzurLaneAutoScript

Alas，一个带GUI的碧蓝航线脚本（支持国服, 国际服, 日服, 台服, 可以支持其他服务器），为 7x24 运行的场景而设计，能接管近乎全部的碧蓝航线玩法。碧蓝航线，作为一个手游，已经进入了生命周期的晚期。从现在到关服的这段时间里，请减少花费在碧蓝航线上的时间，把一切都交给 Alas。

Alas 是一款免费开源软件，地址：https://github.com/LmeSzinc/AzurLaneAutoScript

EN support, thanks **[@whoamikyo](https://gi"
SerpentAI,"![](https://s3.ca-central-1.amazonaws.com/serpent-ai-assets/SerpentFBCover.png)

# Serpent.AI - Game Agent Framework (Python)

[![](https://img.shields.io/badge/project-website-brightgreen.svg?colorB=1bcc6f&longCache=true)](http://serpent.ai)
[![](https://img.shields.io/badge/project-blog-brightgreen.svg?colorB=1bcc6f&longCache=true)](http://blog.serpent.ai)
[![](https://img.shields.io/badge/project-wiki-brightgreen.svg?colorB=1bcc6f&longCache=true)](https://github.com/SerpentAI/SerpentAI/wiki)    
[![](https://img.shields.io/badge/pypi-v2018.1.2-brightgreen.svg?colorB=007ec6&longCache=true)]()
[![](https://img.shields.io/badge/python-3.6-brightgreen.svg?colorB=007ec6&longCache=true)]()
[![](https://img.shields.io/badge/license-MIT-brightgreen.svg?colorB=007ec6&longCache=true)]()  
[![](https://img.shields.io/badge/twitter-@Serpent__AI-brightgreen.svg?colorB=1da1f2&longCache=true)](https://twitter.com/Serpent_AI)

## Update: Revival (May 2020)

Development work has resumed on the frame"
tinydb,".. image:: https://raw.githubusercontent.com/msiemens/tinydb/master/artwork/logo.png
    :scale: 100%
    :height: 150px

|Build Status| |Coverage| |Version|

Quick Links
***********

- `Example Code`_
- `Supported Python Versions`_
- `Documentation <http://tinydb.readthedocs.org/>`_
- `Changelog <https://tinydb.readthedocs.io/en/latest/changelog.html>`_
- `Extensions <https://tinydb.readthedocs.io/en/latest/extensions.html>`_
- `Contributing`_

Introduction
************

TinyDB is a lightweight document oriented database optimized for your happiness :)
It's written in pure Python and has no external dependencies. The target are
small apps that would be blown away by a SQL-DB or an external database server.

TinyDB is:

- **tiny:** The current source code has 1800 lines of code (with about 40%
  documentation) and 1600 lines tests.

- **document oriented:** Like MongoDB_, you can store any document
  (represented as ``dict``) in TinyDB.

- **optimized for your happiness:** TinyDB is de"
mealie,"[![Latest Release][latest-release-shield]][latest-release-url]
[![Contributors][contributors-shield]][contributors-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![AGPL License][license-shield]][license-url]
[![Docker Pulls][docker-pull]][docker-url]
[![GHCR Pulls][ghcr-pulls]][ghcr-url]

<!-- PROJECT LOGO -->
<br />
<p align=""center"">
  <a href=""https://github.com/mealie-recipes/mealie"">
<svg style=""width:100px;height:100px"" viewBox=""0 0 24 24"">
    <path fill=""currentColor"" d=""M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z"" />
</svg>
  </a>

  <h3 align=""center"">Mealie</h3>

  <p align=""center"">
    A Place For All Your Recipes
    <br />
 "
get_subscribe,"# ⏰ 免费机场 免费梯子 翻墙VPN

## ⚠️ 注意

- 欢迎无产阶级革命斗士免费使用本订阅
- 链接来自网络，仅作学习使用
- 使用页面所提供的任意资源时，请务必遵守当地法律

## 🚀 每12小时更新一次

- clash订阅链接：`https://git.io/emzclash`

- v2ray订阅链接：`https://git.io/emzv2ray`

手机用户无法访问上方短链接时可以用下面的长链接

- clash订阅链接：`https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/clash.yml`

- v2ray订阅链接：`https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/v2ray.txt`


## 📘 客户端使用方法

- 📱 [Android](https://www.ermao.net/skill/clashforandroid/)
- 🖥 [Windows](https://www.ermao.net/uncategorized/clash-for-windows/)

## 💸 付费订阅

我搜罗的一些比较便宜的机场（月消费10块以下），觉得免费订阅不好使的朋友们可以在这里面找找。

[https://www.ermao.net/posts/vpn](https://www.ermao.net/posts/vpn)

| 链接 | 价位 | 备注 |
|----|----|----|
|[m.ssone.io](https://hello-ssone.com/register?aff=aBHsE1pF)|10元 100G/月|正常访问|
|[https://www.efcloud.bio](https://www.efcloud.bio/#/register?code=kbbSUTvm)|	9 元 350G/月|正常访问|
|[https://www.fccloud.cc](https://www.fccloud.cc/#/register?code=AYsN4z5L)|	10 元 150G/月|正常访问|
|[https://ss.vgsseven"
ffsubsync,"FFsubsync
=======

[![CI Status](https://github.com/smacke/ffsubsync/workflows/ffsubsync/badge.svg)](https://github.com/smacke/ffsubsync/actions)
[![Support Ukraine](https://badgen.net/badge/support/UKRAINE/?color=0057B8&labelColor=FFD700)](https://github.com/vshymanskyy/StandWithUkraine/blob/main/docs/README.md)
[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-maroon.svg)](https://opensource.org/licenses/MIT)
[![Python Versions](https://img.shields.io/pypi/pyversions/ffsubsync.svg)](https://pypi.org/project/ffsubsync)
[![Documentation Status](https://readthedocs.org/projects/ffsubsync/badge/?version=latest)](https://ffsubsync.readthedocs.io/en/latest/?badge=latest)
[![PyPI Version](https://img.shields.io/pypi/v/ffsubsync.svg)](https://pypi.org/project/ffsubsync)


Language"
faust,".. XXX Need to change this image to readthedocs before release

.. image:: https://raw.githubusercontent.com/robinhood/faust/8ee5e209322d9edf5bdb79b992ef986be2de4bb4/artwork/banner-alt1.png

===========================
 Deprecation Notice
===========================

This library has been deprecated and no longer managed or supported. The current active community project can be found at https://github.com/faust-streaming/faust

===========================
 Python Stream Processing
===========================

|build-status| |coverage| |license| |wheel| |pyversion| |pyimp|

:Version: 1.10.4
:Web: http://faust.readthedocs.io/
:Download: http://pypi.org/project/faust
:Source: http://github.com/robinhood/faust
:Keywords: distributed, stream, async, processing, data, queue, state management


.. sourcecode:: python

    # Python Streams
    # Forever scalable event processing & in-memory durable K/V store;
    # as a library w/ asyncio & static typing.
    import faust

**Faust** is a strea"
OpenNMT-py,"# Announcement: OpenNMT-py is no longer actively supported.

We started a new project [Eole](https://eole-nlp.github.io/eole/) available on [Github](https://github.com/eole-nlp/eole)

It is a spin-off of OpenNMT-py in terms of features but we revamped a lot of stuff.

Eole handles NMT, LLM, Encoders as well as a new concept of Estimator within a NMT Model See this [post](https://medium.com/p/05b00b271a47) and this [news](https://www.linkedin.com/posts/vincentnguyenngoc_embarrassingly-small-english-to-german-model-activity-7203400634727841792-FCre?utm_source=share&utm_medium=member_desktop)

If you are a developer, switch now. If you are a user only, then we will publish the first py-pi versions shortly.


# OpenNMT-py: Open-Source Neural Machine Translation and (Large) Language Models

[![Build Status](https://github.com/OpenNMT/OpenNMT-py/workflows/Lint%20&%20Tests/badge.svg)](https://github.com/OpenNMT/OpenNMT-py/actions)
[![Documentation](https://img.shields.io/badge/docs-latest-blu"
python,"# Kubernetes Python Client

[![Build Status](https://travis-ci.org/kubernetes-client/python.svg?branch=master)](https://travis-ci.org/kubernetes-client/python)
[![PyPI version](https://badge.fury.io/py/kubernetes.svg)](https://badge.fury.io/py/kubernetes)
[![codecov](https://codecov.io/gh/kubernetes-client/python/branch/master/graph/badge.svg)](https://codecov.io/gh/kubernetes-client/python ""Non-generated packages only"")
[![pypi supported versions](https://img.shields.io/pypi/pyversions/kubernetes.svg)](https://pypi.python.org/pypi/kubernetes)
[![Client Capabilities](https://img.shields.io/badge/Kubernetes%20client-Silver-blue.svg?style=flat&colorB=C0C0C0&colorA=306CE8)](http://bit.ly/kubernetes-client-capabilities-badge)
[![Client Support Level](https://img.shields.io/badge/kubernetes%20client-beta-green.svg?style=flat&colorA=306CE8)](http://bit.ly/kubernetes-client-support-badge)

Python client for the [kubernetes](http://kubernetes.io/) API.

## Installation

From source:

```
git c"
vits,"# VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech

### Jaehyeon Kim, Jungil Kong, and Juhee Son

In our recent [paper](https://arxiv.org/abs/2106.06103), we propose VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.

Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duratio"
Auto_Bangumi,"<p align=""center"">
    <img src=""docs/image/icons/light-icon.svg#gh-light-mode-only"" width=50%/ alt="""">
    <img src=""docs/image/icons/dark-icon.svg#gh-dark-mode-only"" width=50%/ alt="""">
</p>
<p align=""center"">
    <img title=""docker build version"" src=""https://img.shields.io/docker/v/estrellaxd/auto_bangumi"" alt="""">
    <img title=""release date"" src=""https://img.shields.io/github/release-date/estrellaxd/auto_bangumi"" alt="""">
    <img title=""docker pull"" src=""https://img.shields.io/docker/pulls/estrellaxd/auto_bangumi"" alt="""">
    <img title=""python version"" src=""https://img.shields.io/badge/python-3.11-blue"" alt="""">
</p>

<p align=""center"">
  <a href=""https://www.autobangumi.org"">官方网站</a> | <a href=""https://www.autobangumi.org/deploy/quick-start.html"">快速开始</a> | <a href=""https://www.autobangumi.org/changelog/3.0.html"">更新日志</a> | <a href=""https://t.me/autobangumi_update"">更新推送</a> | <a href=""https://t.me/autobangumi"">TG 群组</a>
</p>

# 项目说明

<p align=""center"">
    <img title=""AutoBangumi"
eve,"Eve
====
.. image:: https://img.shields.io/pypi/v/eve.svg?style=flat-square
    :target: https://pypi.org/project/eve

.. image:: https://github.com/pyeve/eve/workflows/CI/badge.svg
  :target: https://github.com/pyeve/eve/actions?query=workflow%3ACI

.. image:: https://img.shields.io/pypi/pyversions/eve.svg?style=flat-square
    :target: https://pypi.org/project/eve

.. image:: https://img.shields.io/badge/license-BSD-blue.svg?style=flat-square
    :target: https://en.wikipedia.org/wiki/BSD_License

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

Eve is an open source Python REST API framework designed for human beings. It
allows to effortlessly build and deploy highly customizable, fully featured
RESTful Web Services. Eve offers native support for MongoDB, and SQL backends
via community extensions.

Eve is Simple
-------------
.. code-block:: python

    from eve import Eve

    app = Eve()
    app.run()

The API is now"
BasicSR,"<p align=""center"">
  <img src=""assets/basicsr_xpixel_logo.png"" height=120>
</p>

## <div align=""center""><b><a href=""README.md"">English</a> | <a href=""README_CN.md"">简体中文</a></b></div>

<div align=""center"">

[![LICENSE](https://img.shields.io/github/license/xinntao/basicsr.svg)](https://github.com/xinntao/BasicSR/blob/master/LICENSE.txt)
[![PyPI](https://img.shields.io/pypi/v/basicsr)](https://pypi.org/project/basicsr/)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/xinntao/BasicSR.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/xinntao/BasicSR/context:python)
[![python lint](https://github.com/xinntao/BasicSR/actions/workflows/pylint.yml/badge.svg)](https://github.com/xinntao/BasicSR/blob/master/.github/workflows/pylint.yml)
[![Publish-pip](https://github.com/xinntao/BasicSR/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/xinntao/BasicSR/blob/master/.github/workflows/publish-pip.yml)
[![gitee mirror](https://github.com/xinntao/BasicSR/act"
ragas,"<h1 align=""center"">
  <img style=""vertical-align:middle"" height=""200""
  src=""./docs/_static/imgs/logo.png"">
</h1>
<p align=""center"">
  <i>Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines</i>
</p>

<p align=""center"">
    <a href=""https://github.com/explodinggradients/ragas/releases"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/explodinggradients/ragas.svg"">
    </a>
    <a href=""https://www.python.org/"">
            <img alt=""Build"" src=""https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple"">
    </a>
    <a href=""https://github.com/explodinggradients/ragas/blob/master/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/github/license/explodinggradients/ragas.svg?color=green"">
    </a>
    <a href=""https://colab.research.google.com/github/explodinggradients/ragas/blob/main/docs/quickstart.ipynb"">
        <img alt=""Open In Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg"">
    "
tensorflow_practice,"Tensroflow练习
======

相关数据集下载地址：链接:https://pan.baidu.com/s/1GMv7_3qruoVZBJMvN-afGA  密码:ako7
基于tf1.4

目录

1、基础<br>
[基本语法<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/basic.py)
[tensorBoard使用<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/tensorBoard.py)
[dropout<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/dropout.py)
[模型保存与重载<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/save2file.py)
[基本神经网络<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/first_nerual_network.py)
[卷积神经网络<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/CNN.py)
<br>
2、自然语言相关<br>
[static_RNN<br>](https://github.com/princewen/tensorflow_practice/blob/master/nlp/RNN_static_cell.py)
[dynamic_RNN<br>](https://github.com/princewen/tensorflow_practice/blob/master/nlp/RNN_dynamic_cell.py)
[LSTM<br>](https://github.com/princewen/tensorflow_practice/blob/master/nlp/LSTM.py)
[LSTM_"
backgroundremover,"# BackgroundRemover
![Background Remover](https://raw.githubusercontent.com/nadermx/backgroundremover/main/examplefiles/backgroundremoverexample.png)
<img alt=""background remover video"" src=""https://raw.githubusercontent.com/nadermx/backgroundremover/main/examplefiles/backgroundremoverprocessed.gif"" height=""200"" /><br>
BackgroundRemover is a command line tool to remove background from [image](https://github.com/nadermx/backgroundremover#image) and [video](https://github.com/nadermx/backgroundremover#video) using AI, made by [nadermx](https://john.nader.mx) to power [https://BackgroundRemoverAI.com](https://backgroundremoverai.com). If you wonder why it was made read this [short blog post](https://johnathannader.com/my-first-open-source-project/).<br>


### Requirements

* python >= 3.6
* python3.6-dev #or what ever version of python you use
* torch and torchvision stable version (https://pytorch.org)
* ffmpeg 4.4+

* To clarify, you must install both python and whatever dev version of "
The-Grand-Complete-Data-Science-Materials,"# The Grand Complete Data Science Guide With Videos And Materials

## 1. Complete Python Playlist For Data Analytics And Data Science

- Python In English: https://www.youtube.com/watch?v=bPrmA1SEN2k&list=PLZoTAELRMXVNUL99R4bDlVYsncUNvwUBB
- Python In Hindi: https://www.youtube.com/watch?v=MJd9d9Mpxg0&list=PLTDARY42LDV4qqiJd1Z1tShm3mp9-rP4v

## 2. Complete Stats Playlist For Data Analytics And Data Science

- Stats In English One Shot: https://www.youtube.com/watch?v=LZzq1zSL1bs
- Stats In English Detailed Playlist: https://www.youtube.com/watch?v=zRUliXuwJCQ&list=PLZoTAELRMXVMhVyr3Ri9IQ-t5QPBtxzJO
- Stats In Hindi Detailed Playlist: https://www.youtube.com/watch?v=7y3XckjaVOw&list=PLTDARY42LDV6YHSRo669_uDDGmUEmQnDJ

## 3. Complete SQL For Data Analytics And Data Science

- Complete SQl Detailed Playlist English: https://www.youtube.com/watch?v=us1XyayQ6fU&list=PLZoTAELRMXVNMRWlVf0bDDSxNEn38u9Cl
- Complete SQL Detailed Playlist Hindi : **Coming Soon**
- Complete SQL One Shot : **Coming"
DeepPavlov,"# DeepPavlov 1.0

[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
![Python 3.6, 3.7, 3.8, 3.9, 3.10, 3.11](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-green.svg)
[![Downloads](https://pepy.tech/badge/deeppavlov)](https://pepy.tech/project/deeppavlov)
[![Static Badge](https://img.shields.io/badge/DeepPavlov%20Community-blue)](https://forum.deeppavlov.ai/)
[![Static Badge](https://img.shields.io/badge/DeepPavlov%20Demo-blue)](https://demo.deeppavlov.ai/)


DeepPavlov 1.0 is an open-source NLP framework built on [PyTorch](https://pytorch.org/) and [transformers](https://github.com/huggingface/transformers). DeepPavlov 1.0 is created for modular and configuration-driven development of state-of-the-art NLP models and supports a wide range of NLP model applications. DeepPavlov 1.0 is designed for practitioners with limited knowledge of NLP/ML.

## Quick Links

|name|Description|
|--|--|
| ⭐"
Github-Ranking,"[Github Ranking](./README.md)
==========

**A list of the most github stars and forks repositories.**

*Last Automatic Update Time: 2024-09-26T03:04:35Z*

## Table of Contents

* [Most Stars](#most-stars)
* [Most Forks](#most-forks)
* [ActionScript](#actionscript)
* [C](#c)
* [C\#](#c-1)
* [C\+\+](#c-2)
* [Clojure](#clojure)
* [CoffeeScript](#coffeescript)
* [CSS](#css)
* [Dart](#dart)
* [DM](#dm)
* [Elixir](#elixir)
* [Go](#go)
* [Groovy](#groovy)
* [Haskell](#haskell)
* [HTML](#html)
* [Java](#java)
* [JavaScript](#javascript)
* [Julia](#julia)
* [Kotlin](#kotlin)
* [Lua](#lua)
* [MATLAB](#matlab)
* [Objective\-C](#objective-c)
* [Perl](#perl)
* [PHP](#php)
* [PowerShell](#powershell)
* [Python](#python)
* [R](#r)
* [Ruby](#ruby)
* [Rust](#rust)
* [Scala](#scala)
* [Shell](#shell)
* [Swift](#swift)
* [TeX](#tex)
* [TypeScript](#typeScript)
* [Vim script](#vim-script)
## Most Stars

This is top 10, for more click **[Top 100 Stars](Top100/Top-100-stars.md)**

| Ranking | Project Name |"
patroni,"|Tests Status| |Coverage Status|

Patroni: A Template for PostgreSQL HA with ZooKeeper, etcd or Consul
--------------------------------------------------------------------

You can find a version of this documentation that is searchable and also easier to navigate at `patroni.readthedocs.io <https://patroni.readthedocs.io>`__.


There are many ways to run high availability with PostgreSQL; for a list, see the `PostgreSQL Documentation <https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling>`__.

Patroni is a template for high availability (HA) PostgreSQL solutions using Python. For maximum accessibility, Patroni supports a variety of distributed configuration stores like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in datacent"
DeepLearningFlappyBird,"# Using Deep Q-Network to Learn How To Play Flappy Bird

<img src=""./images/flappy_bird_demp.gif"" width=""250"">

7 mins version: [DQN for flappy bird](https://www.youtube.com/watch?v=THhUXIhjkCM)

## Overview
This project follows the description of the Deep Q Learning algorithm described in Playing Atari with Deep Reinforcement Learning [2] and shows that this learning algorithm can be further generalized to the notorious Flappy Bird.

## Installation Dependencies:
* Python 2.7 or 3
* TensorFlow 0.7
* pygame
* OpenCV-Python

## How to Run?
```
git clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git
cd DeepLearningFlappyBird
python deep_q_network.py
```

## What is Deep Q-Network?
It is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.

For those who are interested in deep reinforcement learning, I highly recommend to read the following post:

[Demystifying Deep Re"
werkzeug,"# Werkzeug

*werkzeug* German noun: ""tool"". Etymology: *werk* (""work""), *zeug* (""stuff"")

Werkzeug is a comprehensive [WSGI][] web application library. It began as
a simple collection of various utilities for WSGI applications and has
become one of the most advanced WSGI utility libraries.

It includes:

-   An interactive debugger that allows inspecting stack traces and
    source code in the browser with an interactive interpreter for any
    frame in the stack.
-   A full-featured request object with objects to interact with
    headers, query args, form data, files, and cookies.
-   A response object that can wrap other WSGI applications and handle
    streaming data.
-   A routing system for matching URLs to endpoints and generating URLs
    for endpoints, with an extensible system for capturing variables
    from URLs.
-   HTTP utilities to handle entity tags, cache control, dates, user
    agents, cookies, files, and more.
-   A threaded WSGI server for use while developing appl"
monkey,"# Infection Monkey
[![GitHub release (latest by date)](https://img.shields.io/github/v/release/guardicore/monkey)](https://github.com/guardicore/monkey/releases)

[![Build Status](https://app.travis-ci.com/guardicore/monkey.svg?branch=develop)](https://app.travis-ci.com/guardicore/monkey)
[![codecov](https://codecov.io/gh/guardicore/monkey/branch/develop/graph/badge.svg)](https://codecov.io/gh/guardicore/monkey)

![GitHub stars](https://img.shields.io/github/stars/guardicore/monkey)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/guardicore/monkey)

Welcome to Infection Monkey!  We're glad you could swing by.🐒 Here's all the
info you'll need to start monkeying around.

## What is Infection Monkey?
Infection Monkey is an open-source adversary emulation platform that helps you
improve your security posture using empirical data. The Monkey uses various
methods to self-propagate across a network and reports its activities to a
centralized command and control serve"
icloud_photos_downloader,"# iCloud Photos Downloader [![Quality Checks](https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Quality%20Checks/badge.svg)](https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/quality-checks.yml) [![Multi Platform Docker Build](https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Docker%20Build/badge.svg)](https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/docker-build.yml) [![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

- A command-line tool to download all your iCloud photos.
- Works on Linux, Windows, and macOS; laptop, desktop, and NAS
- Available as an executable for direct downloading and through package managers/ecosystems ([Docker](https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker), [PyPI](https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi), [AUR](http"
ScoutSuite,"<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/4206926/49877604-10457580-fe26-11e8-92d7-cd876c4f6454.png"" width=350/>
</p>

#

[![Workflow](https://github.com/nccgroup/ScoutSuite/workflows/CI%20Workflow/badge.svg)](https://github.com/nccgroup/ScoutSuite/actions)
[![CodeCov](https://codecov.io/gh/nccgroup/ScoutSuite/branch/master/graph/badge.svg)](https://codecov.io/gh/nccgroup/ScoutSuite)

[![PyPI version](https://badge.fury.io/py/ScoutSuite.svg)](https://badge.fury.io/py/ScoutSuite)
[![PyPI downloads](https://img.shields.io/pypi/dm/scoutsuite)](https://img.shields.io/pypi/dm/scoutsuite)
[![Docker Hub](https://img.shields.io/badge/Docker%20Hub-rossja%2Fncc--scoutsuite-blue)](https://hub.docker.com/r/rossja/ncc-scoutsuite/)
[![Docker Pulls](https://img.shields.io/docker/pulls/rossja/ncc-scoutsuite.svg?style=flat-square)](https://hub.docker.com/r/rossja/ncc-scoutsuite/)

## Description

Scout Suite is an open source multi-cloud security-auditing tool, which enab"
pix2pixHD,"<img src='imgs/teaser_720.gif' align=""right"" width=360>

<br><br><br><br>

# pix2pixHD
### [Project](https://tcwang0509.github.io/pix2pixHD/) | [Youtube](https://youtu.be/3AIpPlzM_qs) | [Paper](https://arxiv.org/pdf/1711.11585.pdf) <br>
Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic image-to-image translation. It can be used for turning semantic label maps into photo-realistic images or synthesizing portraits from face label maps. <br><br>
[High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://tcwang0509.github.io/pix2pixHD/)  
 [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyuliu.net/)<sup>1</sup>, [Jun-Yan Zhu](http://people.eecs.berkeley.edu/~junyanz/)<sup>2</sup>, Andrew Tao<sup>1</sup>, [Jan Kautz](http://jankautz.com/)<sup>1</sup>, [Bryan Catanzaro](http://catanzaro.name/)<sup>1</sup>  
 <sup>1</sup>NVIDIA Corporation, <sup>2</sup>UC Berkeley  
 In CVPR 20"
aws-devops-zero-to-hero,"# aws-devops-zero-to-hero

Complete YouTube playlist - https://www.youtube.com/playlist?list=PLdpzxOOAlwvLNOxX0RfndiYSt1Le9azze

AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples. Each day's class will provide real-time knowledge on AWS services, allowing you to apply what you've learned and gain practical skills in working with AWS in a DevOps context.

## Day 1: Introduction to AWS

You will learn what is private and public cloud. Why companies are moving to public cloud, what are the advantages of moving to cloud.

Also, you will be introduced to the basics of AWS, including the core services and their significance in DevOps practices. Finally learn how to set up an AWS account and navigate the AWS Management Console.

## Day 2: IAM (Identity and Access Management)

You will explore IAM, which is used for managing access to AWS resources. You'll learn how to create IAM users, gro"
boxmot,"# BoxMOT: pluggable SOTA tracking modules for segmentation, object detection and pose estimation models

<div align=""center"">
  <p>
  <img src=""assets/images/track_all_seg_1280_025conf.gif"" width=""400""/>
  </p>
  <br>
  <div>
  <a href=""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml""><img src=""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml/badge.svg"" alt=""CI CPU testing""></a>
  <a href=""https://pepy.tech/project/boxmot""><img src=""https://static.pepy.tech/badge/boxmot""></a>
  <br>
  <a href=""https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a>
<a href=""https://doi.org/10.5281/zenodo.8132989""><img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.8132989.svg"" alt=""DOI""></a>
<a href=""https://hub.docker.com/r/boxmot/boxmot""><img src=""https://img.shields.io/docker/pulls/boxmot/boxmot?logo=docker"" alt=""Ultralytic"
jupytext,"![](https://github.com/mwouts/jupytext/blob/17aea37c612f33a4e27eeee4b81966f1506920fd/docs/images/logo_large.png?raw=true)

<!-- INDEX-START -->

[![CI](https://github.com/mwouts/jupytext/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/mwouts/jupytext/actions)
[![Documentation Status](https://readthedocs.org/projects/jupytext/badge/?version=latest)](https://jupytext.readthedocs.io/en/latest/?badge=latest)
[![codecov.io](https://codecov.io/github/mwouts/jupytext/coverage.svg?branch=main)](https://codecov.io/gh/mwouts/jupytext/branch/main)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![GitHub language count](https://img.shields.io/github/languages/count/mwouts/jupytext)](docs/languages.md)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/jupytext.svg)](https://anaconda.org/conda-forge/jupytext)
[![Pypi](https://img.shields.io/pypi/v/jupytext.svg)](https://pypi.python.org/pypi/jupytext)
[!"
autocut,"# AutoCut: 通过字幕来剪切视频

AutoCut 对你的视频自动生成字幕。然后你选择需要保留的句子，AutoCut 将对你视频中对应的片段裁切并保存。你无需使用视频编辑软件，只需要编辑文本文件即可完成剪切。

**2024.03.10更新**：支持 pip 安装和提供 import 转录相关的功能

```shell
# Install
pip install autocut-sub
```

```python
from autocut import Transcribe, load_audio
```


**2023.10.14更新**：支持 faster-whisper 和指定依赖（但由于 Action 限制暂时移除了 faster-whisper 的测试运行）

```shell
# for whisper only
pip install .

# for whisper and faster-whisper
pip install '.[faster]'

# for whisper and openai-whisper
pip install '.[openai]'

# for all
pip install '.[all]'
```

```shell
# using faster-whisper
autocut -t xxx --whisper-mode=faster
```

```shell
# using openai api
export OPENAI_API_KEY=sk-xxx
autocut -t xxx --whisper-mode=openai --openai-rpm=3
```

**2023.8.13更新**：支持调用 Openai Whisper API
```shell
export OPENAI_API_KEY=sk-xxx
autocut -t xxx --whisper-mode=openai --openai-rpm=3
```

## 使用例子

假如你录制的视频放在 `2022-11-04/` 这个文件夹里。那么运行

```bash
autocut -d 2022-11-04
```

> 提示：如果你使用 OBS 录屏，可以在 `设置->高级->录像->文件名格式` 中将空格改成 `/`，即"
RedditVideoMakerBot,"# Reddit Video Maker Bot 🎥

All done WITHOUT video editing or asset compiling. Just pure ✨programming magic✨.

Created by Lewis Menelaws & [TMRRW](https://tmrrwinc.ca)

<a target=""_blank"" href=""https://tmrrwinc.ca"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://user-images.githubusercontent.com/6053155/170528535-e274dc0b-7972-4b27-af22-637f8c370133.png"">
  <source media=""(prefers-color-scheme: light)"" srcset=""https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png"">
  <img src=""https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png"" width=""350"">
</picture>

</a>

## Video Explainer

[![lewisthumbnail](https://user-images.githubusercontent.com/6053155/173631669-1d1b14ad-c478-4010-b57d-d79592a789f2.png)
](https://www.youtube.com/watch?v=3gjcY_00U1w)

## Motivation 🤔

These videos on TikTok, YouTube and Instagram get MILLIONS of views across all platforms and require very "
skypilot,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png"">
    <img alt=""SkyPilot"" src=""https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png"" width=55%>
  </picture>
</p>

<p align=""center"">
  <a href=""https://skypilot.readthedocs.io/en/latest/"">
    <img alt=""Documentation"" src=""https://readthedocs.org/projects/skypilot/badge/?version=latest"">
  </a>

  <a href=""https://github.com/skypilot-org/skypilot/releases"">
    <img alt=""GitHub Release"" src=""https://img.shields.io/github/release/skypilot-org/skypilot.svg"">
  </a>

  <a href=""http://slack.skypilot.co"">
    <img alt=""Join Slack"" src=""https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack"">
  </a>

</p>

<h3 align=""center"">
    Run AI on Any Infra — Unified, Faster, Cheaper
</h3>

----
:fire: *News* :fire:
- [Sep, 2024] Point, L"
PyQt,"# 各种各样的PyQt测试和例子

[![Blog](https://img.shields.io/badge/blog-pyqt-green.svg)](https://pyqt.site)
[![codebeat badge](https://codebeat.co/badges/d23d0dc8-aef3-43d2-96aa-e3215b2c9861)](https://codebeat.co/projects/github-com-pyqt5-pyqt-master)
[![Badge](https://img.shields.io/badge/link-996.icu-%23FF4D5B.svg?style=flat-square)](https://996.icu/#/zh_CN)
[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/996icu/996.ICU/blob/master/LICENSE)

[https://pyqt.site](https://pyqt.site) 论坛是专门针对PyQt5学习和提升开设的网站，分享大家平时学习中记录的笔记和例子，以及对遇到的问题进行收集整理。

[![GitHub watchers](https://img.shields.io/github/watchers/PyQt5/PyQt.svg?style=social&label=Watch)](https://github.com/PyQt5/PyQt)
[![GitHub stars](https://img.shields.io/github/stars/PyQt5/PyQt.svg?style=social)](https://github.com/PyQt5/PyQt)
[![GitHub forks](https://img.shields.io/github/forks/PyQt5/PyQt.svg?style=social)](https://github.com/PyQt5/PyQt/fork)

如果您觉得这里的东西对您有帮助，别忘了帮忙点一颗:star:小星星:star:
"
DeepSeek-Coder,"<p align=""center"">
<img width=""1000px"" alt=""DeepSeek Coder"" src=""pictures/logo.png"">
</p>
<p align=""center""><a href=""https://www.deepseek.com/"">[<img src=""pictures/home.png"" width=""20px""> Homepage]</a> | <a href=""https://coder.deepseek.com/"">[🤖 Chat with DeepSeek Coder]</a> | <a href=""https://huggingface.co/deepseek-ai"">[🤗 Models Download]</a> | <a href=""https://discord.gg/Tc7c45Zzu5"">[Discord]</a> | <a href=""https://github.com/guoday/assert/blob/main/QR.png?raw=true"">[WeChat (微信)]</a></p>
<p align=""center"">
  <a href=""https://huggingface.co/papers/2401.14196""><b>Paper Link</b>👁️</a>
</p>
<hr>


### 1. Introduction of DeepSeek Coder

DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and"
stock,"### 说明，项目迁移到了Gitee 啦，最后一次修改，2023-06-02 执行存档

项目迁移到这里了：此项目后续更新访问这里：

https://gitee.com/pythonstock/stock

github项目后续就Archives存档了，不再更新了！

csdn的pythonstock专栏地址，相关资料都在这里有说明：

https://blog.csdn.net/freewebsys/category_9285317.html


### pythonstock V2 项目简介


**特别说明：股市有风险投资需谨慎，本项目只能用于Python代码学习，股票分析，投资失败亏钱不负责，不算BUG。**

```
项目地址：https://github.com/pythonstock/stock
PythonStock V2 是基于Python的pandas，akshare，bokeh，tornado，stockstats，ta-lib等框架开发的全栈股票系统。
项目创建于2017年7月17日，每月不定期更新。
1）可以直接使用docker直接本地部署运行，整个项目在docker hub上压缩后200MB，本地占用500MB磁盘空间。
2）使用Docker解决了Python库安装问题，使用Mariadb（MySQL）存储数据。借助akshare抓取数据。
3）使用cron做定时任务，每天进行数据抓取计算，每天18点开始进行数据计算，计算当日数据，使用300天数据进行计算，大约需要15分钟计算完毕。
4）股票数据接口防止被封，按天进行数据缓存，储存最近3天数据，每天定时清除，同时使用read_pickle to_pickle 的gzip压缩模式存储。
5）使用tornado开发web系统，支持每日股票数据-东财，龙虎榜-个股上榜-新浪，数据中心-大宗交易行情等。
6）数据展示系统，是通用数据展示系统，配置字典模板之后，页面自动加载数据，并完成数据展示，后续自己开发的指标数据可以加入进去。
7）增加曲线数据分析，在查看股票中，可以直接跳转到东方财富页面查看相关信息，点击指标之后使用Bokeh将多达 17 个指标的数据绘图，进行图表展示。
8) 2.0 最大的更新在于替换tushare库（因部分库不能使用），使用akshare进行数据抓取。

基础库版本
"
tenacity,"Tenacity
========
.. image:: https://img.shields.io/pypi/v/tenacity.svg
    :target: https://pypi.python.org/pypi/tenacity

.. image:: https://circleci.com/gh/jd/tenacity.svg?style=svg
    :target: https://circleci.com/gh/jd/tenacity

.. image:: https://img.shields.io/endpoint.svg?url=https://api.mergify.com/badges/jd/tenacity&style=flat
   :target: https://mergify.io
   :alt: Mergify Status

**Please refer to the** `tenacity documentation <https://tenacity.readthedocs.io/en/latest/>`_ **for a better experience.**

Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in
Python, to simplify the task of adding retry behavior to just about anything.
It originates from `a fork of retrying
<https://github.com/rholder/retrying/issues/65>`_ which is sadly no longer
`maintained <https://julien.danjou.info/python-tenacity/>`_. Tenacity isn't
api compatible with retrying but adds significant new functionality and
fixes a number of longstanding bugs.

The simplest use case"
cryptography,"pyca/cryptography
=================

.. image:: https://img.shields.io/pypi/v/cryptography.svg
    :target: https://pypi.org/project/cryptography/
    :alt: Latest Version

.. image:: https://readthedocs.org/projects/cryptography/badge/?version=latest
    :target: https://cryptography.io
    :alt: Latest Docs

.. image:: https://github.com/pyca/cryptography/workflows/CI/badge.svg?branch=main
    :target: https://github.com/pyca/cryptography/actions?query=workflow%3ACI+branch%3Amain


``cryptography`` is a package which provides cryptographic recipes and
primitives to Python developers. Our goal is for it to be your ""cryptographic
standard library"". It supports Python 3.7+ and PyPy3 7.3.11+.

``cryptography`` includes both high level recipes and low level interfaces to
common cryptographic algorithms such as symmetric ciphers, message digests, and
key derivation functions. For example, to encrypt something with
``cryptography``'s high level symmetric encryption recipe:

.. code-block:: "
streaming-llm,"# Efficient Streaming Language Models with Attention Sinks 
[[paper](http://arxiv.org/abs/2309.17453)] [[slides](assets/StreamingLLM.pdf)][[video](https://youtu.be/hvJsEzP34o8)]

![schemes](figures/schemes.png)

https://github.com/mit-han-lab/streaming-llm/assets/40906949/2bd1cda4-a0bd-47d1-a023-fbf7779b8358

## TL;DR
We deploy LLMs for infinite-length inputs without sacrificing efficiency and performance.

## News

- [2024/02] StreamingLLM is covered by [MIT News as a spotlight](https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213)!
- [2024/01] StreamingLLM is integrated by HPC-AI Tech [SwiftInfer](https://github.com/hpcaitech/SwiftInfer) to support infinite input length for LLM inference.
- [2024/01] StreamingLLM is integrated by NVIDIA [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm)!
- [2023/12] StreamingLLM is integrated by CMU, UW, and OctoAI, enabling endless and efficient LLM generati"
DjangoBlog,"# DjangoBlog

🌍
*[English](/docs/README-en.md) ∙ [简体中文](README.md)*

基于`python3.10`和`Django4.0`的博客。   

[![Django CI](https://github.com/liangliangyy/DjangoBlog/actions/workflows/django.yml/badge.svg)](https://github.com/liangliangyy/DjangoBlog/actions/workflows/django.yml) [![CodeQL](https://github.com/liangliangyy/DjangoBlog/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/liangliangyy/DjangoBlog/actions/workflows/codeql-analysis.yml) [![codecov](https://codecov.io/gh/liangliangyy/DjangoBlog/branch/master/graph/badge.svg)](https://codecov.io/gh/liangliangyy/DjangoBlog)  [![license](https://img.shields.io/github/license/liangliangyy/djangoblog.svg)]()  

## 主要功能：
- 文章，页面，分类目录，标签的添加，删除，编辑等。文章、评论及页面支持`Markdown`，支持代码高亮。
- 支持文章全文搜索。
- 完整的评论功能，包括发表回复评论，以及评论的邮件提醒，支持`Markdown`。
- 侧边栏功能，最新文章，最多阅读，标签云等。
- 支持Oauth登陆，现已有Google,GitHub,facebook,微博,QQ登录。
- 支持`Redis`缓存，支持缓存自动刷新。
- 简单的SEO功能，新建文章等会自动通知Google和百度。
- 集成了简单的图床功能。
- 集成`django-compressor`，自动压缩`css`，`js`。
- 网站异常邮件提醒，若有未捕捉"
django-extensions,"===================
 Django Extensions
===================

.. image:: https://img.shields.io/pypi/l/django-extensions.svg
   :target: https://raw.githubusercontent.com/django-extensions/django-extensions/master/LICENSE

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/compile_catalog.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/linters.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/precommit.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/pytest.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-e"
pyWhat,"<p align='center'>
<img src='images/logo.png'>
<p align=""center"">➡️ <a href=""http://discord.skerritt.blog"">Discord</a> ⬅️<br>
<i>The easiest way to identify anything</i><br>
<code>pip3 install pywhat && pywhat --help</code>
</p>

<p align=""center"">
  <a href=""http://discord.skerritt.blog""><img alt=""Discord"" src=""https://img.shields.io/discord/754001738184392704""></a> <a href=""https://pypi.org/project/pywhat/""><img alt=""PyPI - Downloads"" src=""https://pepy.tech/badge/pywhat/month""></a>  <a href=""https://twitter.com/bee_sec_san""><img alt=""Twitter Follow"" src=""https://img.shields.io/twitter/follow/bee_sec_san?style=social""></a> <a href=""https://pypi.org/project/pywhat/""><img alt=""PyPI - Python Version"" src=""https://img.shields.io/pypi/pyversions/pywhat""></a> <a href=""https://pypi.org/project/pywhat/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/pywhat""></a>
</p>
<hr>

# 🤔 `What` is this?

![](images/main_demo.gif)

Imagine this: You come across some mysterious text 🧙‍♂️ `0x5290840009"
ipex-llm,"> [!IMPORTANT]
> ***`bigdl-llm` has now become `ipex-llm` (see the migration guide [here](docs/mddocs/Quickstart/bigdl_llm_migration.md)); you may find the original `BigDL` project [here](https://github.com/intel-analytics/BigDL-2.x).***
 
---

#  💫 Intel® LLM Library for PyTorch* 
<p>
  <b>< English</b> | <a href='./README.zh-CN.md'>中文</a> >
</p>

**`IPEX-LLM`** is a PyTorch library for running **LLM** on Intel CPU and GPU *(e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max)* with very low latency[^1]. 
> [!NOTE]
> - *It is built on top of the excellent work of **`llama.cpp`**, **`transformers`**, **`bitsandbytes`**, **`vLLM`**, **`qlora`**, **`AutoGPTQ`**, **`AutoAWQ`**, etc.*
> - *It provides seamless integration with [llama.cpp](docs/mddocs/Quickstart/llama_cpp_quickstart.md), [Ollama](docs/mddocs/Quickstart/ollama_quickstart.md), [Text-Generation-WebUI](docs/mddocs/Quickstart/webui_quickstart.md), [HuggingFace transformers](python/llm/example/GPU/HuggingFace), [Lang"
lm-evaluation-harness,"# Language Model Evaluation Harness

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10256836.svg)](https://doi.org/10.5281/zenodo.10256836)

---

*Latest News 📣*

- [2024/09] We are prototyping allowing users of LM Evaluation Harness to create and evaluate on text+image multimodal input, text output tasks, and have just added the `hf-multimodal` and `vllm-vlm` model types and `mmmu` task as a prototype feature. We welcome users to try out this in-progress feature and stress-test it for themselves, and suggest they check out [`lmms-eval`](https://github.com/EvolvingLMMs-Lab/lmms-eval), a wonderful project originally forking off of the lm-evaluation-harness, for a broader range of multimodal tasks, models, and features.
- [2024/07] [API model](docs/API_guide.md) support has been updated and refactored, introducing support for batched and async requests, and making it significantly easier to customize and use for your own purposes. **To run Llama 405B, we recommend using VLLM's OpenA"
SlowFast,"# PySlowFast

PySlowFast is an open source video understanding codebase from FAIR that provides state-of-the-art video classification models with efficient training. This repository includes implementations of the following methods:

- [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982)
- [Non-local Neural Networks](https://arxiv.org/abs/1711.07971)
- [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1912.00998)
- [X3D: Progressive Network Expansion for Efficient Video Recognition](https://arxiv.org/abs/2004.04730)
- [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)
- [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)
- [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526)
- [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)
- [Masked Autoencod"
r0capture,"# r0capture

安卓应用层抓包通杀脚本

## 简介

- 仅限安卓平台，测试安卓7、8、9、10、11、12、13、14 可用 ；
- 无视所有证书校验或绑定，不用考虑任何证书的事情；
- 通杀TCP/IP四层模型中的应用层中的全部协议；
- 通杀协议包括：Http,WebSocket,Ftp,Xmpp,Imap,Smtp,Protobuf等等、以及它们的SSL版本；
- 通杀所有应用层框架，包括HttpUrlConnection、Okhttp1/3/4、Retrofit/Volley等等；
- 无视加固，不管是整体壳还是二代壳或VMP，不用考虑加固的事情；
- 如果有抓不到的情况欢迎提issue，或者直接加vx：r0ysue，进行反馈~

### June.18th 2023 update：测试Pixel4/安卓13/KernelSU/Frida16 功能工作正常 正常抓包 导出证书

### January.14th 2021 update：增加几个辅助功能

- 增加App收发包函数定位功能
- 增加App客户端证书导出功能
- 新增host连接方式“-H”，用于Frida-server监听在非标准端口时的连接

## 用法

- 推荐环境：[https://github.com/r0ysue/AndroidSecurityStudy/blob/master/FRIDA/A01/README.md](https://github.com/r0ysue/AndroidSecurityStudy/blob/master/FRIDA/A01/README.md)

切记仅限安卓平台7、8、9、10、11 可用 ，禁止使用模拟器。

- Spawn 模式：

`$ python3 r0capture.py -U -f com.coolapk.market -v`

- Attach 模式，抓包内容保存成pcap文件供后续分析： 

`$ python3 r0capture.py -U 酷安 -v -p iqiyi.pcap` 

建议使用`Attach`模式，从感兴趣的地方开始抓包，并且保存成`pcap`文件，供后续使用Wireshark进行分析。
> 老版本Frida使用包名，新版本Frida使用APP名。APP名必须是点开app后，frida-ps -"
pkuseg-python,"# pkuseg：一个多领域中文分词工具包 [**(English Version)**](readme/readme_english.md)

pkuseg 是基于论文[[Luo et. al, 2019](#论文引用)]的工具包。其简单易用，支持细分领域分词，有效提升了分词准确度。



## 目录

* [主要亮点](#主要亮点)
* [编译和安装](#编译和安装)
* [各类分词工具包的性能对比](#各类分词工具包的性能对比)
* [使用方式](#使用方式)
* [论文引用](#论文引用)
* [作者](#作者)
* [常见问题及解答](#常见问题及解答)



## 主要亮点

pkuseg具有如下几个特点：

1. 多领域分词。不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了新闻领域，网络领域，医药领域，旅游领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。各领域分词样例可参考 [**example.txt**](https://github.com/lancopku/pkuseg-python/blob/master/example.txt)。
2. 更高的分词准确率。相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。
3. 支持用户自训练模型。支持用户使用全新的标注数据进行训练。
4. 支持词性标注。


## 编译和安装

- 目前**仅支持python3**
- **为了获得好的效果和速度，强烈建议大家通过pip install更新到目前的最新版本**

1. 通过PyPI安装(自带模型文件)：
	```
	pip3 install pkuseg
	之后通过import pkuseg来引用
	```
   **建议更新到最新版本**以获得更好的开箱体验：
   	```
	pip3 install -U pkuseg
	```
2. 如果PyPI官方源下载速度不理想，建议使用镜像源，比如：   
  "
mycroft-core,"[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE.md) 
[![CLA](https://img.shields.io/badge/CLA%3F-Required-blue.svg)](https://mycroft.ai/cla) 
[![Team](https://img.shields.io/badge/Team-Mycroft_Core-violetblue.svg)](https://github.com/MycroftAI/contributors/blob/master/team/Mycroft%20Core.md) 
![Status](https://img.shields.io/badge/-Production_ready-green.svg)

![Unit Tests](https://github.com/mycroftai/mycroft-core/workflows/Unit%20Tests/badge.svg)
[![codecov](https://codecov.io/gh/MycroftAI/mycroft-core/branch/dev/graph/badge.svg?token=zQzRlkXxAr)](https://codecov.io/gh/MycroftAI/mycroft-core)

# This project is no longer actively maintained

Mycroft core is no longer maintaiend and probably likely not work on your computer anymore. [Open Voice OS](https://openvoiceos.org) and [Neon-core](https://github.com/NeonGeckoCom/NeonCore) are both spiritual successors to Mycroft. (And some of the old code may live on there.)

# Old Readme

Mycroft is a hackable"
lerobot,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""media/lerobot-logo-thumbnail.png"">
    <source media=""(prefers-color-scheme: light)"" srcset=""media/lerobot-logo-thumbnail.png"">
    <img alt=""LeRobot, Hugging Face Robotics Library"" src=""media/lerobot-logo-thumbnail.png"" style=""max-width: 100%;"">
  </picture>
  <br/>
  <br/>
</p>

<div align=""center"">

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shield"
boltons,"# Boltons

*boltons should be builtins.*

<a href=""https://boltons.readthedocs.io/en/latest/""><img src=""https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat""></a>
<a href=""https://pypi.python.org/pypi/boltons""><img src=""https://img.shields.io/pypi/v/boltons.svg""></a>
<a href=""https://anaconda.org/conda-forge/boltons""><img src=""https://img.shields.io/conda/vn/conda-forge/boltons.svg""></a>
<a href=""https://ports.macports.org/port/py-boltons/summary""><img src=""https://repology.org/badge/version-for-repo/macports/python:boltons.svg?header=MacPorts""></a>
<a href=""https://pypi.python.org/pypi/boltons""><img src=""https://img.shields.io/pypi/pyversions/boltons.svg""></a>
<a href=""http://calver.org""><img src=""https://img.shields.io/badge/calver-YY.MINOR.MICRO-22bfda.svg""></a>

**Boltons** is a set of over 230 BSD-licensed, pure-Python utilities
in the same spirit as — and yet conspicuously missing from —
[the standard library][stdlib], including:

  * [Atomic file saving][atomic], "
watchdog,"Watchdog
========

|Build Status|
|CirrusCI Status|

Python API and shell utilities to monitor file system events.

Works on 3.9+.

Example API Usage
-----------------

A simple program that uses watchdog to monitor directories specified
as command-line arguments and logs events generated:

.. code-block:: python

    import time

    from watchdog.events import FileSystemEvent, FileSystemEventHandler
    from watchdog.observers import Observer


    class MyEventHandler(FileSystemEventHandler):
        def on_any_event(self, event: FileSystemEvent) -> None:
            print(event)


    event_handler = MyEventHandler()
    observer = Observer()
    observer.schedule(event_handler, ""."", recursive=True)
    observer.start()
    try:
        while True:
            time.sleep(1)
    finally:
        observer.stop()
        observer.join()


Shell Utilities
---------------

Watchdog comes with an *optional* utility script called ``watchmedo``.
Please type ``watchmedo --help`` at the shel"
aws-sam-cli,"<p align=""center"">
</p>

# AWS SAM CLI

![Apache 2.0 License](https://img.shields.io/github/license/aws/aws-sam-cli)
![SAM CLI Version](https://img.shields.io/github/release/aws/aws-sam-cli.svg?label=CLI%20Version)
![Install](https://img.shields.io/badge/brew-aws/tap/aws--sam--cli-orange)
![pip](https://img.shields.io/badge/pip-aws--sam--cli-9cf)

[Installation](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html) | [Blogs](https://serverlessland.com/blog?tag=AWS%20SAM) | [Videos](https://serverlessland.com/video?tag=AWS%20SAM) | [AWS Docs](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html) | [Roadmap](https://github.com/aws/aws-sam-cli/wiki/SAM-CLI-Roadmap) | [Try It Out](https://s12d.com/jKo46elk) | [Slack Us](https://join.slack.com/t/awsdevelopers/shared_invite/zt-yryddays-C9fkWrmguDv0h2EEDzCqvw)

The AWS Serverless Application Model (SAM) CLI is an open-source CLI tool that help"
PyTorch-VAE,"<h1 align=""center"">
  <b>PyTorch VAE</b><br>
</h1>

<p align=""center"">
      <a href=""https://www.python.org/"">
        <img src=""https://img.shields.io/badge/Python-3.5-ff69b4.svg"" /></a>
       <a href= ""https://pytorch.org/"">
        <img src=""https://img.shields.io/badge/PyTorch-1.3-2BAF2B.svg"" /></a>
       <a href= ""https://github.com/AntixK/PyTorch-VAE/blob/master/LICENSE.md"">
        <img src=""https://img.shields.io/badge/license-Apache2.0-blue.svg"" /></a>
         <a href= ""https://twitter.com/intent/tweet?text=PyTorch-VAE:%20Collection%20of%20VAE%20models%20in%20PyTorch.&url=https://github.com/AntixK/PyTorch-VAE"">
        <img src=""https://img.shields.io/twitter/url/https/shields.io.svg?style=social"" /></a>

</p>

**Update 22/12/2021:** Added support for PyTorch Lightning 1.5.6 version and cleaned up the code.

A collection of Variational AutoEncoders (VAEs) implemented in pytorch with focus on reproducibility. The aim of this project is to provide
a quick and simple working "
TikTokDownload,"
![项目图](https://tvax2.sinaimg.cn/large/006908GAly1hgn9zod1yuj30zk0hstmf.jpg)

<h1 align=""center"">✨ 抖音去水印作品下载 ✨</h1>
<div align=""center"">

[English](README-EN.md) | 简体中文

[![License: MIT](https://img.shields.io/github/license/johnserf-seed/tiktokdownload?style=for-the-badge)](https://github.com/Johnserf-Seed/TikTokDownload/blob/main/LICENSE)
![Release Download](https://img.shields.io/github/downloads/Johnserf-Seed/TikTokDownload/total?style=for-the-badge)
![GitHub Repo size](https://img.shields.io/github/repo-size/Johnserf-Seed/TikTokDownload?style=for-the-badge&color=3cb371)
[![GitHub Repo Languages](https://img.shields.io/github/languages/top/Johnserf-Seed/TikTokDownload?style=for-the-badge)](https://github.com/BeyondDimension/SteamTools/search?l=c%23)
[![Python v3.11.1](https://img.shields.io/badge/python-v3.11.1-orange?style=for-the-badge)](https://github.com/Johnserf-Seed/TikTokDownload)
![Terminal: wt](https://img.shields.io/badge/Terminal-wt-blue?style=for-the-badge)

[![GitHub S"
boto,"####
Deprecation notice
####

**This package is no longer maintained and has been replaced by** `Boto3 <https://github.com/boto/boto3>`__.
**Issues and pull requests are not reviewed. If you are having an issue with the** `Boto3 <https://github.com/boto/boto3>`__ **package or the** `AWS CLI <https://github.com/aws/aws-cli>`__, **please open an issue on their respective repositories.**

####
boto
####
boto 2.49.0

Released: 11-July-2018

.. image:: https://pypip.in/d/boto/badge.svg
        :target: https://pypi.python.org/pypi/boto/


************
Introduction
************

Boto is a Python package that provides interfaces to Amazon Web Services.
Currently, all features work with Python 2.6 and 2.7. Work is under way to
support Python 3.3+ in the same codebase. Modules are being ported one at
a time with the help of the open source community, so please check below
for compatibility with Python 3.3+.

To port a module to Python 3.3+, please view our `Contributing Guidelines`_
and the `Po"
point-e,"# Point·E

![Animation of four 3D point clouds rotating](point_e/examples/paper_banner.gif)

This is the official code and model release for [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751).

# Usage

Install with `pip install -e .`.

To get started with examples, see the following notebooks:

 * [image2pointcloud.ipynb](point_e/examples/image2pointcloud.ipynb) - sample a point cloud, conditioned on some example synthetic view images.
 * [text2pointcloud.ipynb](point_e/examples/text2pointcloud.ipynb) - use our small, worse quality pure text-to-3D model to produce 3D point clouds directly from text descriptions. This model's capabilities are limited, but it does understand some simple categories and colors.
 * [pointcloud2mesh.ipynb](point_e/examples/pointcloud2mesh.ipynb) - try our SDF regression model for producing meshes from point clouds.

For our P-FID and P-IS evaluation scripts, see:

 * [evaluate_pfid.py](point_e/evals/scr"
ngxtop,"================================================================
``ngxtop`` - **real-time** metrics for nginx server (and others)
================================================================

**ngxtop** parses your nginx access log and outputs useful, ``top``-like, metrics of your nginx server.
So you can tell what is happening with your server in real-time.

    ``ngxtop`` is designed to run in a short-period time just like the ``top`` command for troubleshooting and monitoring
    your Nginx server at the moment. If you need a long running monitoring process or storing your webserver stats in external
    monitoring / graphing system, you can try `Luameter <https://luameter.com>`_.

``ngxtop`` tries to determine the correct location and format of nginx access log file by default, so you can just run
``ngxtop`` and having a close look at all requests coming to your nginx server. But it does not limit you to nginx
and the default top view. ``ngxtop`` is flexible enough for you to c"
marimo,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg"">
</p>

<p align=""center"">
  <em>A reactive Python notebook that's reproducible, git-friendly, and deployable as scripts or apps.</em>

<p align=""center"">
  <a href=""https://docs.marimo.io"" target=""_blank""><strong>Docs</strong></a> ·
  <a href=""https://discord.gg/JE7nhX6mD8"" target=""_blank""><strong>Discord</strong></a> ·
  <a href=""https://github.com/marimo-team/marimo/tree/main/examples"" target=""_blank""><strong>Examples</strong></a>
</p>

<p align=""center"">
  <b>English | </b>
  <a href=""https://github.com/marimo-team/marimo/blob/main/README_Chinese.md"" target=""_blank""><b>简体中文</b></a>
</p>

<p align=""center"">
<a href=""https://pypi.org/project/marimo/""><img src=""https://img.shields.io/pypi/v/marimo?color=%2334D058&label=pypi"" /></a>
<a href=""https://anaconda.org/conda-forge/marimo""><img src=""https://img.shields.io/conda/vn/conda-forge/marimo.svg""/></a>
<a href"
grip,"Grip -- GitHub Readme Instant Preview
=====================================

[![Current version on PyPI](http://img.shields.io/pypi/v/grip.svg)][pypi]
[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/joeyespo)

Render local readme files before sending off to GitHub.

Grip is a command-line server application written in Python that uses the
[GitHub markdown API][markdown] to render a local readme file. The styles
and rendering come directly from GitHub, so you'll know exactly how it will appear.
Changes you make to the Readme will be instantly reflected in the browser without
requiring a page refresh.


Motivation
----------

Sometimes you just want to see the exact readme
result before committing and pushing to GitHub.

Especially when doing [Readme-driven development][rdd].


Installation
------------

To install grip, simply:

```console
$ pip install grip
```

On OS X, you can also install with Homebrew:

```console
$ brew install grip"
buku,"<h1 align=""center"">buku</h1>

<p align=""center"">
<a href=""https://github.com/jarun/buku/releases/latest""><img src=""https://img.shields.io/github/release/jarun/buku.svg?maxAge=600"" alt=""Latest release"" /></a>
<a href=""https://repology.org/project/buku/versions""><img src=""https://repology.org/badge/tiny-repos/buku.svg?header=repos"" alt=""Availability""></a>
<a href=""https://pypi.org/project/buku/""><img src=""https://img.shields.io/pypi/v/buku.svg?maxAge=600"" alt=""PyPI"" /></a>
<a href=""https://circleci.com/gh/jarun/workflows/buku""><img src=""https://img.shields.io/circleci/project/github/jarun/buku.svg"" alt=""Build Status"" /></a>
<a href=""https://buku.readthedocs.io/en/latest/?badge=latest""><img src=""https://readthedocs.org/projects/buku/badge/?version=latest"" alt=""Docs Status"" /></a>
<a href=""https://en.wikipedia.org/wiki/Privacy-invasive_software""><img src=""https://img.shields.io/badge/privacy-✓-crimson"" alt=""Privacy Awareness"" /></a>
<a href=""https://github.com/jarun/buku/blob/master/LICENS"
pyinstrument,"pyinstrument
============

[![PyPI version](https://badge.fury.io/py/pyinstrument.svg)](https://badge.fury.io/py/pyinstrument)
[![.github/workflows/test.yml](https://github.com/joerick/pyinstrument/actions/workflows/test.yml/badge.svg)](https://github.com/joerick/pyinstrument/actions/workflows/test.yml)
[![Build wheels](https://github.com/joerick/pyinstrument/actions/workflows/wheels.yml/badge.svg)](https://github.com/joerick/pyinstrument/actions/workflows/wheels.yml)

[Documentation](https://pyinstrument.readthedocs.io/)

<!-- MARK intro start -->

[![Screenshot](https://github.com/joerick/pyinstrument/raw/main/docs/img/screenshot.jpg)](https://github.com/joerick/pyinstrument/raw/main/docs/img/screenshot.jpg)

Pyinstrument is a Python profiler. A profiler is a tool to help you optimize
your code - make it faster. To get the biggest speed increase you should
[focus on the slowest part of your program](https://en.wikipedia.org/wiki/Amdahl%27s_law).
Pyinstrument helps you find it!

> ☕️ "
sqlglot,"![SQLGlot logo](sqlglot.png)

SQLGlot is a no-dependency SQL parser, transpiler, optimizer, and engine. It can be used to format SQL or translate between [23 different dialects](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/__init__.py) like [DuckDB](https://duckdb.org/), [Presto](https://prestodb.io/) / [Trino](https://trino.io/), [Spark](https://spark.apache.org/) / [Databricks](https://www.databricks.com/), [Snowflake](https://www.snowflake.com/en/), and [BigQuery](https://cloud.google.com/bigquery/). It aims to read a wide variety of SQL inputs and output syntactically and semantically correct SQL in the targeted dialects.

It is a very comprehensive generic SQL parser with a robust [test suite](https://github.com/tobymao/sqlglot/blob/main/tests/). It is also quite [performant](#benchmarks), while being written purely in Python.

You can easily [customize](#custom-dialects) the parser, [analyze](#metadata) queries, traverse expression trees, and programmatically [bu"
metaseq,"

# Metaseq
A codebase for working with [Open Pre-trained Transformers](projects/OPT), originally forked from [fairseq](https://github.com/facebookresearch/fairseq).


## Community Integrations

### Using OPT with 🤗 Transformers

The OPT 125M--66B models are now available in [Hugging Face Transformers](https://github.com/huggingface/transformers/releases/tag/v4.19.0). You can access them under the `facebook` organization on the [Hugging Face Hub](https://huggingface.co/facebook)

### Using OPT-175B with Alpa

The OPT 125M--175B models are now supported in the [Alpa project](https://alpa-projects.github.io/tutorials/opt_serving.html), which 
enables serving OPT-175B with more flexible parallelisms on older generations of GPUs, such as 40GB A100, V100, T4, M60, etc.

### Using OPT with Colossal-AI

The OPT models are now supported in the [Colossal-AI](https://github.com/hpcaitech/ColossalAI#OPT), which helps users to efficiently and quickly deploy OPT models training and inference, reduc"
isort,"[![isort - isort your imports, so you don't have to.](https://raw.githubusercontent.com/pycqa/isort/main/art/logo_large.png)](https://pycqa.github.io/isort/)

------------------------------------------------------------------------

[![PyPI version](https://badge.fury.io/py/isort.svg)](https://badge.fury.io/py/isort)
[![Test Status](https://github.com/pycqa/isort/workflows/Test/badge.svg?branch=develop)](https://github.com/pycqa/isort/actions?query=workflow%3ATest)
[![Lint Status](https://github.com/pycqa/isort/workflows/Lint/badge.svg?branch=develop)](https://github.com/pycqa/isort/actions?query=workflow%3ALint)
[![Code coverage Status](https://codecov.io/gh/pycqa/isort/branch/main/graph/badge.svg)](https://codecov.io/gh/pycqa/isort)
[![License](https://img.shields.io/github/license/mashape/apistatus.svg)](https://pypi.org/project/isort/)
[![Join the chat at https://gitter.im/timothycrosley/isort](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/timothycrosley/isort?utm_so"
jrnl,"<!--
Copyright © 2012-2023 jrnl contributors
License: https://www.gnu.org/licenses/gpl-3.0.html
-->

<p align=""center"">
<a href=""https://jrnl.sh"">
<img align=""center"" src=""https://raw.githubusercontent.com/jrnl-org/jrnl/develop/docs_theme/assets/readme-header.png""/>
</a>
</p>

jrnl
 [![Testing](https://github.com/jrnl-org/jrnl/workflows/Testing/badge.svg)](https://github.com/jrnl-org/jrnl/actions?query=workflow%3ATesting)
 [![Downloads](https://pepy.tech/badge/jrnl)](https://pepy.tech/project/jrnl)
 [![Version](http://img.shields.io/pypi/v/jrnl.svg?style=flat)](https://pypi.python.org/pypi/jrnl/)
 [![Homebrew](https://img.shields.io/homebrew/v/jrnl?style=flat-square)](https://formulae.brew.sh/formula/jrnl)
 [![Gitter](https://img.shields.io/gitter/room/jrnl-org/jrnl)](https://gitter.im/jrnl-org/jrnl)
 [![Changelog](https://img.shields.io/badge/changelog-on%20github-green)](https://github.com/jrnl-org/jrnl/blob/develop/CHANGELOG.md)
====

_To get help, [submit an issue](https://github.c"
uiautomator2,"# uiautomator2
[![PyPI](https://img.shields.io/pypi/v/uiautomator2.svg)](https://pypi.python.org/pypi/uiautomator2)
![PyPI](https://img.shields.io/pypi/pyversions/uiautomator2.svg)
[![codecov](https://codecov.io/gh/openatx/uiautomator2/graph/badge.svg?token=d0ZLkqorBu)](https://codecov.io/gh/openatx/uiautomator2)

QQ交流群: **815453846**
Discord: <https://discord.gg/PbJhnZJKDd>

> 有段时间没有维护这个项目了（可能有两年了），但是最近工作需要又重新研究一下Android原生自动化，当然又调研了Appium，对比下来一看，发现uiautomator2这个项目的运行速度是真的好快，从检测元素到点击，都是毫秒级的，代码也比较好理解。真是没想到以前竟然写出了这么神奇的项目，这么好的项目怎么能让它落灰呢，得好好整一整，一些垃圾代码清理清理。所以项目版本从2.x.x升级到了3.x.x

还在用2.x.x版本的用户，可以先看一下[2to3](docs/2to3.md) 再决定是否要升级3.x.x （我个人还是非常建议升级的）

2到3毕竟是大版本升级，很多的函数删掉了。首先删掉的就是atx-agent，其次还有一堆atx-agent相关的函数。废弃的功能比如init.

各种依赖库的版本号

- [![PyPI](https://img.shields.io/pypi/v/uiautomator2.svg?label=uiautomator2)](https://pypi.python.org/pypi/uiautomator2)
- [![PyPI](https://img.shields.io/pypi/v/adbutils.svg?label=adbutils)](https://github.com/openatx/adbutils)
- [![GitHub tag (latest SemVer)](h"
Time-Series-Library,"# Time Series Library (TSLib)
TSLib is an open-source library for deep learning researchers, especially for deep time series analysis.

We provide a neat code base to evaluate advanced deep time series models or develop your model, which covers five mainstream tasks: **long- and short-term forecasting, imputation, anomaly detection, and classification.**

:triangular_flag_on_post:**News** (2024.07) We wrote a comprehensive survey of [[Deep Time Series Models]](https://arxiv.org/abs/2407.13278) with a rigorous benchmark based on TSLib. In this paper, we summarized the design principles of current time series models supported by insightful experiments, hoping to be helpful to future research.

:triangular_flag_on_post:**News** (2024.04) Many thanks for the great work from [frecklebars](https://github.com/thuml/Time-Series-Library/pull/378). The famous sequential model [Mamba](https://arxiv.org/abs/2312.00752) has been included in our library. See [this file](https://github.com/thuml/Time"
krita-ai-diffusion,"<h1><img width=""64px"" src=""ai_diffusion/icons/logo-128.png""> Generative AI <i>for Krita</i></h1>

✨[Features](#features) | ⭳ [Download](https://github.com/Acly/krita-ai-diffusion/releases/latest) | 🛠️[Installation](https://www.interstice.cloud/plugin) | 🎞️ [Video](https://youtu.be/Ly6USRwTHe0) | 🖼️[Gallery](#gallery) | 📖[Wiki](https://github.com/Acly/krita-ai-diffusion/wiki) | 💬[Discussion](https://github.com/Acly/krita-ai-diffusion/discussions) | 🗣️[Discord](https://discord.gg/pWyzHfHHhU)

This is a plugin to use generative AI in image painting and editing workflows from within Krita. For a more visual introduction, see [**www.interstice.cloud**](https://www.interstice.cloud)

The main goals of this project are:
* **Precision and Control.** Creating entire images from text can be unpredictable.
  To get the result you envision, you can restrict generation to selections,
  refine existing content with a variable degree of strength, focus text on image
  regions, and guide generation wi"
