repo_name,readme_content
public-apis,"# Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

<br >

<p>
    <a href=""https://apilayer.com"">
        <div>
            <img src="".github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png"" width=""100%"" alt=""APILayer Logo"" />
        </div>
    </a>
  </p>

[APILayer](https://apilayer.com/?utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo) is the fastest way to integrate APIs into any product. There are a lot of APIs available at [APILayer Marketplace](https://apilayer.com/#bestSellers&utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo).

<br >

## APILayer APIs
| API | Description | Call this"
system-design-primer,*[English](README.md) âˆ™ [æ—¥æœ¬èª](README-ja.md) âˆ™ [ç®€ä½“ä¸­æ–‡](README-zh-Hans.md) âˆ™ [ç¹é«”ä¸­æ–‡](README-zh-TW.md) | [Ø§Ù„Ø¹ÙØ±ÙØ¨ÙÙŠÙÙ‘Ø©â€](https://github.com/donnemartin/system-design-primer/issues/170) âˆ™ [à¦¬à¦¾à¦‚à¦²à¦¾](https://github.com/donnemartin/system-design-primer/issues/220) âˆ™ [PortuguÃªs do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) âˆ™ [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) âˆ™ [ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](https://github.com/donnemartin/system-design-primer/issues/130) âˆ™ [×¢×‘×¨×™×ª](https://github.com/donnemartin/system-design-primer/issues/272) âˆ™ [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) âˆ™ [í•œêµ­ì–´](https://github.com/donnemartin/system-design-primer/issues/102) âˆ™ [ÙØ§Ø±Ø³ÛŒ](https://github.com/donnemartin/system-design-primer/issues/110) âˆ™ [Polski](https://github.com/donnemartin/system-design-primer/issues/68) âˆ™ [Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](https://github.com/donnemartin/system-design-primer/issues/87) âˆ™ [EspaÃ±ol](https://github.com/donnemartin/system-desig
awesome-python,"# Awesome Python [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

An opinionated list of awesome Python frameworks, libraries, software and resources.

Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).

- [Awesome Python](#awesome-python)
    - [Admin Panels](#admin-panels)
    - [Algorithms and Design Patterns](#algorithms-and-design-patterns)
    - [ASGI Servers](#asgi-servers)
    - [Asynchronous Programming](#asynchronous-programming)
    - [Audio](#audio)
    - [Authentication](#authentication)
    - [Build Tools](#build-tools)
    - [Built-in Classes Enhancement](#built-in-classes-enhancement)
    - [Caching](#caching)
    - [ChatOps Tools](#chatops-tools)
    - [CMS](#cms)
    - [Code Analysis](#code-analysis)
    - [Command-line Interface Development](#command-line-interface-development)
    - [Command-line Tools](#command-line-tools)
    - [Computer Visio"
Python,"<div align=""center"">
<!-- Title: -->
  <a href=""https://github.com/TheAlgorithms/"">
    <img src=""https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg"" height=""100"">
  </a>
  <h1><a href=""https://github.com/TheAlgorithms/"">The Algorithms</a> - Python</h1>
<!-- Labels: -->
  <!-- First row: -->
  <a href=""https://gitpod.io/#https://github.com/TheAlgorithms/Python"">
    <img src=""https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&style=flat-square"" height=""20"" alt=""Gitpod Ready-to-Code"">
  </a>
  <a href=""https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md"">
    <img src=""https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square"" height=""20"" alt=""Contributions Welcome"">
  </a>
  <img src=""https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&style=flat-square"" height=""20"">
  <a href=""https://the-algorithms.com/discord"">
 "
AutoGPT,"# AutoGPT: Build, Deploy, and Run AI Agents

[![Discord Follow](https://dcbadge.vercel.app/api/server/autogpt?style=flat)](https://discord.gg/autogpt) &ensp;
[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. 

## Hosting Options 
   - Download to self-host
   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta  

## How to Setup for Self-Hosting
> [!NOTE]
> Setting up and hosting the AutoGPT Platform yourself is a technical process. 
> If you'd rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.

https://github.com/user-attachments/assets/d04273a5-b36a-4a37-818e-f631ce72d603

This tutorial as"
Python-100-Days,"## Python - 100å¤©ä»æ–°æ‰‹åˆ°å¤§å¸ˆ

> **ä½œè€…**ï¼šéª†æ˜Š
>
> **è¯´æ˜**ï¼šä»é¡¹ç›®ä¸Šçº¿åˆ°è·å¾—8w+æ˜Ÿæ ‡ä»¥æ¥ï¼Œä¸€ç›´æ”¶åˆ°åé¦ˆè¯´åŸºç¡€éƒ¨åˆ†ï¼ˆå‰15å¤©çš„å†…å®¹ï¼‰å¯¹æ–°æ‰‹æ¥è¯´æ˜¯æ¯”è¾ƒå›°éš¾çš„ï¼Œå»ºè®®æœ‰é…å¥—è§†é¢‘è¿›è¡Œè®²è§£ã€‚æœ€è¿‘æŠŠåŸºç¡€éƒ¨åˆ†çš„å†…å®¹é‡æ–°åˆ¶ä½œäº†ä¸€ä¸ªåä¸º[â€œPython-Core-50-Coursesâ€](<https://github.com/jackfrued/Python-Core-50-Courses>)çš„é¡¹ç›®ï¼Œç”¨æ›´ä¸ºç®€å•é€šä¿—çš„æ–¹å¼é‡å†™äº†è¿™éƒ¨åˆ†å†…å®¹å¹¶é™„å¸¦äº†è§†é¢‘è®²è§£ï¼Œåˆå­¦è€…å¯ä»¥çœ‹çœ‹è¿™ä¸ªæ–°çš„ä»“åº“ã€‚å›½å†…ç”¨æˆ·å¦‚æœè®¿é—®GitHubæ¯”è¾ƒæ…¢çš„è¯ï¼Œå¯ä»¥å…³æ³¨æˆ‘çš„**çŸ¥ä¹å·[Python-Jack](https://www.zhihu.com/people/jackfrued)**ï¼Œä¸Šé¢çš„[â€œä»é›¶å¼€å§‹å­¦Pythonâ€](<https://zhuanlan.zhihu.com/c_1216656665569013760>)ä¸“æ æ¯”è¾ƒé€‚åˆåˆå­¦è€…ï¼Œå…¶ä»–çš„ä¸“æ å¦‚â€œæ•°æ®æ€ç»´å’Œç»Ÿè®¡æ€ç»´â€ã€â€œåŸºäºPythonçš„æ•°æ®åˆ†æâ€ç­‰ä¹Ÿåœ¨æŒç»­åˆ›ä½œå’Œæ›´æ–°ä¸­ï¼Œæ¬¢è¿å¤§å®¶å…³æ³¨ã€ç‚¹èµå’Œè¯„è®ºã€‚
>
> æƒ³è·å–å­¦ä¹ è§†é¢‘çš„å°ä¼™ä¼´ï¼Œå¤§å®¶å¯ä»¥æ‰«æä¸‹é¢çš„äºŒç»´ç è¿›å…¥å¾®ä¿¡å°ç¨‹åºï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰é€‚åˆè‡ªå·±çš„å†…å®¹ã€‚å¤§å®¶å¿ƒå¿ƒå¿µå¿µçš„æœºå™¨å­¦ä¹ çš„å†…å®¹åœ¨å°ç¨‹åºä¸­éƒ½å¯ä»¥æ‰¾åˆ°ï¼Œç”±æˆ‘å’Œæˆ‘çš„åŒäº‹ä¸ºå¤§å®¶å½•åˆ¶çš„ã€‚
>
> <img src=""res/study_card.png"" style=""zoom:20%;"">
>
> å¤§å®¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¦‚æœé‡åˆ°ä¸€äº›æ£˜æ‰‹çš„é—®é¢˜æˆ–è€…éœ€è¦ç›¸å…³çš„å­¦ä¹ èµ„æºï¼Œå¯ä»¥åŠ å…¥ä¸‹é¢çš„QQäº¤æµç¾¤ï¼Œä¸‰ä¸ªç¾¤æ˜¯ä¸€æ ·çš„åŠ å…¥ä¸€ä¸ªå³å¯ï¼Œè¯·ä¸è¦é‡å¤åŠ ç¾¤ï¼Œä¹Ÿä¸è¦åœ¨ç¾¤é‡Œå‘å¸ƒå¹¿å‘Šå’Œå…¶ä»–è‰²æƒ…ã€ä½ä¿—æˆ–æ•æ„Ÿå†…å®¹ã€‚**å¦‚æœç¼ºä¹è‡ªå¾‹æ€§ï¼Œæœ‰ä»˜è´¹å­¦ä¹ çš„éœ€æ±‚ï¼Œå¯ä»¥æ·»åŠ æˆ‘çš„å¾®ä¿¡ï¼ˆjackfruedï¼‰ç§èŠï¼Œå¤‡æ³¨å¥½è‡ªå·±çš„ç§°å‘¼å’Œéœ€æ±‚ï¼Œæˆ‘ä¼šç»™å¤§å®¶æä¾›ä¸€äº›å­¦ä¹ æ–¹æ¡ˆå’ŒèŒä¸šè§„åˆ’æ–¹é¢çš„æŒ‡å¯¼**ã€‚
>
> <img src=""res/python_study_qq_group.png"" style=""zoom:30%;"">
>
> é…å¥—çš„è§†é¢‘åœ¨æŠ–éŸ³å’ŒBç«™æŒç»­æ›´æ–°ä¸­ï¼Œæœ‰å…´è¶£çš„å°ä¼™ä¼´å¯ä»¥å…³æ³¨æˆ‘çš„æŠ–éŸ³æˆ–Bç«™è´¦å·ï¼Œæœ€è¿‘åˆšåˆšèµ·å·ï¼Œè¿˜å¸Œæœ›å¤§å®¶å¤šå¤šæ”¯æŒï¼Œéå¸¸æ„Ÿè°¢æ‚¨ï¼
>
> <img src=""res/qrcode.JPG"" style=""zoom:20%;"">
>
> å¤§å®¶ä¸€ç›´å‚¬æ›´çš„ã€Šæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ã€‹å› ä¸ªäººå’Œå…¬"
stable-diffusion-webui,"# Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- T"
transformers,"<!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg"">
    <img alt=""Hugging Face Transformers Library"" src="
youtube-dl,"[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI)


youtube-dl - download videos from youtube.com or other video platforms

- [INSTALLATION](#installation)
- [DESCRIPTION](#description)
- [OPTIONS](#options)
- [CONFIGURATION](#configuration)
- [OUTPUT TEMPLATE](#output-template)
- [FORMAT SELECTION](#format-selection)
- [VIDEO SELECTION](#video-selection)
- [FAQ](#faq)
- [DEVELOPER INSTRUCTIONS](#developer-instructions)
- [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)

# INSTALLATION

To install it right away for all UNIX users (Linux, macOS, etc.), type:

    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
    sudo chmod a+rx /usr/local/bin/youtube-dl

If you do not have curl, you can alternatively use a recent wget:

    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
  "
HelloGitHub,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif""/>
  <br>ä¸­æ–‡ | <a href=""README_en.md"">English</a> | <a href=""README_ja.md"">æ—¥æœ¬èª</a>
  <br>åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚<br>å…´è¶£æ˜¯æœ€å¥½çš„è€å¸ˆï¼Œè¿™é‡Œèƒ½å¤Ÿå¸®ä½ æ‰¾åˆ°ç¼–ç¨‹çš„å…´è¶£ï¼
</p>

<p align=""center"">
  <a href=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png""><img src=""https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square"" alt=""WeiXin""></a>
  <a href=""https://github.com/521xueweihan/HelloGitHub/stargazers""><img src=""https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square"" alt=""GitHub stars""></a>
  <a href=""https://github.com/521xueweihan/HelloGitHub/issues""><img src=""https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square"" alt=""GitHub issues""></a>
    <a href=""https://weibo.com/hellogithub""><img src=""https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square"""
thefuck,"# The Fuck [![Version][version-badge]][version-link] [![Build Status][workflow-badge]][workflow-link] [![Coverage][coverage-badge]][coverage-link] [![MIT License][license-badge]](LICENSE.md)

*The Fuck* is a magnificent app, inspired by a [@liamosaur](https://twitter.com/liamosaur/)
[tweet](https://twitter.com/liamosaur/status/506975850596536320),
that corrects errors in previous console commands.


Is *The Fuck* too slow? [Try the experimental instant mode!](#experimental-instant-mode)

[![gif with examples][examples-link]][examples-link]

More examples:

```bash
âœ apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

âœ fuck
sudo apt-get install vim [enter/â†‘/â†“/ctrl+c]
[sudo] password for nvbn:
Reading package lists... Done
...
```

```bash
âœ git push
fatal: The current branch master has no upstream branch.
To push the current branch and set the remote as upstream"
yt-dlp,"<!-- MANPAGE: BEGIN EXCLUDED SECTION -->
<div align=""center"">

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&label=Download&style=for-the-badge)](#installation ""Installation"")
[![PyPi](https://img.shields.io/badge/-PyPi-blue.svg?logo=pypi&labelColor=555555&style=for-the-badge)](https://pypi.org/project/yt-dlp ""PyPi"")
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&labelColor=555555&style=for-the-badge)](Collaborators.md#collaborators ""Donate"")
[![Matrix](https://img.shields.io/matrix/yt-dlp:matrix.org?color=brightgreen&labelColor=555555&label=&logo=element&style=for-the-badge)](https://matrix.to/#/#yt-dlp:matrix.org ""Matrix"")
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&labelColor=555555&label=&logo=discord&style=for-the-badge)](https://discord.gg/H5MNcFW63r ""Discord"")
[![Supported Sites]("
pytorch,"![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)

--------------------------------------------------------------------------------

PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.

Our trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).

<!-- toc -->

- [More About PyTorch](#more-about-pytorch)
  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)
  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)
  - [Python First](#python-first)
  - [Imperative Experiences](#imperative-experiences)
  - [Fast and Lean](#fast-and-lean)
  - [Extensions Without Pain](#ex"
django,"======
Django
======

Django is a high-level Python web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.

All documentation is in the ""``docs``"" directory and online at
https://docs.djangoproject.com/en/stable/. If you're just getting started,
here's how we recommend you read the docs:

* First, read ``docs/intro/install.txt`` for instructions on installing Django.

* Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,
  ``docs/intro/tutorial02.txt``, etc.).

* If you want to set up an actual deployment server, read
  ``docs/howto/deployment/index.txt`` for instructions.

* You'll probably want to read through the topical guides (in ``docs/topics``)
  next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific
  problems, and check out the reference (``docs/ref``) for gory details.

* See ``docs/README`` for instructions on building an HTML version of the docs.

Docs are updated rigorously. If yo"
models,"<div align=""center"">
  <img src=""https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png"">
</div>

[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)
[![tf-models-official PyPI](https://badge.fury.io/py/tf-models-official.svg)](https://badge.fury.io/py/tf-models-official)


# Welcome to the Model Garden for TensorFlow

The TensorFlow Model Garden is a repository with a number of different
implementations of state-of-the-art (SOTA) models and modeling solutions for
TensorFlow users. We aim to demonstrate the best practices for modeling so that
TensorFlow users can take full advantage of TensorFlow for their research and
product development.

To improve the transparency and reproducibility of our models, training logs on
[TensorBoard.dev](https://tensorboard.dev) are also provided for models to the
extent possible though not all models are suitable.

| Directory | Description |
|-----------|-------------|
"
fastapi,"<p align=""center"">
  <a href=""https://fastapi.tiangolo.com""><img src=""https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png"" alt=""FastAPI""></a>
</p>
<p align=""center"">
    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster"" target=""_blank"">
    <img src=""https://github.com/fastapi/fastapi/workflows/Test/badge.svg?event=push&branch=master"" alt=""Test"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg"" alt=""Coverage"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package"" alt=""Package version"">
</a>
<a href=""https://pypi.org/project/fastapi"" target=""_blank"">
    <img src=""https://i"
core,"Home Assistant |Chat Status|
=================================================================================

Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.

Check out `home-assistant.io <https://home-assistant.io>`__ for `a
demo <https://demo.home-assistant.io>`__, `installation instructions <https://home-assistant.io/getting-started/>`__,
`tutorials <https://home-assistant.io/getting-started/automation/>`__ and `documentation <https://home-assistant.io/docs/>`__.

This is a project of the `Open Home Foundation <https://www.openhomefoundation.org/>`__.

|screenshot-states|

Featured integrations
---------------------

|screenshot-integrations|

The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the `section on architecture <https://developers.home-assistant.io/docs/architec"
whisper,"# Whisper

[[Blog]](https://openai.com/blog/whisper)
[[Paper]](https://arxiv.org/abs/2212.04356)
[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)
[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.


## Approach

![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-proc"
funNLP,"<center>
    <img style=""border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"" 
    src=""./data/.logoå›¾ç‰‡/.img.jpg""width=""180"">
    <br>
    <div style=""color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"">NLPæ°‘å·¥çš„ä¹å›­</div>
</center>
<br>

[![](https://img.shields.io/github/stars/fighting41love/funnlp?style=social)](https://github.com/fighting41love/funnlp)
[![](https://img.shields.io/badge/dynamic/json?color=blue&label=%E7%9F%A5%E4%B9%8E%E5%85%B3%E6%B3%A8&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dzhihu%26queryKey%3Dmountain-blue-64)](https://www.zhihu.com/people/mountain-blue-64)
[![](data/.logoå›¾ç‰‡/.æèµ å›¾ç‰‡/.Citations-487-red.svg)](https://scholar.google.com/citations?hl=en&user=aqZdfDUAAAAJ)

[![](data/.logoå›¾ç‰‡/.æèµ å›¾ç‰‡/.Home-%E4%BA%BA%E7%94%9F%E6%B5%AA%E8%B4%B9%E6%8C%87%E5%8D%97-brightgreen.svg)](http://fighting41love.github.io/archives/)
[![]"
flask,"# Flask

Flask is a lightweight [WSGI][] web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around [Werkzeug][]
and [Jinja][], and has become one of the most popular Python web
application frameworks.

Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.

[WSGI]: https://wsgi.readthedocs.io/
[Werkzeug]: https://werkzeug.palletsprojects.com/
[Jinja]: https://jinja.palletsprojects.com/


## A Simple Example

```python
# save this as app.py
from flask import Flask

app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello, World!""
```

```
$ flask run
  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```


## Donate

The Pallets organization develops and sup"
devops-exercises,"<p align=""center""><img src=""images/devops_exercises.png""/></p>

:information_source: &nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE

:bar_chart: &nbsp;There are currently **2624** exercises and questions

:warning: &nbsp;You can use these for preparing for an interview but most of the questions and exercises don't represent an actual interview. Please read [FAQ page](faq.md) for more details

:stop_sign: &nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it's not about learning all the topics and technologies mentioned in this repository

:pencil: &nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines [here](CONTRIBUTING.md)

****

<!-- ALL-TOPICS-LIST:START -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<center>
<table>
  <tr>
    <td align=""center""><a href=""t"
awesome-machine-learning,"# Awesome Machine Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![Track Awesome List](https://www.trackawesomelist.com/badge.svg)](https://www.trackawesomelist.com/josephmisiti/awesome-machine-learning/)

A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by `awesome-php`.

_If you want to contribute to this list (please do), send me a pull request or contact me [@josephmisiti](https://twitter.com/josephmisiti)._
Also, a listed repository should be deprecated if:

* Repository's owner explicitly says that ""this library is not maintained"".
* Not committed for a long time (2~3 years).

Further resources:

* For a list of free machine learning books available for download, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md).

* For a list of professional machine learning events, go [h"
gpt_academic,"> [!IMPORTANT]
> 2024.6.1: ç‰ˆæœ¬3.80åŠ å…¥æ’ä»¶äºŒçº§èœå•åŠŸèƒ½ï¼ˆè¯¦è§wikiï¼‰  
> 2024.5.1: åŠ å…¥Doc2xç¿»è¯‘PDFè®ºæ–‡çš„åŠŸèƒ½ï¼Œ[æŸ¥çœ‹è¯¦æƒ…](https://github.com/binary-husky/gpt_academic/wiki/Doc2x)  
> 2024.3.11: å…¨åŠ›æ”¯æŒQwenã€GLMã€DeepseekCoderç­‰ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ SoVitsè¯­éŸ³å…‹éš†æ¨¡å—ï¼Œ[æŸ¥çœ‹è¯¦æƒ…](https://www.bilibili.com/video/BV1Rp421S7tF/) 
> 2024.1.17: å®‰è£…ä¾èµ–æ—¶ï¼Œè¯·é€‰æ‹©`requirements.txt`ä¸­**æŒ‡å®šçš„ç‰ˆæœ¬**ã€‚ å®‰è£…å‘½ä»¤ï¼š`pip install -r requirements.txt`ã€‚æœ¬é¡¹ç›®å®Œå…¨å¼€æºå…è´¹ï¼Œæ‚¨å¯é€šè¿‡è®¢é˜…[åœ¨çº¿æœåŠ¡](https://github.com/binary-husky/gpt_academic/wiki/online)çš„æ–¹å¼é¼“åŠ±æœ¬é¡¹ç›®çš„å‘å±•ã€‚

<br>

<div align=center>
<h1 aligh=""center"">
<img src=""docs/logo.png"" width=""40""> GPT å­¦æœ¯ä¼˜åŒ– (GPT Academic)
</h1>

[![Github][Github-image]][Github-url]
[![License][License-image]][License-url]
[![Releases][Releases-image]][Releases-url]
[![Installation][Installation-image]][Installation-url]
[![Wiki][Wiki-image]][Wiki-url]
[![PR][PRs-image]][PRs-url]

[Github-image]: https://img.shields.io/badge/github-12100E.svg?style=flat-square
[License-image]: https://img.shields.io/github/license/binary-husky/gpt_academic?label=License&style=flat-square&co"
cpython,"This is Python version 3.14.0 alpha 0
=====================================

.. image:: https://github.com/python/cpython/actions/workflows/build.yml/badge.svg?branch=main&event=push
   :alt: CPython build status on GitHub Actions
   :target: https://github.com/python/cpython/actions

.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main
   :alt: CPython build status on Azure DevOps
   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=main

.. image:: https://img.shields.io/badge/discourse-join_chat-brightgreen.svg
   :alt: Python Discourse chat
   :target: https://discuss.python.org/


Copyright Â© 2001-2024 Python Software Foundation.  All rights reserved.

See the end of this file for further copyright and license information.

.. contents::

General Information
-------------------

- Website: https://www.python.org
- Source code: https://github.com/python/cpython
- Issue tracker: https://github.c"
ansible,"[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![A"
manim,"<p align=""center"">
    <a href=""https://github.com/3b1b/manim"">
        <img src=""https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png"">
    </a>
</p>

[![pypi version](https://img.shields.io/pypi/v/manimgl?logo=pypi)](https://pypi.org/project/manimgl/)
[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)
[![Manim Subreddit](https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=ff4301&label=reddit&logo=reddit)](https://www.reddit.com/r/manim/)
[![Manim Discord](https://img.shields.io/discord/581738731934056449.svg?label=discord&logo=discord)](https://discord.com/invite/bYCyhM9Kz2)
[![docs](https://github.com/3b1b/manim/workflows/docs/badge.svg)](https://3b1b.github.io/manim/)

Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.

Note, there are two versions of manim.  This repository began as a personal project by the author of [3Blue1Brown"
d2l-zh,"# åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ï¼ˆDive into Deep Learningï¼ŒD2L.aiï¼‰

[ç¬¬äºŒç‰ˆï¼šzh.D2L.ai](https://zh.d2l.ai)  | [ç¬¬ä¸€ç‰ˆï¼šzh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  å®‰è£…å’Œä½¿ç”¨ä¹¦ä¸­æºä»£ç ï¼š [ç¬¬äºŒç‰ˆ](https://zh.d2l.ai/chapter_installation/index.html) [ç¬¬ä¸€ç‰ˆ](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)

<h5 align=""center""><i>ç†è§£æ·±åº¦å­¦ä¹ çš„æœ€ä½³æ–¹æ³•æ˜¯å­¦ä»¥è‡´ç”¨ã€‚</i></h5>

<p align=""center"">
  <img width=""200""  src=""static/frontpage/_images/eq.jpg"">
  <img width=""200""  src=""static/frontpage/_images/figure.jpg"">
  <img width=""200""  src=""static/frontpage/_images/code.jpg"">
  <img width=""200""  src=""static/frontpage/_images/notebook.gif"">
</p>

æœ¬å¼€æºé¡¹ç›®ä»£è¡¨äº†æˆ‘ä»¬çš„ä¸€ç§å°è¯•ï¼šæˆ‘ä»¬å°†æ•™ç»™è¯»è€…æ¦‚å¿µã€èƒŒæ™¯çŸ¥è¯†å’Œä»£ç ï¼›æˆ‘ä»¬å°†åœ¨åŒä¸€ä¸ªåœ°æ–¹é˜è¿°å‰–æé—®é¢˜æ‰€éœ€çš„æ‰¹åˆ¤æ€§æ€ç»´ã€è§£å†³é—®é¢˜æ‰€éœ€çš„æ•°å­¦çŸ¥è¯†ï¼Œä»¥åŠå®ç°è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„å·¥ç¨‹æŠ€èƒ½ã€‚

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªä¸ºå®ç°ä»¥ä¸‹ç›®æ ‡çš„ç»Ÿä¸€èµ„æºï¼š
1. æ‰€æœ‰äººå‡å¯åœ¨ç½‘ä¸Šå…è´¹è·å–ï¼›
1. æä¾›è¶³å¤Ÿçš„æŠ€æœ¯æ·±åº¦ï¼Œä»è€Œå¸®åŠ©è¯»è€…å®é™…æˆä¸ºæ·±åº¦å­¦ä¹ åº”ç”¨ç§‘å­¦å®¶ï¼šæ—¢ç†è§£æ•°å­¦åŸç†ï¼Œåˆèƒ½å¤Ÿå®ç°å¹¶ä¸æ–­æ”¹è¿›æ–¹æ³•ï¼›
1. åŒ…å«å¯è¿è¡Œçš„ä»£ç ï¼Œä¸ºè¯»è€…å±•ç¤ºå¦‚ä½•åœ¨å®é™…ä¸­è§£å†³é—®é¢˜ã€‚è¿™æ ·ä¸ä»…ç›´æ¥å°†æ•°å­¦å…¬å¼å¯¹åº”æˆå®é™…ä»£ç ï¼Œè€Œä¸”å¯ä»¥ä¿®æ”¹ä»£ç ã€è§‚å¯Ÿç»“æœå¹¶åŠæ—¶è·å–ç»éªŒï¼›
1. å…è®¸æˆ‘ä»¬å’Œæ•´ä¸ªç¤¾åŒºä¸æ–­å¿«é€Ÿè¿­ä»£å†…å®¹ï¼Œä»è€Œç´§è·Ÿä»åœ¨é«˜é€Ÿå‘å±•çš„æ·±åº¦å­¦ä¹ é¢†åŸŸï¼›
1. ç”±åŒ…å«æœ‰å…³æŠ€æœ¯ç»†èŠ‚é—®ç­”çš„è®ºå›ä½œä¸ºè¡¥å……ï¼Œä½¿å¤§å®¶å¯ä»¥ç›¸äº’ç­”ç–‘å¹¶äº¤æ¢ç»éªŒã€‚

<h5 align=""center"">å°†æœ¬ä¹¦ï¼ˆä¸­è‹±æ–‡ç‰ˆï¼‰ç”¨ä½œæ•™ææˆ–å‚è€ƒä¹¦çš„å¤§å­¦</h5>
<p align=""center"">
  <img width=""400""  src"
keras,"# Keras 3: Deep Learning for Humans

Keras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, and PyTorch.
Effortlessly build and train models for computer vision, natural language processing, audio processing,
timeseries forecasting, recommender systems, etc.

- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras
and the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.
- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),
leverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).
- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.

Join nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.


## Installation

### Install "
PayloadsAllTheThings,"# Payloads All The Things 

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !    
I :heart: pull requests :)

You can also contribute with a :beers: IRL, or using the sponsor button 

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.git"
gpt4free,"![248433934-7886223b-c1d1-4260-82aa-da5741f303bb](https://github.com/xtekky/gpt4free/assets/98614666/ea012c87-76e0-496a-8ac4-e2de090cc6c9)

<a href=""https://trendshift.io/repositories/1692"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/1692"" alt=""xtekky%2Fgpt4free | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

---

Written by [@xtekky](https://github.com/xtekky) & maintained by [@hlohaus](https://github.com/hlohaus)

<div id=""top""></div>

> By using this repository or any code related to it, you agree to the [legal notice](https://github.com/xtekky/gpt4free/blob/main/LEGAL_NOTICE.md). The author is **not responsible for the usage of this repository nor endorses it**, nor is the author responsible for any copies, forks, re-uploads made by other users, or anything else related to GPT4Free. This is the author's only account and repository. To prevent impersonation or irresponsible actions, please comply with the GNU GPL license th"
scikit-learn,".. -*- mode: rst -*-

|Azure| |CirrusCI| |Codecov| |CircleCI| |Nightly wheels| |Black| |PythonVersion| |PyPi| |DOI| |Benchmark|

.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main
   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main

.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield
   :target: https://circleci.com/gh/scikit-learn/scikit-learn

.. |CirrusCI| image:: https://img.shields.io/cirrus/github/scikit-learn/scikit-learn/main?label=Cirrus%20CI
   :target: https://cirrus-ci.com/github/scikit-learn/scikit-learn/main

.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9
   :target: https://codecov.io/gh/scikit-learn/scikit-learn

.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=sche"
sherlock,"<p align=center>
  <br>
  <a href=""https://sherlock-project.github.io/"" target=""_blank""><img src=""images/sherlock-logo.png""/></a>
  <br>
  <span>Hunt down social media accounts by username across <a href=""https://sherlockproject.xyz/sites"">400+ social networks</a></span>
  <br>
</p>

<p align=""center"">
  <a href=""https://sherlockproject.xyz/installation"">Installation</a>
  &nbsp;&nbsp;&nbsp;â€¢&nbsp;&nbsp;&nbsp;
  <a href=""https://sherlockproject.xyz/usage"">Usage</a>
  &nbsp;&nbsp;&nbsp;â€¢&nbsp;&nbsp;&nbsp;
  <a href=""https://sherlockproject.xyz/contribute"">Contributing</a>
</p>

<p align=""center"">
<img width=""70%"" height=""70%"" src=""images/demo.png""/>
</a>
</p>


## Installation


| | Command | Notes |
| - | - | - |
| PyPI | `pipx install sherlock-project` | `pip` may be used in place of `pipx` |
| Docker | `docker pull sherlock/sherlock` | |
| Debian family | `apt install sherlock` | Kali, Parrot, Debian Testing and Sid |
| BlackArch | `pacman -S sherlock` |  |
| Homebrew | `brew install"
screenshot-to-code,"# screenshot-to-code

A simple tool to convert screenshots, mockups and Figma designs into clean, functional code using AI. **Now supporting Claude Sonnet 3.5 and GPT-4O!**

https://github.com/abi/screenshot-to-code/assets/23818/6cebadae-2fe3-4986-ac6a-8fb9db030045

Supported stacks:

- HTML + Tailwind
- HTML + CSS
- React + Tailwind
- Vue + Tailwind
- Bootstrap
- Ionic + Tailwind
- SVG

Supported AI models:

- Claude Sonnet 3.5 - Best model!
- GPT-4O - also recommended!
- GPT-4 Turbo (Apr 2024)
- GPT-4 Vision (Nov 2023)
- Claude 3 Sonnet
- DALL-E 3 for image generation

See the [Examples](#-examples) section below for more demos.

We also just added experimental support for taking a video/screen recording of a website in action and turning that into a functional prototype.

![google in app quick 3](https://github.com/abi/screenshot-to-code/assets/23818/8758ffa4-9483-4b9b-bb66-abd6d1594c33)

[Learn more about video here](https://github.com/abi/screenshot-to-code/wiki/Screen-Recording-t"
llama,"## **Note of deprecation**

Thank you for developing with Llama models. As part of the Llama 3.1 release, weâ€™ve consolidated GitHub repos and added some additional repos as weâ€™ve expanded Llamaâ€™s functionality into being an e2e Llama Stack. Please use the following repos going forward:
- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations 
- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations
- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications
-"
localstack,"<p align=""center"">
:zap: We are thrilled to announce the release of <a href=""https://blog.localstack.cloud/2024-08-29-localstack-release-v-3-7-0/"">LocalStack 3.7</a> :zap:
</p>

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/localstack/localstack/master/docs/localstack-readme-banner.svg"" alt=""LocalStack - A fully functional local cloud stack"">
</p>

<p align=""center"">
  <a href=""https://circleci.com/gh/localstack/localstack""><img alt=""CircleCI"" src=""https://img.shields.io/circleci/build/gh/localstack/localstack/master?logo=circleci""></a>
  <a href=""https://coveralls.io/github/localstack/localstack?branch=master""><img alt=""Coverage Status"" src=""https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master""></a>
  <a href=""https://pypi.org/project/localstack/""><img alt=""PyPI Version"" src=""https://img.shields.io/pypi/v/localstack?color=blue""></a>
  <a href=""https://hub.docker.com/r/localstack/localstack""><img alt=""Docker Pulls"" src=""https://img.shields."
annotated_deep_learning_paper_implementations,"[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)
[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/labmlai)

# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)

This is a collection of simple PyTorch implementations of
neural networks and related algorithms.
These implementations are documented with explanations,

[The website](https://nn.labml.ai/index.html)
renders these as side-by-side formatted notes.
We believe these would help you understand these algorithms better.

![Screenshot](https://nn.labml.ai/dqn-light.png)

We are actively maintaining this repo and adding new 
implementations almost weekly.
[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.

## Paper Implementations

#### âœ¨ [Transformers](https://nn.labml.ai/transformers/index.html)

* [Multi-"
private-gpt,"# ğŸ”’ PrivateGPT ğŸ“‘

[![Tests](https://github.com/zylon-ai/private-gpt/actions/workflows/tests.yml/badge.svg)](https://github.com/zylon-ai/private-gpt/actions/workflows/tests.yml?query=branch%3Amain)
[![Website](https://img.shields.io/website?up_message=check%20it&down_message=down&url=https%3A%2F%2Fdocs.privategpt.dev%2F&label=Documentation)](https://docs.privategpt.dev/)
[![Discord](https://img.shields.io/discord/1164200432894234644?logo=discord&label=PrivateGPT)](https://discord.gg/bK6mRVpErU)
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/ZylonPrivateGPT)](https://twitter.com/ZylonPrivateGPT)

![Gradio UI](/fern/docs/assets/ui.png?raw=true)

PrivateGPT is a production-ready AI project that allows you to ask questions about your documents using the power
of Large Language Models (LLMs), even in scenarios without an Internet connection. 100% private, no data leaves your
execution environment at any point.

>[!TIP]
> If you are looking for an **enterprise-ready, fu"
face_recognition,"# Face Recognition

_You can also read a translated version of this file [in Chinese ç®€ä½“ä¸­æ–‡ç‰ˆ](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md) or [in Korean í•œêµ­ì–´](https://github.com/ageitgey/face_recognition/blob/master/README_Korean.md) or [in Japanese æ—¥æœ¬èª](https://github.com/m-i-k-i/face_recognition/blob/master/README_Japanese.md)._

Recognize and manipulate faces from Python or from the command line with
the world's simplest face recognition library.

Built using [dlib](http://dlib.net/)'s state-of-the-art face recognition
built with deep learning. The model has an accuracy of 99.38% on the
[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.

This also provides a simple `face_recognition` command line tool that lets
you do face recognition on a folder of images from the command line!


[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)
[![Build Status](https://github.com"
scrapy,".. image:: https://scrapy.org/img/scrapylogo.png
   :target: https://scrapy.org/
   
======
Scrapy
======

.. image:: https://img.shields.io/pypi/v/Scrapy.svg
   :target: https://pypi.python.org/pypi/Scrapy
   :alt: PyPI Version

.. image:: https://img.shields.io/pypi/pyversions/Scrapy.svg
   :target: https://pypi.python.org/pypi/Scrapy
   :alt: Supported Python Versions

.. image:: https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg
   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu
   :alt: Ubuntu

.. .. image:: https://github.com/scrapy/scrapy/workflows/macOS/badge.svg
   .. :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS
   .. :alt: macOS


.. image:: https://github.com/scrapy/scrapy/workflows/Windows/badge.svg
   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows
   :alt: Windows

.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg
   :target: https://pypi.python.org/pypi/Scrapy
   :al"
open-interpreter,"<h1 align=""center"">â— Open Interpreter</h1>

<p align=""center"">
    <a href=""https://discord.gg/Hvz9Axh84z"">
        <img alt=""Discord"" src=""https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white""/></a>
    <a href=""docs/README_JA.md""><img src=""https://img.shields.io/badge/ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ-æ—¥æœ¬èª-white.svg"" alt=""JA doc""/></a>
    <a href=""docs/README_ZH.md""><img src=""https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡ç‰ˆ-white.svg"" alt=""ZH doc""/></a>
    <a href=""docs/README_ES.md""> <img src=""https://img.shields.io/badge/EspaÃ±ol-white.svg"" alt=""ES doc""/></a>
    <a href=""docs/README_UK.md""><img src=""https://img.shields.io/badge/Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°-white.svg"" alt=""UK doc""/></a>
    <a href=""docs/README_IN.md""><img src=""https://img.shields.io/badge/Hindi-white.svg"" alt=""IN doc""/></a>
    <a href=""LICENSE""><img src=""https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat"" alt=""License""/></a>
    <br>
    <br><a href=""https://0ggfznkwh4j.typeform.com/to/G21i9lJ2"">Get e"
Real-Time-Voice-Cloning,"# Real-Time Voice Cloning
This repository is an implementation of [Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master's thesis](https://matheo.uliege.be/handle/2268.2/6801).

SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.

**Video demonstration** (click the picture):

[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)



### Papers implemented  
| URL | Designation | Title | Implementation source |
| --- | ----------- | ----- | --------------------- |
|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To"
gpt-engineer,"# gpt-engineer

[![GitHub Repo stars](https://img.shields.io/github/stars/gpt-engineer-org/gpt-engineer?style=social)](https://github.com/gpt-engineer-org/gpt-engineer)
[![Discord Follow](https://dcbadge.vercel.app/api/server/8tcDQ89Ej2?style=flat)](https://discord.gg/8tcDQ89Ej2)
[![License](https://img.shields.io/github/license/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/LICENSE)
[![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/issues)
![GitHub Release](https://img.shields.io/github/v/release/gpt-engineer-org/gpt-engineer)
[![Twitter Follow](https://img.shields.io/twitter/follow/antonosika?style=social)](https://twitter.com/antonosika)

gpt-engineer lets you:
- Specify software in natural language
- Sit back and watch as an AI writes and executes the code
- Ask the AI to implement improvements

## Getting Started

### Install gpt-engine"
requests,"# Requests

**Requests** is a simple, yet elegant, HTTP library.

```python
>>> import requests
>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{""authenticated"": true, ...'
>>> r.json()
{'authenticated': True, ...}
```

Requests allows you to send HTTP/1.1 requests extremely easily. Thereâ€™s no need to manually add query strings to your URLs, or to form-encode your `PUT` & `POST` data â€” but nowadays, just use the `json` method!

Requests is one of the most downloaded Python packages today, pulling in around `30M downloads / week`â€” according to GitHub, Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D) by `1,000,000+` repositories. You may certainly put your trust in this code.

[![Downloads](https://static.pepy.tech/badge/requests/month)](https://pepy."
faceswap,"# deepfakes_faceswap
<p align=""center"">
  <a href=""https://faceswap.dev""><img src=""https://i.imgur.com/zHvjHnb.png""></img></a>
<br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.
</p>
<p align=""center"">
<img src = ""https://i.imgur.com/nWHFLDf.jpg""></img>
</p>

<p align=""center"">
<a href=""https://www.patreon.com/bePatron?u=23238350""><img src=""https://c5.patreon.com/external/logo/become_a_patron_button.png""></img></a>
&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://discord.gg/FC54sYg""><img src=""https://i.imgur.com/gIpztkv.png""></img></a></p>

<p align=""center"">
  <a href=""https://www.dailymotion.com/video/x810mot""><img src=""https://user-images.githubusercontent.com/36920800/178301720-b69841bb-a1ca-4c20-91db-a2a10f5692ca.png""></img></a>
<br />Emma Stone/Scarlett Johansson FaceSwap using the Phaze-A model
</p>

<p align=""center"">
  <a href=""https://www.youtube.com/watch?v=r1jng79a5xc""><img src=""https://img.youtube.com/vi/r1jng79a5xc/0.jpg""></im"
ComfyUI,"<div align=""center"">

# ComfyUI
**The most powerful and modular diffusion model GUI and backend.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Matrix][matrix-shield]][matrix-url]
<br>
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&logo=matrix&logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
<!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 -->
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue"
you-get,"# You-Get

[![Build Status](https://github.com/soimort/you-get/workflows/develop/badge.svg)](https://github.com/soimort/you-get/actions)
[![PyPI version](https://img.shields.io/pypi/v/you-get.svg)](https://pypi.python.org/pypi/you-get/)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/soimort/you-get?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

**NOTICE (30 May 2022): Support for Python 3.5, 3.6 and 3.7 will eventually be dropped. ([see details here](https://github.com/soimort/you-get/wiki/TLS-1.3-post-handshake-authentication-(PHA)))**

**NOTICE (8 Mar 2019): Read [this](https://github.com/soimort/you-get/blob/develop/CONTRIBUTING.md) if you are looking for the conventional ""Issues"" tab.**

---

[You-Get](https://you-get.org/) is a tiny command-line utility to download media contents (videos, audios, images) from the Web, in case there is no other handy way to do it.

Here's how you use `you-get` to download a video from [YouTube]("
yolov5,"<div align=""center"">
  <p>
    <a href=""https://ultralytics.com/events/yolovision"" target=""_blank"">
      <img width=""100%"" src=""https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png""></a>
  </p>

[ä¸­æ–‡](https://docs.ultralytics.com/zh) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko) | [æ—¥æœ¬èª](https://docs.ultralytics.com/ja) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [FranÃ§ais](https://docs.ultralytics.com/fr) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar)

<div>
    <a href=""https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml""><img src=""https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg"" alt=""YOLOv5 CI""></a>
    <a href=""https://zenodo.org/badge/latestdoi/264818686""><img src=""https://zenodo."
hackingtool,"### All in One Hacking tool For HackersğŸ¥‡
![](https://img.shields.io/github/license/Z4nzu/hackingtool)
![](https://img.shields.io/github/issues/Z4nzu/hackingtool)
![](https://img.shields.io/github/issues-closed/Z4nzu/hackingtool)
![](https://img.shields.io/badge/Python-3-blue)
![](https://img.shields.io/github/forks/Z4nzu/hackingtool)
![](https://img.shields.io/github/stars/Z4nzu/hackingtool)
![](https://img.shields.io/github/last-commit/Z4nzu/hackingtool)
[![HitCount](http://hits.dwyl.com/Z4nzu/hackingtool.svg)](http://hits.dwyl.com/Z4nzu/hackingtool)
![](https://img.shields.io/badge/platform-Linux%20%7C%20KaliLinux%20%7C%20ParrotOs-blue)

#### Install Kali Linux in WIndows10 Without VirtualBox [YOUTUBE](https://youtu.be/BsFhpIDcd9I) or use Docker

## Update Available V1.2.0 ğŸš€ 
- [âœ”] Installation Bug Fixed
- [x] Added New Tools 
    - [x] Reverse Engineering
    - [x] RAT Tools
    - [x] Web Crawling 
    - [x] Payload Injector
- [x] Multitor Tools update
- [X] Added Tool in wifijammin"
openpilot,"<div align=""center"" style=""text-align: center;"">

<h1>openpilot</h1>

<p>
  <b>openpilot is an operating system for robotics.</b>
  <br>
  Currently, it upgrades the driver assistance system in 275+ supported cars.
</p>

<h3>
  <a href=""https://docs.comma.ai"">Docs</a>
  <span> Â· </span>
  <a href=""https://docs.comma.ai/contributing/roadmap/"">Roadmap</a>
  <span> Â· </span>
  <a href=""https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md"">Contribute</a>
  <span> Â· </span>
  <a href=""https://discord.comma.ai"">Community</a>
  <span> Â· </span>
  <a href=""https://comma.ai/shop"">Try it on a comma 3X</a>
</h3>

Quick start: `bash <(curl -fsSL openpilot.comma.ai)`

![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)
[![codecov](https://codecov.io/gh/commaai/openpilot/branch/master/graph/badge.svg)](https://codecov.io/gh/commaai/openpilot)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Fol"
grok-1,"# Grok-1

This repository contains JAX example code for loading and running the Grok-1 open-weights model.

Make sure to download the checkpoint and place the `ckpt-0` directory in `checkpoints` - see [Downloading the weights](#downloading-the-weights)

Then, run

```shell
pip install -r requirements.txt
python run.py
```

to test the code.

The script loads the checkpoint and samples from the model on a test input.

Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code.
The implementation of the MoE layer in this repository is not efficient. The implementation was chosen to avoid the need for custom kernels to validate the correctness of the model.

# Model Specifications

Grok-1 is currently designed with the following specifications:

- **Parameters:** 314B
- **Architecture:** Mixture of 8 Experts (MoE)
- **Experts Utilization:** 2 experts used per token
- **Layers:** 64
- **Attention Heads:** 48 fo"
rich,"[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rich/13.2.0)](https://pypi.org/project/rich/) [![PyPI version](https://badge.fury.io/py/rich.svg)](https://badge.fury.io/py/rich)

[![Downloads](https://pepy.tech/badge/rich/month)](https://pepy.tech/project/rich)
[![codecov](https://img.shields.io/codecov/c/github/Textualize/rich?label=codecov&logo=codecov)](https://codecov.io/gh/Textualize/rich)
[![Rich blog](https://img.shields.io/badge/blog-rich%20news-yellowgreen)](https://www.willmcgugan.com/tag/rich/)
[![Twitter Follow](https://img.shields.io/twitter/follow/willmcgugan.svg?style=social)](https://twitter.com/willmcgugan)

![Logo](https://github.com/textualize/rich/raw/master/imgs/logo.svg)

[English readme](https://github.com/textualize/rich/blob/master/README.md)
 â€¢ [ç®€ä½“ä¸­æ–‡ readme](https://github.com/textualize/rich/blob/master/README.cn.md)
 â€¢ [æ­£é«”ä¸­æ–‡ readme](https://github.com/textualize/rich/blob/master/README.zh-tw.md)
 â€¢ [Lengua espaÃ±ola readme](https://github"
professional-programming,"<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
## Table of Contents

- [Professional Programming - about this list](#professional-programming---about-this-list)
  - [Principles](#principles)
  - [Contributing to this list](#contributing-to-this-list)
  - [Must-read books](#must-read-books)
  - [Must-read articles](#must-read-articles)
  - [Other general material and list of resources](#other-general-material-and-list-of-resources)
    - [Other lists](#other-lists)
    - [Books](#books)
    - [Articles](#articles)
    - [Axioms](#axioms)
    - [Courses](#courses)
  - [Topics](#topics)
    - [Algorithm and data structures](#algorithm-and-data-structures)
    - [API design & development](#api-design--development)
    - [Attitude, habits, mindset](#attitude-habits-mindset)
      - [Procrastination](#procrastination)
    - [Authentication/authorization](#authenticationauthorization)
    - [A"
big-list-of-naughty-strings,"# Big List of Naughty Strings
The Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer [walks into a bar](http://www.sempf.net/post/On-Testing1).

## Why Test Naughty Strings?

Even multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a [zero-width space](https://en.wikipedia.org/wiki/Zero-width_space) (U+200B) on Twitter:

![](http://i.imgur.com/HyDg2eV.gif)

Although this is not a malicious error, and typical users aren't Tweeting weird unicode, an ""internal server error"" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.

## Usage

`blns.txt` "
MetaGPT,"
# MetaGPT: The Multi-Agent Framework

<p align=""center"">
<a href=""""><img src=""docs/resources/MetaGPT-new-log.png"" alt=""MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks."" width=""150px""></a>
</p>

<p align=""center"">
<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>
</p>

<p align=""center"">
<a href=""docs/README_CN.md""><img src=""https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡ç‰ˆ-blue.svg"" alt=""CN doc""></a>
<a href=""README.md""><img src=""https://img.shields.io/badge/document-English-blue.svg"" alt=""EN doc""></a>
<a href=""docs/README_JA.md""><img src=""https://img.shields.io/badge/ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ-æ—¥æœ¬èª-blue.svg"" alt=""JA doc""></a>
<a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-blue.svg"" alt=""License: MIT""></a>
<a href=""docs/ROADMAP.md""><img src=""https://img.shields.io/badge/ROADMAP-è·¯çº¿å›¾-blue"" alt=""roadmap""></a>
<a href=""https://discord.gg/DYn29wFk9z""><img src=""https://dcbadge.vercel.app/ap"
pandas,"<picture align=""center"">
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://pandas.pydata.org/static/img/pandas_white.svg"">
  <img alt=""Pandas Logo"" src=""https://pandas.pydata.org/static/img/pandas.svg"">
</picture>

-----------------

# pandas: powerful Python data analysis toolkit

| | |
| --- | --- |
| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |
| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads"
PaddleOCR,"[English](README_en.md) | ç®€ä½“ä¸­æ–‡

<p align=""center"">
 <img src=""https://github.com/PaddlePaddle/PaddleOCR/releases/download/v2.8.0/PaddleOCR_logo.png"" align=""middle"" width = ""600""/>
<p align=""center"">
<p align=""center"">
    <a href=""https://discord.gg/z9xaRVjdbD""><img src=""https://img.shields.io/badge/Chat-on%20discord-7289da.svg?sanitize=true"" alt=""Chat""></a>
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleOCR/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleOCR?color=ffa""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""https://pypi.org/project/PaddleOCR/""><img src=""https://img.shields.io/pypi/dm/PaddleOCR?color=9cf""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleOCR/stargazers""><img src=""https://img.shields"
30-Days-Of-Python,"# ğŸ 30 Days Of Python

|# Day | Topics                                                    |
|------|:---------------------------------------------------------:|
| 01  |  [Introduction](./readme.md)|
| 02  |  [Variables, Built-in Functions](./02_Day_Variables_builtin_functions/02_variables_builtin_functions.md)|
| 03  |  [Operators](./03_Day_Operators/03_operators.md)|
| 04  |  [Strings](./04_Day_Strings/04_strings.md)|
| 05  |  [Lists](./05_Day_Lists/05_lists.md)|
| 06  |  [Tuples](./06_Day_Tuples/06_tuples.md)|
| 07  |  [Sets](./07_Day_Sets/07_sets.md)|
| 08  |  [Dictionaries](./08_Day_Dictionaries/08_dictionaries.md)|
| 09  |  [Conditionals](./09_Day_Conditionals/09_conditionals.md)|
| 10  |  [Loops](./10_Day_Loops/10_loops.md)|
| 11  |  [Functions](./11_Day_Functions/11_functions.md)|
| 12  |  [Modules](./12_Day_Modules/12_modules.md)|
| 13  |  [List Comprehension](./13_Day_List_comprehension/13_list_comprehension.md)|
| 14  |  [Higher Order Functions](./14_Day_Higher_order_function"
ChatGLM-6B,"# ChatGLM-6B

<p align=""center"">
   ğŸŒ <a href=""https://chatglm.cn/blog"" target=""_blank"">Blog</a> â€¢ ğŸ¤— <a href=""https://huggingface.co/THUDM/chatglm-6b"" target=""_blank"">HF Repo</a> â€¢ ğŸ¦ <a href=""https://twitter.com/thukeg"" target=""_blank"">Twitter</a> â€¢ ğŸ“„<a href=""https://arxiv.org/pdf/2406.12793"" target=""_blank""> Report </a> <br>
</p>
<p align=""center"">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„  <a href=""https://discord.gg/fK2dz4bg"" target=""_blank"">Discord</a> å’Œ <a href=""resources/WECHAT.md"" target=""_blank"">WeChat</a>
</p>
<p align=""center"">
ğŸ“åœ¨ <a href=""https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9"">æ™ºè°±AIå¼€æ”¾å¹³å°</a> ä½“éªŒå’Œä½¿ç”¨æ›´å¤§è§„æ¨¡çš„ GLM å•†ä¸šæ¨¡å‹ã€‚
</p>

*Read this in [English](README_en.md).*

## GLM-4 å¼€æºæ¨¡å‹å’ŒAPI

æˆ‘ä»¬å·²ç»å‘å¸ƒæœ€æ–°çš„ **GLM-4** å¤§è¯­è¨€å¯¹è¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæœ‰äº†æ–°çš„çªç ´ï¼Œæ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ¸ é“ä½“éªŒæˆ‘ä»¬çš„æœ€æ–°æ¨¡å‹ã€‚

+ [GLM-4 å¼€æºæ¨¡å‹](https://github.com/THUDM/GLM-4) æˆ‘ä»¬å·²ç»å¼€æºäº† GLM-4-9B ç³»åˆ—æ¨¡å‹ï¼Œåœ¨å„é¡¹æŒ‡æ ‡çš„ceæ˜¯ä¸Šæœ‰æ˜æ˜¾æå‡ï¼Œæ¬¢è¿å°è¯•ã€‚
+ [æ™ºè°±æ¸…è¨€](https://chatglm.cn/main/detail?fr=ecology_x) ä½“éªŒæœ€æ–°ç‰ˆ GLM-4ï¼ŒåŒ…æ‹¬ **GLMsï¼ŒAll tools**ç­‰åŠŸèƒ½ã€‚
+ [APIå¹³å°](https://open.bigmodel.cn/?utm_campaign=ope"
Fooocus,"<div align=center>
<img src=""https://github.com/lllyasviel/Fooocus/assets/19834515/483fb86d-c9a2-4c20-997c-46dafc124f25"">
</div>

# Fooocus

[>>> Click Here to Install Fooocus <<<](#download)

Fooocus is an image generating software (based on [Gradio](https://www.gradio.app/) <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>).

Fooocus presents a rethinking of image generator designs. The software is offline, open source, and free, while at the same time, similar to many online image generators like Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images. Fooocus has also simplified the installation: between pressing ""download"" and generating the first image, the number of needed mouse clicks is strictly limited to less than 3. Minimal GPU memory requirement is 4GB (Nvidia).

**Recently many fake websites exist on Google when you search â€œfooocusâ€. Do not trust those â€“ here i"
python-patterns,"python-patterns
===============

A collection of design patterns and idioms in Python.

Remember that each pattern has its own trade-offs. And you need to pay attention more to why you're choosing a certain pattern than to how to implement it.

Current Patterns
----------------

__Creational Patterns__:

| Pattern | Description |
|:-------:| ----------- |
| [abstract_factory](patterns/creational/abstract_factory.py) | use a generic function with specific factories |
| [borg](patterns/creational/borg.py) | a singleton with shared-state among instances |
| [builder](patterns/creational/builder.py) | instead of using multiple constructors, builder object receives parameters and returns constructed objects |
| [factory](patterns/creational/factory.py) | delegate a specialized function/method to create instances |
| [lazy_evaluation](patterns/creational/lazy_evaluation.py) | lazily-evaluated property pattern in Python |
| [pool](patterns/creational/pool.py) | preinstantiate and maintain a g"
text-generation-webui,"# Text generation web UI

A Gradio web UI for Large Language Models.

Its goal is to become the [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) of text generation.

|![Image1](https://github.com/oobabooga/screenshots/raw/main/print_instruct.png) | ![Image2](https://github.com/oobabooga/screenshots/raw/main/print_chat.png) |
|:---:|:---:|
|![Image1](https://github.com/oobabooga/screenshots/raw/main/print_default.png) | ![Image2](https://github.com/oobabooga/screenshots/raw/main/print_parameters.png) |

## Features

* Multiple backends for text generation in a single UI and API, including [Transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/ggerganov/llama.cpp) (through [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)), [ExLlamaV2](https://github.com/turboderp/exllamav2), [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). [AutoAW"
ailearning,"<p align=""center"">
    <a href=""https://www.apachecn.org"">
        <img width=""200"" src=""docs/img/logo.jpg"">
    </a>
    <br >
    <a href=""https://www.apachecn.org/""><img src=""https://img.shields.io/badge/%3E-HOME-green.svg""></a>
    <a href=""https://home.apachecn.org/about/""><img src=""https://img.shields.io/badge/%3E-ABOUT-green.svg""></a>
    <a href=""mailto:apache@163.com""><img src=""https://img.shields.io/badge/%3E-Email-green.svg""></a>
</p>

<h1 align=""center""><a href=""https://github.com/apachecn/AiLearning"">AI learning</a></h1>

> åè®®ï¼š[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)
> 
> ä¸€ç§æ–°æŠ€æœ¯ä¸€æ—¦å¼€å§‹æµè¡Œï¼Œä½ è¦ä¹ˆåä¸Šå‹è·¯æœºï¼Œè¦ä¹ˆæˆä¸ºé“ºè·¯çŸ³ã€‚â€”â€”Stewart Brand

* [åœ¨çº¿é˜…è¯»](https://ailearning.apachecn.org)
* [åœ¨çº¿é˜…è¯»ï¼ˆv1ï¼‰](https://alv1.apachecn.org/)
* [QuantLearning](https://qlearn.apachecn.org/#/)
* [ApacheCN ä¸­æ–‡ç¿»è¯‘ç»„ 713436582](https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt)
* [Apa"
ColossalAI,"# Colossal-AI
<div id=""top"" align=""center"">

   [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)

   Colossal-AI: Making large AI models cheaper, faster, and more accessible

   <h3> <a href=""https://arxiv.org/abs/2110.14883""> Paper </a> |
   <a href=""https://www.colossalai.org/""> Documentation </a> |
   <a href=""https://github.com/hpcaitech/ColossalAI/tree/main/examples""> Examples </a> |
   <a href=""https://github.com/hpcaitech/ColossalAI/discussions""> Forum </a> |
   <a href=""https://cloud.luchentech.com/"">GPU Cloud Playground </a> |
   <a href=""https://hpc-ai.com/blog""> Blog </a></h3>

   [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)
   [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/actions/wor"
black,"[![Black Logo](https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png)](https://black.readthedocs.io/en/stable/)

<h2 align=""center"">The Uncompromising Code Formatter</h2>

<p align=""center"">
<a href=""https://github.com/psf/black/actions""><img alt=""Actions Status"" src=""https://github.com/psf/black/workflows/Test/badge.svg""></a>
<a href=""https://black.readthedocs.io/en/stable/?badge=stable""><img alt=""Documentation Status"" src=""https://readthedocs.org/projects/black/badge/?version=stable""></a>
<a href=""https://coveralls.io/github/psf/black?branch=main""><img alt=""Coverage Status"" src=""https://coveralls.io/repos/github/psf/black/badge.svg?branch=main""></a>
<a href=""https://github.com/psf/black/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://black.readthedocs.io/en/stable/_static/license.svg""></a>
<a href=""https://pypi.org/project/black/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/black""></a>
<a href=""https://pepy.tech/project/black""><img alt=""Downloa"
sentry,"<p align=""center"">
  <p align=""center"">
    <a href=""https://sentry.io/?utm_source=github&utm_medium=logo"" target=""_blank"">
      <img src=""https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png"" alt=""Sentry"" width=""280"" height=""84"" />
    </a>
  </p>
  <p align=""center"">
    Users and logs provide clues. Sentry provides answers.
  </p>
</p>

# What's Sentry?

Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.

<p align=""center"">
  <img src=""https://github.com/getsentry/sentry/raw/master/.github/screenshots/projects.png"" width=""270"" />
  <img src=""https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png"" width=""270"" />
  <img src=""https://github.com/getsentry/sentry/raw/master/.github/screenshots/transaction-summary.png"" width=""270"" />
  <img src=""https://github.com/getsentry/sentry/raw/master/.gith"
stablediffusion,"# Stable Diffusion Version 2
![t2i](assets/stable-samples/txt2img/768/merged-0006.png)
![t2i](assets/stable-samples/txt2img/768/merged-0002.png)
![t2i](assets/stable-samples/txt2img/768/merged-0005.png)

This repository contains [Stable Diffusion](https://github.com/CompVis/stable-diffusion) models trained from scratch and will be continuously updated with
new checkpoints. The following list provides an overview of all currently available models. More coming soon.

## News


**March 24, 2023**

*Stable UnCLIP 2.1*

- New stable diffusion finetune (_Stable unCLIP 2.1_, [Hugging Face](https://huggingface.co/stabilityai/)) at 768x768 resolution,  based on SD2.1-768. This model allows for image variations and mixing operations as described in [*Hierarchical Text-Conditional Image Generation with CLIP Latents*](https://arxiv.org/abs/2204.06125), and, thanks to its modularity, can be combined with other models such as [KARLO](https://github.com/kakaobrain/karlo). Comes in two variants: [*Sta"
cheat.sh,"

![cheat.sh logo](http://cheat.sh/files/big-logo-v2-fixed.png)

Unified access to the best community driven cheat sheets repositories of the world.

Let's imagine for a moment that there is such a thing as an ideal cheat sheet.
What should it look like?
What features should it have?

* **Concise** â€” It should only contain the things you need, and nothing else.
* **Fast** â€” It should be possible to use it instantly.
* **Comprehensive** â€” It should contain answers for every possible question.
* **Universal** â€” It should be available everywhere, anytime, without any preparations.
* **Unobtrusive** â€” It should not distract you from your main task.
* **Tutoring** â€” It should help you to learn the subject.
* **Inconspicuous** â€” It should be possible to use it completely unnoticed.

Such a thing exists! It's easy to [install](#installation) and there's even [auto-complete](#tab-completion).


## Features

**cheat.sh**

* Has a simple curl/browser/editor interface.
* Covers 56 programming lan"
Deep-Learning-Papers-Reading-Roadmap,"# Deep Learning Papers Reading Roadmap

>If you are a newcomer to the Deep Learning area, the first question you may have is ""Which paper should I start reading from?""

>Here is a reading roadmap of Deep Learning papers!

The roadmap is constructed in accordance with the following four guidelines:

- From outline to detail
- From old to state-of-the-art
- from generic to specific areas
- focus on state-of-the-art

You will find many papers that are quite new but really worth reading.

I would continue adding papers to this roadmap.


---------------------------------------

# 1 Deep Learning History and Basics

## 1.0 Book

**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. ""**Deep learning**."" An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:

## 1.1 Survey

**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. ""**Deep learning**."" Nature"
bert,"# BERT

**\*\*\*\*\* New March 11th, 2020: Smaller BERT Models \*\*\*\*\***

This is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).

We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.

Our goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.

You "
odoo,"[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&colorA=8F8F8F)](https://www.odoo.com/documentation/17.0)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo
----

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an <a href=""https://www.odoo.com/page/crm"">Open Source CRM</a>,
<a href=""https://www.odoo.com/app/website"">Website Builder</a>,
<a href=""https://www.odoo.com/app/ecommerce"">eCommerce</a>,
<a href=""https://www.odoo.com/app/inventory"">Warehouse Management</a>,
<a href=""https://www.odoo.com/app/project"">Project Management</a>,
<a href=""https://www.odoo.com/app/accounting"">Billing &amp; Accounting</a>,
<a href=""htt"
Open-Assistant,"<h1 align=""center"">
    <span>Open-Assistant</span>
  <img width=""auto"" height=""50px"" src=""https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png""/>
</h1>

<blockquote>
<p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href=""https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed"">blog post</a> for more information. The final published oasst2 dataset can be found on HuggingFace at <a href=""https://huggingface.co/datasets/OpenAssistant/oasst2"">OpenAssistant/oasst2</a></p>
</blockquote>

<div align=""center"">

<a href=""https://github.com/LAION-AI/Open-Assistant/stargazers"">![GitHub Repo stars](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)</a>
<a href=""https://laion-ai.github.io/Open-Assistant/"">![Docs](https://img.shields.io/badge/docs-laion--ai.github.io%2FOpen--Assistant%2F-green)</a>
<a href=""https://github.com/L"
diagrams,"![diagrams logo](assets/img/diagrams.png)

# Diagrams

[![license](https://img.shields.io/badge/license-MIT-blue.svg)](/LICENSE)
[![pypi version](https://badge.fury.io/py/diagrams.svg)](https://badge.fury.io/py/diagrams)
![python version](https://img.shields.io/badge/python-%3E%3D%203.6-blue?logo=python)
![Run tests](https://github.com/mingrammer/diagrams/workflows/Run%20tests/badge.svg?branch=master)
[![todos](https://badgen.net/https/api.tickgit.com/badgen/github.com/mingrammer/diagrams?label=todos)](https://www.tickgit.com/browse?repo=github.com/mingrammer/diagrams)
![contributors](https://img.shields.io/github/contributors/mingrammer/diagrams)

<a href=""https://www.buymeacoffee.com/mingrammer"" target=""_blank""><img src=""https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png"" alt=""Buy Me A Coffee"" style=""height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;"" ></a>

**Diagram as Code**.

Diagrams lets you draw th"
Deep-Live-Cam,"
![demo-gif](demo.gif)
![demo-gif](avgpcperformancedemo.gif)

## Deep Live Cam

Real-time face swap and video deepfake with a single click and only a single image.

## Disclaimer

This software is intended as a productive contribution to the AI-generated media industry. It aims to assist artists with tasks like animating custom characters or using them as models for clothing, etc.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to law and ethics. We may shut down the project or add watermarks if legally required.

Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user ac"
interview_internal_reference,"
## 2023å¹´æœ€æ–°æ€»ç»“ï¼Œé˜¿é‡Œï¼Œè…¾è®¯ï¼Œç™¾åº¦ï¼Œç¾å›¢ï¼Œå¤´æ¡ç­‰æŠ€æœ¯é¢è¯•é¢˜ç›®ï¼Œä»¥åŠç­”æ¡ˆï¼Œä¸“å®¶å‡ºé¢˜äººåˆ†ææ±‡æ€»ã€‚æŒç»­æ›´æ–°ä¸­ã€‚

* [é˜¿é‡Œç¯‡](#1)
* [åä¸ºç¯‡](#2)
* [ç™¾åº¦ç¯‡](#3)
* [è…¾è®¯ç¯‡](#4)
* [ç¾å›¢ç¯‡](#5)
* [å¤´æ¡ç¯‡](#6)
* [æ»´æ»´ç¯‡](#7)
* [äº¬ä¸œç¯‡](#8)
* [MySQLç¯‡](#9)
* [Redisç¯‡](#10)
* [MongoDBç¯‡](#11)
* [Zookeeperç¯‡](#12)
* [Nginxç¯‡](#13)
* [ç®—æ³•ç¯‡](#14)
* [å†…å­˜ç¯‡](#15)
* [cpuç¯‡](#16)
* [ç£ç›˜ç¯‡](#17)
* [ç½‘ç»œé€šä¿¡ç¯‡](#18)
* [å®‰å…¨ç¯‡](#19)
* [å¹¶å‘ç¯‡](#20)

<h3 id=""1"">é˜¿é‡Œç¯‡</h3> 

---

##### [1.1.1 å¦‚ä½•å®ç°ä¸€ä¸ªé«˜æ•ˆçš„å•å‘é“¾è¡¨é€†åºè¾“å‡ºï¼Ÿ](01.é˜¿é‡Œç¯‡/1.1.1%20%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E7%9A%84%E5%8D%95%E5%90%91%E9%93%BE%E8%A1%A8%E9%80%86%E5%BA%8F%E8%BE%93%E5%87%BA%EF%BC%9F.md)

##### [1.1.2 å·²çŸ¥sqrt(2)çº¦ç­‰äº1.414ï¼Œè¦æ±‚ä¸ç”¨æ•°å­¦åº“ï¼Œæ±‚sqrt(2)ç²¾ç¡®åˆ°å°æ•°ç‚¹å10ä½](01.é˜¿é‡Œç¯‡/1.1.2%20%E5%B7%B2%E7%9F%A5sqrt%282%29%E7%BA%A6%E7%AD%89%E4%BA%8E1.414%EF%BC%8C%E8%A6%81%E6%B1%82%E4%B8%8D%E7%94%A8%E6%95%B0%E5%AD%A6%E5%BA%93%EF%BC%8C%E6%B1%82sqrt%282%29%E7%B2%BE%E7%A1%AE%E5%88%B0%E5%B0%8F%E6%95%B0%E7%82%B9%E5%90%8E10%E4%BD%8D.md)

##### [1.1.3 ç»™å®šä¸€ä¸ªäºŒå‰æœç´¢æ ‘(BST)ï¼Œæ‰¾åˆ°æ ‘ä¸­ç¬¬ K å°çš„èŠ‚ç‚¹](01.é˜¿é‡Œç¯‡/1.1.3%20%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%"
FastChat,"# FastChat
| [**Demo**](https://lmarena.ai/) | [**Discord**](https://discord.gg/HSWAKCrnFx) | [**X**](https://x.com/lmsysorg) |

FastChat is an open platform for training, serving, and evaluating large language model based chatbots.
- FastChat powers Chatbot Arena ([lmarena.ai](https://lmarena.ai)), serving over 10 million chat requests for 70+ LLMs.
- Chatbot Arena has collected over 1.5M human votes from side-by-side LLM battles to compile an online [LLM Elo leaderboard](https://lmarena.ai/?leaderboard).

FastChat's core features include:
- The training and evaluation code for state-of-the-art models (e.g., Vicuna, MT-Bench).
- A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.

## News
- [2024/03] ğŸ”¥ We released Chatbot Arena technical [report](https://arxiv.org/abs/2403.04132).
- [2023/09] We released **LMSYS-Chat-1M**, a large-scale real-world LLM conversation dataset. Read the [report](https://arxiv.org/abs/2309.11998).
- [2023/08] We released"
airflow,"<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 ""License""); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
-->

<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->
# Apache Airflow

[![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)
[![G"
nanoGPT,"
# nanoGPT

![nanoGPT](assets/nanogpt.jpg)

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.

![repro124m](assets/gpt2_124M_loss.png)

Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).

## install

```
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

Dependencies:

- [pytorch](ht"
python-cheatsheet,"Comprehensive Python Cheatsheet
===============================
<sup>[Download text file](https://raw.githubusercontent.com/gto76/python-cheatsheet/main/README.md), [Buy PDF](https://transactions.sendowl.com/products/78175486/4422834F/view), [Fork me on GitHub](https://github.com/gto76/python-cheatsheet) or [Check out FAQ](https://github.com/gto76/python-cheatsheet/wiki/Frequently-Asked-Questions).
</sup>

![Monty Python](web/image_888.jpeg)


Contents
--------
**&nbsp;&nbsp;&nbsp;** **1. Collections:** **&nbsp;** **[`List`](#list)**__,__ **[`Dictionary`](#dictionary)**__,__ **[`Set`](#set)**__,__ **[`Tuple`](#tuple)**__,__ **[`Range`](#range)**__,__ **[`Enumerate`](#enumerate)**__,__ **[`Iterator`](#iterator)**__,__ **[`Generator`](#generator)**__.__  
**&nbsp;&nbsp;&nbsp;** **2. Types:** **&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**  **[`Type`](#type)**__,__ **[`String`](#string)**__,__ **[`Regular_Exp`](#regex)**__,__ **[`Format`](#format)**__,__ **[`Numbers`](#nu"
quivr,"# Quivr - Your Second Brain, Empowered by Generative AI

<div align=""center"">
    <img src=""./logo.png"" alt=""Quivr-logo"" width=""31%""  style=""border-radius: 50%; padding-bottom: 20px""/>
</div>

[![Discord Follow](https://dcbadge.vercel.app/api/server/HUpRgp2HG8?style=flat)](https://discord.gg/HUpRgp2HG8)
[![GitHub Repo stars](https://img.shields.io/github/stars/quivrhq/quivr?style=social)](https://github.com/quivrhq/quivr)
[![Twitter Follow](https://img.shields.io/twitter/follow/StanGirard?style=social)](https://twitter.com/_StanGirard)

Quivr, your second brain, utilizes the power of GenerativeAI to be your personal assistant ! Think of it as Obsidian, but turbocharged with AI capabilities.

[Roadmap here](https://docs.quivr.app/docs/roadmap)

## Key Features ğŸ¯

- **Fast and Efficient**: Designed with speed and efficiency at its core. Quivr ensures rapid access to your data.
- **Secure**: Your data, your control. Always.
- **OS Compatible**: Ubuntu 20 or newer.
- **File Compatibility**"
mitmproxy,"# mitmproxy

[![Continuous Integration Status](https://github.com/mitmproxy/mitmproxy/workflows/CI/badge.svg?branch=main)](https://github.com/mitmproxy/mitmproxy/actions?query=branch%3Amain)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/a38b0325dfb944839c0c8da354f70b1b)](https://app.codacy.com/gh/mitmproxy/mitmproxy/dashboard)
[![autofix.ci: enabled](https://shields.mitmproxy.org/badge/autofix.ci-yes-success?logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSIjZmZmIiB2aWV3Qm94PSIwIDAgMTI4IDEyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCB0cmFuc2Zvcm09InNjYWxlKDAuMDYxLC0wLjA2MSkgdHJhbnNsYXRlKC0yNTAsLTE3NTApIiBkPSJNMTMyNSAtMzQwcS0xMTUgMCAtMTY0LjUgMzIuNXQtNDkuNSAxMTQuNXEwIDMyIDUgNzAuNXQxMC41IDcyLjV0NS41IDU0djIyMHEtMzQgLTkgLTY5LjUgLTE0dC03MS41IC01cS0xMzYgMCAtMjUxLjUgNjJ0LTE5MSAxNjl0LTkyLjUgMjQxcS05MCAxMjAgLTkwIDI2NnEwIDEwOCA0OC41IDIwMC41dDEzMiAxNTUuNXQxODguNSA4MXExNSA5OSAxMDAuNSAxODAuNXQyMTcgMTMwLjV0MjgyLjUgNDlxMTM2IDAgMjU2LjUgLTQ2IHQyMDkgLTEyNy41dDEyOC41IC0xODkuNXExNDk"
wtfpython,"<p align=""center""><img src=""/images/logo.png#gh-light-mode-only"" alt=""""><img src=""/images/logo-dark.png#gh-dark-mode-only"" alt=""""></p>
<h1 align=""center"">What the f*ck Python! ğŸ˜±</h1>
<p align=""center"">Exploring and understanding Python through surprising snippets.</p>


Translations: [Chinese ä¸­æ–‡](https://github.com/leisurelicht/wtfpython-cn) | [Vietnamese Tiáº¿ng Viá»‡t](https://github.com/vuduclyunitn/wtfptyhon-vi) | [Spanish EspaÃ±ol](https://web.archive.org/web/20220511161045/https://github.com/JoseDeFreitas/wtfpython-es) | [Korean í•œêµ­ì–´](https://github.com/buttercrab/wtfpython-ko) | [Russian Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://github.com/satwikkansal/wtfpython/tree/master/translations/ru-russian) | [German Deutsch](https://github.com/BenSt099/wtfpython) | [Add translation](https://github.com/satwikkansal/wtfpython/issues/new?title=Add%20translation%20for%20[LANGUAGE]&body=Expected%20time%20to%20finish:%20[X]%20weeks.%20I%27ll%20start%20working%20on%20it%20from%20[Y].)

Other modes: [Interactive Website](htt"
llama_index,"# ğŸ—‚ï¸ LlamaIndex ğŸ¦™

[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)
[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)
[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)
[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNSIgaGVpZ2h0PSI0IiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxwYXRoIGQ9Ik00LjQzIDEuODgyYTEuNDQgMS40NCAwIDAgMS0uMDk4LjQyNmMtLjA1LjEyMy0uMTE1LjIzLS4xOTIuMzIyLS4wNzUuMDktLjE2LjE2NS0uMjU1LjIyNmExLjM1MyAxLjM1MyAwIDAgMS0uNTk1LjIxMmMtLjA5OS4wMTItLjE5Mi4wMTQtLjI3OS4wMDZsLTEuNTkzLS4xNHYtLjQwNmgxLjY1OGMuMDkuMDAxLjE3LS4xNjkuMjQ2LS4xOTFhLjYwMy42MDMgMCAwIDAgLjItLjEwNi41MjkuNTI5IDAgMCAwIC4xMzgtLjE3LjY1NC42NTQgMCAwIDAgLjA2NS0uMjRsLjAyOC0uMzJhLjkzLjkzIDAgMCAwLS4wMzYtLjI0OS41NjcuNTY3IDAgMCAwLS4xMDMt"
DragGAN,"<p align=""center"">

  <h1 align=""center"">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>
  <p align=""center"">
    <a href=""https://xingangpan.github.io/""><strong>Xingang Pan</strong></a>
    Â·
    <a href=""https://ayushtewari.com/""><strong>Ayush Tewari</strong></a>
    Â·
    <a href=""https://people.mpi-inf.mpg.de/~tleimkue/""><strong>Thomas LeimkÃ¼hler</strong></a>
    Â·
    <a href=""https://lingjie0206.github.io/""><strong>Lingjie Liu</strong></a>
    Â·
    <a href=""https://www.meka.page/""><strong>Abhimitra Meka</strong></a>
    Â·
    <a href=""http://www.mpi-inf.mpg.de/~theobalt/""><strong>Christian Theobalt</strong></a>
  </p>
  <h2 align=""center"">SIGGRAPH 2023 Conference Proceedings</h2>
  <div align=""center"">
    <img src=""DragGAN.gif"", width=""600"">
  </div>

  <p align=""center"">
  <br>
    <a href=""https://pytorch.org/get-started/locally/""><img alt=""PyTorch"" src=""https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white""></a>
  "
GFPGAN,"<p align=""center"">
  <img src=""assets/gfpgan_logo.png"" height=130>
</p>

## <div align=""center""><b><a href=""README.md"">English</a> | <a href=""README_CN.md"">ç®€ä½“ä¸­æ–‡</a></b></div>

<div align=""center"">
<!-- <a href=""https://twitter.com/_Xintao_"" style=""text-decoration:none;"">
    <img src=""https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png"" width=""4%"" alt="""" />
</a> -->

[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)
[![PyPI](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)
[![Open issue](https://img.shields.io/github/issues/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)
[![Closed issue](https://img.shields.io/github/issues-closed/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)
[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/TencentARC/GFPGAN/blob/master/LIC"
MockingBird,"> ğŸš§ While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. Join me at https://discord.gg/wrAGwSH5 .
>
![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)


[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)

> English | [ä¸­æ–‡](README-CN.md)| [ä¸­æ–‡Linux](README-LINUX-CN.md)

## Features
ğŸŒ **Chinese** supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.

ğŸ¤© **PyTorch** worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060

ğŸŒ **Windows + Linux** run in both Windows OS and linux OS (even in M1 MACOS)

ğŸ¤© **Easy & Awesome** effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder

ğŸŒ **Webserver Ready** to serve your result with remote calling

### [DEMO"
DeepSpeed,"[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)
[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)
[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)
[![Twitter](https://img.shields.io/twitter/follow/MSFTDeepSpeed)](https://twitter.com/intent/follow?screen_name=MSFTDeepSpeed)
[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40MSFTDeepSpeedJP-blue)](https://twitter.com/MSFTDeepSpeedJP)
[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)


<div align=""center"">
 <img src=""docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only"" width=""400px"">
 <img src=""docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-"
streamlit,"<br>

<img src=""https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png"" alt=""Streamlit logo"" style=""margin-top:50px""></img>

# Welcome to Streamlit ğŸ‘‹

**A faster way to build and share data apps.**

## What is Streamlit?

Streamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once youâ€™ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.

### Why choose Streamlit?

- **Simple and Pythonic:** Write beautiful, easy-to-read code.
- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.
- **Live editing:** See your app update instantly as you edit your script.
- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.

## Installation

Open a terminal and run:

```bash
$ pip install streamlit
$ "
gym,"[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Important Notice

### The team that has been maintaining Gym since 2021 has moved all future development to [Gymnasium](https://github.com/Farama-Foundation/Gymnasium), a drop in replacement for Gym (import gymnasium as gym), and Gym will not be receiving any future updates. Please switch over to Gymnasium as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out [this blog post](https://farama.org/Announcing-The-Farama-Foundation).

## Gym

Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with t"
TaskMatrix,"# TaskMatrix

**TaskMatrix** connects ChatGPT and a series of Visual Foundation Models to enable **sending** and **receiving** images during chatting.

See our paper: [<font size=5>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</font>](https://arxiv.org/abs/2303.04671)

<a src=""https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"" href=""https://huggingface.co/spaces/microsoft/visual_chatgpt"">
    <img src=""https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"" alt=""Open in Spaces"">
</a>

<a src=""https://colab.research.google.com/assets/colab-badge.svg"" href=""https://colab.research.google.com/drive/1P3jJqKEWEaeNcZg8fODbbWeQ3gxOHk2-?usp=sharing"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open in Colab"">
</a>

## Updates:
- Now TaskMatrix supports [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) and [segment-anything](https://github.com/facebookresearch/segment-anything)! Thanks **@jordd"
12306,"### 12306 è´­ç¥¨å°åŠ©æ‰‹
#### pythonç‰ˆæœ¬
  - [ ] 2.7.10 - 2.7.15
  - [x] 3.6 - 3.7.4
  - [ ] 2.7.9

#### å·²æœ‰åŠŸèƒ½
  - [x] è‡ªåŠ¨æ‰“ç 
  - [x] è‡ªåŠ¨ç™»å½•
  - [x] å‡†ç‚¹é¢„å”®å’Œæ¡æ¼
  - [x] æ™ºèƒ½å€™è¡¥
  - [x] é‚®ä»¶é€šçŸ¥
  - [x] serveré…±é€šçŸ¥

#### ä¾èµ–åº“
  - éªŒè¯ç ç›®å‰å¯ä»¥æœ¬åœ°è¯†åˆ«ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œæ”¾äºé¡¹ç›®æ ¹ç›®å½•ï¼Œå…¨éƒ¨ä»£ç æ¥æºäºæ­¤é¡¹ç›® [ä¼ é€é—¨](https://github.com/zhaipro/easy12306)ï¼Œè¡¨ç¤ºæ„Ÿè°¢
    ```
      1. æ¨¡å‹ä¸‹è½½é“¾æ¥:https://pan.baidu.com/s/1rS155VjweWVWIJogakechA  å¯†ç :bmlm
         ç¾¤é‡Œé¢ä¹Ÿå¯ä»¥ä¸‹è½½
      2. gitä»“åº“ä¸‹è½½ï¼šhttps://github.com/testerSunshine/12306model.git
    ```
  - è‡ªæ‰˜ç®¡äº‘æ‰“ç æœåŠ¡å™¨æ­å»ºï¼š[12306_code_server](https://github.com/YinAoXiong/12306_code_server)
    - å¦‚æœå¤§å®¶æœ‰ç©ºé—²çš„æœåŠ¡å™¨ï¼Œå¯æ­å»ºä¹‹ååœ¨è¿™ä¸ª [issues](https://github.com/testerSunshine/12306/issues/446) é‡Œé¢å¡«å…¥è‡ªå·±çš„æœåŠ¡å™¨(è¯·æ³¨æ„æœåŠ¡å™¨å®‰å…¨ï¼)
  - é¡¹ç›®ä¾èµ– [requirements.txt](requirements.txt)
  - å®‰è£…æ–¹æ³•x:
      - rootç”¨æˆ·(é¿å…å¤špythonç¯å¢ƒäº§ç”Ÿé—®é¢˜): `pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`
      - érootç”¨æˆ·ï¼ˆé¿å…å®‰è£…å’Œè¿è¡Œæ—¶ä½¿ç”¨äº†ä¸åŒç¯å¢ƒï¼‰: `pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt`
      - è®¸å¤šwindowsçš„ç”¨æˆ·è£…ä¸äº†tensorflowçš„è¯ï¼Œå¯ä»¥é€‚å½“é™ä½ç‰ˆæœ¬æˆ–è€…å‡é«˜ç‰ˆæœ¬éƒ½æ˜¯å¯ä»¥çš„
        "
TTS,"
## ğŸ¸Coqui.ai News
- ğŸ“£ â“TTSv2 is here with 16 languages and better performance across the board.
- ğŸ“£ â“TTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).
- ğŸ“£ â“TTS can now stream with <200ms latency.
- ğŸ“£ â“TTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)
- ğŸ“£ [ğŸ¶Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)
- ğŸ“£ You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with ğŸ¸TTS.
- ğŸ“£ ğŸ¸TTS now supports ğŸ¢Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)

<div align=""center"">
<img src=""https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124"
HanLP,"<h2 align=""center"">HanLP: Han Language Processing</h2>

<div align=""center"">
    <a href=""https://github.com/hankcs/HanLP/actions"">
       <img alt=""Unit Tests"" src=""https://github.com/hankcs/hanlp/actions/workflows/unit-tests.yml/badge.svg?branch=master"">
    </a>
    <a href=""https://pypi.org/project/hanlp/"">
        <img alt=""PyPI Version"" src=""https://img.shields.io/pypi/v/hanlp?color=blue"">
    </a>
    <a href=""https://pypi.org/project/hanlp/"">
        <img alt=""Python Versions"" src=""https://img.shields.io/pypi/pyversions/hanlp?colorB=blue"">
    </a>
    <a href=""https://pepy.tech/project/hanlp"">
        <img alt=""Downloads"" src=""https://static.pepy.tech/badge/hanlp"">
    </a>
    <a href=""https://mybinder.org/v2/gh/hankcs/HanLP/doc-zh?filepath=plugins%2Fhanlp_demo%2Fhanlp_demo%2Fzh%2Ftutorial.ipynb"">
        <img alt=""åœ¨çº¿è¿è¡Œ"" src=""https://mybinder.org/badge_logo.svg"">
    </a>
</div>
<h4 align=""center"">
    <a href=""https://github.com/hankcs/HanLP/tree/master"">English</a> |
    <a"
shadowsocks,"Removed according to regulations.
"
cli,"<h2 align=""center"">
    <a href=""https://httpie.io"" target=""blank_"">
        <img height=""100"" alt=""HTTPie"" src=""https://raw.githubusercontent.com/httpie/cli/master/docs/httpie-logo.svg"" />
    </a>
    <br>
    HTTPie CLI: human-friendly HTTP client for the API era
</h2>

<div align=""center"">

[![HTTPie for Desktop](https://img.shields.io/static/v1?label=HTTPie&message=Desktop&color=4B78E6)](https://httpie.io/product)
[![](https://img.shields.io/static/v1?label=HTTPie&message=Web%20%26%20Mobile&color=73DC8C)](https://httpie.io/app)
[![](https://img.shields.io/static/v1?label=HTTPie&message=CLI&color=FA9BFA)](https://httpie.io/cli)
[![Twitter](https://img.shields.io/twitter/follow/httpie?style=flat&color=%234B78E6&logoColor=%234B78E6)](https://twitter.com/httpie)
[![Chat](https://img.shields.io/discord/725351238698270761?style=flat&label=Chat%20on%20Discord&color=%23FA9BFA)](https://httpie.io/discord)

</div>


<div align=""center"">

[![Docs](https://img.shields.io/badge/stable%20docs-h"
WeChatMsg,"<a href=""https://hellogithub.com/repository/93df3704446343068e67fc174a34be47"" target=""_blank""><img src=""https://api.hellogithub.com/v1/widgets/recommend.svg?rid=93df3704446343068e67fc174a34be47&claim_uid=AzZ0bVgHmTOEej5"" alt=""Featuredï½œHelloGitHub"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a>
<h1 align=""center"">æˆ‘çš„æ•°æ®æˆ‘åšä¸»</h1>
<div align=""center"">
    <a href=""https://github.com/LC044/WeChatMsg/stargazers"">
        <img src=""https://img.shields.io/github/stars/LC044/WeChatMsg.svg"" />
    </a>
    <a href=""https://memotrace.cn/"" target=""_blank"">
        <img alt=""GitHub forks"" src=""https://img.shields.io/github/forks/LC044/WeChatMsg?color=eb6ea5"">
    </a>
    <a href=""https://memotrace.cn/"" target=""_blank"">
        <img src=""https://img.shields.io/badge/WeChat-ç•™ç—•-blue.svg"">
    </a>
    <a target=""_blank"" href=""https://memotrace.cn/"">
        <img alt=""Hits"" src=""https://hits.b3log.org/LC044/memotrace.svg"">
    </a>
    <a href=""https://memotrace.cn/"" target=""_blank"">"
ray,".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png

.. image:: https://readthedocs.org/projects/ray/badge/?version=master
    :target: http://docs.ray.io/en/master/?badge=master

.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue
    :target: https://forms.gle/9TSdDYUgxYs8SA9e8

.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue
    :target: https://discuss.ray.io/

.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter
    :target: https://twitter.com/raydistributed

.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3q"
jieba,"jieba
========
â€œç»“å·´â€ä¸­æ–‡åˆ†è¯ï¼šåšæœ€å¥½çš„ Python ä¸­æ–‡åˆ†è¯ç»„ä»¶

""Jieba"" (Chinese for ""to stutter"") Chinese text segmentation: built to be the best Python Chinese word segmentation module.

- _Scroll down for English documentation._


ç‰¹ç‚¹
========
* æ”¯æŒå››ç§åˆ†è¯æ¨¡å¼ï¼š
    * ç²¾ç¡®æ¨¡å¼ï¼Œè¯•å›¾å°†å¥å­æœ€ç²¾ç¡®åœ°åˆ‡å¼€ï¼Œé€‚åˆæ–‡æœ¬åˆ†æï¼›
    * å…¨æ¨¡å¼ï¼ŒæŠŠå¥å­ä¸­æ‰€æœ‰çš„å¯ä»¥æˆè¯çš„è¯è¯­éƒ½æ‰«æå‡ºæ¥, é€Ÿåº¦éå¸¸å¿«ï¼Œä½†æ˜¯ä¸èƒ½è§£å†³æ­§ä¹‰ï¼›
    * æœç´¢å¼•æ“æ¨¡å¼ï¼Œåœ¨ç²¾ç¡®æ¨¡å¼çš„åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†ï¼Œæé«˜å¬å›ç‡ï¼Œé€‚åˆç”¨äºæœç´¢å¼•æ“åˆ†è¯ã€‚
    * paddleæ¨¡å¼ï¼Œåˆ©ç”¨PaddlePaddleæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒåºåˆ—æ ‡æ³¨ï¼ˆåŒå‘GRUï¼‰ç½‘ç»œæ¨¡å‹å®ç°åˆ†è¯ã€‚åŒæ—¶æ”¯æŒè¯æ€§æ ‡æ³¨ã€‚paddleæ¨¡å¼ä½¿ç”¨éœ€å®‰è£…paddlepaddle-tinyï¼Œ`pip install paddlepaddle-tiny==1.6.1`ã€‚ç›®å‰paddleæ¨¡å¼æ”¯æŒjieba v0.40åŠä»¥ä¸Šç‰ˆæœ¬ã€‚jieba v0.40ä»¥ä¸‹ç‰ˆæœ¬ï¼Œè¯·å‡çº§jiebaï¼Œ`pip install jieba --upgrade` ã€‚[PaddlePaddleå®˜ç½‘](https://www.paddlepaddle.org.cn/)
* æ”¯æŒç¹ä½“åˆ†è¯
* æ”¯æŒè‡ªå®šä¹‰è¯å…¸
* MIT æˆæƒåè®®

å®‰è£…è¯´æ˜
=======

ä»£ç å¯¹ Python 2/3 å‡å…¼å®¹

* å…¨è‡ªåŠ¨å®‰è£…ï¼š`easy_install jieba` æˆ–è€… `pip install jieba` / `pip3 install jieba`
* åŠè‡ªåŠ¨å®‰è£…ï¼šå…ˆä¸‹è½½ http://pypi.python.org/pypi/jieba/ ï¼Œè§£å‹åè¿è¡Œ `python setup.py install`
* æ‰‹åŠ¨å®‰è£…ï¼šå°† jieba ç›®å½•æ”¾ç½®äºå½“å‰ç›®å½•æˆ–è€… site-packages ç›®å½•
* é€šè¿‡ `import jieba` æ¥å¼•ç”¨
* å¦‚æœéœ€è¦ä½¿ç”¨paddleæ¨¡å¼ä¸‹çš„åˆ†è¯å’Œè¯æ€§æ ‡æ³¨åŠŸèƒ½ï¼Œè¯·å…ˆå®‰è£…paddlepaddle-tinyï¼Œ`pip install paddlepaddl"
GPT-SoVITS,"<div align=""center"">


<h1>GPT-SoVITS-WebUI</h1>
A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.<br><br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/RVC-Boss/GPT-SoVITS)

<a href=""https://trendshift.io/repositories/7033"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/7033"" alt=""RVC-Boss%2FGPT-SoVITS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

<!-- img src=""https://counter.seku.su/cmoe?name=gptsovits&theme=r34"" /><br> -->

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb)
[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-online%20demo-yel"
XX-Net,":rocket: XX-Net (ç¿»å¢™VPN)
=========
è¿™æ˜¯ä¸€ä¸ªç¨³å¥å¯é çš„ç¿»å¢™ç³»ç»Ÿï¼Œå·²ç»è¿ç»­è¿è¡Œ 9 å¹´ï¼  
æˆ‘ä»¬ä¸å»ç ”ç©¶å¢™æœ‰ä»€ä¹ˆç¼ºé™·ï¼Œå› ä¸ºæ‰€æœ‰çš„ç¼ºé™·éƒ½ä¼šè¢«æ…¢æ…¢çš„è¡¥ä¸Šã€‚  
æˆ‘ä»¬çš„ç­–ç•¥æ˜¯åŒ–èº«ä¸ºæ™®é€šæµé‡ï¼Œå®Œå…¨æ— æ³•åŒºåˆ†ï¼Œæœ€ç»ˆéšèº«åœ¨èŒ«èŒ«çš„ç½‘ç»œè¿æ¥ä¸­ã€‚ã€‚ã€‚

:electric_plug: åŠŸèƒ½ç‰¹æ€§
=========
* æ”¯æŒå¤šå¹³å°ï¼š Android/iOS/Windows/Mac/Linux   
* é‡‡ç”¨ç‹¬ç‰¹çš„æ··æ·†ç®—æ³•ï¼Œè®©æ‚¨çš„æµé‡åœ¨ç½‘ç»œä¸­æ— æ³•è¢«è¯†åˆ«  
* å¼€æºç»¿è‰²è½¯ä»¶ï¼Œæ— éœ€å®‰è£…ï¼Œå¯ä»¥æ”¯æŒå¤šå°è®¾å¤‡åŒæ—¶è¿æ¥
* æ¨¡æ‹ŸChromeæµè§ˆå™¨è¡Œä¸ºï¼Œå®Œå…¨æ— æ³•è¯†åˆ«ï¼Œç¨³å®šç¿»å¢™
* å†…ç½® ChatGPTï¼Œæ¯ä¸ªå¥—é¤èµ é€ ChatGPT-3.5 ä¸€ç™¾ä¸‡token 


<br>

### å®˜ç½‘ä¸‹è½½: [https://xx-net.com](https://xx-net.com)
### Telegram: [https://t.me/xxnetshare](https://t.me/xxnetshare)
### Twitter: [https://twitter.com/XXNetDev](https://twitter.com/XXNetDev)
###
###### [ä¸­æ–‡å¸®åŠ©æ–‡æ¡£](https://github.com/XX-net/XX-Net/wiki/%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3) &nbsp; &nbsp; &nbsp;[English Document](https://github.com/XX-net/XX-Net/wiki/English-Home-Page) &nbsp; &nbsp; &nbsp;[ÙØ§Ø±Ø³ÛŒ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ](https://github.com/XX-net/XX-Net/wiki/Persian-home-page) 

<br>


### æœ€æ–°å…¬å‘Šï¼š
 2024-03-06
* æœ€æ–°ç‰ˆ5.9.10, æ›´æ–°é»‘åå•åˆ—è¡¨ã€‚
* 5.9.0 å‡çº§GAEæœåŠ¡ç«¯åˆ°python3
* 5.8.8 æ”¹è¿›iOSä¸‹è¿æ¥æ€§èƒ½
* 5.7.0 ä¸ºX-Tunnelå¢åŠ æ–°é€šé“
* 5.6.0 é‡æ„ä»£ç ï¼Œå‡å°‘ç³»ç»Ÿèµ„æºæ¶ˆè€—
* 5.1.0ï¼Œå†…ç½®ChatGPT
* åŸæ¥æ˜¯4.x.x è€ç‰ˆæœ¬çš„ï¼Œéœ€è¦é‡æ–°ä¸‹è½½æ–°"
ccxt,"# CCXT â€“ CryptoCurrency eXchange Trading Library

[![Build Status](https://img.shields.io/travis/com/ccxt/ccxt)](https://travis-ci.com/ccxt/ccxt) [![npm](https://img.shields.io/npm/v/ccxt.svg)](https://npmjs.com/package/ccxt) [![PyPI](https://img.shields.io/pypi/v/ccxt.svg)](https://pypi.python.org/pypi/ccxt) [![NPM Downloads](https://img.shields.io/npm/dy/ccxt.svg)](https://www.npmjs.com/package/ccxt) [![Discord](https://img.shields.io/discord/690203284119617602?logo=discord&logoColor=white)](https://discord.gg/ccxt) [![Supported Exchanges](https://img.shields.io/badge/exchanges-108-blue.svg)](https://github.com/ccxt/ccxt/wiki/Exchange-Markets) [![Twitter Follow](https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&label=CCXT)](https://twitter.com/ccxt_official)

A JavaScript / Python / PHP / C# library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs.

### [Install](#install) Â· [Usage](#usage) Â· ["
gradio,"<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/1)getting_started/1)quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->

<div align=""center"">

[<img src=""readme_files/gradio.svg"" alt=""gradio"" width=400>](https://gradio.app)<br>

[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)
[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml)
 [![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)
[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)
![Python version](https://img.shields.io/badge/python-3.8+-important)
[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)

[Website](ht"
OpenHands,"<a name=""readme-top""></a>

<div align=""center"">
  <img src=""./docs/static/img/logo.png"" alt=""Logo"" width=""200"">
  <h1 align=""center"">OpenHands: Code Less, Make More</h1>
</div>


<div align=""center"">
  <a href=""https://github.com/All-Hands-AI/OpenHands/graphs/contributors""><img src=""https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&color=blue"" alt=""Contributors""></a>
  <a href=""https://github.com/All-Hands-AI/OpenHands/stargazers""><img src=""https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&color=blue"" alt=""Stargazers""></a>
  <a href=""https://codecov.io/github/All-Hands-AI/OpenHands?branch=main""><img alt=""CodeCov"" src=""https://img.shields.io/codecov/c/github/All-Hands-AI/OpenHands?style=for-the-badge&color=blue""></a>
  <a href=""https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE""><img src=""https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&color=blue"" alt=""MIT License""></a>
  <br/>
  <"
sqlmap,"# sqlmap ![](https://i.imgur.com/fe85aVR.png)

[![.github/workflows/tests.yml](https://github.com/sqlmapproject/sqlmap/actions/workflows/tests.yml/badge.svg)](https://github.com/sqlmapproject/sqlmap/actions/workflows/tests.yml) [![Python 2.6|2.7|3.x](https://img.shields.io/badge/python-2.6|2.7|3.x-yellow.svg)](https://www.python.org/) [![License](https://img.shields.io/badge/license-GPLv2-red.svg)](https://raw.githubusercontent.com/sqlmapproject/sqlmap/master/LICENSE) [![Twitter](https://img.shields.io/badge/twitter-@sqlmap-blue.svg)](https://twitter.com/sqlmap)

sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester, and a broad range of switches including database fingerprinting, over data fetching from the database, accessing the underlying file system, and executing commands on t"
pytorch-image-models,"# PyTorch Image Models
- [What's New](#whats-new)
- [Introduction](#introduction)
- [Models](#models)
- [Features](#features)
- [Results](#results)
- [Getting Started (Documentation)](#getting-started-documentation)
- [Train, Validation, Inference Scripts](#train-validation-inference-scripts)
- [Awesome PyTorch Resources](#awesome-pytorch-resources)
- [Licenses](#licenses)
- [Citing](#citing)

## What's New

### Aug 21, 2024
* Updated SBB ViT models trained on ImageNet-12k and fine-tuned on ImageNet-1k, challenging quite a number of much larger, slower models

| model | top1 | top5 | param_count | img_size |
| -------------------------------------------------- | ------ | ------ | ----------- | -------- |
| [vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k) | 87.438 | 98.256 | 64.11 | 384 |
| [vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patc"
LLaMA-Factory,"![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/hiyouga/LLaMA-Factory)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-91-green)](#projects-using-llama-factory)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)
[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Open in Colab](https://colab.research.googl"
certbot,".. This file contains a series of comments that are used to include sections of this README in other files. Do not modify these comments unless you know what you are doing. tag:intro-begin

|build-status|

.. |build-status| image:: https://img.shields.io/azure-devops/build/certbot/ba534f81-a483-4b9b-9b4e-a60bec8fee72/5/master
   :target: https://dev.azure.com/certbot/certbot/_build?definitionId=5
   :alt: Azure Pipelines CI status
 
.. image:: https://raw.githubusercontent.com/EFForg/design/master/logos/eff-certbot-lockup.png
  :width: 200
  :alt: EFF Certbot Logo

Certbot is part of EFFâ€™s effort to encrypt the entire Internet. Secure communication over the Web relies on HTTPS, which requires the use of a digital certificate that lets browsers verify the identity of web servers (e.g., is that really google.com?). Web servers obtain their certificates from trusted third parties called certificate authorities (CAs). Certbot is an easy-to-use client that fetches a certificate from Letâ€™s E"
poetry,"# Poetry: Python packaging and dependency management made easy

[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/)
[![Stable Version](https://img.shields.io/pypi/v/poetry?label=stable)][PyPI Releases]
[![Pre-release Version](https://img.shields.io/github/v/release/python-poetry/poetry?label=pre-release&include_prereleases&sort=semver)][PyPI Releases]
[![Python Versions](https://img.shields.io/pypi/pyversions/poetry)][PyPI]
[![Download Stats](https://img.shields.io/pypi/dm/poetry)](https://pypistats.org/packages/poetry)
[![Discord](https://img.shields.io/discord/487711540787675139?logo=discord)][Discord]

Poetry helps you declare, manage and install dependencies of Python projects,
ensuring you have the right stack everywhere.

![Poetry Install](https://raw.githubusercontent.com/python-poetry/poetry/master/assets/install.gif)

Poetry replaces `setup.py`, `requirements.txt`, `setup.cfg`, `MANIFEST.in` and `Pipfile` with a "
Python,"# My Python Eggs ğŸ ğŸ˜„

<hr>

I do not consider myself as a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me: craig@geekcomputers.co.uk.

<hr>

This repository contains a collection of Python scripts that are designed to reduce human workload and serve as educational examples for beginners to get started with Python. The code documentation is aligned correctly for viewing in [Notepad++](https://notepad-plus-plus.org/) :spiral_notepad:

Feel free to explore the scripts and use them for your learning and automation needs!

## List of Scripts:

1. [batch_file_rename.py](https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py) - Batch rename a group of files in a specified directory, changing their extensions.
2. [create_dir_if_not_there.py](htt"
ChatTTS,"<div align=""center"">

<a href=""https://trendshift.io/repositories/10489"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/10489"" alt=""2noise%2FChatTTS | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

# ChatTTS
A generative speech model for daily dialogue.

[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)

[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)
[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoC"
OpenBB,"<br />
<img src=""https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only"" alt=""OpenBB Terminal logo"" width=""600"">
<img src=""https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only"" alt=""OpenBB Terminal logo"" width=""600"">
<br />
<br />

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://twitter.com/openbb_finance)
![Discord Shield](https://discordapp.com/api/guilds/831165782750789672/widget.png?style=shield)
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)
<a href=""https://codespaces.new/OpenBB-finance/OpenBBTerminal"">
  <img src=""https://github.com/codespaces/badge.svg"" height=""20"" />"
fairseq,"<p align=""center"">
  <img src=""docs/fairseq_logo.png"" width=""150"">
  <br />
  <br />
  <a href=""https://opensource.fb.com/support-ukraine""><img alt=""Support Ukraine"" src=""https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB"" /></a>
  <a href=""https://github.com/pytorch/fairseq/blob/main/LICENSE""><img alt=""MIT License"" src=""https://img.shields.io/badge/license-MIT-blue.svg"" /></a>
  <a href=""https://github.com/pytorch/fairseq/releases""><img alt=""Latest Release"" src=""https://img.shields.io/github/release/pytorch/fairseq.svg"" /></a>
  <a href=""https://github.com/pytorch/fairseq/actions?query=workflow:build""><img alt=""Build Status"" src=""https://github.com/pytorch/fairseq/workflows/build/badge.svg"" /></a>
  <a href=""https://fairseq.readthedocs.io/en/latest/?badge=latest""><img alt=""Documentation Status"" src=""https://readthedocs.org/projects/fairseq/badge/?version=latest"" /></a>
  <a href=""https://app.circleci.com/pipelines/github/facebookresearch/fairseq/""><img al"
chatgpt-on-wechat,"# ç®€ä»‹

> chatgpt-on-wechatï¼ˆç®€ç§°CoWï¼‰é¡¹ç›®æ˜¯åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯æœºå™¨äººï¼Œæ”¯æŒå¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰æ¥å…¥ï¼Œå¯é€‰æ‹©GPT3.5/GPT4.0/Claude/Gemini/LinkAI/ChatGLM/KIMI/æ–‡å¿ƒä¸€è¨€/è®¯é£æ˜Ÿç«/é€šä¹‰åƒé—®/LinkAIï¼Œèƒ½å¤„ç†æ–‡æœ¬ã€è¯­éŸ³å’Œå›¾ç‰‡ï¼Œé€šè¿‡æ’ä»¶è®¿é—®æ“ä½œç³»ç»Ÿå’Œäº’è”ç½‘ç­‰å¤–éƒ¨èµ„æºï¼Œæ”¯æŒåŸºäºè‡ªæœ‰çŸ¥è¯†åº“å®šåˆ¶ä¼ä¸šAIåº”ç”¨ã€‚

æœ€æ–°ç‰ˆæœ¬æ”¯æŒçš„åŠŸèƒ½å¦‚ä¸‹ï¼š

-  âœ…   **å¤šç«¯éƒ¨ç½²ï¼š** æœ‰å¤šç§éƒ¨ç½²æ–¹å¼å¯é€‰æ‹©ä¸”åŠŸèƒ½å®Œå¤‡ï¼Œç›®å‰å·²æ”¯æŒå¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰ç­‰éƒ¨ç½²æ–¹å¼
-  âœ…   **åŸºç¡€å¯¹è¯ï¼š** ç§èŠåŠç¾¤èŠçš„æ¶ˆæ¯æ™ºèƒ½å›å¤ï¼Œæ”¯æŒå¤šè½®ä¼šè¯ä¸Šä¸‹æ–‡è®°å¿†ï¼Œæ”¯æŒ GPT-3.5, GPT-4o-mini, GPT-4o,  GPT-4, Claude-3.5, Gemini, æ–‡å¿ƒä¸€è¨€, è®¯é£æ˜Ÿç«, é€šä¹‰åƒé—®ï¼ŒChatGLM-4ï¼ŒKimi(æœˆä¹‹æš—é¢), MiniMax
-  âœ…   **è¯­éŸ³èƒ½åŠ›ï¼š** å¯è¯†åˆ«è¯­éŸ³æ¶ˆæ¯ï¼Œé€šè¿‡æ–‡å­—æˆ–è¯­éŸ³å›å¤ï¼Œæ”¯æŒ azure, baidu, google, openai(whisper/tts) ç­‰å¤šç§è¯­éŸ³æ¨¡å‹
-  âœ…   **å›¾åƒèƒ½åŠ›ï¼š** æ”¯æŒå›¾ç‰‡ç”Ÿæˆã€å›¾ç‰‡è¯†åˆ«ã€å›¾ç”Ÿå›¾ï¼ˆå¦‚ç…§ç‰‡ä¿®å¤ï¼‰ï¼Œå¯é€‰æ‹© Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, visionæ¨¡å‹
-  âœ…   **ä¸°å¯Œæ’ä»¶ï¼š** æ”¯æŒä¸ªæ€§åŒ–æ’ä»¶æ‰©å±•ï¼Œå·²å®ç°å¤šè§’è‰²åˆ‡æ¢ã€æ–‡å­—å†’é™©ã€æ•æ„Ÿè¯è¿‡æ»¤ã€èŠå¤©è®°å½•æ€»ç»“ã€æ–‡æ¡£æ€»ç»“å’Œå¯¹è¯ã€è”ç½‘æœç´¢ç­‰æ’ä»¶
-  âœ…   **çŸ¥è¯†åº“ï¼š** é€šè¿‡ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶è‡ªå®šä¹‰ä¸“å±æœºå™¨äººï¼Œå¯ä½œä¸ºæ•°å­—åˆ†èº«ã€æ™ºèƒ½å®¢æœã€ç§åŸŸåŠ©æ‰‹ä½¿ç”¨ï¼ŒåŸºäº [LinkAI](https://link-ai.tech) å®ç°

## å£°æ˜

1. æœ¬é¡¹ç›®éµå¾ª [MITå¼€æºåè®®](/LICENSE)ï¼Œä»…ç”¨äºæŠ€æœ¯ç ”ç©¶å’Œå­¦ä¹ ï¼Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶éœ€éµå®ˆæ‰€åœ¨åœ°æ³•å¾‹æ³•è§„ã€ç›¸å…³æ”¿ç­–ä»¥åŠä¼ä¸šç« ç¨‹ï¼Œç¦æ­¢ç”¨äºä»»ä½•è¿æ³•æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º
2. å¢ƒå†…ä½¿ç”¨è¯¥é¡¹ç›®æ—¶ï¼Œè¯·ä½¿ç”¨å›½å†…å‚å•†çš„å¤§æ¨¡å‹æœåŠ¡ï¼Œå¹¶è¿›è¡Œå¿…è¦çš„å†…å®¹å®‰å…¨å®¡æ ¸åŠè¿‡æ»¤
3. æœ¬é¡¹ç›®ä¸»è¦æ¥å…¥ååŒåŠå…¬å¹³å°ï¼Œæ¨èä½¿ç”¨å…¬ä¼—å·ã€ä¼å¾®è‡ªå»ºåº”ç”¨ã€é’‰é’‰ã€é£ä¹¦ç­‰æ¥å…¥é€šé“ï¼Œå…¶ä»–é€šé“ä¸ºå†å²äº§ç‰©å·²ä¸ç»´æŠ¤
4. ä»»ä½•ä¸ªäººã€å›¢é˜Ÿå’Œä¼ä¸šï¼Œæ— è®ºä»¥ä½•ç§"
detectron2,"<img src="".github/Detectron2-Logo-Horz.svg"" width=""300"" >

<a href=""https://opensource.facebook.com/support-ukraine"">
  <img src=""https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB"" alt=""Support Ukraine - Help Provide Humanitarian Aid to Ukraine."" />
</a>

Detectron2 is Facebook AI Research's next generation library
that provides state-of-the-art detection and segmentation algorithms.
It is the successor of
[Detectron](https://github.com/facebookresearch/Detectron/)
and [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/).
It supports a number of computer vision research projects and production applications in Facebook.

<div align=""center"">
  <img src=""https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png""/>
</div>
<br>

## Learn More about Detectron2

Explain Like Iâ€™m 5: Detectron2            |  Using Machine Learning with Detectron2
:-------------------------:|:----------------------"
jax,"<div align=""center"">
<img src=""https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png"" alt=""logo""></img>
</div>

# Transformable numerical computing at scale

![Continuous integration](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg)
![PyPI version](https://img.shields.io/pypi/v/jax)

[**Quickstart**](#quickstart-colab-in-the-cloud)
| [**Transformations**](#transformations)
| [**Install guide**](#installation)
| [**Neural net libraries**](#neural-network-libraries)
| [**Change logs**](https://jax.readthedocs.io/en/latest/changelog.html)
| [**Reference docs**](https://jax.readthedocs.io/en/latest/)


## What is JAX?

JAX is a Python library for accelerator-oriented array computation and program transformation,
designed for high-performance numerical computing and large-scale machine learning.

With its updated version of [Autograd](https://github.com/hips/autograd),
JAX can automatically differentiate native
Python and NumPy functions. It can"
gpt-pilot,"<div align=""center"">

# ğŸ§‘â€âœˆï¸ GPT PILOT ğŸ§‘â€âœˆï¸

</div>

---

<div align=""center"">

[![Discord Follow](https://dcbadge.vercel.app/api/server/HaqXugmxr9?style=flat)](https://discord.gg/HaqXugmxr9)
[![GitHub Repo stars](https://img.shields.io/github/stars/Pythagora-io/gpt-pilot?style=social)](https://github.com/Pythagora-io/gpt-pilot)
[![Twitter Follow](https://img.shields.io/twitter/follow/HiPythagora?style=social)](https://twitter.com/HiPythagora)

</div>

---

<div align=""center"">
<a href=""https://www.ycombinator.com/"" target=""_blank""><img src=""https://s3.amazonaws.com/assets.pythagora.ai/yc/PNG/Black.png"" alt=""Pythagora-io%2Fgpt-pilot | Trendshift"" style=""width: 250px; height: 93px;""/></a>
</div>
<br>
<div align=""center"">
<a href=""https://trendshift.io/repositories/466"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/466"" alt=""Pythagora-io%2Fgpt-pilot | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>
</div>

<br>
<br>

<div align=""cent"
pytorch-tutorial,"<p align=""center""><img width=""40%"" src=""logo/pytorch_logo_2018.svg"" /></p>

--------------------------------------------------------------------------------

This repository provides tutorial code for deep learning researchers to learn [PyTorch](https://github.com/pytorch/pytorch). In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish [Official Pytorch Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).


<br/>

## Table of Contents

#### 1. Basics
* [PyTorch Basics](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/pytorch_basics/main.py)
* [Linear Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/linear_regression/main.py#L22-L23)
* [Logistic Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/logistic_regression/main.py#L33-L34)
* [Feedforward Neural Network](https://gi"
ControlNet,"# News: A nightly version of ControlNet 1.1 is released!

[ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly) is released. Those new models will be merged to this repo after we make sure that everything is good.

# Below is ControlNet 1.0

Official implementation of [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet is a neural network structure to control diffusion models by adding extra conditions.

![img](github_page/he.png)

It copys the weights of neural network blocks into a ""locked"" copy and a ""trainable"" copy. 

The ""trainable"" one learns your condition. The ""locked"" one preserves your model. 

Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models.

The ""zero convolution"" is 1Ã—1 convolution with both weight and bias initialized as zeros. 

Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.

No "
linux-insides,"linux-insides
===============

A book-in-progress about the linux kernel and its insides.

**The goal is simple** - to share my modest knowledge about the insides of the linux kernel and help people who are interested in linux kernel insides, and other low-level subject matter. Feel free to go through the book [Start here](https://github.com/0xAX/linux-insides/blob/master/SUMMARY.md)

**Questions/Suggestions**: Feel free about any questions or suggestions by pinging me at twitter [@0xAX](https://twitter.com/0xAX), adding an [issue](https://github.com/0xAX/linux-insides/issues/new) or just drop me an [email](mailto:anotherworldofworld@gmail.com).

Generating eBooks and PDFs - [documentation](https://github.com/GitbookIO/gitbook/blob/master/docs/ebook.md)

# Mailing List

We have a Google Group mailing list for learning the kernel source code. Here are some instructions about how to use it.

#### Join

Send an email with any subject/content to `kernelhacking+subscribe@googlegroups.com`. "
spaCy,"<a href=""https://explosion.ai""><img src=""https://explosion.ai/assets/img/logo.svg"" width=""125"" height=""125"" align=""right"" /></a>

# spaCy: Industrial-strength NLP

spaCy is a library for **advanced Natural Language Processing** in Python and
Cython. It's built on the very latest research, and was designed from day one to
be used in real products.

spaCy comes with [pretrained pipelines](https://spacy.io/models) and currently
supports tokenization and training for **70+ languages**. It features
state-of-the-art speed and **neural network models** for tagging, parsing,
**named entity recognition**, **text classification** and more, multi-task
learning with pretrained **transformers** like BERT, as well as a
production-ready [**training system**](https://spacy.io/usage/training) and easy
model packaging, deployment and workflow management. spaCy is commercial
open-source software, released under the
[MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).

ğŸ’« **Version 3.7 ou"
langflow,"<!-- markdownlint-disable MD030 -->

# [![Langflow](./docs/static/img/hero.png)](https://www.langflow.org)

<p align=""center"" style=""font-size: 12px;"">
    Langflow is a low-code app builder for RAG and multi-agent AI applications. Itâ€™s Python-based and agnostic to any model, API, or database.
</p>

<p align=""center"" style=""font-size: 12px;"">
    <a href=""https://docs.langflow.org"" style=""text-decoration: underline;"">Docs</a> -
    <a href=""https://astra.datastax.com/signup?type=langflow"" style=""text-decoration: underline;"">Free Cloud Service</a> -
    <a href=""https://docs.langflow.org/getting-started-installation"" style=""text-decoration: underline;"">Self Managed</a>
    
</p>

<div align=""center"">
  <a href=""./README.md""><img alt=""README in English"" src=""https://img.shields.io/badge/English-d9d9d9""></a>
  <a href=""./README.PT.md""><img alt=""README in Portuguese"" src=""https://img.shields.io/badge/Portuguese-d9d9d9""></a>
  <a href=""./README.ES.md""><img alt=""README in Spanish"" src=""https"
stanford_alpaca,"
<p align=""center"" width=""100%"">
<img src=""assets/logo.png"" alt=""Stanford-Alpaca"" style=""width: 50%; min-width: 300px; display: block; margin: auto;"">
</p>

# Stanford Alpaca: An Instruction-following LLaMA Model

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)
[![Weight Diff License](https://img.shields.io/badge/Weight%20Diff%20License-CC%20By%20NC%204.0-yellow)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/WEIGHT_DIFF_LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

This is the repo for the Stanford Alpaca project, which aims t"
interactive-coding-challenges,"<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/interactive-coding-challenges/master/images/cover_challenge.gif"">
</p>

interactive-coding-challenges
============

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/donnemartin/interactive-coding-challenges/master)

**120+ continually updated, interactive, and test-driven coding challenges**, with [Anki flashcards](#anki-flashcards-coding-and-design).

Challenges focus on **algorithms** and **data structures** found in **coding interviews**.

Each challenge has one or more reference solutions that are:

* Fully functional
* Unit tested
* Easy-to-understand

Challenges will soon provide on-demand [incremental hints](https://github.com/donnemartin/interactive-coding-challenges/issues/22) to help you arrive at the optimal solution.

Notebooks also detail:

* Constraints
* Test cases
* Algorithms
* Big-O time and space complexities

Also included are **unit tested reference impleme"
mmdetection,"<div align=""center"">
  <img src=""resources/mmdet-logo.png"" width=""600""/>
  <div>&nbsp;</div>
  <div align=""center"">
    <b><font size=""5"">OpenMMLab website</font></b>
    <sup>
      <a href=""https://openmmlab.com"">
        <i><font size=""4"">HOT</font></i>
      </a>
    </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <b><font size=""5"">OpenMMLab platform</font></b>
    <sup>
      <a href=""https://platform.openmmlab.com"">
        <i><font size=""4"">TRY IT OUT</font></i>
      </a>
    </sup>
  </div>
  <div>&nbsp;</div>

[![PyPI](https://img.shields.io/pypi/v/mmdet)](https://pypi.org/project/mmdet)
[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection.readthedocs.io/en/latest/)
[![badge](https://github.com/open-mmlab/mmdetection/workflows/build/badge.svg)](https://github.com/open-mmlab/mmdetection/actions)
[![codecov](https://codecov.io/gh/open-mmlab/mmdetection/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmdetection)
[![license](https://img.shields.io/"
ultralytics,"<div align=""center"">
  <p>
    <a href=""https://www.ultralytics.com/events/yolovision"" target=""_blank"">
      <img width=""100%"" src=""https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png"" alt=""YOLO Vision banner""></a>
  </p>

[ä¸­æ–‡](https://docs.ultralytics.com/zh) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko) | [æ—¥æœ¬èª](https://docs.ultralytics.com/ja) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [FranÃ§ais](https://docs.ultralytics.com/fr) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar) <br>

<div>
    <a href=""https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml""><img src=""https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg"" alt=""Ultralytics CI""></a>
    <a href=""https://zenodo.org/badge/latestdoi/2"
OpenVoice,"<div align=""center"">
  <div>&nbsp;</div>
  <img src=""resources/openvoicelogo.jpg"" width=""400""/> 

[Paper](https://arxiv.org/abs/2312.01479) |
[Website](https://research.myshell.ai/open-voice) 

</div>

## Introduction

### OpenVoice V1

As we detailed in our [paper](https://arxiv.org/abs/2312.01479) and [website](https://research.myshell.ai/open-voice), the advantages of OpenVoice are three-fold:

**1. Accurate Tone Color Cloning.**
OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.

**2. Flexible Voice Style Control.**
OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation. 

**3. Zero-shot Cross-lingual Voice Cloning.**
Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.

### OpenVoice V2

In April 2024, we relea"
tqdm,"|Logo|

tqdm
====

|Py-Versions| |Versions| |Conda-Forge-Status| |Docker| |Snapcraft|

|Build-Status| |Coverage-Status| |Branch-Coverage-Status| |Codacy-Grade| |Libraries-Rank| |PyPI-Downloads|

|LICENCE| |OpenHub-Status| |binder-demo| |awesome-python|

``tqdm`` derives from the Arabic word *taqaddum* (ØªÙ‚Ø¯Ù‘Ù…) which can mean ""progress,""
and is an abbreviation for ""I love you so much"" in Spanish (*te quiero demasiado*).

Instantly make your loops show a smart progress meter - just wrap any
iterable with ``tqdm(iterable)``, and you're done!

.. code:: python

    from tqdm import tqdm
    for i in tqdm(range(10000)):
        ...

``76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Â Â Â Â Â Â  | 7568/10000 [00:33<00:10, 229.00it/s]``

``trange(N)`` can be also used as a convenient shortcut for
``tqdm(range(N))``.

|Screenshot|
    |Video| |Slides| |Merch|

It can also be executed as a module with pipes:

.. code:: sh

    $ seq 9999999 | tqdm --bytes | wc -l
    75.2MB [00:00, 217MB/s]
    9999999

    $ tar -zcf -"
freqtrade,"# ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/workflows/Freqtrade%20CI/badge.svg)](https://github.com/freqtrade/freqtrade/actions/)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)
[![Maintainability](https://api.codeclimate.com/v1/badges/5737e6d668200b7518ff/maintainability)](https://codeclimate.com/github/freqtrade/freqtrade/maintainability)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plottin"
django-rest-framework,"# [Django REST framework][docs]

[![build-status-image]][build-status]
[![coverage-status-image]][codecov]
[![pypi-version]][pypi]

**Awesome web-browsable Web APIs.**

Full documentation for the project is available at [https://www.django-rest-framework.org/][docs].

---

# Funding

REST framework is a *collaboratively funded project*. If you use
REST framework commercially we strongly encourage you to invest in its
continued development by [signing up for a paid plan][funding].

The initial aim is to provide a single full-time position on REST framework.
*Every single sign-up makes a significant impact towards making that possible.*

[![][sentry-img]][sentry-url]
[![][stream-img]][stream-url]
[![][spacinov-img]][spacinov-url]
[![][retool-img]][retool-url]
[![][bitio-img]][bitio-url]
[![][posthog-img]][posthog-url]
[![][cryptapi-img]][cryptapi-url]
[![][fezto-img]][fezto-url]
[![][svix-img]][svix-url]
[![][zuplo-img]][zuplo-url]

Many thanks to all our [wonderful sponsors][sponsors], "
roop,"## This project has been discontinued

Yes, it still works, you can still use this software. It just won't recieve any updates now.

> I do not have the interest or time to oversee the development of this software. I thank all the amazing people who contributed to this project and made what it is in it's final form.

# Roop

> Take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training.

[![Build Status](https://img.shields.io/github/actions/workflow/status/s0md3v/roop/ci.yml.svg?branch=main)](https://github.com/s0md3v/roop/actions?query=workflow:ci)

<img src=""https://i.ibb.co/4RdPYwQ/Untitled.jpg""/>

## Installation

Be aware, the installation needs technical skills and is not for beginners. Please do not open platform and installation related issues on GitHub.

[Basic](https://github.com/s0md3v/roop/wiki/1.-Installation) - It is more likely to work on your computer, but will be quite slow

[Acceleration](ht"
pytorch-lightning,"<div align=""center"">

<img alt=""Lightning"" src=""https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png"" width=""800px"" style=""max-width: 100%;"">

<br/>
<br/>

**The deep learning framework to pretrain, finetune and deploy AI models.**

**NEW- Deploying models? Check out [LitServe](https://github.com/Lightning-AI/litserve), the PyTorch Lightning for model serving**

______________________________________________________________________

<p align=""center"">
    <a href=""#quick-start"" style=""margin: 0 10px;"">Quick start</a> â€¢
  <a href=""#examples"">Examples</a> â€¢
  <a href=""#why-pytorch-lightning"">PyTorch Lightning</a> â€¢
  <a href=""#lightning-fabric-expert-control"">Fabric</a> â€¢
  <a href=""https://lightning.ai/"">Lightning AI</a> â€¢   
  <a href=""#community"">Community</a> â€¢
  <a href=""https://pytorch-lightning.readthedocs.io/en/stable/"">Docs</a>
</p>

<!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL -->

[![PyPI - Python Version](https://im"
ChatGPT,"# ChatGPT <img src=""https://github.com/acheong08/ChatGPT/blob/main/logo.png?raw=true"" width=""15%""></img>

English - [ä¸­æ–‡](./README_zh.md) - [Spanish](./README_sp.md) - [æ—¥æœ¬èª](./README_ja.md) - [í•œêµ­ì–´](./README_ko.md)

[![PyPi](https://img.shields.io/pypi/v/revChatGPT.svg)](https://pypi.python.org/pypi/revChatGPT)
[![Support_Platform](https://img.shields.io/pypi/pyversions/revChatGPT)](https://pypi.python.org/pypi/revChatGPT)
[![Downloads](https://static.pepy.tech/badge/revchatgpt)](https://pypi.python.org/pypi/revChatGPT)

Reverse Engineered ChatGPT API by OpenAI. Extensible for chatbots etc.

[![](https://github.com/acheong08/ChatGPT/blob/main/docs/view.gif?raw=true)](https://pypi.python.org/pypi/revChatGPT)

# Installation

```
python -m pip install --upgrade revChatGPT
```

### Suport Python Version

- Minimum - Python3.9
- Recommend - Python3.11+

<details>

  <summary>

# V1 Standard ChatGPT

V1 uses a cloudflare bypass proxy to make life convenient for everyone. The proxy is open sou"
Real-ESRGAN,"<p align=""center"">
  <img src=""assets/realesrgan_logo.png"" height=120>
</p>

## <div align=""center""><b><a href=""README.md"">English</a> | <a href=""README_CN.md"">ç®€ä½“ä¸­æ–‡</a></b></div>

<div align=""center"">

ğŸ‘€[**Demos**](#-demos-videos) **|** ğŸš©[**Updates**](#-updates) **|** âš¡[**Usage**](#-quick-inference) **|** ğŸ°[**Model Zoo**](docs/model_zoo.md) **|** ğŸ”§[Install](#-dependencies-and-installation)  **|** ğŸ’»[Train](docs/Training.md) **|** â“[FAQ](docs/FAQ.md) **|** ğŸ¨[Contribution](docs/CONTRIBUTING.md)

[![download](https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg)](https://github.com/xinntao/Real-ESRGAN/releases)
[![PyPI](https://img.shields.io/pypi/v/realesrgan)](https://pypi.org/project/realesrgan/)
[![Open issue](https://img.shields.io/github/issues/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)
[![Closed issue](https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)
[![LICENSE](https://img.shi"
CheatSheetSeries,"# Welcome to the OWASP Cheat Sheet Series

[![OWASP Flagship](https://img.shields.io/badge/owasp-flagship%20project-48A646.svg)](https://www.owasp.org/index.php/OWASP_Project_Inventory#tab=Flagship_Projects)
[![Creative Commons License](https://img.shields.io/github/license/OWASP/CheatSheetSeries)](https://creativecommons.org/licenses/by-sa/4.0/ ""CC BY-SA 4.0"")

Welcome to the official repository for the Open Web Application Security ProjectÂ® (OWASP) Cheat Sheet Series project. The project focuses on providing good security practices for builders in order to secure their applications.

In order to read the cheat sheets and **reference** them, use the project [official website](https://cheatsheetseries.owasp.org). The project details can be viewed on the [OWASP main website](https://owasp.org/www-project-cheat-sheets/) without the cheat sheets.

:triangular_flag_on_post: Markdown files are the working sources and aren't intended to be referenced in any external documentation, books or w"
numpy,"<h1 align=""center"">
<img src=""https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg"" width=""300"">
</h1><br>


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)


NumPy is "
vllm,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png"">
    <img alt=""vLLM"" src=""https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png"" width=55%>
  </picture>
</p>

<h3 align=""center"">
Easy, fast, and cheap LLM serving for everyone
</h3>

<p align=""center"">
| <a href=""https://docs.vllm.ai""><b>Documentation</b></a> | <a href=""https://vllm.ai""><b>Blog</b></a> | <a href=""https://arxiv.org/abs/2309.06180""><b>Paper</b></a> | <a href=""https://discord.gg/jz7wjKhh6g""><b>Discord</b></a> | <a href=""https://x.com/vllm_project""><b>Twitter/X</b></a> |

</p>


---

**vLLM, AMD, Anyscale Meet & Greet at [Ray Summit 2024](http://raysummit.anyscale.com) (Monday, Sept 30th, 5-7pm PT) at Marriott Marquis San Francisco**

We are excited to announce our special vLLM event in collaboration with AMD and Anyscale.
Join"
data-science-ipython-notebooks,"<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif"">
</p>

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png"">
  <br/>
</p>

# data-science-ipython-notebooks

## Index

* [deep-learning](#deep-learning)
    * [tensorflow](#tensor-flow-tutorials)
    * [theano](#theano-tutorials)
    * [keras](#keras-tutorials)
    * [caffe](#deep-learning-misc)
* [scikit-learn](#scikit-learn)
* [statistical-inference-scipy](#statistical-inference-scipy)
* [pandas](#pandas)
* [matplotlib](#matplotlib)
* [numpy](#numpy)
* [python-data](#python-data)
* [kaggle-and-business-analyses](#kaggle-and-business-analyses)
* [spark](#spark)
* [mapreduce-python](#mapreduce-python)
* [amazon web services](#aws)
* [command lines](#commands)
* [misc](#misc)
* [notebook-installation](#notebook-installation)
* [credits](#credits)
* [con"
python-fire,"# Python Fire [![PyPI](https://img.shields.io/pypi/pyversions/fire.svg?style=plastic)](https://github.com/google/python-fire)

_Python Fire is a library for automatically generating command line interfaces
(CLIs) from absolutely any Python object._

-   Python Fire is a simple way to create a CLI in Python.
    [[1]](docs/benefits.md#simple-cli)
-   Python Fire is a helpful tool for developing and debugging Python code.
    [[2]](docs/benefits.md#debugging)
-   Python Fire helps with exploring existing code or turning other people's
    code into a CLI. [[3]](docs/benefits.md#exploring)
-   Python Fire makes transitioning between Bash and Python easier.
    [[4]](docs/benefits.md#bash)
-   Python Fire makes using a Python REPL easier by setting up the REPL with the
    modules and variables you'll need already imported and created.
    [[5]](docs/benefits.md#repl)

## Installation

To install Python Fire with pip, run: `pip install fire`

To install Python Fire with conda, run: `conda "
hosts,"**Take Note!**

With the exception of issues and PRs regarding changes to
`hosts/data/StevenBlack/hosts`, all other issues regarding the content of the
produced hosts files should be made with the appropriate data source that
contributed the content in question. The contact information for all of the data
sources can be found in the `hosts/data/` directory.

---

![Logo](https://raw.githubusercontent.com/StevenBlack/hosts/master/.github/logo.png)

[![latest release](https://img.shields.io/github/release/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/releases)
[![license](https://img.shields.io/github/license/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/blob/master/license.txt)
[![repo size](https://img.shields.io/github/repo-size/StevenBlack/hosts)](https://github.com/StevenBlack/hosts)
[![contributors](https://img.shields.io/github/contributors/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/graphs/contributors)
[![Build Status](https://img.shields.io/gi"
glances,"===============================
Glances - An eye on your system
===============================

|  |pypi| |test| |contributors| |quality|
|  |starts| |docker| |pypistat|
|  |sponsors| |twitter|

.. |pypi| image:: https://img.shields.io/pypi/v/glances.svg
    :target: https://pypi.python.org/pypi/Glances

.. |starts| image:: https://img.shields.io/github/stars/nicolargo/glances.svg
    :target: https://github.com/nicolargo/glances/
    :alt: Github stars

.. |docker| image:: https://img.shields.io/docker/pulls/nicolargo/glances
    :target: https://hub.docker.com/r/nicolargo/glances/
    :alt: Docker pull

.. |pypistat| image:: https://pepy.tech/badge/glances/month
    :target: https://pepy.tech/project/glances
    :alt: Pypi downloads

.. |test| image:: https://github.com/nicolargo/glances/actions/workflows/ci.yml/badge.svg?branch=develop
    :target: https://github.com/nicolargo/glances/actions
    :alt: Linux tests (GitHub Actions)

.. |contributors| image:: https://img.shields.io/g"
tinygrad,"<div align=""center"">

<picture>
  <source media=""(prefers-color-scheme: light)"" srcset=""/docs/logo_tiny_light.svg"">
  <img alt=""tiny corp logo"" src=""/docs/logo_tiny_dark.svg"" width=""50%"" height=""50%"">
</picture>

tinygrad: For something between [PyTorch](https://github.com/pytorch/pytorch) and [karpathy/micrograd](https://github.com/karpathy/micrograd). Maintained by [tiny corp](https://tinygrad.org).

<h3>

[Homepage](https://github.com/tinygrad/tinygrad) | [Documentation](https://docs.tinygrad.org/) | [Discord](https://discord.gg/ZjZadyC7PK)

</h3>

[![GitHub Repo stars](https://img.shields.io/github/stars/tinygrad/tinygrad)](https://github.com/tinygrad/tinygrad/stargazers)
[![Unit Tests](https://github.com/tinygrad/tinygrad/actions/workflows/test.yml/badge.svg)](https://github.com/tinygrad/tinygrad/actions/workflows/test.yml)
[![Discord](https://img.shields.io/discord/1068976834382925865)](https://discord.gg/ZjZadyC7PK)

</div>

---

This may not be the best deep learning framework,"
mindsdb,"<a name=""readme-top""></a>

<div align=""center"">
	<a href=""https://pypi.org/project/MindsDB/"" target=""_blank""><img src=""https://badge.fury.io/py/MindsDB.svg"" alt=""MindsDB Release""></a>
	<a href=""https://www.python.org/downloads/"" target=""_blank""><img src=""https://img.shields.io/badge/python-3.8.x%7C%203.9.x%7C%203.10.x%7C%203.11.x-brightgreen.svg"" alt=""Python supported""></a>
	<a href=""https://ossrank.com/p/630""><img src=""https://shields.io/endpoint?url=https://ossrank.com/shield/630""></a>
	<img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dm/Mindsdb"">
	<a href=""https://hub.docker.com/u/mindsdb"" target=""_blank""><img src=""https://img.shields.io/docker/pulls/mindsdb/mindsdb"" alt=""Docker pulls""></a>

  <br />
  <br />

  <a href=""https://github.com/mindsdb/mindsdb"">
    <img src=""/docs/assets/mindsdb_logo.jpg"" alt=""MindsDB"" width=""300"">
  </a>

  <p align=""center"">
    <br />
    <a href=""https://www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo"">"
llama3,"<p align=""center"">
  <img src=""https://github.com/meta-llama/llama3/blob/main/Llama3_Repo.jpeg"" width=""400""/>
</p>

<p align=""center"">
        ğŸ¤— <a href=""https://huggingface.co/meta-Llama""> Models on Hugging Face</a>&nbsp | <a href=""https://ai.meta.com/blog/""> Blog</a>&nbsp |  <a href=""https://llama.meta.com/"">Website</a>&nbsp | <a href=""https://llama.meta.com/get-started/"">Get Started</a>&nbsp
<br>

---

## **Note of deprecation**

Thank you for developing with Llama models. As part of the Llama 3.1 release, weâ€™ve consolidated GitHub repos and added some additional repos as weâ€™ve expanded Llamaâ€™s functionality into being an e2e Llama Stack. Please use the following repos going forward:
- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mit"
Detectron,"**Detectron is deprecated. Please see [detectron2](https://github.com/facebookresearch/detectron2), a ground-up rewrite of Detectron in PyTorch.**

# Detectron

Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.

At FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](http"
DeepFaceLive,"<table align=""center"" border=""0"">

<tr><td colspan=2 align=""center"">

![](doc/deepfacelive_intro.png)

![](doc/logo_onnx.png)![](doc/logo_directx.png)![](doc/logo_python.png)

</td></tr>
</table>
<table align=""center"" border=""0"">

<tr><td colspan=2 align=""center"">

## Face Swap (DFM)

You can swap your face from a webcam or the face in the video using trained face models.

Here is a list of available ready-to-use public face models.

These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he's breathtaking!
</td></tr>

<tr><td colspan=2 align=""center"">

<table align=""center"" border=""0"">
<tr><td align=""center"">
Keanu Reeves

<img src=""doc/celebs/Keanu_Reeves/Keanu_Reeves.png"" width=128></img>

<a href=""doc/celebs/Keanu_Reeves/examples.md"">examples</a>
</td><td align=""center"">
Irina Arty

<img src=""doc/celebs/Irina_Arty/Irina_Arty.png"" width=128></img>

examples
</td><td align=""center"">
Millie Park

<img src=""doc/celebs/Millie_Park/M"
redash,"<p align=""center"">
  <img title=""Redash"" src='https://redash.io/assets/images/logo.png' width=""200px""/>
</p>

[![Documentation](https://img.shields.io/badge/docs-redash.io/help-brightgreen.svg)](https://redash.io/help/)
[![GitHub Build](https://github.com/getredash/redash/actions/workflows/ci.yml/badge.svg)](https://github.com/getredash/redash/actions)

Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.

Redash features:

1. **Browser-based**: Everything in your browser, with a shareable URL.
2. **Ease-of-use**: Become immediately productive with data without the need to master complex software.
3. **Qu"
python-telegram-bot,".. image:: https://raw.githubusercontent.com/python-telegram-bot/logos/master/logo-text/png/ptb-logo-text_768.png
   :align: center
   :target: https://python-telegram-bot.org
   :alt: python-telegram-bot Logo

.. image:: https://img.shields.io/pypi/v/python-telegram-bot.svg
   :target: https://pypi.org/project/python-telegram-bot/
   :alt: PyPi Package Version

.. image:: https://img.shields.io/pypi/pyversions/python-telegram-bot.svg
   :target: https://pypi.org/project/python-telegram-bot/
   :alt: Supported Python versions

.. image:: https://img.shields.io/badge/Bot%20API-7.10-blue?logo=telegram
   :target: https://core.telegram.org/bots/api-changelog
   :alt: Supported Bot API version

.. image:: https://img.shields.io/pypi/dm/python-telegram-bot
   :target: https://pypistats.org/packages/python-telegram-bot
   :alt: PyPi Package Monthly Download

.. image:: https://readthedocs.org/projects/python-telegram-bot/badge/?version=stable
   :target: https://docs.python-telegram-bot.org/"
Depix,"# Depix

Depix is a PoC for a technique to recover plaintext from pixelized screenshots.

This implementation works on pixelized images that were created with a linear box filter.
In [this article](https://www.spipm.nl/2030.html) I cover background information on pixelization and similar research.

## Example

![image](docs/img/Recovering_prototype_latest.png)

## Updates

* 27 nov '23: Refactored and removed all this pip stuff. I like scripts I can just run. If a package can't be found, just install it. Also added `tool_show_boxes.py` to show how bad the box detector is (you have to really cut out the pixels exactly). Made a TODO to create a version that just cuts out boxes of static size.

## Installation

* Install the dependencies
* Run Depix:

```sh
python3 depix.py \
    -p /path/to/your/input/image.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png \
    -o /path/to/your/output.png
```

## Example usage

* Depixelize example image created with Notep"
Umi-OCR,"<p align=""left"">
    <span>
        <b>ä¸­æ–‡</b>
    </span>
    <span> â€¢ </span>
    <a href=""README_en.md"">
        English
    </a>
    <span> â€¢ </span>
    <a href=""README_ja.md"">
        æ—¥æœ¬èª
    </a>
</p>

<p align=""center"">
  <a href=""https://github.com/hiroi-sora/Umi-OCR"">
    <img width=""200"" height=""128"" src=""https://tupian.li/images/2022/10/27/icon---256.png"" alt=""Umi-OCR"">
  </a>
</p>

<h1 align=""center"">Umi-OCR æ–‡å­—è¯†åˆ«å·¥å…·</h1>

<p align=""center"">
  <a href=""https://github.com/hiroi-sora/Umi-OCR/releases/latest"">
    <img src=""https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square"" alt=""Umi-OCR"">
  </a>
  <a href=""https://github.com/hiroi-sora/Umi-OCR/blob/main/LICENSE"">
    <img src=""https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square"" alt=""LICENSE"">
  </a>
  <a href=""#ä¸‹è½½å‘è¡Œç‰ˆ"">
    <img src=""https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square"" alt=""forks"">
  </a>
  <a href=""https://star-history.com/#hiroi-s"
Hello-Python,"# Hello Python

[![Python](https://img.shields.io/badge/Python-3.10+-yellow?style=for-the-badge&logo=python&logoColor=white&labelColor=101010)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.88.0+-00a393?style=for-the-badge&logo=fastapi&logoColor=white&labelColor=101010)](https://fastapi.tiangolo.com)
[![MongoDB](https://img.shields.io/badge/MongoDB-6.0+-00684A?style=for-the-badge&logo=mongodb&logoColor=white&labelColor=101010)](https://www.mongodb.com)
[![ChatGPT](https://img.shields.io/badge/ChatGPT-GPT--4-7CF178?style=for-the-badge&logo=openai&logoColor=white&labelColor=101010)](https://platform.openai.com)
[![Reflex](https://img.shields.io/badge/Reflex-0.4.6+-5646ED?style=for-the-badge&logo=reflex&logoColor=white&labelColor=101010)](https://reflex.dev)

## Curso para aprender el lenguaje de programaciÃ³n Python desde cero y para principiantes

![](./Images/header.jpg)

### Proyecto realizado durante emisiones en directo desde [Twitch](https://twitch.tv/moured"
cascadia-code,"![Cascadia Code](images/cascadia-code.png)

# About Cascadia Code
Cascadia is a fun new coding font that comes bundled with [Windows Terminal](https://github.com/microsoft/terminal), and is now the default font in Visual Studio as well. 

# Font Variants
-  `Cascadia Code`: standard version of Cascadia
-  `Cascadia Mono`: a version of Cascadia that doesn't have ligatures
-  `Cascadia (Code|Mono) PL`: a version of Cascadia that has embedded Powerline symbols
-  `Cascadia (Code|Mono) NF`: a version of Cascadia that has Nerd Font symbols

For the italic, there is a standard `italic` and a `cursive` variant accessible via `ss01` (see [below](https://github.com/microsoft/cascadia-code/blob/main/README.md#to-enable-the-cursive-form-of-the-italic-heres-the-code-you-should-use)). 

# Font features
![Coding Ligatures](images/ligatures.png)

![Arrow Support](images/arrow_support.png)

![Stylistic Sets](images/stylistic_set.png)

Enabling stylistic sets will [vary between appl"
spleeter,"<img src=""https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png"" height=""80"" />

[![Github actions](https://github.com/deezer/spleeter/workflows/pytest/badge.svg)](https://github.com/deezer/spleeter/actions) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spleeter) [![PyPI version](https://badge.fury.io/py/spleeter.svg)](https://badge.fury.io/py/spleeter) [![Conda](https://img.shields.io/conda/vn/deezer-research/spleeter)](https://anaconda.org/deezer-research/spleeter) [![Docker Pulls](https://img.shields.io/docker/pulls/deezer/spleeter)](https://hub.docker.com/r/deezer/spleeter) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/spleeter/community) [![status](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b/status.svg)](https://joss.theoj.org/papers/"
ItChat,"# itchat

[![Gitter][gitter-picture]][gitter] ![py27][py27] ![py35][py35] [English version][english-version]

itchatæ˜¯ä¸€ä¸ªå¼€æºçš„å¾®ä¿¡ä¸ªäººå·æ¥å£ï¼Œä½¿ç”¨pythonè°ƒç”¨å¾®ä¿¡ä»æœªå¦‚æ­¤ç®€å•ã€‚

ä½¿ç”¨ä¸åˆ°ä¸‰åè¡Œçš„ä»£ç ï¼Œä½ å°±å¯ä»¥å®Œæˆä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ‰€æœ‰ä¿¡æ¯çš„å¾®ä¿¡æœºå™¨äººã€‚

å½“ç„¶ï¼Œè¯¥apiçš„ä½¿ç”¨è¿œä¸æ­¢ä¸€ä¸ªæœºå™¨äººï¼Œæ›´å¤šçš„åŠŸèƒ½ç­‰ç€ä½ æ¥å‘ç°ï¼Œæ¯”å¦‚[è¿™äº›][tutorial2]ã€‚

è¯¥æ¥å£ä¸å…¬ä¼—å·æ¥å£[itchatmp][itchatmp]å…±äº«ç±»ä¼¼çš„æ“ä½œæ–¹å¼ï¼Œå­¦ä¹ ä¸€æ¬¡æŒæ¡ä¸¤ä¸ªå·¥å…·ã€‚

å¦‚ä»Šå¾®ä¿¡å·²ç»æˆä¸ºäº†ä¸ªäººç¤¾äº¤çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼Œå¸Œæœ›è¿™ä¸ªé¡¹ç›®èƒ½å¤Ÿå¸®åŠ©ä½ æ‰©å±•ä½ çš„ä¸ªäººçš„å¾®ä¿¡å·ã€æ–¹ä¾¿è‡ªå·±çš„ç”Ÿæ´»ã€‚

## å®‰è£…

å¯ä»¥é€šè¿‡æœ¬å‘½ä»¤å®‰è£…itchatï¼š

```python
pip install itchat
```

## ç®€å•å…¥é—¨å®ä¾‹

æœ‰äº†itchatï¼Œå¦‚æœä½ æƒ³è¦ç»™æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹å‘ä¸€æ¡ä¿¡æ¯ï¼Œåªéœ€è¦è¿™æ ·ï¼š

```python
import itchat

itchat.auto_login()

itchat.send('Hello, filehelper', toUserName='filehelper')
```

å¦‚æœä½ æƒ³è¦å›å¤å‘ç»™è‡ªå·±çš„æ–‡æœ¬æ¶ˆæ¯ï¼Œåªéœ€è¦è¿™æ ·ï¼š

```python
import itchat

@itchat.msg_register(itchat.content.TEXT)
def text_reply(msg):
    return msg.text

itchat.auto_login()
itchat.run()
```

ä¸€äº›è¿›é˜¶åº”ç”¨å¯ä»¥åœ¨ä¸‹é¢çš„å¼€æºæœºå™¨äººçš„æºç å’Œè¿›é˜¶åº”ç”¨ä¸­çœ‹åˆ°ï¼Œæˆ–è€…ä½ ä¹Ÿå¯ä»¥é˜…è§ˆ[æ–‡æ¡£][document]ã€‚

## è¯•ä¸€è¯•

è¿™æ˜¯ä¸€ä¸ªåŸºäºè¿™ä¸€é¡¹ç›®çš„[å¼€æºå°æœºå™¨äºº][robot-source-code]ï¼Œç™¾é—»ä¸å¦‚ä¸€è§ï¼Œæœ‰å…´è¶£å¯ä»¥å°è¯•ä¸€ä¸‹ã€‚

ç”±äºå¥½å‹æ•°é‡å®åœ¨å¢é•¿è¿‡å¿«ï¼Œè‡ªåŠ¨é€šè¿‡å¥½å‹éªŒè¯çš„åŠŸèƒ½æ¼”ç¤ºæš‚æ—¶å…³é—­ã€‚

![QRCode][robot-qr]

## æˆªå±

![file-autoreply][robot-demo-file] ![login-page][robot-demo-login]

## è¿›é˜¶åº”ç”¨

### ç‰¹æ®Šçš„å­—å…¸ä½¿ç”¨æ–¹å¼
"
YouCompleteMe,"YouCompleteMe: a code-completion engine for Vim
===============================================

[![Gitter room](https://img.shields.io/gitter/room/Valloric/YouCompleteMe.svg)](https://gitter.im/Valloric/YouCompleteMe)
[![Build status](https://dev.azure.com/YouCompleteMe/YCM/_apis/build/status/ycm-core.YouCompleteMe?branchName=master)](https://dev.azure.com/YouCompleteMe/YCM/_build?definitionId=3&branchName=master)
[![Coverage status](https://img.shields.io/codecov/c/github/ycm-core/YouCompleteMe/master.svg)](https://codecov.io/gh/ycm-core/YouCompleteMe)

Help, Advice, Support
---------------------

Looking for help, advice, or support? Having problems getting YCM to work?

First carefully read the [installation instructions](#installation) for your OS.
We recommend you use the supplied `install.py` - the ""full"" installation guide
is for rare, advanced use cases and most users should use `install.py`.

If the server isn't starting and you're getting a ""YouCompleteMe unavailable""
error,"
so-vits-svc,"<div align=""center"">
<img alt=""LOGO"" src=""https://avatars.githubusercontent.com/u/127122328?s=400&u=5395a98a4f945a3a50cb0cc96c2747505d190dbc&v=4"" width=""300"" height=""300"" />
  
# SoftVC VITS Singing Voice Conversion

[**English**](./README.md) | [**ä¸­æ–‡ç®€ä½“**](./README_zh_CN.md)

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/svc-develop-team/so-vits-svc/blob/4.1-Stable/sovits4_for_colab.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-AGPL3.0-green.svg?style=for-the-badge)](https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/LICENSE)

This round of limited time update is coming to an end, the warehouse will enter the Archieve state, please know

</div>

> âœ¨ A studio that contains visible f0 editor, speaker mix timeline editor and other features (Where the Onnx models are used) : [MoeVoiceStudio](https://github.com/NaruseMioShirakana/MoeVoiceStudio)

"
MiniGPT-4,"# MiniGPT-V

<font size='5'>**MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning**</font>

Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiongâ˜¨, Mohamed Elhoseinyâ˜¨

â˜¨equal last author

<a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/abs/2310.09478.pdf'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>  <a href='https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'> <a href='https://minigpt-v2.github.io'><img src='https://img.shields.io/badge/Gradio-Demo-blue'></a> [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=atFCwV2hSY4)


<font size='5'> **MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models**</font>

Deyao Zhu*, J"
diffusers,"<!---
Copyright 2022 - The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg"" width=""400""/>
    <br>
<p>
<p align=""center"">
    <a href=""https://github.com/huggingface/diffusers/blob/main/LICENSE""><img alt=""GitHub"" src=""https://img.shields.io/github/license/huggingface/datasets.svg?color=blue""></a>
    <a href=""https://github.com/hugg"
jumpserver,"<div align=""center"">
  <a name=""readme-top""></a>
  <a href=""https://jumpserver.org/index-en.html""><img src=""https://download.jumpserver.org/images/jumpserver-logo.svg"" alt=""JumpServer"" width=""300"" /></a>
  
## An open-source PAM tool (Bastion Host)

[![][license-shield]][license-link]
[![][discord-shield]][discord-link]
[![][docker-shield]][docker-link]
[![][github-release-shield]][github-release-link]
[![][github-stars-shield]][github-stars-link]

**English** Â· [ç®€ä½“ä¸­æ–‡](./README.zh-CN.md)
</div>
<br/>

## What is JumpServer?

JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.

![JumpServer Overview](https://github.com/jumpserver/jumpserver/assets/32935519/35a371cb-8590-40ed-88ec-f351f8cf9045)

## Quickstart

Prepare a clean Linux Server ( 64 bit, >= 4c8g )

```sh
curl -sSL https://github.com/jumpserver/jumpserver/releases/l"
textual,"


![Textual splash image](https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png)

[![Discord](https://img.shields.io/discord/1026214085173461072)](https://discord.gg/Enf6Z3qhVr)


# Textual

Textual is a *Rapid Application Development* framework for Python.

Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a [web browser](https://github.com/Textualize/textual-web)!


<details>
  <summary> ğŸ¬ Demonstration </summary>
  <hr>

A quick run through of some Textual features.



https://user-images.githubusercontent.com/554369/197355913-65d3c125-493d-4c05-a590-5311f16c40ff.mov



 </details>


## About

Textual adds interactivity to [Rich](https://github.com/Textualize/rich) with an API inspired by modern web development.

On modern terminal software (installed by default on most systems), Textual apps can use **16.7 million** colors with mouse support and smooth flicker-free animation. A powerful layout engine and re-usable "
pipenv,"Pipenv: Python Development Workflow for Humans
==============================================

[![image](https://img.shields.io/pypi/v/pipenv.svg)](https://python.org/pypi/pipenv)
[![image](https://img.shields.io/pypi/l/pipenv.svg)](https://python.org/pypi/pipenv)
[![CI](https://github.com/pypa/pipenv/actions/workflows/ci.yaml/badge.svg)](https://github.com/pypa/pipenv/actions/workflows/ci.yaml)
[![image](https://img.shields.io/pypi/pyversions/pipenv.svg)](https://python.org/pypi/pipenv)

------------------------------------------------------------------------

**Pipenv** is a Python virtualenv management tool that supports a multitude of systems and nicely bridges the gaps between pip, python (using system python, pyenv or asdf) and virtualenv.
*Linux, macOS, and Windows are all first-class citizens in pipenv.*

Pipenv automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your `Pipfile` as you install/uninstall packages. It also genera"
locust,"# Locust

[![PyPI](https://img.shields.io/pypi/v/locust.svg)](https://pypi.org/project/locust/)<!--![Python Version from PEP 621 TOML](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Flocustio%2Flocust%2Fmaster%2Fpyproject.toml)-->[![Downloads](https://pepy.tech/badge/locust/week)](https://pepy.tech/project/locust)
[![Build Status](https://github.com/locustio/locust/workflows/Tests/badge.svg)](https://github.com/locustio/locust/actions?query=workflow%3ATests)
[![GitHub contributors](https://img.shields.io/github/contributors/locustio/locust.svg)](https://github.com/locustio/locust/graphs/contributors)
[![Support Ukraine Badge](https://bit.ly/support-ukraine-now)](https://github.com/support-ukraine/support-ukraine)

Locust is an open source performance/load testing tool for HTTP and other protocols. Its developer-friendly approach lets you define your tests in regular Python code.

Locust tests can be run from command line or usi"
celery,".. image:: https://docs.celeryq.dev/en/latest/_images/celery-banner-small.png

|build-status| |coverage| |license| |wheel| |semgrep| |pyversion| |pyimp| |ocbackerbadge| |ocsponsorbadge|

:Version: 5.5.0b3 (immunity)
:Web: https://docs.celeryq.dev/en/stable/index.html
:Download: https://pypi.org/project/celery/
:Source: https://github.com/celery/celery/
:Keywords: task, queue, job, async, rabbitmq, amqp, redis,
  python, distributed, actors

Donations
=========

This project relies on your generous donations.

If you are using Celery to create a commercial product, please consider becoming our `backer`_ or our `sponsor`_ to ensure Celery's future.

.. _`backer`: https://opencollective.com/celery#backer
.. _`sponsor`: https://opencollective.com/celery#sponsor

For enterprise
==============

Available as part of the Tidelift Subscription.

The maintainers of ``celery`` and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open sour"
Mask_RCNN,"# Mask R-CNN for Object Detection and Segmentation

This is an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870) on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.

![Instance Segmentation Sample](assets/street.png)

The repository includes:
* Source code of Mask R-CNN built on FPN and ResNet101.
* Training code for MS COCO
* Pre-trained weights for MS COCO
* Jupyter notebooks to visualize the detection pipeline at every step
* ParallelModel class for multi-GPU training
* Evaluation on MS COCO metrics (AP)
* Example of training on your own dataset


The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released [Matterport3D](https://matterport.com/blog/2017/09/20/announcin"
vnpy,"# VeighNa - By Traders, For Traders.

<p align=""center"">
  <img src =""https://vnpy.oss-cn-shanghai.aliyuncs.com/veighna-logo.png""/>
</p>

ğŸ’¬ Want to read this in **english** ? Go [**here**](README_ENG.md)

<p align=""center"">
    <img src =""https://img.shields.io/badge/version-3.9.2-blueviolet.svg""/>
    <img src =""https://img.shields.io/badge/platform-windows|linux|macos-yellow.svg""/>
    <img src =""https://img.shields.io/badge/python-3.10|3.11.|3.12-blue.svg"" />
    <img src =""https://img.shields.io/github/actions/workflow/status/vnpy/vnpy/pythonapp.yml?branch=master""/>
    <img src =""https://img.shields.io/github/license/vnpy/vnpy.svg?color=orange""/>
</p>

VeighNaæ˜¯ä¸€å¥—åŸºäºPythonçš„å¼€æºé‡åŒ–äº¤æ˜“ç³»ç»Ÿå¼€å‘æ¡†æ¶ï¼Œåœ¨å¼€æºç¤¾åŒºæŒç»­ä¸æ–­çš„è´¡çŒ®ä¸‹ä¸€æ­¥æ­¥æˆé•¿ä¸ºå¤šåŠŸèƒ½é‡åŒ–äº¤æ˜“å¹³å°ï¼Œè‡ªå‘å¸ƒä»¥æ¥å·²ç»ç§¯ç´¯äº†ä¼—å¤šæ¥è‡ªé‡‘èæœºæ„æˆ–ç›¸å…³é¢†åŸŸçš„ç”¨æˆ·ï¼ŒåŒ…æ‹¬ç§å‹ŸåŸºé‡‘ã€è¯åˆ¸å…¬å¸ã€æœŸè´§å…¬å¸ç­‰ã€‚

:rocket: :rocket: :rocket: **é¢å‘ä¸“ä¸šäº¤æ˜“å‘˜çš„ã€VeighNa Eliteé‡åŒ–ç»ˆç«¯ã€‘å·²ç»æ­£å¼å‘å¸ƒï¼Œé’ˆå¯¹ä¸“ä¸šäº¤æ˜“å‘˜ç¾¤ä½“åœ¨æµ·é‡ç­–ç•¥å¹¶å‘ã€æ™ºèƒ½ç§»ä»“æ¢æœˆã€ç®—æ³•æ‹†å•æ‰§è¡Œã€å¤šè´¦æˆ·äº¤æ˜“æ”¯æŒç­‰æ–¹é¢çš„éœ€æ±‚æä¾›äº†å®Œå–„æ”¯æŒã€‚äº†è§£æ›´è¯¦ç»†çš„ä¿¡æ¯è¯·æ‰«æä¸‹æ–¹äºŒç»´ç å…³æ³¨åï¼Œç‚¹å‡»èœå•æ çš„ã€ç¤¾åŒºäº¤æµ -> Eliteä¼šå‘˜æœåŠ¡ã€‘å³å¯**ï¼š

<p align=""center"">
  <img src =""https://vnpy.oss-cn-shangha"
wttr.in,"
*wttr.in â€” the right way to ~check~ `curl` the weather!*

wttr.in is a console-oriented weather forecast service that supports various information
representation methods like terminal-oriented ANSI-sequences for console HTTP clients
(curl, httpie, or wget), HTML for web browsers, or PNG for graphical viewers.

Originally started as a small project, a wrapper for [wego](https://github.com/schachmat/wego),
intended to demonstrate the power of the console-oriented services,
*wttr.in* became a popular weather reporting service, handling tens of millions of queries daily.

You can see it running here: [wttr.in](https://wttr.in).

[Documentation](https://wttr.in/:help) | [Usage](https://github.com/chubin/wttr.in#usage) | [One-line output](https://github.com/chubin/wttr.in#one-line-output) | [Data-rich output format](https://github.com/chubin/wttr.in#data-rich-output-format-v2) | [Map view](https://github.com/chubin/wttr.in#map-view-v3) | [Output formats](https://github.com/chubin/wttr.in#di"
generative-models,"# Generative Models by Stability AI

![sample1](assets/000.jpg)

## News


**July 24, 2024**
- We are releasing **[Stable Video 4D (SV4D)](https://huggingface.co/stabilityai/sv4d)**, a video-to-4D diffusion model for novel-view video synthesis. For research purposes:
    - **SV4D** was trained to generate 40 frames (5 video frames x 8 camera views) at 576x576 resolution, given 5 context frames (the input video), and 8 reference views (synthesised from the first frame of the input video, using a multi-view diffusion model like SV3D) of the same size, ideally white-background images with one object.
    - To generate longer novel-view videos (21 frames), we propose a novel sampling method using SV4D, by first sampling 5 anchor frames and then densely sampling the remaining frames while maintaining temporal consistency.
    - To run the community-build gradio demo locally, run `python -m scripts.demo.gradio_app_sv4d`.
    - Please check our [project page](https://sv4d.github.io), [tech re"
algorithms,"[![PyPI version](https://badge.fury.io/py/algorithms.svg)](https://badge.fury.io/py/algorithms)
[![Open Source Helpers](https://www.codetriage.com/keon/algorithms/badges/users.svg)](https://www.codetriage.com/keon/algorithms)
[![Build Status](https://travis-ci.org/keon/algorithms.svg?branch=master)](https://travis-ci.org/keon/algorithms)
[![Coverage Status](https://coveralls.io/repos/github/keon/algorithms/badge.svg?branch=master)](https://coveralls.io/github/keon/algorithms?branch=master)

<p align=""center""><img src=""https://raw.githubusercontent.com/keon/algorithms/master/docs/source/_static/logo/logotype1blue.png""></p>

Pythonic Data Structures and Algorithms
=========================================

Minimal and clean example implementations of data structures and algorithms in Python 3.

## Contributing
Thanks for your interest in contributing! There are many ways to contribute to this project. [Get started here](CONTRIBUTING.md)

## Tests

### Use unittest
For running all tests w"
kitty,"= kitty - the fast, feature-rich, cross-platform, GPU based terminal

See https://sw.kovidgoyal.net/kitty/[the kitty website].

image:https://github.com/kovidgoyal/kitty/workflows/CI/badge.svg[""Build status"", link=""https://github.com/kovidgoyal/kitty/actions?query=workflow%3ACI""]

https://sw.kovidgoyal.net/kitty/faq/[Frequently Asked Questions]

To ask other questions about kitty usage, use either the https://github.com/kovidgoyal/kitty/discussions/[discussions on GitHub] or the
https://www.reddit.com/r/KittyTerminal[Reddit community]

Packaging status in various repositories:

image:https://repology.org/badge/vertical-allrepos/kitty.svg?columns=3&header=kitty[""Packaging status"", link=""https://repology.org/project/kitty/versions""]
"
ML-From-Scratch,"# Machine Learning From Scratch

## About
Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.

The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.

## Table of Contents
- [Machine Learning From Scratch](#machine-learning-from-scratch)
  * [About](#about)
  * [Table of Contents](#table-of-contents)
  * [Installation](#installation)
  * [Examples](#examples)
    + [Polynomial Regression](#polynomial-regression)
    + [Classification With CNN](#classification-with-cnn)
    + [Density-Based Clustering](#density-based-clustering)
    + [Generating Handwritten Digits](#generating-handwritten-digits)
    + [Deep Reinforcement Learning](#deep-reinforcement-learning)
    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)
    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-"
EasyOCR,"# EasyOCR

[![PyPI Status](https://badge.fury.io/py/easyocr.svg)](https://badge.fury.io/py/easyocr)
[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/JaidedAI/EasyOCR/blob/master/LICENSE)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.to/easyocr)
[![Tweet](https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR)
[![Twitter](https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat)](https://twitter.com/JaidedAI)

Ready-to-use OCR with 80+ [supported languages](https://www.jaided.ai/easyocr) and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.

[Try Demo on our website](https://www.jaided.ai/easyocr)

Integrated into [Huggingface Spaces ğŸ¤—](https://huggingface.co/spaces) using [Gradio](https://githu"
JARVIS,"# JARVIS


[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2303.17580)
[![Open in Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/microsoft/HuggingGPT)

The mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community.

## What's New

+  [2024.01.15] We release Easytool for easier tool usage.
   + The code and datasets are available at [EasyTool](/easytool).
   + The paper is available at [EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction](https://arxiv.org/abs/2401.06201).
+  [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs.
   + The code and datasets are available at [TaskBench](/taskbench).
   + The paper is available at [TaskBench: Benchmarking Large Language Models for Task Automation](https://arxiv.org/abs/2311.18760).
+  [2023.07.28] We are now in the process of plann"
d2l-en,"<div align=""left"">
  <img src=""https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png"" width=""350"">
</div>

# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions

[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)

[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)

<h5 align=""center""><i>The best way to understand deep learning is learning by doing.</i></h5>

<p align=""center"">
  <img width=""200""  src=""static/frontpage/_images/eq.jpg"">
  <img width=""200""  src=""static/frontpage/_images/figure.jpg"">
  <img width=""200""  src=""static/frontpage/_images/code.jpg"">
  <img width=""200""  src=""static/frontpage/_images/notebook.gif"">
</p>

This open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the c"
Retrieval-based-Voice-Conversion-WebUI,"<div align=""center"">

<h1>Retrieval-based-Voice-Conversion-WebUI</h1>
ä¸€ä¸ªåŸºäºVITSçš„ç®€å•æ˜“ç”¨çš„å˜å£°æ¡†æ¶<br><br>

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange
)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)

<img src=""https://counter.seku.su/cmoe?name=rvc&theme=r34"" /><br>

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/Retrieval_based_Voice_Conversion_WebUI.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/ğŸ¤—%20-Spaces-yellow.svg?style=for-the-badge)](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)

[![Discord](https://img.shields.io/badge/RVC%20Developers-Discor"
insightface,"
# InsightFace: 2D and 3D Face Analysis Project

<div align=""left"">
  <img src=""https://insightface.ai/assets/img/custom/logo3.jpg"" width=""240""/>
</div>

[InsightFace](https://insightface.ai) project is mainly maintained By [Jia Guo](mailto:guojia@gmail.com?subject=[GitHub]%20InsightFace%20Project) and [Jiankang Deng](https://jiankangdeng.github.io/). 

For all main contributors, please check [contributing](#contributing).

## License

The code of InsightFace is released under the MIT License. There is no limitation for both academic and commercial usage.

The training data containing the annotation (and the models trained with these data) are available for non-commercial research purposes only.

Both manual-downloading models from our github repo and auto-downloading models with our [python-library](python-package) follow the above license policy(which is for non-commercial research purposes only).

## Top News

**`2024-08-01`** We have integrated our most advanced face-swapping model"
algo,"# æ•°æ®ç»“æ„å’Œç®—æ³•å¿…çŸ¥å¿…ä¼šçš„50ä¸ªä»£ç å®ç°
### å¾®ä¿¡æœç´¢æˆ‘çš„å…¬ä¼—å·â€œå°äº‰å“¥â€ï¼Œæˆ–è€…å¾®ä¿¡æ‰«æä¸‹é¢äºŒç»´ç å…³æ³¨
### å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼Œå›å¤â€PDFâ€œè·å–ç‹¬å®¶ç®—æ³•èµ„æ–™ã€‚
### å‰Googleå·¥ç¨‹å¸ˆï¼Œ10ä¸‡äººè·Ÿç€å­¦çš„ã€Šæ•°æ®ç»“æ„å’Œç®—æ³•ä¹‹ç¾ã€‹ã€Šè®¾è®¡æ¨¡å¼ä¹‹ç¾ã€‹ä¸“æ ä½œè€…
![t2](https://github.com/wangzheng0822/markdownphotos/blob/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg)

## æ•°ç»„
* å®ç°ä¸€ä¸ªæ”¯æŒåŠ¨æ€æ‰©å®¹çš„æ•°ç»„
* å®ç°ä¸€ä¸ªå¤§å°å›ºå®šçš„æœ‰åºæ•°ç»„ï¼Œæ”¯æŒåŠ¨æ€å¢åˆ æ”¹æ“ä½œ
* å®ç°ä¸¤ä¸ªæœ‰åºæ•°ç»„åˆå¹¶ä¸ºä¸€ä¸ªæœ‰åºæ•°ç»„

## é“¾è¡¨
* å®ç°å•é“¾è¡¨ã€å¾ªç¯é“¾è¡¨ã€åŒå‘é“¾è¡¨ï¼Œæ”¯æŒå¢åˆ æ“ä½œ
* å®ç°å•é“¾è¡¨åè½¬
* å®ç°ä¸¤ä¸ªæœ‰åºçš„é“¾è¡¨åˆå¹¶ä¸ºä¸€ä¸ªæœ‰åºé“¾è¡¨
* å®ç°æ±‚é“¾è¡¨çš„ä¸­é—´ç»“ç‚¹

## æ ˆ
* ç”¨æ•°ç»„å®ç°ä¸€ä¸ªé¡ºåºæ ˆ
* ç”¨é“¾è¡¨å®ç°ä¸€ä¸ªé“¾å¼æ ˆ
* ç¼–ç¨‹æ¨¡æ‹Ÿå®ç°ä¸€ä¸ªæµè§ˆå™¨çš„å‰è¿›ã€åé€€åŠŸèƒ½

## é˜Ÿåˆ—
* ç”¨æ•°ç»„å®ç°ä¸€ä¸ªé¡ºåºé˜Ÿåˆ—
* ç”¨é“¾è¡¨å®ç°ä¸€ä¸ªé“¾å¼é˜Ÿåˆ—
* å®ç°ä¸€ä¸ªå¾ªç¯é˜Ÿåˆ—

## é€’å½’
* ç¼–ç¨‹å®ç°æ–æ³¢é‚£å¥‘æ•°åˆ—æ±‚å€¼f(n)=f(n-1)+f(n-2)
* ç¼–ç¨‹å®ç°æ±‚é˜¶ä¹˜n!
* ç¼–ç¨‹å®ç°ä¸€ç»„æ•°æ®é›†åˆçš„å…¨æ’åˆ—

## æ’åº
* å®ç°å½’å¹¶æ’åºã€å¿«é€Ÿæ’åºã€æ’å…¥æ’åºã€å†’æ³¡æ’åºã€é€‰æ‹©æ’åº
* ç¼–ç¨‹å®ç°O(n)æ—¶é—´å¤æ‚åº¦å†…æ‰¾åˆ°ä¸€ç»„æ•°æ®çš„ç¬¬Kå¤§å…ƒç´ 

## äºŒåˆ†æŸ¥æ‰¾
* å®ç°ä¸€ä¸ªæœ‰åºæ•°ç»„çš„äºŒåˆ†æŸ¥æ‰¾ç®—æ³•
* å®ç°æ¨¡ç³ŠäºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼ˆæ¯”å¦‚å¤§äºç­‰äºç»™å®šå€¼çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰

## æ•£åˆ—è¡¨
* å®ç°ä¸€ä¸ªåŸºäºé“¾è¡¨æ³•è§£å†³å†²çªé—®é¢˜çš„æ•£åˆ—è¡¨
* å®ç°ä¸€ä¸ªLRUç¼“å­˜æ·˜æ±°ç®—æ³•

## å­—ç¬¦ä¸²
* å®ç°ä¸€ä¸ªå­—ç¬¦é›†ï¼ŒåªåŒ…å«aï½zè¿™26ä¸ªè‹±æ–‡å­—æ¯çš„Trieæ ‘
* å®ç°æœ´ç´ çš„å­—ç¬¦ä¸²åŒ¹é…ç®—æ³•

## äºŒå‰æ ‘
* å®ç°ä¸€ä¸ªäºŒå‰æŸ¥æ‰¾æ ‘ï¼Œå¹¶ä¸”æ”¯æŒæ’å…¥ã€åˆ é™¤ã€æŸ¥æ‰¾æ“ä½œ
* å®ç°æŸ¥æ‰¾äºŒå‰æŸ¥æ‰¾æ ‘ä¸­æŸä¸ªèŠ‚ç‚¹çš„åç»§ã€å‰é©±èŠ‚ç‚¹
* å®ç°äºŒå‰æ ‘å‰ã€ä¸­ã€ååºä»¥åŠæŒ‰å±‚éå†

## å †
* å®ç°ä¸€ä¸ªå°é¡¶å †ã€å¤§é¡¶å †ã€ä¼˜å…ˆçº§é˜Ÿåˆ—
* å®ç°å †æ’åº
* åˆ©ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—åˆå¹¶Kä¸ªæœ‰åºæ•°ç»„
* æ±‚ä¸€ç»„åŠ¨æ€æ•°æ®é›†åˆçš„æœ€å¤§Top K

## å›¾
* å®ç°æœ‰å‘å›¾ã€æ— å‘å›¾ã€æœ‰æƒå›¾ã€æ— æƒå›¾çš„é‚»æ¥çŸ©é˜µå’Œé‚»æ¥è¡¨è¡¨ç¤ºæ–¹æ³•
* å®ç°å›¾çš„æ·±åº¦ä¼˜å…ˆæœç´¢ã€å¹¿åº¦ä¼˜å…ˆæœç´¢
* å®ç°Dijkstraç®—æ³•ã€"
PythonRobotics,"<img src=""https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"" align=""right"" width=""300"" alt=""header pic""/>

# PythonRobotics
![GitHub_Action_Linux_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg)
![GitHub_Action_MacOS_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg)
![GitHub_Action_Windows_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)
[![codecov](https://codecov.io/gh/AtsushiSakai/PythonRobotics/branch/master/graph/badge.svg)](https://codecov.io/gh/AtsushiSakai/PythonRobotics)

Python codes for robotics algorithm.


# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * "
pytorch-CycleGAN-and-pix2pix,"
<img src='imgs/horse2zebra.gif' align=""right"" width=384>

<br><br><br>

# CycleGAN and pix2pix in PyTorch

**New**:  Please check out [img2img-turbo](https://github.com/GaParmar/img2img-turbo) repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDiffusion-Turbo model. The inference time for 512x512 image is 0.29 sec on A6000 and 0.11 sec on A100.

Please check out [contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.

We provide PyTorch implementations for both unpaired and paired image-to-image translation.

The code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesungp), and supported by [Tongzhou Wang](https://github.com/Ss"
NLP-progress,"# Tracking Progress in Natural Language Processing

## Table of contents

### English

- [Automatic speech recognition](english/automatic_speech_recognition.md)
- [CCG](english/ccg.md)
- [Common sense](english/common_sense.md)
- [Constituency parsing](english/constituency_parsing.md)
- [Coreference resolution](english/coreference_resolution.md)
- [Data-to-Text Generation](english/data_to_text_generation.md)
- [Dependency parsing](english/dependency_parsing.md)
- [Dialogue](english/dialogue.md)
- [Domain adaptation](english/domain_adaptation.md)
- [Entity linking](english/entity_linking.md)
- [Grammatical error correction](english/grammatical_error_correction.md)
- [Information extraction](english/information_extraction.md)
- [Intent Detection and Slot Filling](english/intent_detection_slot_filling.md) 
- [Keyphrase Extraction and Generation](english/keyphrase_extraction_generation.md)
- [Language modeling](english/language_modeling.md)
- [Lexical normalization](english/lexical_normaliz"
deep-learning-for-image-processing,"# æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨æ•™ç¨‹

## å‰è¨€
* æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚
* æœ¬æ•™ç¨‹ä¼šä»¥è§†é¢‘çš„æ–¹å¼è¿›è¡Œåˆ†äº«ï¼Œæ•™å­¦æµç¨‹å¦‚ä¸‹ï¼š  
1ï¼‰ä»‹ç»ç½‘ç»œçš„ç»“æ„ä¸åˆ›æ–°ç‚¹  
2ï¼‰ä½¿ç”¨Pytorchè¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ  
3ï¼‰ä½¿ç”¨Tensorflowï¼ˆå†…éƒ¨çš„kerasæ¨¡å—ï¼‰è¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ 
* è¯¾ç¨‹ä¸­æ‰€æœ‰PPTéƒ½æ”¾åœ¨`course_ppt`æ–‡ä»¶å¤¹ä¸‹ï¼Œéœ€è¦çš„è‡ªè¡Œä¸‹è½½ã€‚


## æ•™ç¨‹ç›®å½•ï¼Œç‚¹å‡»è·³è½¬ç›¸åº”è§†é¢‘ï¼ˆåæœŸä¼šæ ¹æ®å­¦ä¹ å†…å®¹å¢åŠ ï¼‰

* å›¾åƒåˆ†ç±»
  * LeNetï¼ˆå·²å®Œæˆï¼‰
    * [Pytorchå®˜æ–¹demo(Lenet)](https://www.bilibili.com/video/BV187411T7Ye)
    * [Tensorflow2å®˜æ–¹demo](https://www.bilibili.com/video/BV1n7411T7o6)

  * AlexNetï¼ˆå·²å®Œæˆï¼‰
    * [AlexNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1p7411T7Pc)
    * [Pytorchæ­å»ºAlexNet](https://www.bilibili.com/video/BV1W7411T7qc)
    * [Tensorflow2æ­å»ºAlexnet](https://www.bilibili.com/video/BV1s7411T7vs)

  * VggNetï¼ˆå·²å®Œæˆï¼‰
    * [VggNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1q7411T7Y6)
    * [Pytorchæ­å»ºVGGç½‘ç»œ](https://www.bilibili.com/video/BV1i7411T7ZN)
    * [Tensorflow2æ­å»ºVGGç½‘ç»œ](https://www.bilibili.com/video/BV1q7411T76b)

  * GoogLeNetï¼ˆå·²å®Œæˆï¼‰
    * [GoogLeNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1z7411T7ie)
    * [Pytorchæ­å»ºGoogLeNetç½‘ç»œ]"
labelImg,".. image:: /readme/images/labelimg.png
        :target: https://github.com/heartexlabs/label-studio

Label Studio is a modern, multi-modal data annotation tool
=======

LabelImg, the popular image annotation tool created by Tzutalin with the help of dozens contributors, is no longer actively being developed and has become part of the Label Studio community. Check out `Label Studio <https://github.com/heartexlabs/label-studio>`__, the most flexible open source data labeling tool for images, text, hypertext, audio, video and time-series data. `Install <https://labelstud.io/guide/install.html>`__ Label Studio and join the `slack community <https://label-studio.slack.com/>`__ to get started.

.. image:: /readme/images/label-studio-1-6-player-screenshot.png
        :target: https://github.com/heartexlabs/label-studio

About LabelImg
========

.. image:: https://img.shields.io/pypi/v/labelimg.svg
        :target: https://pypi.python.org/pypi/labelimg

.. image:: https://img.shields.io/github"
cookiecutter,"<h1 align=""center"">
    <img alt=""cookiecutter Logo"" width=""200px"" src=""https://raw.githubusercontent.com/cookiecutter/cookiecutter/3ac078356adf5a1a72042dfe72ebfa4a9cd5ef38/logo/cookiecutter_medium.png"">
</h1>

<div align=""center"">

[![pypi](https://img.shields.io/pypi/v/cookiecutter.svg)](https://pypi.org/project/cookiecutter/)
[![python](https://img.shields.io/pypi/pyversions/cookiecutter.svg)](https://pypi.org/project/cookiecutter/)
[![Build Status](https://github.com/cookiecutter/cookiecutter/actions/workflows/tests.yml/badge.svg?branch=main)](https://github.com/cookiecutter/cookiecutter/actions)
[![codecov](https://codecov.io/gh/cookiecutter/cookiecutter/branch/main/graphs/badge.svg?branch=main)](https://codecov.io/github/cookiecutter/cookiecutter?branch=main)
[![discord](https://img.shields.io/badge/Discord-cookiecutter-5865F2?style=flat&logo=discord&logoColor=white)](https://discord.gg/9BrxzPKuEW)
[![docs](https://readthedocs.org/projects/cookiecutter/badge/?version=latest)](htt"
gpt-2,"**Status:** Archive (code is provided as-is, no updates expected)

# gpt-2

Code and models from the paper [""Language Models are Unsupervised Multitask Learners""](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).

You can read about GPT-2 and its staged release in our [original blog post](https://openai.com/research/better-language-models/), [6 month follow-up post](https://openai.com/blog/gpt-2-6-month-follow-up/), and [final post](https://www.openai.com/blog/gpt-2-1-5b-release/).

We have also [released a dataset](https://github.com/openai/gpt-2-output-dataset) for researchers to study their behaviors.

<sup>*</sup> *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.*

## Usage

This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.

For basic information, see our [mode"
supervision,"<div align=""center"">
  <p>
    <a align=""center"" href="""" target=""https://supervision.roboflow.com"">
      <img
        width=""100%""
        src=""https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529""
      >
    </a>
  </p>

  <br>

[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)

  <br>

[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)
[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)
[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)
[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)
[![python-version](https://img.shields.io/pypi/pyversions/supervi"
examples,"# PyTorch Examples

![Run Examples](https://github.com/pytorch/examples/workflows/Run%20Examples/badge.svg)

https://pytorch.org/examples/

`pytorch/examples` is a repository showcasing examples of using [PyTorch](https://github.com/pytorch/pytorch). The goal is to have curated, short, few/no dependencies _high quality_ examples that are substantially different from each other that can be emulated in your existing work.

- For tutorials: https://github.com/pytorch/tutorials
- For changes to pytorch.org: https://github.com/pytorch/pytorch.github.io
- For a general model hub: https://pytorch.org/hub/ or https://huggingface.co/models
- For recipes on how to run PyTorch in production: https://github.com/facebookresearch/recipes
- For general Q&A and support: https://discuss.pytorch.org/

## Available models

- [Image classification (MNIST) using Convnets](./mnist/README.md)
- [Word-level Language Modeling using RNN and Transformer](./word_language_model/README.md)
- [Training Imagenet Clas"
openai-python,"# OpenAI Python API library

[![PyPI version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)

The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+
application. The library includes type definitions for all request params and response fields,
and offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).

It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).

## Documentation

The REST API documentation can be found on [platform.openai.com](https://platform.openai.com/docs). The full API of this library can be found in [api.md](api.md).

## Installation

> [!IMPORTANT]
> The SDK was rewritten in v1, which was released November 6th 2023. See the [v1 migration guide](https://github.com/openai/openai-python/discussions/742), which includes scripts to automatically update your code.

```sh
# ins"
Awesome-Linux-Software,"# Awesome Linux Software

![Tux](img/tux.png)

ğŸ§ This repo is a collection of **AWESOME** Linux applications and tools for **any users/developers**.

ğŸ§ Feel free to **contribute** / **star** / **fork** / **pull request** . Any **recommendations** and **suggestions** are welcome.

**Acknowledgement:** _Everything written below is from my own experience in college and after reading various materials. I am neither a professional nor an expert, but a passionate user. Anyone can open a discussion in the issue section, or a pull request if something should be modified or added._

- Brazilian Portuguese version : [here](https://github.com/LewisVo/Awesome-Linux-Software/blob/master/README_pt-BR.md).
- Chinese version : [here](https://github.com/LewisVo/Awesome-Linux-Software/blob/master/README_zh-CN.md) or [here](https://github.com/eniqiz/Awesome-Linux-Software-zh_CN).
- Spanish version : [here](https://github.com/LewisVo/Awesome-Linux-Software/blob/master/README_es-ES.md) or [here](https://gi"
mem0,"<p align=""center"">
  <a href=""https://github.com/mem0ai/mem0"">
  <img src=""docs/images/banner-sm.png"" width=""800px"" alt=""Mem0 - The Memory Layer for Personalized AI"">
  </a>
<p align=""center""><a href=https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps target='_blank'><img alt=Launch YC: Mem0 - Open Source Memory Layer for AI Apps src=https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps/upvote_embed.svg/></a></p>


  <p align=""center"">
    <a href=""https://mem0.ai"">Learn more</a>
    Â·
    <a href=""https://mem0.ai/discord"">Join Discord</a>
  </p>
</p>

<p align=""center"">
  <a href=""https://mem0.ai/discord"">
    <img src=""https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat"" alt=""Mem0 Discord"">
  </a>
  <a href=""https://pepy.tech/project/mem0ai"">
    <img src=""https://img.shields.io/pypi/dm/mem0ai"" alt=""Mem0 PyPI - Downloads"" >
  </a>
  <a href=""https://pypi.org/project/mem0ai"" target=""_blank"">
        <img src=""https://"
Hitomi-Downloader,"<p align=""center"">
  <img src=""imgs/card_crop.png"" width=""50%""/>
  <br>
</p>

[![GitHub release](https://img.shields.io/github/release/KurtBestor/Hitomi-Downloader.svg?logo=github)](https://github.com/KurtBestor/Hitomi-Downloader/releases/latest)
[![GitHub downloads](https://img.shields.io/github/downloads/KurtBestor/Hitomi-Downloader/latest/total.svg?logo=github)](https://github.com/KurtBestor/Hitomi-Downloader/releases/latest)
[![GitHub downloads](https://img.shields.io/github/downloads/KurtBestor/Hitomi-Downloader/total.svg?logo=github)](https://github.com/KurtBestor/Hitomi-Downloader/releases)

## Links
- [Download](https://github.com/KurtBestor/Hitomi-Downloader/releases/latest)
- [Issues](https://github.com/KurtBestor/Hitomi-Downloader/issues)
- [Scripts & Plugins](https://github.com/KurtBestor/Hitomi-Downloader/wiki/Scripts-&-Plugins)
- [Chrome Extension](https://github.com/KurtBestor/Hitomi-Downloader/wiki/Chrome-Extension)

## Demo
<img src=""imgs/how_to_download.gif"">

## Feat"
Open-Sora,"<p align=""center"">
    <img src=""./assets/readme/icon.png"" width=""250""/>
</p>
<div align=""center"">
    <a href=""https://github.com/hpcaitech/Open-Sora/stargazers""><img src=""https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social""></a>
    <a href=""https://hpcaitech.github.io/Open-Sora/""><img src=""https://img.shields.io/badge/Gallery-View-orange?logo=&amp""></a>
    <a href=""https://discord.gg/kZakZzrSUT""><img src=""https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp""></a>
    <a href=""https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA""><img src=""https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp""></a>
    <a href=""https://twitter.com/yangyou1991/status/1769411544083996787?s=61&t=jT0Dsx2d-MS5vS9rNM5e5g""><img src=""https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp""></a>
    <a href=""https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png""><img src"
tornado,"Tornado Web Server
==================

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/tornadoweb/tornado
   :target: https://gitter.im/tornadoweb/tornado?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

`Tornado <http://www.tornadoweb.org>`_ is a Python web framework and
asynchronous networking library, originally developed at `FriendFeed
<http://friendfeed.com>`_.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
`long polling <http://en.wikipedia.org/wiki/Push_technology#Long_Polling>`_,
`WebSockets <http://en.wikipedia.org/wiki/WebSocket>`_, and other
applications that require a long-lived connection to each user.

Hello, world
------------

Here is a simple ""Hello, world"" example web app for Tornado:

.. code-block:: python

    import asyncio
    import tornado

    class MainHandler(tornado.web.RequestHandler):
        def get(self):
       "
GPT_API_free,"<div align=""center"">
<img src=""./images/logo.png"" alt=""icon"" width=""50px""/>
<h1 align=""center"">GPT-API-free</h1>

æ”¯æŒ **GPT-4** / GPT-3.5-Turbo / GPT-3.5-Turbo-16K / embeddings / DALLÂ·E / whisper / text-davinci

å›½å†…åŠ¨æ€åŠ é€Ÿ ç›´è¿æ— éœ€ä»£ç†

[å¿«é€Ÿå¼€å§‹](#å¦‚ä½•ä½¿ç”¨) / [APIæ–‡æ¡£](https://chatanywhere.apifox.cn/) / [ç”³è¯·å†…æµ‹å…è´¹Key](https://api.chatanywhere.org/v1/oauth/free/github/render) / [æ”¯æŒä»˜è´¹Key](https://buyca.shop/) / [æœåŠ¡å¯ç”¨æ€§](https://status.chatanywhere.tech/)

[QQç¾¤: 1009368550](https://qm.qq.com/cgi-bin/qm/qr?k=IUo12Iwbb9X8FGdAJevKIanOkAb7EcAV&jump_from=webapi&authKey=mUS0pJ45r7qiVufMlylLOdi6FmL9M4PdMc6wz6Jk8r2Yr7DZGk0QcjsCedNOShRq)

[![](https://status.chatanywhere.org/api/badge/6/uptime/24?labelPrefix=ä»˜è´¹API:gpt-4:)](https://status.chatanywhere.tech/)
[![](https://status.chatanywhere.org/api/badge/3/uptime/24?labelPrefix=ä»˜è´¹API:gpt-3.5-turbo:)](https://status.chatanywhere.tech/)
[![](https://status.chatanywhere.org/api/badge/8/uptime/24?labelPrefix=ä»˜è´¹API:gpt-3.5-turbo(Azure):)](https://status.chatanywhere.tech/)

[!"
proxy_pool,"
ProxyPool çˆ¬è™«ä»£ç†IPæ± 
=======
[![Build Status](https://travis-ci.org/jhao104/proxy_pool.svg?branch=master)](https://travis-ci.org/jhao104/proxy_pool)
[![](https://img.shields.io/badge/Powered%20by-@j_hao104-green.svg)](http://www.spiderpy.cn/blog/)
[![Packagist](https://img.shields.io/packagist/l/doctrine/orm.svg)](https://github.com/jhao104/proxy_pool/blob/master/LICENSE)
[![GitHub contributors](https://img.shields.io/github/contributors/jhao104/proxy_pool.svg)](https://github.com/jhao104/proxy_pool/graphs/contributors)
[![](https://img.shields.io/badge/language-Python-green.svg)](https://github.com/jhao104/proxy_pool)

    ______                        ______             _
    | ___ \_                      | ___ \           | |
    | |_/ / \__ __   __  _ __   _ | |_/ /___   ___  | |
    |  __/|  _// _ \ \ \/ /| | | ||  __// _ \ / _ \ | |
    | |   | | | (_) | >  < \ |_| || |  | (_) | (_) || |___
    \_|   |_|  \___/ /_/\_\ \__  |\_|   \___/ \___/ \_____\
                           __ / "
zulip,"# Zulip overview

[Zulip](https://zulip.com) is an open-source team collaboration tool with unique
[topic-based threading][why-zulip] that combines the best of email and chat to
make remote work productive and delightful. Fortune 500 companies, [leading open
source projects][rust-case-study], and thousands of other organizations use
Zulip every day. Zulip is the only [modern team chat app][features] that is
designed for both live and asynchronous conversations.

Zulip is built by a distributed community of developers from all around the
world, with 74+ people who have each contributed 100+ commits. With
over 1000 contributors merging over 500 commits a month, Zulip is the
largest and fastest growing open source team chat project.

Come find us on the [development community chat](https://zulip.com/development-community/)!

[![GitHub Actions build status](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml/badge.svg)](https://github.com/zulip/zulip/actions/workflows/zulip-ci.ym"
dash,"# Dash

[![CircleCI](https://img.shields.io/circleci/project/github/plotly/dash/master.svg)](https://circleci.com/gh/plotly/dash)
[![GitHub](https://img.shields.io/github/license/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/blob/master/LICENSE)
[![PyPI](https://img.shields.io/pypi/v/dash.svg?color=dark-green)](https://pypi.org/project/dash/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dash.svg?color=dark-green)](https://pypi.org/project/dash/)
[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/graphs/contributors)

#### *Dash is the most downloaded, trusted Python framework for building ML & data science web apps*.

Built on top of [Plotly.js](https://github.com/plotly/plotly.js), [React](https://reactjs.org/) and [Flask](https://palletsprojects.com/p/flask/), Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Pyth"
GitHub520,"# GitHub520

<p align=""center"">
<a href=""https://hellogithub.com/repository/d05ff820bf36470581c02cda5cbd17ea"" target=""_blank""><img src=""https://api.hellogithub.com/v1/widgets/recommend.svg?rid=d05ff820bf36470581c02cda5cbd17ea&claim_uid=8MKvZoxaWt"" alt=""Featuredï½œHelloGitHub"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a><br>
ğŸ˜˜ è®©ä½ â€œçˆ±â€ä¸Š GitHubï¼Œè§£å†³è®¿é—®æ—¶å›¾è£‚ã€åŠ è½½æ…¢çš„é—®é¢˜ã€‚
</p>

> æœåŠ¡å™¨å·²ç»­è´¹åˆ° 2024.12 å…±èŠ±äº†ï¼š1500+ğŸ’° [ç‚¹å‡»æ‰«ç èµåŠ©](https://raw.hellogithub.com/code.png)ï¼Œæ„Ÿè°¢ğŸ™

## ä¸€ã€ä»‹ç»
å¯¹ GitHub è¯´""çˆ±""å¤ªéš¾äº†ï¼šè®¿é—®æ…¢ã€å›¾ç‰‡åŠ è½½ä¸å‡ºæ¥ã€‚

**æœ¬é¡¹ç›®æ— éœ€å®‰è£…ä»»ä½•ç¨‹åºï¼Œä»…éœ€ 5 åˆ†é’Ÿã€‚**

é€šè¿‡ä¿®æ”¹æœ¬åœ° hosts æ–‡ä»¶ï¼Œè¯•å›¾è§£å†³ï¼š
- GitHub è®¿é—®é€Ÿåº¦æ…¢çš„é—®é¢˜
- GitHub é¡¹ç›®ä¸­çš„å›¾ç‰‡æ˜¾ç¤ºä¸å‡ºçš„é—®é¢˜

è®©ä½ ""çˆ±""ä¸Š GitHubã€‚



*æ³¨ï¼š* æœ¬é¡¹ç›®è¿˜å¤„äºæµ‹è¯•é˜¶æ®µï¼Œä»…åœ¨æœ¬æœºæµ‹è¯•é€šè¿‡ï¼Œå¦‚æœ‰é—®é¢˜æ¬¢è¿æ [issues](https://github.com/521xueweihan/GitHub520/issues/new)


## äºŒã€ä½¿ç”¨æ–¹æ³•

ä¸‹é¢çš„åœ°å€æ— éœ€è®¿é—® GitHub å³å¯è·å–åˆ°æœ€æ–°çš„ hosts å†…å®¹ï¼š

- æ–‡ä»¶ï¼š`https://raw.hellogithub.com/hosts`
- JSONï¼š`https://raw.hellogithub.com/hosts.json`

### 2.1 æ‰‹åŠ¨æ–¹å¼

#### 2.1.1 å¤åˆ¶ä¸‹é¢çš„å†…å®¹

```bash
# GitHub520 Host Start
140.82.113.25                 alive.github.com
140.82.113.6                  api.github"
manim,"<p align=""center"">
    <a href=""https://www.manim.community/""><img src=""https://raw.githubusercontent.com/ManimCommunity/manim/main/logo/cropped.png""></a>
    <br />
    <br />
    <a href=""https://pypi.org/project/manim/""><img src=""https://img.shields.io/pypi/v/manim.svg?style=flat&logo=pypi"" alt=""PyPI Latest Release""></a>
    <a href=""https://hub.docker.com/r/manimcommunity/manim""><img src=""https://img.shields.io/docker/v/manimcommunity/manim?color=%23099cec&label=docker%20image&logo=docker"" alt=""Docker image""> </a>
    <a href=""https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb""><img src=""https://mybinder.org/badge_logo.svg""></a>
    <a href=""http://choosealicense.com/licenses/mit/""><img src=""https://img.shields.io/badge/license-MIT-red.svg?style=flat"" alt=""MIT License""></a>
    <a href=""https://www.reddit.com/r/manim/""><img src=""https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=orange&label=reddit&logo=reddit"" alt="""
posthog,"<p align=""center"">
  <img alt=""posthoglogo"" src=""https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png"">
</p>
<p align=""center"">
  <!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
<a href='https://posthog.com/contributors'><img src='https://img.shields.io/badge/all_contributors-251-orange.svg?style=flat-square' /></a>
<!-- ALL-CONTRIBUTORS-BADGE:END -->
  <a href='http://makeapullrequest.com'><img alt='PRs Welcome' src='https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields'/></a>
  <img alt=""Docker Pulls"" src=""https://img.shields.io/docker/pulls/posthog/posthog""/>
  <img alt=""GitHub commit activity"" src=""https://img.shields.io/github/commit-activity/m/posthog/posthog""/>
  <img alt=""GitHub closed issues"" src=""https://img.shields.io/github/issues-closed/posthog/posthog""/>
</p>

<p align=""center"">
  <a href=""https://posthog.com/docs"">Docs</a> - <a href=""https://posthog.com/community"">Community</a>"
pytorch_geometric,"<p align=""center"">
  <img height=""150"" src=""https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true"" />
</p>

______________________________________________________________________

[![PyPI Version][pypi-image]][pypi-url]
[![Testing Status][testing-image]][testing-url]
[![Linting Status][linting-image]][linting-url]
[![Docs Status][docs-image]][docs-url]
[![Contributing][contributing-image]][contributing-url]
[![Slack][slack-image]][slack-url]

**[Documentation](https://pytorch-geometric.readthedocs.io)** | **[Paper](https://arxiv.org/abs/1903.02428)** | **[Colab Notebooks and Video Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)** | **[External Resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html)** | **[OGB Examples](https://github.com/snap-stanford/ogb/tree/master/examples)**

**PyG** *(PyTorch Geometric)* is a library built upon [PyTorch](ht"
chatgpt-retrieval-plugin,"# ChatGPT Retrieval Plugin

Build Custom GPTs with a Retrieval Plugin backend to give ChatGPT access to personal documents.
![Example Custom GPT Screenshot](/assets/example.png)

## Introduction

The ChatGPT Retrieval Plugin repository provides a flexible solution for semantic search and retrieval of personal or organizational documents using natural language queries. It is a standalone retrieval backend, and can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.com/?model=gpt-4-plugins). ChatGPT and the Assistants API both natively support retrieval from uploaded files, so you should use the Retrieval Plugin as a backend only if you want more granular contro"
jina,"<p align=""center"">
<a href=""https://docs.jina.ai""><img src=""https://github.com/jina-ai/jina/blob/master/docs/_static/logo-light.svg?raw=true"" alt=""Jina logo: Build multimodal AI services via cloud native technologies Â· Model Serving Â· Generative AI Â· Neural Search Â· Cloud Native"" width=""150px""></a>
</p>

<p align=""center"">
<b>Build multimodal AI applications with cloud-native technologies</b>
</p>

<p align=center>
<a href=""https://pypi.org/project/jina/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/jina?label=Release&style=flat-square""></a>
<!--<a href=""https://codecov.io/gh/jina-ai/jina""><img alt=""Codecov branch"" src=""https://img.shields.io/codecov/c/github/jina-ai/jina/master?&logo=Codecov&logoColor=white&style=flat-square""></a>-->
<a href=""https://discord.jina.ai""><img src=""https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square""></a>
<a href=""https://pypistats.org/packages/jina""><img alt=""PyPI - Downloads from official pypistats"" src"
ArchiveBox,"<div align=""center"" style=""text-align: center; width: 100%"">
<img src=""https://archivebox.io/icon.png"" height=""90px""/>
<h1>ArchiveBox<br/><sub>Open-source self-hosted web archiving.</sub></h1>

<br/>

â–¶ï¸ <a href=""https://github.com/ArchiveBox/ArchiveBox/wiki/Quickstart"">Quickstart</a> | <a href=""https://demo.archivebox.io"">Demo</a> | <a href=""https://github.com/ArchiveBox/ArchiveBox"">GitHub</a> | <a href=""https://github.com/ArchiveBox/ArchiveBox/wiki"">Documentation</a> | <a href=""#background--motivation"">Info & Motivation</a> | <a href=""https://github.com/ArchiveBox/ArchiveBox/wiki/Web-Archiving-Community"">Community</a>

<br/>

<!--<a href=""http://webchat.freenode.net?channels=ArchiveBox&uio=d4""><img src=""https://img.shields.io/badge/Community_chat-IRC-%2328A745.svg""/></a>-->

<a href=""https://github.com/ArchiveBox/ArchiveBox/blob/dev/LICENSE""><img src=""https://img.shields.io/badge/Open_source-MIT-green.svg?logo=git&logoColor=green""/></a> <a href=""https://github.com/ArchiveBox/ArchiveB"
audiocraft,"# AudioCraft
![docs badge](https://github.com/facebookresearch/audiocraft/workflows/audiocraft_docs/badge.svg)
![linter badge](https://github.com/facebookresearch/audiocraft/workflows/audiocraft_linter/badge.svg)
![tests badge](https://github.com/facebookresearch/audiocraft/workflows/audiocraft_tests/badge.svg)

AudioCraft is a PyTorch library for deep learning research on audio generation. AudioCraft contains inference and training code
for two state-of-the-art AI generative models producing high-quality audio: AudioGen and MusicGen.


## Installation
AudioCraft requires Python 3.9, PyTorch 2.1.0. To install AudioCraft, you can run the following:

```shell
# Best to make sure you have torch installed first, in particular before installing xformers.
# Don't run this if you already have PyTorch installed.
python -m pip install 'torch==2.1.0'
# You might need the following before trying to install the packages
python -m pip install setuptools wheel
# Then proceed to one of the following
"
erpnext,"<div align=""center"">
    <a href=""https://erpnext.com"">
        <img src=""https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/erpnext-logo.png"" height=""128"">
    </a>
    <h2>ERPNext</h2>
    <p align=""center"">
        <p>ERP made simple</p>
    </p>

[![CI](https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule)](https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![codecov](https://codecov.io/gh/frappe/erpnext/branch/develop/graph/badge.svg?token=0TwvyUg3I5)](https://codecov.io/gh/frappe/erpnext)
[![docker pulls](https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg)](https://hub.docker.com/r/frappe/erpnext-worker)

[https://erpnext.com](https://erpnext.com)

</div>

ERPNext as a monolith includes the following areas for managing businesses:

1. [Acco"
saleor,"<div align=""center"" width=""100px"">
 <picture>
   <source media=""(prefers-color-scheme: dark)"" srcset=""https://user-images.githubusercontent.com/4006792/214640818-fd4de9e6-bdee-47f0-ae66-e69ee9ec84bb.png"">
   <source media=""(prefers-color-scheme: light)"" srcset=""https://user-images.githubusercontent.com/4006792/214636328-8e4f83e8-66cb-4114-a3d8-473eb908b9c3.png"">
   <img width=""200"" alt=""saleor-commerce-logo"" src=""https://user-images.githubusercontent.com/4006792/214636328-8e4f83e8-66cb-4114-a3d8-473eb908b9c3.png"">

 </picture>
</div>

<div align=""center"">
  <strong>Commerce that works with your language and stack</strong>
</div>

<div align=""center"">
  GraphQL native, API-only platform for scalable composable commerce.
</div>

<br>

<div align=""center"">
  Join our community: <br>
  <a href=""https://saleor.io/"">Website</a>
  <span> | </span>
  <a href=""https://twitter.com/getsaleor"">Twitter</a>
  <span> | </span>
  <a href=""https://github.com/saleor/saleor/discussions"">GitHub Discussion"
pydantic,"# Pydantic
[![CI](https://img.shields.io/github/actions/workflow/status/pydantic/pydantic/ci.yml?branch=main&logo=github&label=CI)](https://github.com/pydantic/pydantic/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic)
[![pypi](https://img.shields.io/pypi/v/pydantic.svg)](https://pypi.python.org/pypi/pydantic)
[![CondaForge](https://img.shields.io/conda/v/conda-forge/pydantic.svg)](https://anaconda.org/conda-forge/pydantic)
[![downloads](https://static.pepy.tech/badge/pydantic/month)](https://pepy.tech/project/pydantic)
[![versions](https://img.shields.io/pypi/pyversions/pydantic.svg)](https://github.com/pydantic/pydantic)
[![license](https://img.shields.io/github/license/pydantic/pydantic.svg)](https://github.com/pydantic/pydantic/blob/main/LICENSE)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubus"
Gooey,"# Gooey 
  

Turn (almost) any Python 3 Console Program into a GUI application with one line

<p align=""center"">
    <img src=""https://github.com/chriskiehl/GooeyImages/raw/images/readme-images/1-0-4-title-card.png"" />
</p>


Table of Contents
-----------------  

- [Gooey](#gooey)
- [Table of contents](#table-of-contents)
- [Latest Update](#latest-update)
- [Quick Start](#quick-start)
    - [Installation Instructions](#installation-instructions)
    - [Usage](#usage)
    - [Examples](#examples)
- [What It Is](#what-is-it)
- [Why Is It](#why)
- [Who is this for](#who-is-this-for)
- [How does it work](#how-does-it-work)
- [Internationalization](#internationalization)
- [Global Configuration](#global-configuration)
- [Layout Customization](#layout-customization)
- [Run Modes](#run-modes)
    - [Full/Advanced](#advanced)
    - [Basic](#basic)
    - [No Config](#no-config)
- [Menus](#menus)    
- [Dynamic Validation](#dynamic-validation)
- [Lifecycle Events and UI control](#lifecycle-event"
chinese-independent-blogs,"# ä¸­æ–‡ç‹¬ç«‹åšå®¢åˆ—è¡¨

  [![](https://badgen.net/badge/icon/Website?icon=chrome&label)](https://feeds.pub/cn-indie)  [![](https://badgen.net/badge/icon/Telegram?icon=telegram&label)](https://t.me/indieBlogs)  [![](https://badgen.net/badge/icon/Blog?icon=chrome&label)](https://blog.t9t.io/cn-indie-blogs-2019-10-29/)

## Sponsors

[çšè‡´è¿œ](https://github.com/juzhiyuan) | [Bytebase](https://bytebase.com/) | [Madao](https://madao.me/) | [SecondState](https://bit.ly/3gfWwps)

[Become a sponsor](https://github.com/sponsors/timqian)

## ç›®å½•

- [åšå®¢åˆ—è¡¨](#åšå®¢åˆ—è¡¨)
- [ä»€ä¹ˆæ˜¯ç‹¬ç«‹åšå®¢](#ä»€ä¹ˆæ˜¯ç‹¬ç«‹åšå®¢)
  - [å¦‚ä½•æäº¤](#å¦‚ä½•æäº¤)
- [ä¸ºä»€ä¹ˆè¦æ”¶é›†è¿™å¼ åˆ—è¡¨](#ä¸ºä»€ä¹ˆè¦æ”¶é›†è¿™å¼ åˆ—è¡¨)

## åšå®¢åˆ—è¡¨

> æš‚æ—¶æ ¹æ®å„ RSS æœåŠ¡è®¢é˜…æ•°æ®æ’äº†ä¸ªå…ˆåé¡ºåºã€‚ æ¬¢è¿åŠ å…¥ [Telegram ç¾¤](https://t.me/indieBlogs) è®¨è®ºå¦‚ä½•æ›´å¥½åœ°ç»„ç»‡å’Œåˆ©ç”¨è¿™ä¸ªåˆ—è¡¨

| RSS feed | Introduction | Address | tags |
| --- | --- | --- | --- |
| [Feed](https://blog.t9t.io/atom.xml) | é€æ˜åˆ›ä¸šå®éªŒ | https://blog.t9t.io | åˆ›ä¸š; ç¼–ç¨‹; å¼€æº |
| [Feed](http://feeds.feedburner.com/ruanyifeng) | é˜®ä¸€å³°çš„ç½‘ç»œæ—¥å¿— | https://www.ruanyifeng.com/blog/ | åˆ›ä¸š; ç¼–ç¨‹; å‰ç«¯ |
| [Feed](http://coolshell."
ungoogled-chromium,"# ungoogled-chromium

*A lightweight approach to removing Google web service dependency*

**Help is welcome!** See the [docs/contributing.md](docs/contributing.md) document for more information.

## Objectives

In descending order of significance (i.e. most important objective first):

1. **ungoogled-chromium is Google Chromium, sans dependency on Google web services**.
2. **ungoogled-chromium retains the default Chromium experience as closely as possible**. Unlike other Chromium forks that have their own visions of a web browser, ungoogled-chromium is essentially a drop-in replacement for Chromium.
3. **ungoogled-chromium features tweaks to enhance privacy, control, and transparency**. However, almost all of these features must be manually activated or enabled. For more details, see [Feature Overview](#feature-overview).

In scenarios where the objectives conflict, the objective of higher significance should take precedence.

## Content Overview

* [Objectives](#objectives)
* [Motivat"
ddia,"# è®¾è®¡æ•°æ®å¯†é›†å‹åº”ç”¨ - ä¸­æ–‡ç¿»è¯‘ç‰ˆ

[![Webite: ddia](https://img.shields.io/badge/v1-ddia.pigsty.io-slategray?style=flat)](https://ddia.pigsty.io)
[![Webite: ddia2](https://img.shields.io/badge/v2-ddia2.pigsty.io-slategray?style=flat)](https://ddia2.pigsty.io)
[![GitHub Stars](https://img.shields.io/github/stars/Vonng/ddia?style=flat&logo=github&logoColor=black&color=slategray)](https://star-history.com/#Vonng/ddia&Date)

**ä½œè€…**ï¼š [Martin Kleppmann](https://martin.kleppmann.com)ï¼Œ[ã€ŠDesigning Data-Intensive Applications 2nd Editionã€‹](https://learning.oreilly.com/library/view/designing-data-intensive-applications/9781098119058/ch01.html) ï¼š è‹±å›½å‰‘æ¡¥å¤§å­¦åˆ†å¸ƒå¼ç³»ç»Ÿç ”ç©¶å‘˜ï¼Œæ¼”è®²è€…ï¼Œåšä¸»å’Œå¼€æºè´¡çŒ®è€…ï¼Œè½¯ä»¶å·¥ç¨‹å¸ˆå’Œä¼ä¸šå®¶ï¼Œæ›¾åœ¨ LinkedIn å’Œ Rapportive è´Ÿè´£æ•°æ®åŸºç¡€æ¶æ„ã€‚

**è¯‘è€…**ï¼š[å†¯è‹¥èˆª](https://vonng.com) / [Vonng](https://github.com/Vonng) (rh@vonng.com)ï¼š åˆ›ä¸šè€…ï¼Œ[å¼€æºè´¡çŒ®è€…](https://gitstar-ranking.com/Vonng)ï¼ŒPostgreSQL Hackerã€‚å¼€æº RDS PG [Pigsty](https://pigsty.cc/zh/) ä¸å…¬ä¼—å·ã€Š[éæ³•åŠ å†¯](https://mp.weixin.qq.com/s/p4Ys10ZdEDAuqNAiRmcnIQ)ã€‹ä½œè€…ï¼Œ[æ•°æ®åº“è€å¸æœº](https://pigsty.cc/zh/blog/db)ï¼Œ["
OSX-KVM,"### Note

This `README.md` documents the process of creating a `Virtual Hackintosh`
system.

Note: All blobs and resources included in this repository are re-derivable (all
instructions are included!).

:green_heart: Looking for **commercial** support with this stuff? I am [available
over email](mailto:dhiru.kholia@gmail.com?subject=[GitHub]%20OSX-KVM%20Commercial%20Support%20Request&body=Hi%20-%20We%20are%20interested%20in%20purchasing%20commercial%20support%20options%20for%20your%20project.) for a chat for **commercial support options only**. Note: Project sponsors get access to the `Private OSX-KVM` repository, and direct support.

Struggling with `Content Caching` stuff? We can help.

Working with `Proxmox` and macOS? See [Nick's blog for sure](https://www.nicksherlock.com/).

Yes, we support offline macOS installations now - see [this document](./run_offline.md) ğŸ‰


### Contributing Back

This project can always use your help, time and attention. I am looking for
help (pull-reques"
magic-wormhole,"# Magic Wormhole
[![PyPI](http://img.shields.io/pypi/v/magic-wormhole.svg)](https://pypi.python.org/pypi/magic-wormhole)
![Tests](https://github.com/magic-wormhole/magic-wormhole/workflows/Tests/badge.svg)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/w1bdniovwm4egfyg/branch/master?svg=true)](https://ci.appveyor.com/project/warner/magic-wormhole)
[![codecov.io](https://codecov.io/github/magic-wormhole/magic-wormhole/coverage.svg?branch=master)](https://codecov.io/github/magic-wormhole/magic-wormhole?branch=master)
[![Docs](https://readthedocs.org/projects/magic-wormhole/badge/?version=latest)](https://magic-wormhole.readthedocs.io)
[![Irc](https://img.shields.io/badge/irc.libera.chat-%23magic--wormhole-brightgreen)](https://web.libera.chat/)
[![Matrix](https://img.shields.io/badge/matrix.org-%23magic--wormhole-brightgreen)](https://matrix.to/#/#magic-wormhole:matrix.org)


Get things from one computer to another, safely.

This package provides a library and a com"
matplotlib,"[![PyPi](https://img.shields.io/pypi/v/matplotlib)](https://pypi.org/project/matplotlib/)
[![Conda](https://img.shields.io/conda/vn/conda-forge/matplotlib)](https://anaconda.org/conda-forge/matplotlib)
[![Downloads](https://img.shields.io/pypi/dm/matplotlib)](https://pypi.org/project/matplotlib)
[![NUMFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)

[![Discourse help forum](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.matplotlib.org)
[![Gitter](https://badges.gitter.im/matplotlib/matplotlib.svg)](https://gitter.im/matplotlib/matplotlib)
[![GitHub issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/matplotlib/matplotlib/issues)
[![Contributing](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://matplotlib.org/stable/devel/index.html)

[![GitHub actions status](https://github.com/matplotlib/matplotlib/workflows/Tests/badg"
babyagi,"# Translations:

[<img title=""Ø¹Ø±Ø¨ÙŠ"" alt=""Ø¹Ø±Ø¨ÙŠ"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/sa.svg"" width=""30"">](docs/README-ar.md)
[<img title=""FranÃ§ais"" alt=""FranÃ§ais"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/fr.svg"" width=""30"">](docs/README-fr.md)
[<img title=""Polski"" alt=""Polski"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/pl.svg"" width=""30"">](docs/README-pl.md)
[<img title=""Portuguese"" alt=""Portuguese"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/br.svg"" width=""30"">](docs/README-pt-br.md)
[<img title=""Romanian"" alt=""Romanian"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ro.svg"" width=""30"">](docs/README-ro.md)
[<img title=""Russian"" alt=""Russian"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ru.svg"" width=""30"">](docs/README-ru.md)
[<img title=""Slovenian"" alt=""Slovenian"" src=""https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg"
minGPT,"
# minGPT

![mingpt](mingpt.jpg)

A PyTorch re-implementation of [GPT](https://github.com/openai/gpt-2), both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see [mingpt/model.py](mingpt/model.py)). All that's going on is that a sequence of indices feeds into a [Transformer](https://arxiv.org/abs/1706.03762), and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.

**note (Jan 2023)**: though I may continue to accept and change some details, minGPT is in a semi-archived state. For more recent developments see my rewrite [nanoGPT](https://github.com/karpathy/nanoGPT). Basically, minGPT became referenced across a wide varie"
localGPT,"# LocalGPT: Secure, Local Conversations with Your Documents ğŸŒ


[![GitHub Stars](https://img.shields.io/github/stars/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/pulls)
[![License](https://img.shields.io/github/license/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/blob/main/LICENSE)

ğŸš¨ğŸš¨ You can run localGPT on a pre-configured [Virtual Machine](https://bit.ly/localGPT). Make sure to use the code: PromptEngineering to get 50% off. I will get a small commision!

**LocalGPT** is an open-source initiative that allows y"
vit-pytorch,"<img src=""./images/vit.gif"" width=""500px""></img>

## Table of Contents

- [Vision Transformer - Pytorch](#vision-transformer---pytorch)
- [Install](#install)
- [Usage](#usage)
- [Parameters](#parameters)
- [Simple ViT](#simple-vit)
- [NaViT](#navit)
- [Distillation](#distillation)
- [Deep ViT](#deep-vit)
- [CaiT](#cait)
- [Token-to-Token ViT](#token-to-token-vit)
- [CCT](#cct)
- [Cross ViT](#cross-vit)
- [PiT](#pit)
- [LeViT](#levit)
- [CvT](#cvt)
- [Twins SVT](#twins-svt)
- [CrossFormer](#crossformer)
- [RegionViT](#regionvit)
- [ScalableViT](#scalablevit)
- [SepViT](#sepvit)
- [MaxViT](#maxvit)
- [NesT](#nest)
- [MobileViT](#mobilevit)
- [XCiT](#xcit)
- [Masked Autoencoder](#masked-autoencoder)
- [Simple Masked Image Modeling](#simple-masked-image-modeling)
- [Masked Patch Prediction](#masked-patch-prediction)
- [Masked Position Prediction](#masked-position-prediction)
- [Adaptive Token Sampling](#adaptive-token-sampling)
- [Patch Merger](#patch-merger)
- [Vision Transformer for Smal"
unilm,"<!--# Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities-->
## [aka.ms/GeneralAI](https://aka.ms/GeneralAI)
# Hiring
We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and General AI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to <a href=""mailto:fuwei@microsoft.com"" class=""x-hidden-focus"">fuwei@microsoft.com</a>.

# Foundation Architecture
### TorchScale - A Library of Foundation Architectures ([repo](https://github.com/microsoft/torchscale))

Fundamental research to develop new architectures for foundation models and AI, focusing on modeling generality and capability, as well as training stability and efficiency.

> Stability - [**DeepNet**](https://github.com/microsoft/unilm/tree/master/deepnet): scaling Transformers to 1,000 Layers and beyond

> Generality - [**Foundation Transformers (Magneto)**](https://arxi"
loguru,".. raw:: html

    <p align=""center"">
        <a href=""#readme"">
            <img alt=""Loguru logo"" src=""https://raw.githubusercontent.com/Delgan/loguru/master/docs/_static/img/logo.png"">
            <!-- Logo credits: Sambeet from Pixaday -->
            <!-- Logo fonts: Comfortaa + Raleway -->
        </a>
    </p>
    <p align=""center"">
        <a href=""https://pypi.python.org/pypi/loguru""><img alt=""Pypi version"" src=""https://img.shields.io/pypi/v/loguru.svg""></a>
        <a href=""https://pypi.python.org/pypi/loguru""><img alt=""Python versions"" src=""https://img.shields.io/badge/python-3.5%2B%20%7C%20PyPy-blue.svg""></a>
        <a href=""https://loguru.readthedocs.io/en/stable/index.html""><img alt=""Documentation"" src=""https://img.shields.io/readthedocs/loguru.svg""></a>
        <a href=""https://github.com/Delgan/loguru/actions/workflows/tests.yml?query=branch:master""><img alt=""Build status"" src=""https://img.shields.io/github/actions/workflow/status/Delgan/loguru/tests.yml?branch=master"""
crewAI,"<div align=""center"">

![Logo of crewAI, two people rowing on a boat](./docs/crewai_logo.png)

# **crewAI**

ğŸ¤– **crewAI**: Cutting-edge framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.

<h3>

[Homepage](https://www.crewai.com/) | [Documentation](https://docs.crewai.com/) | [Chat with Docs](https://chatg.pt/DWjSBZn) | [Examples](https://github.com/crewAIInc/crewAI-examples) | [Discourse](https://community.crewai.com)

</h3>

[![GitHub Repo stars](https://img.shields.io/github/stars/joaomdmoura/crewAI)](https://github.com/crewAIInc/crewAI)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

</div>

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Description"
LLaVA,"# ğŸŒ‹ LLaVA: Large Language and Vision Assistant

*Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.*

[ğŸ“¢ [LLaVA-NeXT Blog](https://llava-vl.github.io/blog/2024-01-30-llava-next/)] [[Project Page](https://llava-vl.github.io/)] [[Demo](https://llava.hliu.cc/)]  [[Data](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)] [[Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)]

ğŸ¤Community Contributions: [[llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3436)] [[Colab](https://github.com/camenduru/LLaVA-colab)] [[ğŸ¤—Space](https://huggingface.co/spaces/badayvedat/LLaVA)] [[Replicate](https://replicate.com/yorickvp/llava-13b)] [[AutoGen](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb)]  [[BakLLaVA](https://github.com/SkunkworksAI/BakLLaVA)]

**Improved Baselines with Visual Instruction Tuning** [[Paper](https://arxiv.org/abs/2310.03744)] [[HF](https://huggingface.co/papers"
calibre,"# calibre

<img align=""left"" src=""https://raw.githubusercontent.com/kovidgoyal/calibre/master/resources/images/lt.png"" height=""200"" width=""200""/>

calibre is an e-book manager. It can view, convert, edit and catalog e-books 
in all of the major e-book formats. It can also talk to e-book reader 
devices. It can go out to the internet and fetch metadata for your books. 
It can download newspapers and convert them into e-books for convenient 
reading. It is cross platform, running on Linux, Windows and macOS.

For more information, see the [calibre About page](https://calibre-ebook.com/about).

[![Build Status](https://github.com/kovidgoyal/calibre/workflows/CI/badge.svg)](https://github.com/kovidgoyal/calibre/actions?query=workflow%3ACI)

## Screenshots  

[Screenshots page](https://calibre-ebook.com/demo)

## Usage

See the [User Manual](https://manual.calibre-ebook.com).

## Development

[Setting up a development environment for calibre](https://manual.calibre-ebook.com/develop.html).
"
reflex,"```diff
+ Searching for Pynecone? You are in the right repo. Pynecone has been renamed to Reflex. +
```

<div align=""center"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_dark.svg#gh-light-mode-only"" alt=""Reflex Logo"" width=""300px"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_light.svg#gh-dark-mode-only"" alt=""Reflex Logo"" width=""300px"">

<hr>

### **âœ¨ Performant, customizable web apps in pure Python. Deploy in seconds. âœ¨**
[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&label=Discord)](https://discord.gg/T5WSbC2YtQ)
</div>

---

[English](https://github.com/reflex-dev/reflex/blob"
paperless-ngx,"[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png"" w"
aider,"
<!-- Edit README.md, not index.md -->

# Aider is AI pair programming in your terminal

Aider lets you pair program with LLMs,
to edit code in your local git repository.
Start a new project or work with an existing git repo.
Aider works best with GPT-4o & Claude 3.5 Sonnet and can 
[connect to almost any LLM](https://aider.chat/docs/llms.html).

<!-- SCREENCAST START -->
<p align=""center"">
  <img
    src=""https://aider.chat/assets/screencast.svg""
    alt=""aider screencast""
  >
</p>
<!-- SCREENCAST END -->

<!-- VIDEO START
<p align=""center"">
  <video style=""max-width: 100%; height: auto;"" autoplay loop muted playsinline>
    <source src=""/assets/shell-cmds-small.mp4"" type=""video/mp4"">
    Your browser does not support the video tag.
  </video>
</p>
VIDEO END -->

<p align=""center"">
  <a href=""https://discord.gg/Tv2uQnR88V"">
    <img src=""https://img.shields.io/badge/Join-Discord-blue.svg""/>
  </a>
  <a href=""https://aider.chat/docs/install.html"">
    <img src=""https://img.shields.io/b"
mkdocs,"# MkDocs

> *Project documentation with Markdown*

[![PyPI Version][pypi-v-image]][pypi-v-link]
[![Build Status][GHAction-image]][GHAction-link]
[![Coverage Status][codecov-image]][codecov-link]

MkDocs is a **fast**, **simple** and **downright gorgeous** static site
generator that's geared towards building project documentation. Documentation
source files are written in Markdown, and configured with a single YAML
configuration file. It is designed to be easy to use and can be extended with
third-party themes, plugins, and Markdown extensions.

Please see the [Documentation][mkdocs] for an introductory tutorial and a full
user guide.

## Features

- Build static HTML files from Markdown files.
- Use Plugins and Markdown Extensions to enhance MkDocs.
- Use the built-in themes, third party themes or create your own.
- Publish your documentation anywhere that static files can be served.
- Much more!

## Support

If you need help with MkDocs, do not hesitate to get in contact with us!

-  "
magenta,"# Status

This repository is currently inactive and serves only as a supplement some of our papers. We have transitioned to using individual repositories for new projects. For our current work, see the [Magenta website](https://g.co/magenta) and [Magenta GitHub Organization](https://github.com/magenta).

# Magenta

<img src=""magenta-logo-bg.png"" height=""75"">

[![Build Status](https://github.com/magenta/magenta/workflows/build/badge.svg)](https://github.com/magenta/magenta/actions?query=workflow%3Abuild)
 [![PyPI version](https://badge.fury.io/py/magenta.svg)](https://badge.fury.io/py/magenta)

**Magenta** is a research project exploring the role of machine learning
in the process of creating art and music.  Primarily this
involves developing new deep learning and reinforcement learning
algorithms for generating songs, images, drawings, and other materials. But it's also
an exploration in building smart tools and interfaces that allow
artists and musicians to extend (not replace!) their"
datasets,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg"">
    <img alt=""Hugging Face Datasets Library"" src=""https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg"" width=""352"" height=""59"" style=""max-width: 100%;"">
  </picture>
  <br/>
  <br/>
</p>

<p align=""center"">
    <a href=""https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain""><img alt=""Build"" src=""https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main""></a>
    <a href=""https://github.com/huggingface/datasets/blob/main/LICENSE""><img alt=""GitHub"" src=""https://img.shields.io/github/license/huggingface/datasets.svg?color=blue""></a>
    <a href="""
IOPaint,"<h1 align=""center"">IOPaint</h1>
<p align=""center"">A free and open-source inpainting & outpainting tool powered by SOTA AI model.</p>

<p align=""center"">
  <a href=""https://github.com/Sanster/IOPaint"">
    <img alt=""total download"" src=""https://pepy.tech/badge/iopaint"" />
  </a>
  <a href=""https://pypi.org/project/iopaint"">
    <img alt=""version"" src=""https://img.shields.io/pypi/v/iopaint"" />
  </a>
  <a href="""">
    <img alt=""python version"" src=""https://img.shields.io/pypi/pyversions/iopaint"" />
  </a>
  <a href=""https://huggingface.co/spaces/Sanster/iopaint-lama"">
    <img alt=""HuggingFace Spaces"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Spaces-blue"" />
  </a>
  <a href=""https://colab.research.google.com/drive/1TKVlDZiE3MIZnAUMpv2t_S4hLr6TUY1d?usp=sharing"">
    <img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg"" />
  </a>
</p>

|Erase([LaMa](https://www.iopaint.com/models/erase/lama))|Replace Object([PowerPaint](https://www.iopa"
recommenders,"<!--
Copyright (c) Recommenders contributors.
Licensed under the MIT License.
-->
<img src=""https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg"" width=""800"">


[![Documentation status](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment)
[![License](https://img.shields.io/github/license/recommenders-team/recommenders.svg)](https://github.com/recommenders-team/recommenders/blob/main/LICENSE)
[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PyPI Version](https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&logoColor=white)](https://pypi.org/project/recommenders)
[![Python Versions](https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&logoColor=white)](https://pypi.org/project/recommenders)

[<img align=""left"" width=""300"" s"
mlc-llm,"<div align=""center"">

# MLC LLM

[![Installation](https://img.shields.io/badge/docs-latest-green)](https://llm.mlc.ai/docs/)
[![License](https://img.shields.io/badge/license-apache_2-blue)](https://github.com/mlc-ai/mlc-llm/blob/main/LICENSE)
[![Join Discoard](https://img.shields.io/badge/Join-Discord-7289DA?logo=discord&logoColor=white)](""https://discord.gg/9Xpy2HGBuD"")
[![Related Repository: WebLLM](https://img.shields.io/badge/Related_Repo-WebLLM-fafbfc?logo=github)](https://github.com/mlc-ai/web-llm/)

**Universal LLM Deployment Engine with ML Compilation**

[Get Started](https://llm.mlc.ai/docs/get_started/quick_start) | [Documentation](https://llm.mlc.ai/docs) | [Blog](https://blog.mlc.ai/)

</div>

## About

MLC LLM is a machine learning compiler and high-performance deployment engine for large language models.  The mission of this project is to enable everyone to develop, optimize, and deploy AI models natively on everyone's platforms.Â 

<div align=""center"">
<table style=""width"
rasa,"<h1 align=""center"">Rasa Open Source</h1>

<div align=""center"">

[![Join the chat on Rasa Community Forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![PyPI version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)
[![Build Status](https://github.com/RasaHQ/rasa/workflows/Continuous%20Integration/badge.svg)](https://github.com/RasaHQ/rasa/actions)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=RasaHQ_rasa&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=RasaHQ_rasa)
[![Documentation Status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)
![Documentation Build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%2"
nginx-proxy,"[![Test](https://github.com/nginx-proxy/nginx-proxy/actions/workflows/test.yml/badge.svg)](https://github.com/nginx-proxy/nginx-proxy/actions/workflows/test.yml)
[![GitHub release](https://img.shields.io/github/v/release/nginx-proxy/nginx-proxy)](https://github.com/nginx-proxy/nginx-proxy/releases)
[![nginx 1.27.1](https://img.shields.io/badge/nginx-1.27.1-brightgreen.svg?logo=nginx)](https://nginx.org/en/CHANGES)
[![Docker Image Size](https://img.shields.io/docker/image-size/nginxproxy/nginx-proxy?sort=semver)](https://hub.docker.com/r/nginxproxy/nginx-proxy ""Click to view the image on Docker Hub"")
[![Docker stars](https://img.shields.io/docker/stars/nginxproxy/nginx-proxy.svg)](https://hub.docker.com/r/nginxproxy/nginx-proxy ""DockerHub"")
[![Docker pulls](https://img.shields.io/docker/pulls/nginxproxy/nginx-proxy.svg)](https://hub.docker.com/r/nginxproxy/nginx-proxy ""DockerHub"")

nginx-proxy sets up a container running nginx and [docker-gen](https://github.com/nginx-proxy/docker-gen)."
mlflow,"=============================================
MLflow: A Machine Learning Lifecycle Platform
=============================================

MLflow is a platform to streamline machine learning development, including tracking experiments, packaging code
into reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs that can be
used with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you
currently run ML code (e.g. in notebooks, standalone applications or the cloud). MLflow's current components are:

* `MLflow Tracking <https://mlflow.org/docs/latest/tracking.html>`_: An API to log parameters, code, and
  results in machine learning experiments and compare them using an interactive UI.
* `MLflow Projects <https://mlflow.org/docs/latest/projects.html>`_: A code packaging format for reproducible
  runs using Conda and Docker, so you can share your ML code with others.
* `MLflow Models <https://mlflow.org/d"
devika,"<p align=""center"">
  <img src="".assets/devika-avatar.png"" alt=""Devika Logo"" width=""250"">
</p>

<h1 align=""center"">ğŸš€ Devika - Agentic AI Software Engineer ğŸ‘©â€ğŸ’»</h1>

![devika screenshot](.assets/devika-screenshot.png)

> [!IMPORTANT]  
> This project is currently in a very early development/experimental stage. There are a lot of unimplemented/broken features at the moment. Contributions are welcome to help out with the progress!

## Table of Contents

- [About](#about)
- [Key Features](#key-features)
- [System Architecture](#system-architecture)
- [Getting Started](#getting-started)
  - [Requirements](#requirements)
  - [Installation](#installation)
  - [How to use](#how-to-use)
- [Configuration](#configuration)
- [Contributing](#contributing)
- [Help and Support](#help-and-support)
- [License](#license)

## About

Devika is an advanced AI software engineer that can understand high-level human instructions, break them down into steps, research relevant information, and write code to achi"
prophet,"
# Prophet: Automatic Forecasting Procedure

![Build](https://github.com/facebook/prophet/workflows/Build/badge.svg)

[![PyPI Version](https://img.shields.io/pypi/v/prophet.svg)](https://pypi.python.org/pypi/prophet)
[![PyPI Downloads Monthly](https://pepy.tech/badge/prophet/month)](https://pepy.tech/project/prophet)
[![PyPI Downloads All](https://pepy.tech/badge/prophet)](https://pepy.tech/project/prophet)

[![CRAN Version](https://www.r-pkg.org/badges/version/prophet)](https://CRAN.R-project.org/package=prophet)
[![CRAN Downloads Monthly](https://cranlogs.r-pkg.org/badges/prophet?color=brightgreen)](https://cran.r-project.org/package=prophet)
[![CRAN Downloads All](https://cranlogs.r-pkg.org/badges/grand-total/prophet?color=brightgreen)](https://cranlogs.r-pkg.org/badges/grand-total/prophet)

[![Conda_Version](https://anaconda.org/conda-forge/prophet/badges/version.svg)](https://anaconda.org/conda-forge/prophet/)

-----

**2023 Update:** We discuss our plans for the future of Prophet"
ChatPaper,"<div style=""font-size: 1.5rem;"">
  <a href=""./README.md"">ä¸­æ–‡</a> |
  <a href=""./readme_en.md"">English</a>
</div>
</br>


ğŸ’¥ğŸ’¥ğŸ’¥<strong>7.23 [MasterYip](https://github.com/MasterYip) åŒå­¦å¼€æºäº† [ChatPaper2Xmind](https://github.com/MasterYip/ChatPaper2Xmind)! 
å°†è®ºæ–‡PDFé€šè¿‡Chatä¸€é”®ç”Ÿæˆ å›¾ç‰‡+å…¬å¼çš„ç®€è¦XMindç¬”è®°ã€‚
 </strong>

ğŸ’¥ğŸ’¥ğŸ’¥<strong>7.22 ä»“åº“çš„æ–‡ä»¶åšäº†ä¸€ä¸ªæ•´ç†ï¼Œå¯èƒ½ä¼šæœ‰äº›è·¯å¾„å’Œbugï¼Œæ­£åœ¨ä¿®å¤ä¸­ã€‚
å¢åŠ å…¨æ–°çš„æœ¬åœ°PDFå…¨æ–‡ç¿»è¯‘åŠŸèƒ½ï¼[â›ï¸PDFå…¨æ–‡ç¿»è¯‘é…ç½®æ•™ç¨‹](https://github.com/kaixindelele/ChatPaper#%E4%BB%BB%E6%84%8Fpdf%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B)
 </strong>

<details><summary><code><b>å†å²é‡å¤§æ›´æ–°</b></code></summary>

- ğŸŒŸ*2023.07.23*: [MasterYip](https://github.com/MasterYip) åŒå­¦å¼€æºäº† [ChatPaper2Xmind](https://github.com/MasterYip/ChatPaper2Xmind)! 
å°†è®ºæ–‡PDFé€šè¿‡Chatä¸€é”®ç”Ÿæˆ å›¾ç‰‡+å…¬å¼çš„ç®€è¦XMindç¬”è®°ã€‚
- ğŸŒŸ*2023.07.22*: å¢åŠ å…¨æ–°çš„æœ¬åœ°PDFå…¨æ–‡ç¿»è¯‘åŠŸèƒ½ï¼[â›ï¸PDFå…¨æ–‡ç¿»è¯‘é…ç½®æ•™ç¨‹](#ä»»æ„PDFå…¨æ–‡ç¿»è¯‘é…ç½®æ•™ç¨‹)
- ğŸŒŸ*2023.07.21*: ä»“åº“çš„æ–‡ä»¶åšäº†ä¸€ä¸ªæ•´ç†ï¼Œå¯èƒ½ä¼šæœ‰äº›è·¯å¾„å’Œbugï¼Œæ­£åœ¨ä¿®å¤ä¸­ã€‚
- ğŸŒŸ*2023.07.09*: å¸ˆå¼Ÿ[red-tie](https://github.com/red-tie)åœ¨[auto-draft](https://github.com/CCCBora/auto-draft)çš„åŸºç¡€ä¸Šï¼Œä¼˜åŒ–äº†ä¸€æ¬¾[ä¸€é”®æ–‡çŒ®ç»¼è¿°](https://githu"
mypy,"<img src=""docs/source/mypy_light.svg"" alt=""mypy logo"" width=""300px""/>

Mypy: Static Typing for Python
=======================================

[![Stable Version](https://img.shields.io/pypi/v/mypy?color=blue)](https://pypi.org/project/mypy/)
[![Downloads](https://img.shields.io/pypi/dm/mypy)](https://pypistats.org/packages/mypy)
[![Build Status](https://github.com/python/mypy/actions/workflows/test.yml/badge.svg)](https://github.com/python/mypy/actions)
[![Documentation Status](https://readthedocs.org/projects/mypy/badge/?version=latest)](https://mypy.readthedocs.io/en/latest/?badge=latest)
[![Chat at https://gitter.im/python/typing](https://badges.gitter.im/python/typing.svg)](https://gitter.im/python/typing?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/blac"
Chinese-LLaMA-Alpaca,"# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)é¡¹ç›®å¯åŠ¨ï¼

[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki) | [**â“æé—®/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues) | [**ğŸ’¬è®¨è®º/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/discussions) | [**âš”ï¸ç«æŠ€åœº/Arena**](http://llm-arena.ymcui.com/)

<p align=""center"">
    <br>
    <img src=""./pics/banner.png"" width=""700""/>
    <br>
</p>
<p align=""center"">
    <img alt=""GitHub"" src=""https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square"">
    <img alt=""GitHub release (latest by date)"" src=""https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca"">
    <img alt=""GitHub top language"" src=""https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca"">
    <img alt=""GitHub last commit"" src=""https://img.shields.io/github/last-commit/ymcui/Chinese-LLaMA-Alpaca"">
    <a "
facefusion,"FaceFusion
==========

> Industry leading face manipulation platform.

[![Build Status](https://img.shields.io/github/actions/workflow/status/facefusion/facefusion/ci.yml.svg?branch=master)](https://github.com/facefusion/facefusion/actions?query=workflow:ci)
[![Coverage Status](https://img.shields.io/coveralls/facefusion/facefusion.svg)](https://coveralls.io/r/facefusion/facefusion)
![License](https://img.shields.io/badge/license-MIT-green)


Preview
-------

![Preview](https://raw.githubusercontent.com/facefusion/facefusion/master/.github/preview.png?sanitize=true)


Installation
------------

Be aware, the [installation](https://docs.facefusion.io/installation) needs technical skills and is not recommended for beginners. In case you are not comfortable using a terminal, our [Windows Installer](https://windows-installer.facefusion.io) and [macOS Installer](https://macos-installer.facefusion.io) get you started.


Usage
-----

Run the command:

```
python facefusion.py [commands] [opti"
python-spider,"# æ³¨ï¼š2020å¹´æœ€æ–°è¿è½½æ•™ç¨‹è¯·ç§»æ­¥ï¼š[Python Spider 2020](https://github.com/Jack-Cherish/python-spider/tree/master/2020 ""Python Spider 2020"")

å…è´£å£°æ˜ï¼š

å¤§å®¶è¯·ä»¥å­¦ä¹ ä¸ºç›®çš„ä½¿ç”¨æœ¬ä»“åº“ï¼Œçˆ¬è™«è¿æ³•è¿è§„çš„æ¡ˆä»¶ï¼šhttps://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China

æœ¬ä»“åº“çš„æ‰€æœ‰å†…å®¹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒä¹‹ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚ä»»ä½•äººæˆ–ç»„ç»‡ä¸å¾—å°†æœ¬ä»“åº“çš„å†…å®¹ç”¨äºéæ³•ç”¨é€”æˆ–ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šã€‚æœ¬ä»“åº“æ‰€æ¶‰åŠçš„çˆ¬è™«æŠ€æœ¯ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸å¾—ç”¨äºå¯¹å…¶ä»–å¹³å°è¿›è¡Œå¤§è§„æ¨¡çˆ¬è™«æˆ–å…¶ä»–éæ³•è¡Œä¸ºã€‚å¯¹äºå› ä½¿ç”¨æœ¬ä»“åº“å†…å®¹è€Œå¼•èµ·çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæœ¬ä»“åº“ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ä½¿ç”¨æœ¬ä»“åº“çš„å†…å®¹å³è¡¨ç¤ºæ‚¨åŒæ„æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚

# Python Spider

åŸåˆ›æ–‡ç« æ¯å‘¨æœ€å°‘ä¸¤ç¯‡ï¼Œ**åç»­æœ€æ–°æ–‡ç« **ä¼šåœ¨[ã€å…¬ä¼—å·ã€‘](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)é¦–å‘ï¼Œè§†é¢‘[ã€Bç«™ã€‘](https://space.bilibili.com/331507846)é¦–å‘ï¼Œå¤§å®¶å¯ä»¥åŠ æˆ‘[ã€å¾®ä¿¡ã€‘](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)è¿›**äº¤æµç¾¤**ï¼ŒæŠ€æœ¯äº¤æµæˆ–ææ„è§éƒ½å¯ä»¥ï¼Œæ¬¢è¿**Star**ï¼

<p align=""center"">
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/weChat-å¾®ä¿¡ç¾¤-blue.svg"" alt=""å¾®ä¿¡ç¾¤""></a>
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Jack%20Cui"
ragflow,"<div align=""center"">
<a href=""https://demo.ragflow.io/"">
<img src=""web/src/assets/logo-with-text.png"" width=""520"" alt=""ragflow logo"">
</a>
</div>

<p align=""center"">
  <a href=""./README.md"">English</a> |
  <a href=""./README_zh.md"">ç®€ä½“ä¸­æ–‡</a> |
  <a href=""./README_ja.md"">æ—¥æœ¬èª</a> |
  <a href=""./README_ko.md"">í•œêµ­ì–´</a>
</p>

<p align=""center"">
    <a href=""https://github.com/infiniflow/ragflow/releases/latest"">
        <img src=""https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&label=Latest%20Release"" alt=""Latest Release"">
    </a>
    <a href=""https://demo.ragflow.io"" target=""_blank"">
        <img alt=""Static Badge"" src=""https://img.shields.io/badge/Online-Demo-4e6b99""></a>
    <a href=""https://hub.docker.com/r/infiniflow/ragflow"" target=""_blank"">
        <img src=""https://img.shields.io/badge/docker_pull-ragflow:v0.11.0-brightgreen"" alt=""docker pull infiniflow/ragflow:v0.11.0""></a>
    <a href=""https://github.com/infiniflow/ragflow/blob/main/LICENSE"">
    <img height=""21"
sanic,".. image:: https://raw.githubusercontent.com/sanic-org/sanic-assets/master/png/sanic-framework-logo-400x97.png
    :alt: Sanic | Build fast. Run fast.

Sanic | Build fast. Run fast.
=============================

.. start-badges

.. list-table::
    :widths: 15 85
    :stub-columns: 1

    * - Build
      - | |Tests|
    * - Docs
      - | |UserGuide| |Documentation|
    * - Package
      - | |PyPI| |PyPI version| |Wheel| |Supported implementations| |Code style ruff|
    * - Support
      - | |Forums| |Discord| |Awesome|
    * - Stats
      - | |Monthly Downloads| |Weekly Downloads| |Conda downloads|

.. |UserGuide| image:: https://img.shields.io/badge/user%20guide-sanic-ff0068
   :target: https://sanic.dev/
.. |Forums| image:: https://img.shields.io/badge/forums-community-ff0068.svg
   :target: https://community.sanicframework.org/
.. |Discord| image:: https://img.shields.io/discord/812221182594121728?logo=discord&label=Discord&color=5865F2
   :target: https://discord.gg/FARQzAEMAA
.."
wagtail,"<h1 align=""center"">
    <picture>
        <source media=""(prefers-color-scheme: light)"" srcset="".github/wagtail.svg"">
        <source media=""(prefers-color-scheme: dark)"" srcset="".github/wagtail-inverse.svg"">
        <img width=""343"" src="".github/wagtail.svg"" alt=""Wagtail"">
    </picture>
</h1>
<p align=""center"">
    <br>
    <a href=""https://github.com/wagtail/wagtail/actions"">
        <img src=""https://github.com/wagtail/wagtail/workflows/Wagtail%20CI/badge.svg"" alt=""Build Status"" />
    </a>
    <a href=""https://opensource.org/licenses/BSD-3-Clause"">
        <img src=""https://img.shields.io/badge/license-BSD-blue.svg"" alt=""License"" />
    </a>
    <a href=""https://pypi.python.org/pypi/wagtail/"">
        <img src=""https://img.shields.io/pypi/v/wagtail.svg"" alt=""Version"" />
    </a>
    <a href=""https://pypi.python.org/pypi/wagtail/"">
        <img src=""https://img.shields.io/pypi/dm/wagtail?logo=Downloads"" alt=""Monthly downloads"" />
    </a>
    <a href=""https://x.com/WagtailCMS"">
   "
DeOldify,"
# DeOldify

**Quick Start**: The easiest way to colorize images using open source DeOldify
(for free!) is here: [DeOldify Image Colorization on DeepAI](https://deepai.org/machine-learning-model/colorizer)

**Desktop**: Want to run open source DeOldify for photos and videos on the desktop?
* Stable Diffusion Web UI Plugin- Photos and video, cross-platform (NEW!). <https://github.com/SpenserCai/sd-webui-deoldify>
* ColorfulSoft Windows GUI- No GPU required! Photos/Windows only. <https://github.com/ColorfulSoft/DeOldify.NET>.
No GPU required!

The **most advanced** version of DeOldify image colorization is available here,
exclusively.  Try a few images for free! [MyHeritage In Color](https://www.myheritage.com/incolor)

**Replicate:** Image: <a href=""https://replicate.com/arielreplicate/deoldify_image""><img src=""https://replicate.com/arielreplicate/deoldify_image/badge""></a> | Video: <a href=""https://replicate.com/arielreplicate/deoldify_video""><img src=""https://replicate.com/arielreplic"
learn_python3_spider,"
# learn_python3_spider
æ¥ä¸‹æ¥å°±æ˜¯ï¼Œå­¦ä¹ pythonçš„æ­£ç¡®å§¿åŠ¿ï¼

[ç­‰ä¸‹ï¼Œé˜¿é‡Œäº‘æœåŠ¡å™¨/2æ ¸2G/3M/40gï¼Œ99å…ƒ/å¹´ï¼Ÿï¼Ÿï¼Ÿ](https://t.aliyun.com/U/DYTxRF)

peace.

# å¦‚æœä½ ä¹Ÿæƒ³è¦ä¼š Python

å¯ä»¥åŠ å…¥æˆ‘çš„ [Python ä¼šå‘˜ç½‘ç«™](https://fxxkpython.com)!!

# pythonçˆ¬è™«æ•™ç¨‹ä»0åˆ°1

## çˆ¬è™«è´ŸåŸºç¡€

- [pythonçˆ¬è™«ç³»åˆ—æ•™ç¨‹-1 ï½œ ä¸ä¼šä»£ç ä¹Ÿæƒ³çˆ¬æ•°æ®ï¼Ÿè¿™å°±æ•™ä½ ï¼](https://mp.weixin.qq.com/s?__biz=MzkyNTExNzY4NA==&mid=2247484935&idx=1&sn=ad9f68845455ca35c08c0e11f92aa4a6&chksm=c1ca3b9cf6bdb28a8647bc911079221b790780611e019e628613657ebfbc38e1e317f53ab00f&token=1453775207&lang=zh_CN#rd)


## pythonçˆ¬è™«å‰ï¼ŒæŠ“åŒ…

- [pythonçˆ¬è™«ç³»åˆ—æ•™ç¨‹00 | ä»€ä¹ˆæ˜¯çˆ¬è™«ï¼Œæ€ä¹ˆç©çˆ¬è™«ï¼Ÿ](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489892&amp;idx=1&amp;sn=40f3f6b70d467ca72b838939aa63d720&amp;chksm=ceb9e378f9ce6a6e089459fad40e2ef8bdce9f46a0a7b9e8332cdbe6d2bc09a47879dc99dd4c&amp;scene=27#wechat_redirect)
- [pythonçˆ¬è™«ç³»åˆ—æ•™ç¨‹01 | æ•™ä½ åœ¨ Chrome æµè§ˆå™¨è½»æ¾æŠ“åŒ…](http://mp.weixin.qq.com/s?__biz=Mzg2NzYyNjg2Nw==&amp;mid=2247489893&amp;idx=1&amp;sn=32cc4fe30066a148485f40629aff598a&amp;chksm=ceb9e379f9ce6a6f609b95a729d01ff1745c101c14fe005fd2ed73e32dec08e1ed4d102b"
awesome-free-chatgpt,"# Awesome Free ChatGPT

![Awesome](https://cdn.jsdelivr.net/gh/LiLittleCat/PicBed/svg/awesome/badge.svg) [![English](https://cdn.jsdelivr.net/gh/LiLittleCat/PicBed/svg/lang/english.svg)](README_en.md) ![website count](https://img.shields.io/badge/websites-298-blue?style=flat) ![last-commit](https://img.shields.io/github/last-commit/LiLittleCat/awesome-free-chatgpt?style=flat&amp;label=last&nbsp;commit)

> 4 æœˆ 1 æ—¥ï¼ŒOpenAI å®£å¸ƒå¯ä»¥ä¸ç™»å½•å³å¯ä½¿ç”¨ ChatGPT 3.5ï¼Œå‚é˜… [Start using ChatGPT instantly](https://openai.com/blog/start-using-chatgpt-instantly)ã€‚

ğŸ å…è´¹çš„ ChatGPT (<https://chatgpt.com>)(<https://chat.openai.com>) é•œåƒç½‘ç«™åˆ—è¡¨ï¼Œä»¥åŠæ›´å¤šå…è´¹èµ„æºï¼ŒæŒç»­æ›´æ–°ã€‚

æ­¤å¤„åˆ—å‡ºçš„ç½‘ç«™å‡æ¥æºäºäº’è”ç½‘ï¼Œè¯·æ³¨æ„ä¸è¦åœ¨è¿™äº›ç½‘ç«™ä¸Šè¾“å…¥ä»»ä½•ä¸ªäººæ•æ„Ÿä¿¡æ¯ã€‚

ğŸŒˆ æ¬¢è¿è´¡çŒ®

- [æ·»åŠ é•œåƒç«™ç‚¹](https://github.com/LiLittleCat/awesome-free-chatgpt/issues/new?assignees=LiLittleCat&labels=&projects=&template=%E6%B7%BB%E5%8A%A0%E9%95%9C%E5%83%8F%E7%AB%99%E7%82%B9.md&title=%E6%B7%BB%E5%8A%A0%E9%95%9C%E5%83%8F%E7%AB%99%E7%82%B9)
- [åé¦ˆç«™ç‚¹å¤±æ•ˆ](https://github.com/LiLittleCat/awesome-free-chatgpt/issues/new?assignee"
Ciphey,"<p align=""center"">
Translations <br>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/de/README.md>ğŸ‡©ğŸ‡ª DE   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/fr/README.md>ğŸ‡«ğŸ‡· FR   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/hu/README.md>ğŸ‡­ğŸ‡º HU   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/id/README.md>ğŸ‡®ğŸ‡© ID   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/it/README.md>ğŸ‡®ğŸ‡¹ IT   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/nl/README.md>ğŸ‡³ğŸ‡± NL   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/pt-br/README.md>ğŸ‡§ğŸ‡· PT-BR   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/ru/README.md>ğŸ‡·ğŸ‡º RU   </a>
<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/zh/README.md>ğŸ‡¨ğŸ‡³ ZH   </a>
<a href=""https://github.com/Ciphey/Ciphey/tree/master/translations/th/README.md"">ğŸ‡¹ğŸ‡­ TH   </a>
 <br><br>
â¡ï¸
<a href=""https://githu"
pyscript,"# PyScript

## What is PyScript

### Summary

PyScript is a framework that allows users to create rich Python applications in the browser using HTML's interface and the power of [Pyodide](https://pyodide.org/en/stable/), [MicroPython](https://micropython.org/) and [WASM](https://webassembly.org/), and modern web technologies.

To get started see the [Beginning PyScript tutorial](https://docs.pyscript.net/latest/beginning-pyscript/).

For examples see [here](https://pyscript.com/@examples).

Other useful resources:

-   The [official technical docs](https://docs.pyscript.net/).
-   Our current [Home Page](https://pyscript.net/) on the web.
-   A free-to-use [online editor](https://pyscript.com/) for trying PyScript.
-   Our community [Discord Channel](https://discord.gg/BYB2kvyFwm), to keep in touch .

Every Tuesday at 15:30 UTC there is the _PyScript Community Call_ on zoom, where we can talk about PyScript development in the open. Most of the maintainers regularly participate in the c"
luigi,".. figure:: https://raw.githubusercontent.com/spotify/luigi/master/doc/luigi.png
   :alt: Luigi Logo
   :align: center

.. image:: https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fspotify%2Fluigi%2Fbadge&label=build&logo=none&%3Fref%3Dmaster&style=flat
    :target: https://actions-badge.atrox.dev/spotify/luigi/goto?ref=master

.. image:: https://img.shields.io/codecov/c/github/spotify/luigi/master.svg?style=flat
    :target: https://codecov.io/gh/spotify/luigi?branch=master

.. image:: https://img.shields.io/pypi/v/luigi.svg?style=flat
   :target: https://pypi.python.org/pypi/luigi

.. image:: https://img.shields.io/pypi/l/luigi.svg?style=flat
   :target: https://pypi.python.org/pypi/luigi

.. image:: https://readthedocs.org/projects/luigi/badge/?version=stable
    :target: https://luigi.readthedocs.io/en/stable/?badge=stable
    :alt: Documentation Status

Luigi is a Python (3.6, 3.7, 3.8, 3.9, 3.10, 3.11, 3.12 tested) package that helps you build comple"
onnx,"<!--
Copyright (c) ONNX Project Contributors

SPDX-License-Identifier: Apache-2.0
-->

<p align=""center""><img width=""40%"" src=""https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png"" /></p>

[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)
[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)
[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://g"
awesome-quant,"# Awesome Quant

A curated list of insanely awesome libraries, packages and resources for Quants (Quantitative Finance).

[![](https://awesome.re/badge.svg)](https://awesome.re)

## Languages

- [Python](#python)
- [R](#r)
- [Matlab](#matlab)
- [Julia](#julia)
- [Java](#java)
- [JavaScript](#javascript)
- [Haskell](#haskell)
- [Scala](#scala)
- [Ruby](#ruby)
- [Elixir/Erlang](#elixirerlang)
- [Golang](#golang)
- [CPP](#cpp)
- [CSharp](#csharp)
- [Rust](#rust)
- [Frameworks](#frameworks)
- [Reproducing Works](#reproducing-works)

## Python

### Numerical Libraries & Data Structures

- [numpy](https://www.numpy.org) - NumPy is the fundamental package for scientific computing with Python.
- [scipy](https://www.scipy.org) - SciPy (pronounced â€œSigh Pieâ€) is a Python-based ecosystem of open-source software for mathematics, science, and engineering.
- [pandas](https://pandas.pydata.org) - pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures an"
faker,"*Faker* is a Python package that generates fake data for you. Whether
you need to bootstrap your database, create good-looking XML documents,
fill-in your persistence to stress test it, or anonymize data taken from
a production service, Faker is for you.

Faker is heavily inspired by `PHP Faker`_, `Perl Faker`_, and by `Ruby Faker`_.

----

::

    _|_|_|_|          _|
    _|        _|_|_|  _|  _|      _|_|    _|  _|_|
    _|_|_|  _|    _|  _|_|      _|_|_|_|  _|_|
    _|      _|    _|  _|  _|    _|        _|
    _|        _|_|_|  _|    _|    _|_|_|  _|

|pypi| |build| |coverage| |license|

----

Compatibility
-------------

Starting from version ``4.0.0``, ``Faker`` dropped support for Python 2 and from version ``5.0.0``
only supports Python 3.7 and above. If you still need Python 2 compatibility, please install version ``3.0.1`` in the
meantime, and please consider updating your codebase to support Python 3 so you can enjoy the
latest features ``Faker`` has to offer. Please see the `"
inter,"# Inter

Inter is a typeface carefully crafted & designed for computer screens.
Inter features a tall x-height to aid in readability of mixed-case and lower-case text.
Inter is a [variable font](https://rsms.me/inter/#variable) with
several [OpenType features](https://rsms.me/inter/#features), like contextual alternates that adjusts punctuation depending on the shape of surrounding glyphs, slashed zero for when you need to disambiguate ""0"" from ""o"", tabular numbers, etc.

[**Download Inter font filesâ€¦**](https://github.com/rsms/inter/releases/latest)

<br>

[![Sample](misc/readme/intro.png)](https://rsms.me/inter/samples/)


### Quick questions

- **Where can I get Inter?** [Here](https://github.com/rsms/inter/releases/latest)
- **I think I found a bug. How can I let you know?** [Open an issue here](https://github.com/rsms/inter/issues/new?template=bug_report.md)
- **I have a question. Where can I get help?** [Post in Discussions Q&A](https://github.com/rsms/inter/discussions/categorie"
ultimatevocalremovergui,"# Ultimate Vocal Remover GUI v5.6
<img src=""https://raw.githubusercontent.com/Anjok07/ultimatevocalremovergui/master/gui_data/img/UVR_v5.6.png?raw=true"" />

[![Release](https://img.shields.io/github/release/anjok07/ultimatevocalremovergui.svg)](https://github.com/anjok07/ultimatevocalremovergui/releases/latest)
[![Downloads](https://img.shields.io/github/downloads/anjok07/ultimatevocalremovergui/total.svg)](https://github.com/anjok07/ultimatevocalremovergui/releases)

## About

This application uses state-of-the-art source separation models to remove vocals from audio files. UVR's core developers trained all of the models provided in this package (except for the Demucs v3 and v4 4-stem models).

- **Core Developers**
    - [Anjok07](https://github.com/anjok07)
    - [aufr33](https://github.com/aufr33)

- **Support the Project**
    - [Donate](https://www.buymeacoffee.com/uvr5)

## Installation

These bundles contain the UVR interface, Python, PyTorch, and other depen"
game-programmer,"* English [svg](https://miloyip.github.io/game-programmer/game-programmer.svg) [pdf](https://miloyip.github.io/game-programmer/game-programmer.pdf) [jpg](https://miloyip.github.io/game-programmer/game-programmer.jpg) [png](https://miloyip.github.io/game-programmer/game-programmer.png)
* ç®€ä½“ä¸­æ–‡ [svg](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.svg) [pdf](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.pdf) [jpg](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.jpg) [png](https://miloyip.github.io/game-programmer/game-programmer-zh-cn.png) by [tkchu](https://github.com/tkchu)

![ ](game-programmer.jpg)

## Disclaimer

1. This work (the WORK) was created by Milo Yip (the AUTHOR), who has been a game developer for more than 20 years.
2. The books shown in the WORK represent knowledge/skills that may/should be acquired by game programmers. There are other important ways of learning, such as practicing, courses, industrial/academic conferences"
kivy,"Kivy
====

<img align=""right"" height=""256"" src=""https://raw.githubusercontent.com/kivy/kivy/master/kivy/data/logo/kivy-icon-256.png""/>

[Kivy](https://www.kivy.org) is an open-source [Python](https://python.org) framework
for developing GUI apps that work cross-platform, including desktop, mobile and
embedded platforms.

The aim is to allow for quick and easy interaction design and rapid prototyping
whilst making your code reusable and deployable: Innovative user interfaces made
easy.

Kivy is written in Python and [Cython](https://cython.org/) and is built on
[OpenGL ES 2.0](https://www.khronos.org/opengles/). It supports various input 
devices and has an extensive (and extensible) widget library. With the
same codebase, you can target Windows, macOS, Linux (including Raspberry Pi OS),
Android, and iOS. All Kivy widgets are built with multitouch support.

Kivy is [MIT licensed](LICENSE), actively developed by a great community and is
supported by many projects managed by the 
[Kivy Or"
zipline,".. image:: https://media.quantopian.com/logos/open_source/zipline-logo-03_.png
    :target: https://www.zipline.io
    :width: 212px
    :align: center
    :alt: Zipline

=============

|Gitter|
|pypi version status|
|pypi pyversion status|
|travis status|
|appveyor status|
|Coverage Status|

Zipline is a Pythonic algorithmic trading library. It is an event-driven
system for backtesting. Zipline is currently used in production as the backtesting and live-trading
engine powering `Quantopian <https://www.quantopian.com>`_ -- a free,
community-centered, hosted platform for building and executing trading
strategies. Quantopian also offers a `fully managed service for professionals <https://factset.quantopian.com>`_
that includes Zipline, Alphalens, Pyfolio, FactSet data, and more.

- `Join our Community! <https://groups.google.com/forum/#!forum/zipline>`_
- `Documentation <https://www.zipline.io>`_
- Want to Contribute? See our `Development Guidelines <https://www.zipline.io/development-gu"
graphrag,"# GraphRAG

ğŸ‘‰ [Use the GraphRAG Accelerator solution](https://github.com/Azure-Samples/graphrag-accelerator) <br/>
ğŸ‘‰ [Microsoft Research Blog Post](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)<br/>
ğŸ‘‰ [Read the docs](https://microsoft.github.io/graphrag)<br/>
ğŸ‘‰ [GraphRAG Arxiv](https://arxiv.org/pdf/2404.16130)

<div align=""left"">
  <a href=""https://pypi.org/project/graphrag/"">
    <img alt=""PyPI - Version"" src=""https://img.shields.io/pypi/v/graphrag"">
  </a>
  <a href=""https://pypi.org/project/graphrag/"">
    <img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dm/graphrag"">
  </a>
  <a href=""https://github.com/microsoft/graphrag/issues"">
    <img alt=""GitHub Issues"" src=""https://img.shields.io/github/issues/microsoft/graphrag"">
  </a>
  <a href=""https://github.com/microsoft/graphrag/discussions"">
    <img alt=""GitHub Discussions"" src=""https://img.shields.io/github/discussions/microsoft/graphrag"">
  </a>
</div>

## O"
dspy,"<p align=""center"">
  <img align=""center"" src=""docs/images/DSPy8.png"" width=""460px"" />
</p>
<p align=""left"">


## DSPy: _Programming_â€”not promptingâ€”Foundation Models

**[Jun'24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct'23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul'24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun'24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb'24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan'24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec'23] [DSPy Assertions: Computational Constraints for Self-Refining Language Mod"
spotify-downloader,"
<!--- mdformat-toc start --slug=github --->

<!---
!!! IF EDITING THE README, ENSURE TO COPY THE WHOLE FILE TO index.md in `/docs/` AND REMOVE THE REFERENCES TO ReadTheDocs THERE.
--->

<div align=""center"">

# spotDL v4

**spotDL** finds songs from Spotify playlists on YouTube and downloads them - along with album art, lyrics and metadata.


[![MIT License](https://img.shields.io/github/license/spotdl/spotify-downloader?color=44CC11&style=flat-square)](https://github.com/spotDL/spotify-downloader/blob/master/LICENSE)
[![PyPI version](https://img.shields.io/pypi/pyversions/spotDL?color=%2344CC11&style=flat-square)](https://pypi.org/project/spotdl/)
[![PyPi downloads](https://img.shields.io/pypi/dw/spotDL?label=downloads@pypi&color=344CC11&style=flat-square)](https://pypi.org/project/spotdl/)
![Contributors](https://img.shields.io/github/contributors/spotDL/spotify-downloader?style=flat-square)
[![Discord](https://img.shields.io/discord/771628785447337985?label=discord&logo=discord&styl"
changedetection.io,"## Web Site Change Detection, Restock monitoring and notifications.

**_Detect website content changes and perform meaningful actions - trigger notifications via Discord, Email, Slack, Telegram, API calls and many more._**

_Live your data-life pro-actively._ 


[<img src=""https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot.png"" style=""max-width:100%;"" alt=""Self-hosted web site page change monitoring""  title=""Self-hosted web site page change monitoring""  />](https://changedetection.io?src=github)

[![Release Version][release-shield]][release-link] [![Docker Pulls][docker-pulls]][docker-link] [![License][license-shield]](LICENSE.md)

![changedetection.io](https://github.com/dgtlmoon/changedetection.io/actions/workflows/test-only.yml/badge.svg?branch=master)

[**Get started with website page change monitoring straight away. Don't have time? Try our $8.99/month subscription, use our proxies and support!**](https://changedetection.io) , _half the price of o"
haystack,"<div align=""center"">
  <a href=""https://haystack.deepset.ai/""><img src=""https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/banner_20.png"" alt=""Green logo of a stylized white 'H' with the text 'Haystack, by deepset. Haystack 2.0 is live ğŸ‰'Â Abstract green and yellow diagrams in the background.""></a>

|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
sd-webui-controlnet,"# ControlNet for Stable Diffusion WebUI

The WebUI extension for ControlNet and other injection-based SD controls.
![image](https://github.com/Mikubill/sd-webui-controlnet/assets/20929282/261f9a50-ba9c-472f-b398-fced61929c4a)

This extension is for AUTOMATIC1111's [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui), allows the Web UI to add [ControlNet](https://github.com/lllyasviel/ControlNet) to the original Stable Diffusion model to generate images. The addition is on-the-fly, the merging is not required.

# News

- [2024-07-09] ğŸ”¥[v1.1.454] ControlNet union model support [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2989]
- [2024-07-01] ğŸ”¥[v1.1.452] Depth Anything V2 - UDAV2 depth Preprocessor [Pull thread: https://github.com/Mikubill/sd-webui-controlnet/pull/2969]
- [2024-05-19] ğŸ”¥[v1.1.449] Anyline Preprocessor & MistoLine SDXL model [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2907]
"
reddit,"## This repository is archived.

This repository is archived and will not receive any updates or accept issues or pull requests.

To report bugs in reddit.com please make a post in [/r/bugs](http://www.reddit.com/r/bugs).

If you have found a bug that can in some way compromise the security of the
site or its users, please exercise [responsible
disclosure](http://www.reddit.com/wiki/whitehat) and e-mail
security@reddit.com.

---

### API

For notices about reddit API changes and discussion of reddit API client development, subscribe to the [/r/redditdev](http://www.reddit.com/r/redditdev) and [/r/changelog](http://www.reddit.com/r/changelog) subreddits.

To learn more about reddit's API, check out our [automated API documentation](http://www.reddit.com/dev/api) and the [API wiki page](https://github.com/reddit/reddit/wiki/API). Please use a unique User-Agent string and take care to abide by our [API rules](https://github.com/reddit/reddit/wiki/API#wiki-rules).

### Quickstart

To set u"
InstaPy,"<p align=""center"">
  <img src=""https://i.imgur.com/sJzfZsL.jpg"" width=""154"">
  <h1 align=""center"">InstaPy</h1>
  <p align=""center"">Tooling that <b>automates</b> your social media interactions to â€œfarmâ€ Likes, Comments, and Followers on Instagram
Implemented in Python using the Selenium module.<p>
  <p align=""center"">
    <a href=""https://github.com/timgrossmann/InstaPy/blob/master/LICENSE"">
      <img src=""https://img.shields.io/badge/license-GPLv3-blue.svg"" />
    </a>
    <a href=""https://github.com/SeleniumHQ/selenium"">
      <img src=""https://img.shields.io/badge/built%20with-Selenium-yellow.svg"" />
    </a>
    <a href=""https://www.python.org/"">
    	<img src=""https://img.shields.io/badge/built%20with-Python3-red.svg"" />
    </a>
    <a href=""https://www.github.com/timgrossmann/InstaPy#backer"">
	<img src=""https://opencollective.com/instapy/backers/badge.svg"">
    </a>
    <a href=""https://www.github.com/timgrossmann/InstaPy#sponsors"">
	<img src=""https://opencollective.com/instapy/"
MediaCrawler,"> **å…è´£å£°æ˜ï¼š**
> 
> å¤§å®¶è¯·ä»¥å­¦ä¹ ä¸ºç›®çš„ä½¿ç”¨æœ¬ä»“åº“ï¼Œçˆ¬è™«è¿æ³•è¿è§„çš„æ¡ˆä»¶ï¼šhttps://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China  <br>
>
>æœ¬ä»“åº“çš„æ‰€æœ‰å†…å®¹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒä¹‹ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚ä»»ä½•äººæˆ–ç»„ç»‡ä¸å¾—å°†æœ¬ä»“åº“çš„å†…å®¹ç”¨äºéæ³•ç”¨é€”æˆ–ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šã€‚æœ¬ä»“åº“æ‰€æ¶‰åŠçš„çˆ¬è™«æŠ€æœ¯ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸å¾—ç”¨äºå¯¹å…¶ä»–å¹³å°è¿›è¡Œå¤§è§„æ¨¡çˆ¬è™«æˆ–å…¶ä»–éæ³•è¡Œä¸ºã€‚å¯¹äºå› ä½¿ç”¨æœ¬ä»“åº“å†…å®¹è€Œå¼•èµ·çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæœ¬ä»“åº“ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ä½¿ç”¨æœ¬ä»“åº“çš„å†…å®¹å³è¡¨ç¤ºæ‚¨åŒæ„æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚

> ç‚¹å‡»æŸ¥çœ‹æ›´ä¸ºè¯¦ç»†çš„å…è´£å£°æ˜ã€‚[ç‚¹å‡»è·³è½¬](#disclaimer)
# ä»“åº“æè¿°

**å°çº¢ä¹¦çˆ¬è™«**ï¼Œ**æŠ–éŸ³çˆ¬è™«**ï¼Œ **å¿«æ‰‹çˆ¬è™«**ï¼Œ **Bç«™çˆ¬è™«**ï¼Œ **å¾®åšçˆ¬è™«**ï¼Œ**ç™¾åº¦è´´å§çˆ¬è™«**ï¼Œ**çŸ¥ä¹çˆ¬è™«**...ã€‚  
ç›®å‰èƒ½æŠ“å–å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€è´´å§ã€çŸ¥ä¹ç­‰å¹³å°çš„å…¬å¼€ä¿¡æ¯ã€‚

åŸç†ï¼šåˆ©ç”¨[playwright](https://playwright.dev/)æ­æ¡¥ï¼Œä¿ç•™ç™»å½•æˆåŠŸåçš„ä¸Šä¸‹æ–‡æµè§ˆå™¨ç¯å¢ƒï¼Œé€šè¿‡æ‰§è¡ŒJSè¡¨è¾¾å¼è·å–ä¸€äº›åŠ å¯†å‚æ•°
é€šè¿‡ä½¿ç”¨æ­¤æ–¹å¼ï¼Œå…å»äº†å¤ç°æ ¸å¿ƒåŠ å¯†JSä»£ç ï¼Œé€†å‘éš¾åº¦å¤§å¤§é™ä½

[MediaCrawlerPro](https://github.com/MediaCrawlerPro) ç‰ˆæœ¬å·²ç»è¿­ä»£å‡ºæ¥äº†ï¼Œç›¸è¾ƒäºå¼€æºç‰ˆæœ¬çš„ä¼˜åŠ¿ï¼š
- å¤šè´¦å·+IPä»£ç†æ”¯æŒï¼ˆé‡ç‚¹ï¼ï¼‰
- å»é™¤Playwrightä¾èµ–ï¼Œä½¿ç”¨æ›´åŠ ç®€å•
- æ”¯æŒlinuxéƒ¨ç½²ï¼ˆDocker docker-composeï¼‰
- ä»£ç é‡æ„ä¼˜åŒ–ï¼Œæ›´åŠ æ˜“è¯»æ˜“ç»´æŠ¤ï¼ˆè§£è€¦JSç­¾åé€»è¾‘ï¼‰
- å®Œç¾çš„æ¶æ„è®¾è®¡ï¼Œæ›´åŠ æ˜“æ‰©å±•ï¼Œæºç å­¦ä¹ çš„ä»·å€¼æ›´å¤§


MediaCrawlerä»“åº“ç™½é‡‘èµåŠ©å•†:
<a href=""https://mangoproxy.com/?utm_source=mediacrawler&utm_medium=repository&utm_campaign=default"">ã€MangoProxyã€‘å…¨çƒIPä»£ç†ç™½é‡‘æ¨èï¼Œæ”¯æŒ210+å›½å®¶ [MangoProxy](https://mangoproxy.com/?utm_source=mediacrawler&utm_medium=repository&utm_c"
ml-stable-diffusion,"# Core ML Stable Diffusion

Run Stable Diffusion on Apple Silicon with Core ML

[\[Blog Post\]](https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon) [\[BibTeX\]](#bibtex)


This repository comprises:

- `python_coreml_stable_diffusion`, a Python package for converting PyTorch models to Core ML format and performing image generation with Hugging Face [diffusers](https://github.com/huggingface/diffusers) in Python
- `StableDiffusion`, a Swift package that developers can add to their Xcode projects as a dependency to deploy image generation capabilities in their apps. The Swift package relies on the Core ML model files generated by `python_coreml_stable_diffusion`

If you run into issues during installation or runtime, please refer to the [FAQ](#faq) section. Please refer to the [System Requirements](#system-requirements) section before getting started.

<img src=""assets/readme_reel.png"">

## <a name=""system-requirements""></a> System Requirements

<details>
  "
marker,"# Marker

Marker converts PDF to markdown quickly and accurately.

- Supports a wide range of documents (optimized for books and scientific papers)
- Supports all languages
- Removes headers/footers/other artifacts
- Formats tables and code blocks
- Extracts and saves images along with the markdown
- Converts most equations to latex
- Works on GPU, CPU, or MPS

## How it works

Marker is a pipeline of deep learning models:

- Extract text, OCR if necessary (heuristics, [surya](https://github.com/VikParuchuri/surya), tesseract)
- Detect page layout and find reading order ([surya](https://github.com/VikParuchuri/surya))
- Clean and format each block (heuristics, [texify](https://github.com/VikParuchuri/texify)
- Combine blocks and postprocess complete text (heuristics, [pdf_postprocessor](https://huggingface.co/vikp/pdf_postprocessor_t5))

It only uses models where necessary, which improves speed and accuracy.

## Examples

| PDF                                                           "
awesome-oss-alternatives,"# Awesome open-source alternatives to SaaS
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Awesome list of open-source startup alternatives to established SaaS products. Maintained by folks at [![Runa Capital](https://img.shields.io/static/v1?label=&message=%20&style=social&logoWidth=50&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD8AAAAUCAYAAAA6NOUqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAUpSURBVHgBtVhNUuNGFH7d/gkzmSrEKjGzGPkEmKpAUtkgdkBSBTkB4gSYE9icAHMCzAniqWI8kxXKJsXAVGFOgLLBZDWeRSapBNTpT/2EZCEbmcl8VV2S+v97f/1agnLi0qlZU1PldZLCIaUcIZSlSFhoU4p6+rsX3MrO81/evqTHY0kXVxebvwe6dHQ5pM8A8VCH/krNJllq6I4bEdmxUOTr0qy8OZ10w+tkiPq67JAhvklGGI4uv9L/jLHkr9cWtvWjmYt0Gkp5Sty4s93z3ycYda5LT5etRN2lLh7XRfsY8BPf07ok15jTpcZ9eok2i9usqL6YsYHQxJ88LR5o0hv3GoXwVAAzDy7CT0nTKpB6MeWm+jlClTw913zV6w0oH9L9bN7sgAlBOG2KhQPluLpU+b3JfT0y1kLcZvPYCJiznkn+iyflY2UWYx5qoJRoUVkeVjonftaY"
pyspider,"pyspider [![Build Status]][Travis CI] [![Coverage Status]][Coverage]
========

A Powerful Spider(Web Crawler) System in Python.

- Write script in Python
- Powerful WebUI with script editor, task monitor, project manager and result viewer
- [MySQL](https://www.mysql.com/), [MongoDB](https://www.mongodb.org/), [Redis](http://redis.io/), [SQLite](https://www.sqlite.org/), [Elasticsearch](https://www.elastic.co/products/elasticsearch); [PostgreSQL](http://www.postgresql.org/) with [SQLAlchemy](http://www.sqlalchemy.org/) as database backend
- [RabbitMQ](http://www.rabbitmq.com/), [Redis](http://redis.io/) and [Kombu](http://kombu.readthedocs.org/) as message queue
- Task priority, retry, periodical, recrawl by age, etc...
- Distributed architecture, Crawl Javascript pages, Python 2.{6,7}, 3.{3,4,5,6} support, etc...

Tutorial: [http://docs.pyspider.org/en/latest/tutorial/](http://docs.pyspider.org/en/latest/tutorial/)  
Documentation: [http://docs.pyspider.org/](http://docs.pyspider.org/)"
rembg,"# Rembg

[![Downloads](https://img.shields.io/pypi/dm/rembg.svg)](https://img.shields.io/pypi/dm/rembg.svg)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://img.shields.io/badge/License-MIT-blue.svg)
[![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/KenjieDec/RemBG)
[![Streamlit App](https://img.shields.io/badge/ğŸˆ%20Streamlit%20Community-Cloud-blue)](https://bgremoval.streamlit.app/)

Rembg is a tool to remove images background.

<p style=""display: flex;align-items: center;justify-content: center;"">
  <img alt=""example car-1"" src=""https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.jpg"" width=""100"" />
  <img alt=""example car-1.out"" src=""https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-1.out.png"" width=""100"" />
  <img alt=""example car-2"" src=""https://raw.githubusercontent.com/danielgatis/rembg/master/examples/car-2.jpg"" width=""100"" />
  <img alt=""example "
PySnooper,"# PySnooper - Never use print for debugging again

**PySnooper** is a poor man's debugger. If you've used Bash, it's like `set -x` for Python, except it's fancier.

Your story: You're trying to figure out why your Python code isn't doing what you think it should be doing. You'd love to use a full-fledged debugger with breakpoints and watches, but you can't be bothered to set one up right now.

You want to know which lines are running and which aren't, and what the values of the local variables are.

Most people would use `print` lines, in strategic locations, some of them showing the values of variables.

**PySnooper** lets you do the same, except instead of carefully crafting the right `print` lines, you just add one decorator line to the function you're interested in. You'll get a play-by-play log of your function, including which lines ran and   when, and exactly when local variables were changed.

What makes **PySnooper** stand out from all other code intelligence tools? You can us"
PyTorch-GAN,"<p align=""center""><img src=""assets/logo.png"" width=""480""\></p>

**This repository has gone stale as I unfortunately do not have the time to maintain it anymore. If you would like to continue the development of it as a collaborator send me an email at eriklindernoren@gmail.com.**

## PyTorch-GAN
Collection of PyTorch implementations of Generative Adversarial Network varieties presented in research papers. Model architectures will not always mirror the ones proposed in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GANs to implement are very welcomed.

<b>See also:</b> [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)

## Table of Contents
  * [Installation](#installation)
  * [Implementations](#implementations)
    + [Auxiliary Classifier GAN](#auxiliary-classifier-gan)
    + [Adversarial Autoencoder](#adversarial-autoencoder)
    + [BEGAN](#began)
    + [BicycleGAN](#b"
learn-python,"# Playground and Cheatsheet for Learning Python

> ğŸ‡ºğŸ‡¦ UKRAINE [IS BEING ATTACKED](https://war.ukraine.ua/) BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED.
> - Help Ukraine via:
>   - [Serhiy Prytula Charity Foundation](https://prytulafoundation.org/en/)
>   - [Come Back Alive Charity Foundation](https://savelife.in.ua/en/donate-en/)
>   - [National Bank of Ukraine](https://bank.gov.ua/en/news/all/natsionalniy-bank-vidkriv-spetsrahunok-dlya-zboru-koshtiv-na-potrebi-armiyi)
> - More info on [war.ukraine.ua](https://war.ukraine.ua/) and [MFA of Ukraine](https://twitter.com/MFA_Ukraine)

<hr/>

[![Build Status](https://travis-ci.org/trekhleb/learn-python.svg?branch=master)](https://travis-ci.org/trekhleb/learn-python)

> This is a collection of Python scripts that are split by [topics](#table-of-contents) and contain 
code examples with explanations, different use cases and links to further readings.

> _Read this in:_ [_PortuguÃªs_](README.pt-BR.md), [_"
MoneyPrinterTurbo,"<div align=""center"">
<h1 align=""center"">MoneyPrinterTurbo ğŸ’¸</h1>

<p align=""center"">
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/stargazers""><img src=""https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""Stargazers""></a>
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/issues""><img src=""https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""Issues""></a>
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/network/members""><img src=""https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""Forks""></a>
  <a href=""https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE""><img src=""https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge"" alt=""License""></a>
</p>
<br>
<h3>ç®€ä½“ä¸­æ–‡ | <a href=""README-en.md"">English</a></h3>
<div align=""center"">
  <a href=""https://trendshift.io/repositories/8731"" target=""_blank""><i"
ipython,".. image:: https://codecov.io/github/ipython/ipython/coverage.svg?branch=main
    :target: https://codecov.io/github/ipython/ipython?branch=main

.. image:: https://img.shields.io/pypi/v/IPython.svg
    :target: https://pypi.python.org/pypi/ipython

.. image:: https://github.com/ipython/ipython/actions/workflows/test.yml/badge.svg
    :target: https://github.com/ipython/ipython/actions/workflows/test.yml

.. image:: https://www.codetriage.com/ipython/ipython/badges/users.svg
    :target: https://www.codetriage.com/ipython/ipython/

.. image:: https://raster.shields.io/badge/Follows-SPEC--0000-brightgreen.png
    :target: https://scientific-python.org/specs/spec-0000/

.. image:: https://tidelift.com/badges/package/pypi/ipython?style=flat
    :target: https://tidelift.com/subscription/pkg/pypi-ipython


===========================================
 IPython: Productive Interactive Computing
===========================================

Overview
========

Welcome to IPython.  Our full docum"
avatarify-python,"![](docs/mona.gif)

# Avatarify Python

Photorealistic avatars for video-conferencing.

Avatarify Python requires manually downloading and installing some dependencies, and is therefore best suited for users who have some experience with command line applications. [Avatarify Desktop](https://github.com/alievk/avatarify-desktop), which aims to be easier to install and use, is recommended for most users. If you still want to use Avatarify Python, proceed to the [install instructions](docs/).

Based on [First Order Motion Model](https://github.com/AliaksandrSiarohin/first-order-model).

## News
- **7 Mar 2021.** Renamed project to Avatarify Python to distinguish it from other versions of Avatarify
- **14 December 2020.** Released Avatarify Desktop. Check it out [here](https://github.com/alievk/avatarify-desktop).
- **11 July 2020.** Added Docker support. Now you can run Avatarify from Docker on [Linux](https://github.com/alievk/avatarify-python/blob/master/docs/README.md#docker). Thanks t"
autojump,"NAME
----

autojump - a faster way to navigate your filesystem

DESCRIPTION
-----------

autojump is a faster way to navigate your filesystem. It works by
maintaining a database of the directories you use the most from the
command line.

*Directories must be visited first before they can be jumped to.*

USAGE
-----

`j` is a convenience wrapper function around `autojump`. Any option that
can be used with `autojump` can be used with `j` and vice versa.

-   Jump To A Directory That Contains `foo`:

        j foo

-   Jump To A Child Directory:

    Sometimes it's convenient to jump to a child directory
    (sub-directory of current directory) rather than typing out the
    full name.

        jc bar

-   Open File Manager To Directories (instead of jumping):

    Instead of jumping to a directory, you can open a file explorer
    window (Mac Finder, Windows Explorer, GNOME Nautilus, etc.) to the
    directory instead.

        jo music

    Opening a file manager to a child directory is"
voice-changer,"## VC Client

[English](/README_en.md) [Korean](/README_ko.md) [Russian](/README_ru.md)

## What's New!
- å§‰å¦¹å“ã®Text To Speechã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚
  - ç°¡å˜ãªIFã§éŸ³å£°ç”Ÿæˆã‚’æ¥½ã—ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
  - è©³ç´°ã¯[ã“ã¡ã‚‰](https://github.com/w-okada/ttsclient)ã€‚
- Beatrice V2 ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰å…¬é–‹!!!
  - [ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ãƒªãƒã‚¸ãƒˆãƒª](https://huggingface.co/fierce-cats/beatrice-trainer)
  - [ã‚³ãƒ©ãƒœç‰ˆ](https://github.com/w-okada/beatrice-trainer-colab)
- v.2.0.61-alpha
  - [ã“ã¡ã‚‰ã‚’å‚ç…§](https://github.com/w-okada/voice-changer/tree/v.2)
  - feature:
    - ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰ã®æ™‚é–“ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
  - bugfix:
    - ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®éš›ã«ã€ä½¿ç”¨ã—ãªã„ãƒ¢ãƒ‡ãƒ«ã®è¦ç´ ã‚’0ã«ã—ã¦ã‚‚å‹•ãã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
- v.2.0.60-alpha
  - [ã“ã¡ã‚‰ã‚’å‚ç…§](https://github.com/w-okada/voice-changer/tree/v.2)
  - feature:
    - [darkmode](https://github.com/w-okada/voice-changer/issues/1306)
    - [re-introduce pytorch rmvpe](https://github.com/w-okada/voice-changer/issues/1319)
    - [wasapi æ’ä»–ãƒ¢ãƒ¼ãƒ‰é¸æŠ](https://github.com/w-okada/voice-changer/issues/1305)
- v.2.0.58-alpha
  - [ã“ã¡ã‚‰ã‚’å‚ç…§](https://github.com/w-okada/voice-changer/tree/v.2)
  - feature:
    - "
plotly.py,"# plotly.py

<table>
    <tr>
        <td>Latest Release</td>
        <td>
            <a href=""https://pypi.org/project/plotly/""/>
            <img src=""https://badge.fury.io/py/plotly.svg""/>
        </td>
    </tr>
    <tr>
        <td>User forum</td>
        <td>
            <a href=""https://community.plotly.com/""/>
            <img src=""https://img.shields.io/badge/help_forum-discourse-blue.svg""/>
        </td>
    </tr>
    <tr>
        <td>PyPI Downloads</td>
        <td>
            <a href=""https://pepy.tech/project/plotly""/>
            <img src=""https://pepy.tech/badge/plotly/month""/>
        </td>
    </tr>
    <tr>
        <td>License</td>
        <td>
            <a href=""https://opensource.org/licenses/MIT""/>
            <img src=""https://img.shields.io/badge/License-MIT-yellow.svg""/>
        </td>
    </tr>
</table>

<div align=""center"">
  <a href=""https://dash.plotly.com/project-maintenance"">
    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" "
unsloth,"<div align=""center"">

  <a href=""https://unsloth.ai""><picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"">
    <img alt=""unsloth logo"" src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png"" height=""110"" style=""max-width: 100%;"">
  </picture></a>
  
<a href=""https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png"" height=""48""></a>
<a href=""https://discord.gg/unsloth""><img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png"" height=""48""></a>
<a href=""https://ko-fi.com/unsloth""><img src=""https://raw.github"
vision,"# torchvision

[![total torchvision downloads](https://pepy.tech/badge/torchvision)](https://pepy.tech/project/torchvision)
[![documentation](https://img.shields.io/badge/dynamic/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https://pytorch.org/vision/stable/index.html)

The torchvision package consists of popular datasets, model architectures, and common image transformations for computer
vision.

## Installation

Please refer to the [official
instructions](https://pytorch.org/get-started/locally/) to install the stable
versions of `torch` and `torchvision` on your system.

To build source, refer to our [contributing
page](https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md#development-installation).

The following is the corresponding `torchvision` versions and supported Python
versions.

| `torch`            | `torchvision`      | Python              |
| ------------------ | ------------------ "
peft,"<!---
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<h1 align=""center""> <p>ğŸ¤— PEFT</p></h1>
<h3 align=""center"">
    <p>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods</p>
</h3>

Fine-tuning large pretrained models is often prohibitively costly due to their scale. Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a small number of (ext"
prefect,"<p align=""center""><img src=""https://github.com/PrefectHQ/prefect/assets/3407835/c654cbc6-63e8-4ada-a92a-efd2f8f24b85"" width=1000></p>

<p align=""center"">
    <a href=""https://pypi.python.org/pypi/prefect/"" alt=""PyPI version"">
        <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/prefect?color=0052FF&labelColor=090422""></a>
    <a href=""https://github.com/prefecthq/prefect/"" alt=""Stars"">
        <img src=""https://img.shields.io/github/stars/prefecthq/prefect?color=0052FF&labelColor=090422"" /></a>
    <a href=""https://pepy.tech/badge/prefect/"" alt=""Downloads"">
        <img src=""https://img.shields.io/pypi/dm/prefect?color=0052FF&labelColor=090422"" /></a>
    <a href=""https://github.com/prefecthq/prefect/pulse"" alt=""Activity"">
        <img src=""https://img.shields.io/github/commit-activity/m/prefecthq/prefect?color=0052FF&labelColor=090422"" /></a>
    <br>
    <a href=""https://prefect.io/slack"" alt=""Slack"">
        <img src=""https://img.shields.io/badge/slack-join_community-red.svg?c"
neural-networks-and-deep-learning,"# Code samples for ""Neural Networks and Deep Learning""

This repository contains code samples for my book on [""Neural Networks
and Deep Learning""](http://neuralnetworksanddeeplearning.com).

The code is written for Python 2.6 or 2.7. There is a version for 
Python 3.8-3.10 [here](https://github.com/unexploredtest/neural-networks-and-deep-learning). 
I will not be updating the current repository for Python 3 compatibility.

The program `src/network3.py` uses version 0.6 or 0.7 of the Theano
library.  It needs modification for compatibility with later versions
of the library.  I will not be making such modifications.

As the code is written to accompany the book, I don't intend to add
new features. However, bug reports are welcome, and you should feel
free to fork and modify the code.

## License

MIT License

Copyright (c) 2012-2022 Michael Nielsen

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated docume"
codellama,"# Introducing Code Llama

Code Llama is a family of large language models for code based on [Llama 2](https://github.com/facebookresearch/llama) providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model train"
netbox,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/netbox-community/netbox/develop/docs/netbox_logo.svg"" width=""400"" alt=""NetBox logo"" />
  <p><strong>The cornerstone of every automated network</strong></p>
  <a href=""https://github.com/netbox-community/netbox/releases""><img src=""https://img.shields.io/github/v/release/netbox-community/netbox"" alt=""Latest release"" /></a>
  <a href=""https://github.com/netbox-community/netbox/blob/master/LICENSE.txt""><img src=""https://img.shields.io/badge/license-Apache_2.0-blue.svg"" alt=""License"" /></a>
  <a href=""https://github.com/netbox-community/netbox/graphs/contributors""><img src=""https://img.shields.io/github/contributors/netbox-community/netbox?color=blue"" alt=""Contributors"" /></a>
  <a href=""https://github.com/netbox-community/netbox/stargazers""><img src=""https://img.shields.io/github/stars/netbox-community/netbox?style=flat"" alt=""GitHub stars"" /></a>
  <a href=""https://explore.transifex.com/netbox-community/netbox/""><img src=""h"
awesome-python-login-model,"<h2 align=""center""><code>ğŸ‰Life is fantasticğŸ¥³!~</code></h2>

<br>
<p align=""center"">
    <img src=""https://github.com/CriseLYJ/flask-video-streaming-recorder/blob/master/img/main.jpg?raw=true"" 
        alt=""Master"">
</p>

<br>

<p align=""center"">""<i>Did you know all your doors were locked?</i>"" - Riddick (The Chronicles of Riddick)</p>

<br>

<p align=""center"">
  <a href=""https://github.com/CriseLYJ/awesome-python-login-model/tree/master"">
    <img src=""https://img.shields.io/badge/Branch-master-green.svg?longCache=true""
        alt=""Branch"">
  </a>
  <a href=""https://github.com/CriseLYJ/awesome-python-login-model/stargazers"">
    <img src=""https://img.shields.io/github/stars/CriseLYJ/awesome-python-login-model.svg?label=Stars&style=social""
        alt=""Stars"">
  </a>
    <a href=""https://github.com/CriseLYJ/awesome-python-login-model/network/members"">
    <img src=""https://img.shields.io/github/forks/CriseLYJ/awesome-python-login-model.svg?label=Forks&style=social""
        alt=""Forks"">"
twint,"# TWINT - Twitter Intelligence Tool
![2](https://i.imgur.com/iaH3s7z.png)
![3](https://i.imgur.com/hVeCrqL.png)

[![PyPI](https://img.shields.io/pypi/v/twint.svg)](https://pypi.org/project/twint/) [![Build Status](https://travis-ci.org/twintproject/twint.svg?branch=master)](https://travis-ci.org/twintproject/twint) [![Python 3.6|3.7|3.8](https://img.shields.io/badge/Python-3.6%2F3.7%2F3.8-blue.svg)](https://www.python.org/download/releases/3.0/) [![GitHub license](https://img.shields.io/github/license/haccer/tweep.svg)](https://github.com/haccer/tweep/blob/master/LICENSE) [![Downloads](https://pepy.tech/badge/twint)](https://pepy.tech/project/twint) [![Downloads](https://pepy.tech/badge/twint/week)](https://pepy.tech/project/twint/week) [![Patreon](https://img.shields.io/endpoint.svg?url=https:%2F%2Fshieldsio-patreon.herokuapp.com%2Ftwintproject)](https://www.patreon.com/twintproject) ![](https://img.shields.io/twitter/follow/noneprivacy.svg?label=Follow&style=social) 

>No authenticat"
ChatGLM2-6B,"# ChatGLM2-6B

<p align=""center"">
ğŸ¤— <a href=""https://huggingface.co/THUDM/chatglm2-6b"" target=""_blank"">HF Repo</a> â€¢ ğŸ¦ <a href=""https://twitter.com/thukeg"" target=""_blank"">Twitter</a> â€¢ ğŸ“ƒ <a href=""https://arxiv.org/abs/2103.10360"" target=""_blank"">[GLM@ACL 22]</a> <a href=""https://github.com/THUDM/GLM"" target=""_blank"">[GitHub]</a> â€¢ ğŸ“ƒ <a href=""https://arxiv.org/abs/2210.02414"" target=""_blank"">[GLM-130B@ICLR 23]</a> <a href=""https://github.com/THUDM/GLM-130B"" target=""_blank"">[GitHub]</a> <br>
</p>
<p align=""center"">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„  <a href=""https://discord.gg/fK2dz4bg"" target=""_blank"">Discord</a> å’Œ <a href=""resources/WECHAT.md"" target=""_blank"">WeChat</a>
</p>
<p align=""center"">
ğŸ“åœ¨ <a href=""https://www.chatglm.cn"">chatglm.cn</a> ä½“éªŒæ›´å¤§è§„æ¨¡çš„ ChatGLM æ¨¡å‹ã€‚
</p>


*Read this in [English](README_EN.md)*

## GLM-4 å¼€æºæ¨¡å‹å’ŒAPI

æˆ‘ä»¬å·²ç»å‘å¸ƒæœ€æ–°çš„ **GLM-4** æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæœ‰äº†æ–°çš„çªç ´ï¼Œæ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ¸ é“ä½“éªŒæˆ‘ä»¬çš„æœ€æ–°æ¨¡å‹ã€‚

+ [GLM-4 å¼€æºæ¨¡å‹](https://github.com/THUDM/GLM-4) æˆ‘ä»¬å·²ç»å¼€æºäº† GLM-4-9B ç³»åˆ—æ¨¡å‹ï¼Œåœ¨å„é¡¹æŒ‡æ ‡çš„ceæ˜¯ä¸Šæœ‰æ˜æ˜¾æå‡ï¼Œæ¬¢è¿å°è¯•ã€‚
+ [æ™ºè°±æ¸…è¨€](https://chatglm.cn/m"
baselines,"**Status:** Maintenance (expect bug fixes and minor updates)

<img src=""data/logo.jpg"" width=25% align=""right"" /> [![Build status](https://travis-ci.org/openai/baselines.svg?branch=master)](https://travis-ci.org/openai/baselines)

# Baselines

OpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.

These algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. 

## Prerequisites 
Baselines requires python3 (>=3.5) with the development headers. You'll also need system packages CMake, OpenMPI and zlib. Those can be installed as follows
### Ubuntu 
    
```bash
sudo apt-get update && sudo apt-get install cmake "
airbyte,"<p align=""center"">
  <a href=""https://airbyte.com""><img src=""https://assets.website-files.com/605e01bc25f7e19a82e74788/624d9c4a375a55100be6b257_Airbyte_logo_color_dark.svg"" alt=""Airbyte""></a>
</p>
<p align=""center"">
    <em>Data integration platform for ELT pipelines from APIs, databases & files to databases, warehouses & lakes</em>
</p>
<p align=""center"">
<a href=""https://github.com/airbytehq/airbyte/stargazers/"" target=""_blank"">
    <img src=""https://img.shields.io/github/stars/airbytehq/airbyte?style=social&label=Star&maxAge=2592000"" alt=""Test"">
</a>
<a href=""https://github.com/airbytehq/airbyte/releases"" target=""_blank"">
    <img src=""https://img.shields.io/github/v/release/airbytehq/airbyte?color=white"" alt=""Release"">
</a>
<a href=""https://airbytehq.slack.com/"" target=""_blank"">
    <img src=""https://img.shields.io/badge/slack-join-white.svg?logo=slack"" alt=""Slack"">
</a>
<a href=""https://www.youtube.com/c/AirbyteHQ/?sub_confirmation=1"" target=""_blank"">
    <img alt=""YouTube Channel"
Shadowrocket-ADBlock-Rules,"## æœ€å®Œå–„çš„ iOS ç¿»å¢™è§„åˆ™

### åœæ­¢æ›´æ–°å…¬å‘Š

ç»´æŠ¤è¯¥é¡¹ç›®å·²èŠ±è´¹äº†æˆ‘è¿‡å¤šçš„æ—¶é—´ï¼Œè€Œç”Ÿæ´»ä¸­å€¼å¾—èŠ±è´¹æ—¶é—´çš„ä¸œè¥¿å¤ªå¤šï¼Œæ‰€ä»¥ä»å³æ—¥èµ·åœæ­¢æ›´æ–°è¯¥é¡¹ç›®ã€‚

------------------------------------------------------

è¿™é‡Œæ˜¯ä¸€ç³»åˆ—å¥½ç”¨çš„ç¿»å¢™è§„åˆ™ï¼Œé’ˆå¯¹ [Shadowrocket](https://liguangming.com/Shadowrocket) å¼€å‘ï¼Œæ”¯æŒå¹¿å‘Šè¿‡æ»¤ã€‚è§„åˆ™å®šä¹‰äº†å“ªäº›ç½‘ç«™å¯ä»¥ç›´è¿ï¼Œå“ªäº›å¿…é¡»èµ°ä»£ç†ï¼Œè§„åˆ™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œæ— æ³•æä¾›ç¿»å¢™åŠŸèƒ½ã€‚ä½¿ç”¨ Python æŒ‰ç…§ä¸€å®šçš„è§„åˆ™å’Œæ¨¡æ¿å®šæœŸè‡ªåŠ¨ç”Ÿæˆï¼Œå¹¶ä¸”ä½¿ç”¨å¼€æºçš„åŠ›é‡ï¼Œé›†ä¼—äººä¹‹åŠ›é€æ¸å®Œå–„ã€‚

**æ­£åœ¨ä½¿ç”¨æ‰‹æœºæµè§ˆæœ¬é¡µé¢çš„ç”¨æˆ· [è¯·ç‚¹å‡»è¿™é‡Œ](https://github.com/h2y/Shadowrocket-ADBlock-Rules/blob/master/readme.md)ï¼ŒæŸ¥çœ‹å®Œæ•´çš„è¯´æ˜æ–‡æ¡£ã€‚**

**æœ¬è§„åˆ™å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š**

- é»‘åå•ç”±æœ€æ–°ç‰ˆ GFWList è‡ªåŠ¨è½¬æ¢ï¼›ç™½åå•é’ˆå¯¹å…¨çƒ top500 ç«™ç‚¹çš„è¿é€šæƒ…å†µå®šæœŸè‡ªåŠ¨ç”Ÿæˆã€‚
- è‡ªåŠ¨è½¬æ¢æœ€æ–°ç‰ˆæœ¬çš„ `EasyList, Eaylist China, ä¹˜é£è§„åˆ™` ä¸º SR è§„åˆ™ï¼Œå…¨é¢å»é™¤å¹¿å‘Šä¸”å»é™¤é‡å¤ã€‚
- ä¹ŸåŒ…æ‹¬è‡ªå®šä¹‰çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™ï¼Œé’ˆå¯¹ iOS ç«¯çš„ç½‘é¡µå¹¿å‘Šã€App å¹¿å‘Šå’Œè§†é¢‘å¹¿å‘Šã€‚ï¼ˆ[å¸¸è§å¹¿å‘Šè¿‡æ»¤æ•ˆæœç»Ÿè®¡](https://github.com/h2y/Shadowrocket-ADBlock-Rules/issues/40)ï¼‰
- æä¾›å¤šä¸ªè§„åˆ™æ–‡ä»¶è®©å¤§å®¶è‡ªç”±é€‰æ‹©æˆ–è€…è‡ªç”±åˆ‡æ¢ä½¿ç”¨ã€‚
- ä¸“é—¨é’ˆå¯¹ ShadowRocket å¼€å‘ï¼Œå¯ä»¥ä¿è¯ä¸ SR çš„å…¼å®¹æ€§ã€‚


## è§„åˆ™åˆ—è¡¨

![è§„åˆ™é€‰æ‹©æŒ‡å—](https://h2y.github.io/Shadowrocket-ADBlock-Rules/figure/guide.png)

è§„åˆ™ | è§„å®šä»£ç†çš„ç½‘ç«™ | è§„å®šç›´è¿çš„ç½‘ç«™ 
--- | ----------- | ------------- 
[é»‘åå•è§„åˆ™ + å»å¹¿å‘Š](#é»‘åå•è¿‡æ»¤--å¹¿å‘Š) |  è¢«å¢™çš„ç½‘ç«™ï¼ˆGFWListï¼‰ | æ­£å¸¸çš„ç½‘ç«™ 
[é»‘åå•è§„åˆ™](#é»‘åå•è¿‡æ»¤) |   |  
[ç™½åå•è§„åˆ™ + å»å¹¿å‘Š](#ç™½åå•è¿‡æ»¤--å¹¿å‘Š) | å…¶ä»–ç½‘ç«™ | top500"
click,"# $ click_

Click is a Python package for creating beautiful command line interfaces
in a composable way with as little code as necessary. It's the ""Command
Line Interface Creation Kit"". It's highly configurable but comes with
sensible defaults out of the box.

It aims to make the process of writing command line tools quick and fun
while also preventing any frustration caused by the inability to
implement an intended CLI API.

Click in three points:

-   Arbitrary nesting of commands
-   Automatic help page generation
-   Supports lazy loading of subcommands at runtime


## A Simple Example

```python
import click

@click.command()
@click.option(""--count"", default=1, help=""Number of greetings."")
@click.option(""--name"", prompt=""Your name"", help=""The person to greet."")
def hello(count, name):
    """"""Simple program that greets NAME for a total of COUNT times.""""""
    for _ in range(count):
        click.echo(f""Hello, {name}!"")

if __name__ == '__main__':
    hello()
```

```
$ python hello"
gensim,"gensim â€“ Topic Modelling in Python
==================================

<!--
The following image URLs are obfuscated = proxied and cached through
Google because of Github's proxying issues. See:
https://github.com/RaRe-Technologies/gensim/issues/2805
-->

[![Build Status](https://github.com/RaRe-Technologies/gensim/actions/workflows/tests.yml/badge.svg?branch=develop)](https://github.com/RaRe-Technologies/gensim/actions)
[![GitHub release](https://img.shields.io/github/release/rare-technologies/gensim.svg?maxAge=3600)](https://github.com/RaRe-Technologies/gensim/releases)
[![Downloads](https://img.shields.io/pypi/dm/gensim?color=blue)](https://pepy.tech/project/gensim/)
[![DOI](https://zenodo.org/badge/DOI/10.13140/2.1.2393.1847.svg)](https://doi.org/10.13140/2.1.2393.1847)
[![Mailing List](https://img.shields.io/badge/-Mailing%20List-blue.svg)](https://groups.google.com/g/gensim)
[![Follow](https://img.shields.io/twitter/follow/gensim_py.svg?style=social&style=flat&logo=twitter&label=F"
mihomo,"# mihomo
A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.

API url: https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}

## Installation
```
pip install -U git+https://github.com/KT-Yeh/mihomo.git
```

## Usage

### Basic
There are two parsed data formats:
- V1:
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en&version=v1
  - Fetching: use `client.fetch_user_v1(800333171)`
  - Data model: `mihomo.models.v1.StarrailInfoParsedV1`
  - All models defined in `mihomo/models/v1` directory.
- V2: 
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en
  - Fetching: use `client.fetch_user(800333171)`
  - Data model: `mihomo.models.StarrailInfoParsed`
  - All models defined in `mihomo/models` directory.

If you don't want to use `client.get_icon_url` to get the image url everytime, you can use `client.fetch_user(800333171, replace_icon_name_with_url=True)` to get the parsed data with asset urls."
GHunt,"![](assets/long_banner.png)

<br>

#### ğŸŒ GHunt Online version : https://osint.industries

<br>

![Python minimum version](https://img.shields.io/badge/Python-3.10%2B-brightgreen)

# ğŸ˜Š Description

GHunt (v2) is an offensive Google framework, designed to evolve efficiently.\
It's currently focused on OSINT, but any use related with Google is possible.

Features :
- CLI usage and modules
- Python library usage
- Fully async
- JSON export
- Browser extension to ease login

# âœ”ï¸ Requirements
- Python >= 3.10

# âš™ï¸ Installation

```bash
$ pip3 install pipx
$ pipx ensurepath
$ pipx install ghunt
```
It will automatically use venvs to avoid dependency conflicts with other projects.

# ğŸ’ƒ Usage

## Login

First, launch the listener by doing `ghunt login` and choose between 1 of the 2 first methods :
```bash
$ ghunt login

[1] (Companion) Put GHunt on listening mode (currently not compatible with docker)
[2] (Companion) Paste base64-encoded cookies
[3] Enter manually all cookies

Choice =>
```
"
typer,"<p align=""center"">
  <a href=""https://typer.tiangolo.com""><img src=""https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg#only-light"" alt=""Typer""></a>

</p>
<p align=""center"">
    <em>Typer, build great CLIs. Easy to code. Based on Python type hints.</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/typer/actions?query=workflow%3ATest"" target=""_blank"">
    <img src=""https://github.com/fastapi/typer/workflows/Test/badge.svg"" alt=""Test"">
</a>
<a href=""https://github.com/fastapi/typer/actions?query=workflow%3APublish"" target=""_blank"">
    <img src=""https://github.com/fastapi/typer/workflows/Publish/badge.svg"" alt=""Publish"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/typer"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/typer.svg"" alt=""Coverage"">
<a href=""https://pypi.org/project/typer"" target=""_blank"">
    <img src=""https://img.shields.io/pypi/v/typer?color=%2334D058&label=pypi%20package"
aws-cli,"aws-cli
=======

.. image:: https://github.com/aws/aws-cli/actions/workflows/run-tests.yml/badge.svg
   :target: https://github.com/aws/aws-cli/actions/workflows/run-tests.yml
   :alt: Build Status

This package provides a unified command line interface to Amazon Web
Services.

Jump to:

-  `Getting Started <#getting-started>`__
-  `Getting Help <#getting-help>`__
-  `More Resources <#more-resources>`__

Getting Started
---------------

This README is for the AWS CLI version 1. If you are looking for
information about the AWS CLI version 2, please visit the `v2
branch <https://github.com/aws/aws-cli/tree/v2>`__.

Requirements
~~~~~~~~~~~~

The aws-cli package works on Python versions:

-  3.8.x and greater
-  3.9.x and greater
-  3.10.x and greater
-  3.11.x and greater
-  3.12.x and greater

Notices
~~~~~~~

On 2022-05-30, support for Python 3.6 was ended. This follows the
Python Software Foundation `end of support <https://www.python.org/dev/peps/pep-0494/#lifespan>`__
for the runtim"
ranger,"ranger 1.9.3
============

<img src=""https://ranger.github.io/ranger_logo.png"" width=""150"">

[![Build Status](https://travis-ci.org/ranger/ranger.svg?branch=master)](https://travis-ci.org/ranger/ranger)
<a href=""https://repology.org/metapackage/ranger/versions""><img src=""https://repology.org/badge/latest-versions/ranger.svg"" alt=""latest packaged version(s)""></a>
[![Donate via Liberapay](https://img.shields.io/liberapay/patrons/ranger)](https://liberapay.com/ranger)

ranger is a console file manager with VI key bindings.  It provides a
minimalistic and nice curses interface with a view on the directory hierarchy.
It ships with `rifle`, a file launcher that is good at automatically finding
out which program to use for what file type.

![screenshot](https://raw.githubusercontent.com/ranger/ranger-assets/master/screenshots/screenshot.png)

For `mc` aficionados there's also the multi-pane viewmode.

<p>
<img src=""https://raw.githubusercontent.com/ranger/ranger-assets/master/screenshots/twop"
tensor2tensor,"# Tensor2Tensor

[![PyPI
version](https://badge.fury.io/py/tensor2tensor.svg)](https://badge.fury.io/py/tensor2tensor)
[![GitHub
Issues](https://img.shields.io/github/issues/tensorflow/tensor2tensor.svg)](https://github.com/tensorflow/tensor2tensor/issues)
[![Contributions
welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/tensor2tensor/Lobby)
[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)
[![Travis](https://img.shields.io/travis/tensorflow/tensor2tensor.svg)](https://travis-ci.org/tensorflow/tensor2tensor)
[![Run on FH](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run)

[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor), or
[T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library
of deep learning models and datasets designed to m"
SMSBoom,"# SMSBoom - Deprecate

> **Due to judicial reasons, the repository has been suspended!**  
> **å€‰åº«ã¯å¸æ³•ä¸Šã®ç†ç”±ã§åœæ­¢ã•ã‚ŒãŸã€‚**-- Japenese  
> **ç”±äºå¸æ³•åŸå› ï¼Œæ­¤ä»“åº“è¢«åœç”¨ï¼**-- Simplified Chinese  
> **ç”±æ–¼å¸æ³•åŸå› ï¼Œæ­¤å€‰åº«è¢«åœç”¨ï¼** -- Traditional Chinese

For more details, please check: [HK JUDICIARY](https://www.judiciary.hk/zh/home/index.html)

![HK JUDICIARY](https://www.judiciary.hk/images/logo_big.png)

HK JUDICIARY 2024/3/20

<!--
![test](img/test2.gif)

## å…è´£å£°æ˜

1. ä½¿ç”¨æ­¤ç¨‹åºè¯·éµå®ˆå½“åœ°çš„æ³•å¾‹æ³•è§„ï¼Œç¦æ­¢æ»¥ç”¨ã€æ¶æ„ä½¿ç”¨ï¼Œ**è§¦çŠ¯æ³•å¾‹æ‰€é€ æˆçš„é—®é¢˜å‡ç”±ä½¿ç”¨è€…æ‰¿æ‹…**ã€‚  
2. æœ¬ç¨‹åºä»…ä¾›å¨±ä¹,æºç å…¨éƒ¨å¼€æº,**ç¦æ­¢æ»¥ç”¨** å’ŒäºŒæ¬¡ **ç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”**.

## Repair - TODO

1. ä¿®æ”¹æ–‡æ¡£
2. ä¿®ç¼®ä¸»è¦åŠŸèƒ½
3. ä¿®ç¼®åç«¯ä½¿ç”¨ FastAPI å‰ç«¯ä½¿ç”¨ vue3 elementUI
4. GUI ä½¿ç”¨ web æŠ€æœ¯

## Feature

1. é€šè¿‡è‡ªå®šä¹‰ `api.json` çš„æ–¹å¼å®šä¹‰æ¥å£.  
2. æ”¯æŒå…³é”®å­—æ›¿æ¢. **æ—¶é—´æˆ³** `[timestamp]` **æ‰‹æœºå·** `[phone]`  
3. å¤šçº¿ç¨‹/å¼‚æ­¥ è¯·æ±‚.  
4. é€šè¿‡ Flask æä¾›ç½‘é¡µæµ‹è¯•/æ·»åŠ æ¥å£.  
5. å‹å¥½çš„å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ.  
6. é‡‡ç”¨æ–¹ä¾¿çš„ pipenv åŒ…ç®¡ç†.  
7. é€šè¿‡ä»£ç†è°ƒç”¨çŸ­ä¿¡æ¥å£, æ”¯æŒ http, socks4, socks5ä»£ç†.
8. ä½¿ç”¨éšæœºçš„ User-Agent.
9. å¯æŒ‡å®šè½°ç‚¸æ¬¡æ•°, è½°ç‚¸é—´éš”æ—¶é—´.

## Quick Start

### é€‚ç”¨äºå°ç™½

âœ¨æœ¬é¡¹ç›®å·²ç»ä½¿ç”¨ `pyinstaller` æ‰“åŒ…æˆ `EXE` å¯æ‰§è¡Œæ–‡ä»¶!å…å»éƒ¨ç½² Python ç¯å¢ƒçš„çƒ¦æ¼,é€‚åˆç”¨äºå°ç™½ç™½.  

ğŸ”¨ä½œè€…çš„æ‰“åŒ…ç¯å¢ƒä¸º: `Wi"
SuperAGI,"<p align=""center"">
  <a href=""https://superagi.com//#gh-light-mode-only"">
    <img src=""https://superagi.com/wp-content/uploads/2023/05/Logo-dark.svg"" width=""318px"" alt=""SuperAGI logo"" />
  </a>
  <a href=""https://superagi.com//#gh-dark-mode-only"">
    <img src=""https://superagi.com/wp-content/uploads/2023/05/Logo-light.svg"" width=""318px"" alt=""SuperAGI logo"" />
  </a>

</p>

<p align=""center""><i>Open-source framework to build, manage and run useful Autonomous AI Agents</i></p>
    

<p align=""center"">
<a href=""https://superagi.com""> <img src=""https://superagi.com/wp-content/uploads/2023/08/Website.svg""></a>
<a href=""https://app.superagi.com""> <img src=""https://superagi.com/wp-content/uploads/2023/07/Cloud.svg""></a>
<a href=""https://marketplace.superagi.com/""> <img src=""https://superagi.com/wp-content/uploads/2023/08/Marketplace.svg""></a>
<a href=""https://superagi.com/docs/""> <img src=""https://superagi.com/wp-content/uploads/2023/08/Docs.svg""></a>
<a href=""https://documenter.getpostman."
numpy-ml,"# numpy-ml
Ever wish you had an inefficient but somewhat legible collection of machine
learning algorithms implemented exclusively in NumPy? No?

## Installation

### For rapid experimentation
To use this code as a starting point for ML prototyping / experimentation, just clone the repository, create a new [virtualenv](https://pypi.org/project/virtualenv/), and start hacking:

```sh
$ git clone https://github.com/ddbourgin/numpy-ml.git
$ cd numpy-ml && virtualenv npml && source npml/bin/activate
$ pip3 install -r requirements-dev.txt
```

### As a package
If you don't plan to modify the source, you can also install numpy-ml as a
Python package: `pip3 install -u numpy_ml`.

The reinforcement learning agents train on environments defined in the [OpenAI
gym](https://github.com/openai/gym). To install these alongside numpy-ml, you
can use `pip3 install -u 'numpy_ml[rl]'`.

## Documentation
For more details on the available models, see the [project documentation](https://numpy-ml.readthedoc"
CodeFormer,"<p align=""center"">
  <img src=""assets/CodeFormer_logo.png"" height=110>
</p>

## Towards Robust Blind Face Restoration with Codebook Lookup Transformer (NeurIPS 2022)

[Paper](https://arxiv.org/abs/2206.11253) | [Project Page](https://shangchenzhou.com/projects/CodeFormer/) | [Video](https://youtu.be/d3VDpkXlueI)


<a href=""https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""google colab logo""></a> [![Hugging Face](https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/sczhou/CodeFormer) [![Replicate](https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue)](https://replicate.com/sczhou/codeformer) [![OpenXLab](https://img.shields.io/badge/Demo-%F0%9F%90%BC%20OpenXLab-blue)](https://openxlab.org.cn/apps/detail/ShangchenZhou/CodeFormer) ![Visitors](https://api.infinitescript.com/badgen/count?name=sczhou/CodeFormer&ltext=Visitors"
qlib,"[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?"
ChuanhuChatGPT,"<div align=""right"">
  <!-- è¯­è¨€: -->
  ç®€ä½“ä¸­æ–‡ | <a title=""English"" href=""./readme/README_en.md"">English</a> | <a title=""Japanese"" href=""./readme/README_ja.md"">æ—¥æœ¬èª</a> | <a title=""Russian"" href=""./readme/README_ru.md"">Russian</a> | <a title=""Korean"" href=""./readme/README_ko.md"">í•œêµ­ì–´</a>
</div>

<h1 align=""center"">å·è™ Chat ğŸ¯ Chuanhu Chat</h1>
<div align=""center"">
  <a href=""https://github.com/GaiZhenBiao/ChuanhuChatGPT"">
    <img src=""https://github.com/GaiZhenbiao/ChuanhuChatGPT/assets/70903329/aca3a7ec-4f1d-4667-890c-a6f47bf08f63"" alt=""Logo"" height=""156"">
  </a>

<p align=""center"">
    <h3>ä¸ºChatGPTç­‰å¤šç§LLMæä¾›äº†ä¸€ä¸ªè½»å¿«å¥½ç”¨çš„Webå›¾å½¢ç•Œé¢å’Œä¼—å¤šé™„åŠ åŠŸèƒ½</h3>
    <p align=""center"">
      <a href=""https://github.com/GaiZhenbiao/ChuanhuChatGPT/blob/main/LICENSE"">
        <img alt=""Tests Passing"" src=""https://img.shields.io/github/license/GaiZhenbiao/ChuanhuChatGPT"" />
      </a>
      <a href=""https://gradio.app/"">
        <img alt=""GitHub Contributors"" src=""https://img.shields.io/badge/Base-Gradio-fb7d1a?style=flat"" />"
aiohttp,"==================================
Async http client/server framework
==================================

.. image:: https://raw.githubusercontent.com/aio-libs/aiohttp/master/docs/aiohttp-plain.svg
   :height: 64px
   :width: 64px
   :alt: aiohttp logo

|

.. image:: https://github.com/aio-libs/aiohttp/workflows/CI/badge.svg
   :target: https://github.com/aio-libs/aiohttp/actions?query=workflow%3ACI
   :alt: GitHub Actions status for master branch

.. image:: https://codecov.io/gh/aio-libs/aiohttp/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/aio-libs/aiohttp
   :alt: codecov.io status for master branch

.. image:: https://badge.fury.io/py/aiohttp.svg
   :target: https://pypi.org/project/aiohttp
   :alt: Latest PyPI package version

.. image:: https://img.shields.io/pypi/dm/aiohttp
   :target: https://pypistats.org/packages/aiohttp
   :alt: Downloads count

.. image:: https://readthedocs.org/projects/aiohttp/badge/?version=latest
   :target: https://docs.aiohttp.org/
"
Bringing-Old-Photos-Back-to-Life,"# Old Photo Restoration (Official PyTorch Implementation)

<img src='imgs/0001.jpg'/>

### [Project Page](http://raywzy.com/Old_Photo/) | [Paper (CVPR version)](https://arxiv.org/abs/2004.09484) | [Paper (Journal version)](https://arxiv.org/pdf/2009.07047v1.pdf) | [Pretrained Model](https://hkustconnect-my.sharepoint.com/:f:/g/personal/bzhangai_connect_ust_hk/Em0KnYOeSSxFtp4g_dhWdf0BdeT3tY12jIYJ6qvSf300cA?e=nXkJH2) | [Colab Demo](https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing)  | [Replicate Demo & Docker Image](https://replicate.ai/zhangmozhe/bringing-old-photos-back-to-life) :fire:

**Bringing Old Photos Back to Life, CVPR2020 (Oral)**

**Old Photo Restoration via Deep Latent Space Translation, TPAMI 2022**

[Ziyu Wan](http://raywzy.com/)<sup>1</sup>,
[Bo Zhang](https://www.microsoft.com/en-us/research/people/zhanbo/)<sup>2</sup>,
[Dongdong Chen](http://www.dongdongchen.bid/)<sup>3</sup>,
[Pan Zhang](https://panzhang0212.github.io/)<sup>4</sup>,
"
sentence-transformers,"<!--- BADGES: START --->
[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-models-yellow)](https://huggingface.co/models?library=sentence-transformers)
[![GitHub - License](https://img.shields.io/github/license/UKPLab/sentence-transformers?logo=github&style=flat&color=green)][#github-license]
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/sentence-transformers?logo=pypi&style=flat&color=blue)][#pypi-package]
[![PyPI - Package Version](https://img.shields.io/pypi/v/sentence-transformers?logo=pypi&style=flat&color=orange)][#pypi-package]
[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=sentence-transformers)][#docs-package]
<!-- [![PyPI - Downloads](https://img.shields.io/pypi/dm/sentence-transformers?logo=pypi&style=flat&color=green)][#pypi-package] -->

[#github-license]: https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE
[#pypi-package]: https://pypi.org/project/sentence-transformers/
[#"
jupyter,"# Jupyter

*Read this in other languages: [English](README.md), [EspaÃ±ol](README.es-ES.md), [PortuguÃªs](README.pt-BR.md), [FranÃ§ais](README.fr-FR.md)*

Jupyter metapackage for installation and documents

## Documentation structure

This documentation uses the [Sphinx](https://sphinx-doc.org) documentation engine.

The documentation is located in the `docs/source` folder. When you build the documentation, it will be placed in the `docs/build` folder.
It is written in a combination of [reStructuredText](https://docutils.sourceforge.io/rst.html) and [MyST Markdown](https://myst-parser.readthedocs.io/).

## Build the documentation locally

There are a few ways to build the documentation; see below for instructions:

### Build the documentation automatically with `nox`

The easiest way to build the documentation locally is by using the [`nox` command line tool](https://nox.thea.codes/). This tool makes it easy to automate commands in a repository, and we have included a `docs` command to qu"
fabric,"|version| |python| |license| |ci| |coverage|

.. |version| image:: https://img.shields.io/pypi/v/fabric
    :target: https://pypi.org/project/fabric/
    :alt: PyPI - Package Version
.. |python| image:: https://img.shields.io/pypi/pyversions/fabric
    :target: https://pypi.org/project/fabric/
    :alt: PyPI - Python Version
.. |license| image:: https://img.shields.io/pypi/l/fabric
    :target: https://github.com/fabric/fabric/blob/main/LICENSE
    :alt: PyPI - License
.. |ci| image:: https://img.shields.io/circleci/build/github/fabric/fabric/main
    :target: https://app.circleci.com/pipelines/github/fabric/fabric
    :alt: CircleCI
.. |coverage| image:: https://img.shields.io/codecov/c/gh/fabric/fabric
    :target: https://app.codecov.io/gh/fabric/fabric
    :alt: Codecov

Welcome to Fabric!
==================

Fabric is a high level Python (2.7, 3.4+) library designed to execute shell
commands remotely over SSH, yielding useful Python objects in return. It builds
on top of `Invoke <"
pyecharts,"<p align=""center"">
    <img src=""https://user-images.githubusercontent.com/19553554/71825144-2d568180-30d6-11ea-8ee0-63c849cfd934.png"" alt=""pyecharts logo"" width=200 height=200 />
</p>
<h1 align=""center"">pyecharts</h1>
<p align=""center"">
    <em>Python â¤ï¸ ECharts = pyecharts</em>
</p>
<p align=""center"">
    <a href=""https://github.com/pyecharts/pyecharts/actions"">
        <img src=""https://github.com/pyecharts/pyecharts/actions/workflows/python-app.yml/badge.svg"" alt=""Github Actions Status"">
    </a>
    <a href=""https://codecov.io/gh/pyecharts/pyecharts"">
        <img src=""https://codecov.io/gh/pyecharts/pyecharts/branch/master/graph/badge.svg"" alt=""Codecov"">
    </a>
    <a href=""https://badge.fury.io/py/pyecharts"">
        <img src=""https://badge.fury.io/py/pyecharts.svg"" alt=""Package version"">
    </a>
    <a href=""https://pypi.org/project/pyecharts/"">
        <img src=""https://img.shields.io/pypi/pyversions/pyecharts.svg?colorB=brightgreen"" alt=""PyPI - Python Version"">
    </a>
</"
networkx,"NetworkX
========


.. image::
    https://github.com/networkx/networkx/workflows/test/badge.svg?branch=main
    :target: https://github.com/networkx/networkx/actions?query=workflow%3Atest

.. image::
    https://codecov.io/gh/networkx/networkx/branch/main/graph/badge.svg?
    :target: https://app.codecov.io/gh/networkx/networkx/branch/main

.. image::
    https://img.shields.io/pypi/v/networkx.svg?
    :target: https://pypi.python.org/pypi/networkx

.. image::
    https://img.shields.io/pypi/l/networkx.svg?
    :target: https://github.com/networkx/networkx/blob/main/LICENSE.txt

.. image::
    https://img.shields.io/pypi/pyversions/networkx.svg?
    :target: https://pypi.python.org/pypi/networkx

.. image::
    https://img.shields.io/github/labels/networkx/networkx/good%20first%20issue?color=green&label=contribute
    :target: https://github.com/networkx/networkx/contribute


NetworkX is a Python package for the creation, manipulation,
and study of the structure, dynamics, and functio"
dalle-mini,"# DALLÂ·E Mini

<a href=""https://www.craiyon.com/""><img src=""https://www.craiyon.com/thumbnail.png"" width=""300""></a>

## How to use it?

You can use the model on [ğŸ–ï¸ craiyon](https://www.craiyon.com/)

## How does it work?

Refer to our reports:

* [DALLÂ·E mini - Generate Images from Any Text Prompt](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy)
* [DALLÂ·E mini - Explained](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA)
* [DALLÂ·E mega - Training Journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2)

## Development

### Dependencies Installation

For inference only, use `pip install dalle-mini`.

For development, clone the repo and use `pip install -e "".[dev]""`.
Before making a PR, check style with `make style`.

You can experiment with the pipeline step by step through our [`inference pipeline notebook`](tools/inference/i"
discord.py,"discord.py
==========

.. image:: https://discord.com/api/guilds/336642139381301249/embed.png
   :target: https://discord.gg/r3sSKJJ
   :alt: Discord server invite
.. image:: https://img.shields.io/pypi/v/discord.py.svg
   :target: https://pypi.python.org/pypi/discord.py
   :alt: PyPI version info
.. image:: https://img.shields.io/pypi/pyversions/discord.py.svg
   :target: https://pypi.python.org/pypi/discord.py
   :alt: PyPI supported Python versions

A modern, easy to use, feature-rich, and async ready API wrapper for Discord written in Python.

Key Features
-------------

- Modern Pythonic API using ``async`` and ``await``.
- Proper rate limit handling.
- Optimised in both speed and memory.

Installing
----------

**Python 3.8 or higher is required**

To install the library without full voice support, you can just run the following command:

.. code:: sh

    # Linux/macOS
    python3 -m pip install -U discord.py

    # Windows
    py -3 -m pip install -U discord.py

Otherwise to ge"
python-mini-projects,"<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
[![forthebadge](https://forthebadge.com/images/badges/built-by-developers.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/built-with-swag.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://forthebadge.com)

# Python-Mini-Projects

[![All Contributors](https://img.shields.io/github/contributors/Python-World/python-mini-projects)](#contributors-)
![Issues](https://img.shields.io/github/issues/Python-World/python-mini-projects)
![Pull Requests](https://img.shields.io/github/issues-pr/Python-World/python-mini-projects?)
![Forks](https://img.shields.io/github/forks/Python-World/python-mini-projects)
![Stars](https://img.shields.io/github/stars/Python-World/python-mini-projects)
![License](https://img.sh"
evals,"# OpenAI Evals

Evals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.

If you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of [OpenAI's President Greg Brockman](https://twitter.com/gdb/status/1733553161884127435):

<img width=""596"" alt=""https://x.com/gdb/status/1733553161884127435?s=20"" src=""https://github.com/openai/evals/assets/35577566/ce7840ff-43a8-4d88-bb2f-6b207410333b"">

## Setup

To run evals, you will need to set up and specify yo"
DocsGPT,"<h1 align=""center"">
  DocsGPT  ğŸ¦–
</h1>

<p align=""center"">
  <strong>Open-Source Documentation Assistant</strong>
</p>

<p align=""left"">
  <strong><a href=""https://www.docsgpt.cloud/"">DocsGPT</a></strong> is a cutting-edge open-source solution that streamlines the process of finding information in the project documentation. With its integration of the powerful <strong>GPT</strong> models, developers can easily ask questions about a project and receive accurate answers.
  
Say goodbye to time-consuming manual searches, and let <strong><a href=""https://www.docsgpt.cloud/"">DocsGPT</a></strong> help you quickly find the information you need. Try it out and see how it revolutionizes your project documentation experience. Contribute to its development and be a part of the future of AI-powered assistance.
</p>

<div align=""center"">
  
  <a href=""https://github.com/arc53/DocsGPT"">![link to main GitHub showing Stars number](https://img.shields.io/github/stars/arc53/docsgpt?style=social)</a>
  <"
Scrapegraph-ai,"
# ğŸ•·ï¸ ScrapeGraphAI: You Only Scrape Once
[English](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/README.md) | [ä¸­æ–‡](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/chinese.md) | [æ—¥æœ¬èª](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/japanese.md)
| [í•œêµ­ì–´](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/korean.md)
| [Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://github.com/VinciGit00/Scrapegraph-ai/blob/main/docs/russian.md)


[![Downloads](https://img.shields.io/pepy/dt/scrapegraphai?style=for-the-badge)](https://pepy.tech/project/scrapegraphai)
[![linting: pylint](https://img.shields.io/badge/linting-pylint-yellowgreen?style=for-the-badge)](https://github.com/pylint-dev/pylint)
[![Pylint](https://img.shields.io/github/actions/workflow/status/VinciGit00/Scrapegraph-ai/pylint.yml?label=Pylint&logo=github&style=for-the-badge)](https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/pylint.yml)
[![CodeQL](https://img.shields.io/github/actions/workflow/status/VinciGit00"
fauxpilot,"
# FauxPilot

This is an attempt to build a locally hosted alternative to [GitHub Copilot](https://copilot.github.com/). It uses the [SalesForce CodeGen](https://github.com/salesforce/CodeGen) models inside of NVIDIA's [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) with the [FasterTransformer backend](https://github.com/triton-inference-server/fastertransformer_backend/).

<p align=""right"">
  <img width=""50%"" align=""right"" src=""./img/fauxpilot.png"">
</p>

## Prerequisites

You'll need:

* Docker
* `docker compose` >= 1.28
* An NVIDIA GPU with Compute Capability >= 6.0 and enough VRAM to run the model you want.
* [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker)
* `curl` and `zstd` for downloading and unpacking the models.

Note that the VRAM requirements listed by `setup.sh` are *total* -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you *should* be able to run the 6B model by "
mackup,"# Mackupâ„¢

Keep your application settings in sync.

## Table of contents

- [Mackup](#mackup)
  - [Table of contents](#table-of-contents)
  - [WARNING](#warning)
  - [Quickstart](#quickstart)
  - [Usage](#usage)
  - [What does it do](#what-does-it-do)
  - [Bullsh\*t, what does it really do to my files](#bullsht-what-does-it-really-do-to-my-files)
    - [Backup](#backup)
    - [Restore](#restore)
    - [Uninstall](#uninstall)
  - [Supported Storages](#supported-storages)
  - [Unsupported Storages](#unsupported-storages)
  - [Supported Applications](#supported-applications)
  - [Can you support application X](#can-you-support-application-x)
  - [Personalization \& configuration](#personalization--configuration)
  - [Why did you do this](#why-did-you-do-this)
  - [What platforms are supported](#what-platforms-are-supported)
  - [What's up with the weird name](#whats-up-with-the-weird-name)
  - [Where can I find more information](#where-can-i-find-more-information)

## WARNING

âš ï¸ Mackup d"
DeDRM_tools,"# No Longer Maintained
I have not had the time to devote to this project in recent years that I would have liked. I am delighted to find that someone else has taken on the task of keeping the tools updated, and making releases. I shall be using noDRM's version of the tools from now on. 

[Guide] How to remove DRM
Refer to [Wiki Page](https://github.com/apprenticeharper/DeDRM_tools/wiki/Exactly-how-to-remove-DRM)

# DeDRM_tools
DeDRM tools for ebooks

This is a repository that tracks all the scripts and other tools for removing DRM from ebooks that I could find, committed in date order as best as I could manage. (Except for the Requiem tools for Apple's iBooks, and Convert LIT for Microsoft's .lit ebooks.) This includes the tools from a time before Apprentice Alf had a blog, and continues through to when Apprentice Harper (with help) took over maintenance of the tools.

The individual scripts are now released as two plugins for calibre: DeDRM and Obok. 
The DeDRM plugin handles books th"
py12306,"# ğŸš‚ py12306 è´­ç¥¨åŠ©æ‰‹
åˆ†å¸ƒå¼ï¼Œå¤šè´¦å·ï¼Œå¤šä»»åŠ¡è´­ç¥¨

## Features
- [x] å¤šæ—¥æœŸæŸ¥è¯¢ä½™ç¥¨
- [x] è‡ªåŠ¨æ‰“ç ä¸‹å•
- [x] ç”¨æˆ·çŠ¶æ€æ¢å¤
- [x] ç”µè¯è¯­éŸ³é€šçŸ¥
- [x] å¤šè´¦å·ã€å¤šä»»åŠ¡ã€å¤šçº¿ç¨‹æ”¯æŒ
- [x] å•ä¸ªä»»åŠ¡å¤šç«™ç‚¹æŸ¥è¯¢ 
- [x] åˆ†å¸ƒå¼è¿è¡Œ
- [x] Docker æ”¯æŒ
- [x] åŠ¨æ€ä¿®æ”¹é…ç½®æ–‡ä»¶
- [x] é‚®ä»¶é€šçŸ¥
- [x] Web ç®¡ç†é¡µé¢
- [x] å¾®ä¿¡æ¶ˆæ¯é€šçŸ¥
- [ ] ä»£ç†æ± æ”¯æŒ ([pyproxy-async](https://github.com/pjialin/pyproxy-async))

## ä½¿ç”¨
py12306 éœ€è¦è¿è¡Œåœ¨ python 3.6 ä»¥ä¸Šç‰ˆæœ¬ï¼ˆå…¶å®ƒç‰ˆæœ¬æš‚æœªæµ‹è¯•)

**1. å®‰è£…ä¾èµ–**
```bash
git clone https://github.com/pjialin/py12306

pip install -r requirements.txt
```

**2. é…ç½®ç¨‹åº**
```bash
cp env.py.example env.py
```
è‡ªåŠ¨æ‰“ç 

ï¼ˆè‹¥å¿«å·²åœæ­¢æœåŠ¡ï¼Œç›®å‰åªèƒ½è®¾ç½®**free**æ‰“ç æ¨¡å¼ï¼‰
free å·²å¯¹æ¥åˆ°æ‰“ç å…±äº«å¹³å°ï¼Œ[https://py12306-helper.pjialin.com](https://py12306-helper.pjialin.com/)ï¼Œæ¬¢è¿å‚ä¸åˆ†äº«

è¯­éŸ³é€šçŸ¥

è¯­éŸ³éªŒè¯ç ä½¿ç”¨çš„æ˜¯é˜¿é‡Œäº‘ API å¸‚åœºä¸Šçš„ä¸€ä¸ªæœåŠ¡å•†ï¼Œéœ€è¦åˆ° [https://market.aliyun.com/products/56928004/cmapi026600.html](https://market.aliyun.com/products/56928004/cmapi026600.html) è´­ä¹°åå°† appcode å¡«å†™åˆ°é…ç½®ä¸­

**3. å¯åŠ¨å‰æµ‹è¯•**

ç›®å‰æä¾›äº†ä¸€äº›ç®€å•çš„æµ‹è¯•ï¼ŒåŒ…æ‹¬ç”¨æˆ·è´¦å·æ£€æµ‹ï¼Œä¹˜å®¢ä¿¡æ¯æ£€æµ‹ï¼Œè½¦ç«™æ£€æµ‹ç­‰

å¼€å§‹æµ‹è¯• -t 
```bash
python main.py -t
```

æµ‹è¯•é€šçŸ¥æ¶ˆæ¯ (è¯­éŸ³, é‚®ä»¶) -t -n
```bash
# é»˜è®¤ä¸ä¼šè¿›è¡Œé€šçŸ¥æµ‹è¯•ï¼Œè¦å¯¹é€šçŸ¥è¿›è¡Œæµ‹è¯•éœ€è¦åŠ ä¸Š -n å‚æ•° 
python main.py -t -n
```

**4. è¿è¡Œç¨‹åº**
```ba"
imgaug,"# imgaug

This python library helps you with augmenting images for your machine learning projects.
It converts a set of input images into a new, much larger set of slightly altered images.

[![Build Status](https://travis-ci.org/aleju/imgaug.svg?branch=master)](https://travis-ci.org/aleju/imgaug)
[![codecov](https://codecov.io/gh/aleju/imgaug/branch/master/graph/badge.svg)](https://codecov.io/gh/aleju/imgaug)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/1370ce38e99e40af842d47a8dd721444)](https://www.codacy.com/app/aleju/imgaug?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=aleju/imgaug&amp;utm_campaign=Badge_Grade)

<table>

<tr>
<th>&nbsp;</th>
<th>Image</th>
<th>Heatmaps</th>
<th>Seg. Maps</th>
<th>Keypoints</th>
<th>Bounding Boxes,<br>Polygons</th>
</tr>

<!-- Line 1: Original Input -->
<tr>
<td><em>Original Input</em></td>
<td><img src=""https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_image.jpg?raw=true"" heigh"
powerline,"Powerline
=========

**Powerline is a statusline plugin for vim, and provides statuslines and 
prompts for several other applications, including zsh, bash, fish, tmux,
IPython, Awesome, i3 and Qtile.**

+---------+---------------------------------------------------+
| Author  | Kim SilkebÃ¦kken (kim.silkebaekken+vim@gmail.com)  |
+---------+---------------------------------------------------+
| Source  | https://github.com/powerline/powerline            |
+---------+---------------------------------------------------+
| Version | beta                                              |
+---------+---------------------------------------------------+

**Powerline does not support python2 anymore and powerline will stop working with python2 in the near future.**

Features
--------

* **Extensible and feature rich, written in Python.** Powerline was 
  completely rewritten in Python to get rid of as much vimscript as 
  possible. This has allowed much better extensibility, leaner and better 
  c"
the-gan-zoo,"# The GAN Zoo

<p align=""center""><img width=""40%"" src=""The_GAN_Zoo.jpg"" /></p>

Every week, new GAN papers are coming out and it's hard to keep track of them all, not to mention the incredibly creative ways in which researchers are naming these GANs! So, here's a list of what started as a fun activity compiling all named GANs!

<p align=""center""><img width=""50%"" src=""cumulative_gans.jpg"" /></p>

You can also check out the same data in a tabular format with functionality to filter by year or do a quick search by title [here](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv).

Contributions are welcome. Add links through pull requests in gans.tsv file in the same format or create an issue to lemme know something I missed or to start a discussion.

Check out [Deep Hunt](https://deephunt.in) - my weekly AI newsletter for this repo as [blogpost](https://medium.com/deep-hunt/the-gan-zoo-79597dc8c347) and follow me on [Twitter](https://www.twitter.com/hindupuravinash).

* 3"
YYeTsBot,"# YYeTsBot

[![build docker image](https://github.com/tgbot-collection/YYeTsBot/actions/workflows/docker.yaml/badge.svg)](https://github.com/tgbot-collection/YYeTsBot/actions/workflows/docker.yaml)
[![Docker Pulls](https://img.shields.io/docker/pulls/bennythink/yyetsbot)](https://hub.docker.com/r/bennythink/yyetsbot)

![](assets/index.png)

ğŸ‘‰ å‰ç«¯[åœ¨è¿™é‡Œ](https://github.com/tgbot-collection/YYeTsFE) ğŸ‘ˆ

# ä½¿ç”¨è¯´æ˜

ç›´æ¥å‘é€æƒ³è¦çœ‹çš„å‰§é›†åç§°å°±å¯ä»¥äº†ï¼Œå¯é€‰åˆ†äº«ç½‘é¡µæˆ–è€…é“¾æ¥ï¼ˆed2kå’Œç£åŠ›é“¾æ¥ï¼‰ã€‚


æœç´¢èµ„æºæ—¶ï¼Œä¼šæŒ‰ç…§æˆ‘é¢„å®šçš„ä¼˜å…ˆçº§ï¼ˆäººäººå½±è§†ç¦»çº¿ã€å­—å¹•ä¾ ï¼‰è¿›è¡Œæœç´¢ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨å‘½ä»¤å¼ºåˆ¶æŸä¸ªå­—å¹•ç»„ï¼Œå¦‚ `/yyets_offline é€ƒé¿å¯è€»`

## å‘½ä»¤

```
start - å¼€å§‹ä½¿ç”¨
help - å¸®åŠ©
credits - è‡´è°¢
ping - è¿è¡ŒçŠ¶æ€
settings - è·å–å…¬å‘Š
zimuxia_online - å­—å¹•ä¾ åœ¨çº¿æ•°æ®  
newzmz_online - newå­—å¹•ç»„åœ¨çº¿æ•°æ® 
yyets_offline - äººäººå½±è§†ç¦»çº¿æ•°æ®
```

# æˆªå›¾

## å¸¸è§„æœç´¢

![](assets/1.png)

## èµ„æºåˆ†äº«ç«™æˆªå›¾

æœ¬ç½‘ç«™æ°¸ä¹…å…è´¹ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•é™åˆ¶ã€‚
![](assets/new_resource.png)

![](assets/2.png)

æ”¯æŒæ”¶è—åŠŸèƒ½ï¼Œä¼šè·¨è®¾å¤‡åŒæ­¥
![](assets/like.png)

## æŒ‡å®šå­—å¹•ç»„æœç´¢

ç›®å‰åªæ”¯æŒYYeTsOfflineã€ZimuxiaOnlineå’ŒNewzmzOnline

![](assets/3.png)

# å¦‚ä½•ä¸‹è½½ç£åŠ›å’Œç”µé©´èµ„æºï¼Ÿè¿…é›·æç¤ºèµ„æºæ•æ„Ÿ

## ç”µé©´èµ„æº

"
horovod,".. raw:: html

    <p align=""center""><img src=""https://user-images.githubusercontent.com/16640218/34506318-84d0c06c-efe0-11e7-8831-0425772ed8f2.png"" alt=""Logo"" width=""200""/></p>
    <br/>

Horovod
=======

.. raw:: html

   <div align=""center"">

.. image:: https://badge.fury.io/py/horovod.svg
   :target: https://badge.fury.io/py/horovod
   :alt: PyPI Version

.. image:: https://badge.buildkite.com/6f976bc161c69d9960fc00de01b69deb6199b25680a09e5e26.svg?branch=master
   :target: https://buildkite.com/horovod/horovod
   :alt: Build Status

.. image:: https://readthedocs.org/projects/horovod/badge/?version=latest
   :target: https://horovod.readthedocs.io/en/latest/
   :alt: Documentation Status

.. image:: https://img.shields.io/badge/slack-chat-green.svg?logo=slack
   :target: https://forms.gle/cPGvty5hp31tGfg79
   :alt: Slack

.. raw:: html

   </div>

.. raw:: html

   <div align=""center"">

.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg
   :target: https://img.sh"
gpt-researcher,"<div align=""center"">
<!--<h1 style=""display: flex; align-items: center; gap: 10px;"">
  <img src=""https://github.com/assafelovic/gpt-researcher/assets/13554167/a45bac7c-092c-42e5-8eb6-69acbf20dde5"" alt=""Logo"" width=""25"">
  GPT Researcher
</h1>-->
<img src=""https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3"" alt=""Logo"" width=""80"">


####
[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)
[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)
[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted&?compact=true)](https://discord.gg/QgZXvJAccX)
<!--[![Discord Follow](https://img.shields.io/discord/1127851779011391548?style=for-the-badge&logo=discord&label=Chat%20on%20Discord)](https://discord.gg/"
sqlmodel,"<p align=""center"">
  <a href=""https://sqlmodel.tiangolo.com""><img src=""https://sqlmodel.tiangolo.com/img/logo-margin/logo-margin-vector.svg#only-light"" alt=""SQLModel""></a>

</p>
<p align=""center"">
    <em>SQLModel, SQL databases in Python, designed for simplicity, compatibility, and robustness.</em>
</p>
<p align=""center"">
<a href=""https://github.com/fastapi/sqlmodel/actions?query=workflow%3ATest"" target=""_blank"">
    <img src=""https://github.com/fastapi/sqlmodel/workflows/Test/badge.svg"" alt=""Test"">
</a>
<a href=""https://github.com/fastapi/sqlmodel/actions?query=workflow%3APublish"" target=""_blank"">
    <img src=""https://github.com/fastapi/sqlmodel/workflows/Publish/badge.svg"" alt=""Publish"">
</a>
<a href=""https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/sqlmodel"" target=""_blank"">
    <img src=""https://coverage-badge.samuelcolvin.workers.dev/fastapi/sqlmodel.svg"" alt=""Coverage"">
<a href=""https://pypi.org/project/sqlmodel"" target=""_blank"">
    <img src=""https://img.shield"
flux,"# FLUX
by Black Forest Labs: https://blackforestlabs.ai

![grid](assets/grid.jpg)

This repo contains minimal inference code to run text-to-image and image-to-image with our Flux latent rectified flow transformers.

### Inference partners

We are happy to partner with [Replicate](https://replicate.com/), [FAL](https://fal.ai/) and [Mystic](https://www.mystic.ai). You can sample our models using their services.
Below we list relevant links.

Replicate:

- https://replicate.com/collections/flux
- https://replicate.com/collections/flux-fine-tunes
- https://replicate.com/black-forest-labs/flux-pro
- https://replicate.com/black-forest-labs/flux-dev
- https://replicate.com/black-forest-labs/flux-schnell

FAL:

- https://fal.ai/models/fal-ai/flux-pro
- https://fal.ai/models/fal-ai/flux/dev
- https://fal.ai/models/fal-ai/flux/schnell

Mystic:

- https://www.mystic.ai/black-forest-labs
- https://www.mystic.ai/black-forest-labs/flux1-pro
- https://www.mystic.ai/black-forest-labs/flux1-dev
- http"
salt,".. image:: https://img.shields.io/github/license/saltstack/salt
   :alt: Salt Project License: Apache v2.0
   :target: https://github.com/saltstack/salt/blob/master/LICENSE

.. image:: https://img.shields.io/pypi/dm/salt?label=pypi%20downloads
   :alt: PyPi Package Downloads
   :target: https://pypi.org/project/salt

.. image:: https://img.shields.io/lgtm/grade/python/github/saltstack/salt
   :alt: PyPi Package Downloads
   :target: https://lgtm.com/projects/g/saltstack/salt/context:python

.. image:: https://img.shields.io/badge/slack-SaltProject-blue.svg?logo=slack
   :alt: Salt Project Slack Community
   :target: https://via.vmw.com/salt-slack

.. image:: https://img.shields.io/twitch/status/saltprojectoss
   :alt: Salt Project Twitch Channel
   :target: https://www.twitch.tv/saltprojectoss

.. image:: https://img.shields.io/reddit/subreddit-subscribers/saltstack?style=social
   :alt: Salt Project subreddit
   :target: https://www.reddit.com/r/saltstack/

.. image:: https://img.shie"
stylegan,"## StyleGAN &mdash; Official TensorFlow Implementation
![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg?style=plastic)
![TensorFlow 1.10](https://img.shields.io/badge/tensorflow-1.10-green.svg?style=plastic)
![cuDNN 7.3.1](https://img.shields.io/badge/cudnn-7.3.1-green.svg?style=plastic)
![License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-green.svg?style=plastic)

![Teaser image](./stylegan-teaser.png)
**Picture:** *These people are not real &ndash; they were produced by our generator that allows control over different aspects of the image.*

This repository contains the official TensorFlow implementation of the following paper:

> **A Style-Based Generator Architecture for Generative Adversarial Networks**<br>
> Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)<br>
> https://arxiv.org/abs/1812.04948
>
> **Abstract:** *We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literatur"
backtrader,"backtrader
==========

.. image:: https://img.shields.io/pypi/v/backtrader.svg
   :alt: PyPi Version
   :scale: 100%
   :target: https://pypi.python.org/pypi/backtrader/

..  .. image:: https://img.shields.io/pypi/dm/backtrader.svg
       :alt: PyPi Monthly Donwloads
       :scale: 100%
       :target: https://pypi.python.org/pypi/backtrader/

.. image:: https://img.shields.io/pypi/l/backtrader.svg
   :alt: License
   :scale: 100%
   :target: https://github.com/backtrader/backtrader/blob/master/LICENSE
.. image:: https://travis-ci.org/backtrader/backtrader.png?branch=master
   :alt: Travis-ci Build Status
   :scale: 100%
   :target: https://travis-ci.org/backtrader/backtrader
.. image:: https://img.shields.io/pypi/pyversions/backtrader.svg
   :alt: Python versions
   :scale: 100%
   :target: https://pypi.python.org/pypi/backtrader/

**Yahoo API Note**:

  [2018-11-16] After some testing it would seem that data downloads can be
  again relied upon over the web interface (or API ``v7``)
"
newspaper,"Newspaper3k: Article scraping & curation
========================================

.. image:: https://badge.fury.io/py/newspaper3k.svg
    :target: http://badge.fury.io/py/newspaper3k.svg
        :alt: Latest version

.. image:: https://travis-ci.org/codelucas/newspaper.svg
        :target: http://travis-ci.org/codelucas/newspaper/
        :alt: Build status

.. image:: https://coveralls.io/repos/github/codelucas/newspaper/badge.svg?branch=master
        :target: https://coveralls.io/github/codelucas/newspaper
        :alt: Coverage status


Inspired by `requests`_ for its simplicity and powered by `lxml`_ for its speed:

    ""Newspaper is an amazing python library for extracting & curating articles.""
    -- `tweeted by`_ Kenneth Reitz, Author of `requests`_

    ""Newspaper delivers Instapaper style article extraction."" -- `The Changelog`_

.. _`tweeted by`: https://twitter.com/kennethreitz/status/419520678862548992
.. _`The Changelog`: http://thechangelog.com/newspaper-delivers-instap"
albumentations,"# Albumentations

[![PyPI version](https://badge.fury.io/py/albumentations.svg)](https://badge.fury.io/py/albumentations)
![CI](https://github.com/albumentations-team/albumentations/workflows/CI/badge.svg)
[![PyPI Downloads](https://img.shields.io/pypi/dm/albumentations.svg?label=PyPI%20downloads)](
https://pypi.org/project/albumentations/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/albumentations.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/albumentations)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/albumentations)
[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)

[Docs](https://albumentations.ai/docs/) | [Discord](https://discord.gg/AKPrrDYNAt) | [Twitter](https://twitter.com/albumentations) | [LinkedIn](https://www.linkedin.com/company/100504475/)

Albumentations is a Python library for image "
ChatterBot,"![ChatterBot: Machine learning in Python](https://i.imgur.com/b3SCmGT.png)

# ChatterBot

ChatterBot is a machine-learning based conversational dialog engine build in
Python which makes it possible to generate responses based on collections of
known conversations. The language independent design of ChatterBot allows it
to be trained to speak any language.

[![Package Version](https://img.shields.io/pypi/v/chatterbot.svg)](https://pypi.python.org/pypi/chatterbot/)
[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)
[![Django 2.0](https://img.shields.io/badge/Django-2.0-blue.svg)](https://docs.djangoproject.com/en/2.1/releases/2.0/)
[![Requirements Status](https://requires.io/github/gunthercox/ChatterBot/requirements.svg?branch=master)](https://requires.io/github/gunthercox/ChatterBot/requirements/?branch=master)
[![Build Status](https://travis-ci.org/gunthercox/ChatterBot.svg?branch=master)](https://travis-ci.org/gunthe"
ivy,"<div style=""display: block;"" align=""center"">
    <a href=""https://ivy.dev/"">
        <img class=""dark-light"" width=""50%"" src=""https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/ivy-long.svg""/>
    </a>
</div>

------------------------------------------------------------------------

<table align=""center"">
  <tr>
    <td align=""center"">
      <a href=""https://ivy.dev/"">
          <img class=""dark-light"" width=""75"" src=""https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/website.svg"" alt=""Website"">
      </a>
      <br>
      <a href=""https://ivy.dev/"" style=""text-decoration: none;"">Website</a>
    </td>
    <td align=""center"">
      <a href=""https://docs.ivy.dev/"">
          <img class=""dark-light"" width=""70"" src=""https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/docs.svg"" alt=""Docs"">
      </a>
      <br>
      <a href=""https://docs.ivy.dev/"" style=""text-decoration: none;"">Docs</a>
    </td>
  "
nni,"<div align=""center"">
<img src=""docs/img/nni_logo.png"" width=""600""/>
</div>

<br/>

[![MIT licensed](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)
[![Issues](https://img.shields.io/github/issues-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen)
[![Bugs](https://img.shields.io/github/issues/Microsoft/nni/bug.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen+label%3Abug)
[![Pull Requests](https://img.shields.io/github/issues-pr-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/pulls?q=is%3Apr+is%3Aopen)
[![Version](https://img.shields.io/github/release/Microsoft/nni.svg)](https://github.com/Microsoft/nni/releases)
[![Documentation Status](https://readthedocs.org/projects/nni/badge/?version=stable)](https://nni.readthedocs.io/en/stable/?badge=stable)
[![](https://img.shields.io/github/contributors-anon/microsoft/nni)](https://github.com/microsoft/nni/graphs/contributors)



[<img src=""docs/img/readme_bann"
wxpy,"wxpy: ç”¨ Python ç©å¾®ä¿¡
==============================

.. image:: https://badge.fury.io/py/wxpy.svg
    :target: https://badge.fury.io/py/wxpy

.. image:: https://img.shields.io/pypi/pyversions/wxpy.svg
        :target: https://github.com/youfou/wxpy

.. image:: https://readthedocs.org/projects/wxpy/badge/?version=latest
    :target: http://wxpy.readthedocs.io/zh/latest/?badge=latest

å¾®ä¿¡æœºå™¨äºº / å¯èƒ½æ˜¯æœ€ä¼˜é›…çš„å¾®ä¿¡ä¸ªäººå· API
    wxpy åœ¨ itchat çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¤§é‡æ¥å£ä¼˜åŒ–æå‡äº†æ¨¡å—çš„æ˜“ç”¨æ€§ï¼Œå¹¶è¿›è¡Œä¸°å¯Œçš„åŠŸèƒ½æ‰©å±•


..  attention::

    | **å¼ºçƒˆå»ºè®®ä»…ä½¿ç”¨å°å·è¿è¡Œæœºå™¨äººï¼**

    | ä»è¿‘æœŸ (17å¹´6æœˆä¸‹æ—¬) åé¦ˆæ¥çœ‹ï¼Œä½¿ç”¨æœºå™¨äººå­˜åœ¨ä¸€å®šæ¦‚ç‡è¢«é™åˆ¶ç™»å½•çš„å¯èƒ½æ€§ã€‚
    | ä¸»è¦è¡¨ç°ä¸ºæ— æ³•ç™»é™† Web å¾®ä¿¡ (ä½†ä¸å½±å“æ‰‹æœºç­‰å…¶ä»–å¹³å°)ã€‚



ç”¨æ¥å¹²å•¥
----------------

ä¸€äº›å¸¸è§çš„åœºæ™¯

* æ§åˆ¶è·¯ç”±å™¨ã€æ™ºèƒ½å®¶å±…ç­‰å…·æœ‰å¼€æ”¾æ¥å£çš„ç©æ„å„¿
* è¿è¡Œè„šæœ¬æ—¶è‡ªåŠ¨æŠŠæ—¥å¿—å‘é€åˆ°ä½ çš„å¾®ä¿¡
* åŠ ç¾¤ä¸»ä¸ºå¥½å‹ï¼Œè‡ªåŠ¨æ‹‰è¿›ç¾¤ä¸­
* è·¨å·æˆ–è·¨ç¾¤è½¬å‘æ¶ˆæ¯
* è‡ªåŠ¨é™ªäººèŠå¤©
* é€—äººç©
* ...

æ€»è€Œè¨€ä¹‹ï¼Œå¯ç”¨æ¥å®ç°å„ç§å¾®ä¿¡ä¸ªäººå·çš„è‡ªåŠ¨åŒ–æ“ä½œ


..
    ä½“éªŒä¸€ä¸‹
    ----------------

    **è¿™æœ‰ä¸€ä¸ªç°æˆçš„å¾®ä¿¡æœºå™¨äººï¼Œæƒ³ä¸æƒ³è°ƒæˆä¸€ä¸‹ï¼Ÿ**

    è®°å¾—å¡«å†™å…¥ç¾¤å£ä»¤ ğŸ‘‰ [ **wxpy** ]ï¼Œä¸ç¾¤é‡Œçš„å¤§ç¥ä»¬è°ˆç¬‘é£ç”Ÿ ğŸ˜

    ..  image:: https://github.com/youfou/wxpy/raw/master/docs/wechat-group.png


è½»æ¾å®‰è£…
----------------

wxpy æ”¯æŒ Python 3.4-3.6ï¼Œä»¥åŠ 2.7 ç‰ˆæœ¬

å°†ä¸‹æ–¹å‘½ä»¤ä¸­"
awx,"[![CI](https://github.com/ansible/awx/actions/workflows/ci.yml/badge.svg?branch=devel)](https://github.com/ansible/awx/actions/workflows/ci.yml) [![codecov](https://codecov.io/github/ansible/awx/graph/badge.svg?token=4L4GSP9IAR)](https://codecov.io/github/ansible/awx) [![Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-yellow.svg)](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html) [![Apache v2 License](https://img.shields.io/badge/license-Apache%202.0-brightgreen.svg)](https://github.com/ansible/awx/blob/devel/LICENSE.md) [![AWX on the Ansible Forum](https://img.shields.io/badge/mailing%20list-AWX-orange.svg)](https://forum.ansible.com/tag/awx)
[![Ansible Matrix](https://img.shields.io/badge/matrix-Ansible%20Community-blueviolet.svg?logo=matrix)](https://chat.ansible.im/#/welcome) [![Ansible Discourse](https://img.shields.io/badge/discourse-Ansible%20Community-yellowgreen.svg?logo=discourse)](https://forum.ansible.com)

<img src=""https://r"
mailinabox,"Mail-in-a-Box
=============

By [@JoshData](https://github.com/JoshData) and [contributors](https://github.com/mail-in-a-box/mailinabox/graphs/contributors).

Mail-in-a-Box helps individuals take back control of their email by defining a one-click, easy-to-deploy SMTP+everything else server: a mail server in a box.

**Please see [https://mailinabox.email](https://mailinabox.email) for the project's website and setup guide!**

* * *

Our goals are to:

* Make deploying a good mail server easy.
* Promote [decentralization](http://redecentralize.org/), innovation, and privacy on the web.
* Have automated, auditable, and [idempotent](https://web.archive.org/web/20190518072631/https://sharknet.us/2014/02/01/automated-configuration-management-challenges-with-idempotency/) configuration.
* **Not** make a totally unhackable, NSA-proof server.
* **Not** make something customizable by power users.

Additionally, this project has a [Code of Conduct](CODE_OF_CONDUCT.md), which supersedes the goals"
flair,"![alt text](resources/docs/flair_logo_2020_FINAL_day_dpi72.png#gh-light-mode-only)
![alt text](resources/docs/flair_logo_2020_FINAL_night_dpi72.png#gh-dark-mode-only)

[![PyPI version](https://badge.fury.io/py/flair.svg)](https://badge.fury.io/py/flair)
[![GitHub Issues](https://img.shields.io/github/issues/flairNLP/flair.svg)](https://github.com/flairNLP/flair/issues)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)

A very simple framework for **state-of-the-art NLP**. Developed by [Humboldt University of Berlin](https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en/) and friends.

---

Flair is:

* **A powerful NLP library.** Flair allows you to apply our state-of-the-art natural language processing (NLP)
models to your text, such as named entity recognition (NER), sentiment analysis, part-of-speec"
wechat_jump_game,"# æ•™ä½ ç”¨ Python æ¥ç©å¾®ä¿¡è·³ä¸€è·³
[![GitHub stars](https://img.shields.io/github/stars/wangshub/wechat_jump_game.svg)](https://github.com/wangshub/wechat_jump_game/stargazers) [![GitHub forks](https://img.shields.io/github/forks/wangshub/wechat_jump_game.svg)](https://github.com/wangshub/wechat_jump_game/network) [![GitHub license](https://img.shields.io/github/license/wangshub/wechat_jump_game.svg)](https://github.com/wangshub/wechat_jump_game/blob/master/LICENSE)

[![Throughput Graph](https://graphs.waffle.io/wangshub/wechat_jump_game/throughput.svg)](https://waffle.io/wangshub/wechat_jump_game/metrics/throughput) 

## æ¸¸æˆæ¨¡å¼

> 2017 å¹´ 12 æœˆ 28 æ—¥ä¸‹åˆï¼Œå¾®ä¿¡å‘å¸ƒäº† 6.6.1 ç‰ˆæœ¬ï¼ŒåŠ å…¥äº†ã€Œå°æ¸¸æˆã€åŠŸèƒ½ï¼Œå¹¶æä¾›äº†å®˜æ–¹ DEMOã€Œè·³ä¸€è·³ã€ã€‚è¿™æ˜¯ä¸€ä¸ª 2.5D æ’ç”»é£æ ¼çš„ç›Šæ™ºæ¸¸æˆï¼Œç©å®¶å¯ä»¥é€šè¿‡æŒ‰å‹å±å¹•æ—¶é—´çš„é•¿çŸ­æ¥æ§åˆ¶è¿™ä¸ªã€Œå°äººã€è·³è·ƒçš„è·ç¦»ã€‚åˆ†æ•°è¶Šé«˜ï¼Œé‚£ä¹ˆåœ¨å¥½å‹æ’è¡Œæ¦œæ›´åŠ é å‰ã€‚é€šè¿‡ Python è„šæœ¬è‡ªåŠ¨è¿è¡Œï¼Œè®©ä½ è½»æ¾éœ¸æ¦œã€‚

![](./resource/image/jump.gif)

å¯èƒ½åˆšå¼€å§‹ä¸Šæ‰‹çš„æ—¶å€™ï¼Œå› ä¸ºæ—¶é—´è·ç¦»ä¹‹é—´çš„å…³ç³»æŠŠæ¡ä¸æ°å½“ï¼Œåªèƒ½è·³å‡ºå‡ ä¸ªå°±æ‰åˆ°äº†å°å­ä¸‹é¢ã€‚**å¦‚æœèƒ½åˆ©ç”¨å›¾åƒè¯†åˆ«ç²¾ç¡®æµ‹é‡å‡ºèµ·å§‹å’Œç›®æ ‡ç‚¹ä¹‹é—´æµ‹è·ç¦»ï¼Œå°±å¯ä»¥ä¼°è®¡æŒ‰å‹çš„æ—¶é—´æ¥ç²¾ç¡®è·³è·ƒã€‚**

## åŸç†è¯´æ˜

##### ç”±äºå¾®ä¿¡æ£€æµ‹éå¸¸ä¸¥å‰ï¼Œè¿™é‡Œçš„é˜²ç¦ä»£ç å¯èƒ½å·²ç»ä¸èµ·ä½œç”¨ï¼Œä¸»è¦ä¾›å­¦ä¹ ç”¨é€”

1. å°†æ‰‹æœºç‚¹å‡»åˆ°ã€Šè·³ä¸€è·³ã€‹å°ç¨‹åºç•Œé¢

2. ç”¨ ADB å·¥å…·è·å–å½“å‰æ‰‹æœºæˆªå›¾ï¼Œå¹¶ç”¨ AD"
examples-of-web-crawlers,"# <p align=""center"">ä¸€äº›éå¸¸æœ‰è¶£çš„pythonçˆ¬è™«ä¾‹å­,å¯¹æ–°æ‰‹æ¯”è¾ƒå‹å¥½</p>


<p align=""center"">
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers""><img src=""https://img.shields.io/badge/status-updating-brightgreen.svg""></a>
    <a href=""https://github.com/python/cpython""><img src=""https://img.shields.io/badge/Python-3.7-FF1493.svg""></a>
    <a href=""https://opensource.org/licenses/mit-license.php""><img src=""https://badges.frapsoft.com/os/mit/mit.svg""></a>
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers/graphs/contributors""><img src=""https://img.shields.io/github/contributors/shengqiangzhang/examples-of-web-crawlers?color=blue""></a>
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers/stargazers""><img src=""https://img.shields.io/github/stars/shengqiangzhang/examples-of-web-crawlers.svg?logo=github""></a>
    <a href=""https://github.com/shengqiangzhang/examples-of-web-crawlers/network/members""><img src=""https://img.shields.io/github/forks/shengqi"
gaussian-splatting,"# 3D Gaussian Splatting for Real-Time Radiance Field Rendering
Bernhard Kerbl*, Georgios Kopanas*, Thomas LeimkÃ¼hler, George Drettakis (* indicates equal contribution)<br>
| [Webpage](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) | [Full Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf) | [Video](https://youtu.be/T_kXY43VZnk) | [Other GRAPHDECO Publications](http://www-sop.inria.fr/reves/publis/gdindex.php) | [FUNGRAPH project page](https://fungraph.inria.fr) |<br>
| [T&T+DB COLMAP (650MB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/datasets/input/tandt_db.zip) | [Pre-trained Models (14 GB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/datasets/pretrained/models.zip) | [Viewers for Windows (60MB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/binaries/viewers.zip) | [Evaluation Images (7 GB)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/evaluation/images.zip) |<br>
![Teaser image"
yapf,"# YAPF

<p align=""center"">
<a href=""https://badge.fury.io/py/yapf""><img alt=""PyPI Version"" src=""https://badge.fury.io/py/yapf.svg""></a>
<a href=""https://github.com/google/yapf/actions/workflows/ci.yml""><img alt=""Build Status"" src=""https://github.com/google/yapf/actions/workflows/ci.yml/badge.svg""></a>
<a href=""https://github.com/google/yapf/actions/workflows/pre-commit.yml""><img alt=""Actions Status"" src=""https://github.com/google/yapf/actions/workflows/pre-commit.yml/badge.svg""></a>
<a href=""https://coveralls.io/github/google/yapf?branch=main""><img alt=""Coverage Status"" src=""https://coveralls.io/repos/github/google/yapf/badge.svg?branch=main""></a>
</p>


## Introduction

YAPF is a Python formatter based on [`clang-format`](https://clang.llvm.org/docs/ClangFormat.html)
(developed by Daniel Jasper). In essence, the algorithm takes the code and
calculates the best formatting that conforms to the configured style. It takes
away a lot of the drudgery of maintaining your code.

The ultimate "
facenet,"# Face Recognition using Tensorflow [![Build Status][travis-image]][travis]

[travis-image]: http://travis-ci.org/davidsandberg/facenet.svg?branch=master
[travis]: http://travis-ci.org/davidsandberg/facenet

This is a TensorFlow implementation of the face recognizer described in the paper
[""FaceNet: A Unified Embedding for Face Recognition and Clustering""](http://arxiv.org/abs/1503.03832). The project also uses ideas from the paper [""Deep Face Recognition""](http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf) from the [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/) at Oxford.

## Compatibility
The code is tested using Tensorflow r1.7 under Ubuntu 14.04 with Python 2.7 and Python 3.5. The test cases can be found [here](https://github.com/davidsandberg/facenet/tree/master/test) and the results can be found [here](http://travis-ci.org/davidsandberg/facenet).

## News
| Date     | Update |
|----------|--------|
| 2018-04-10 | Added new models trained on Casia-"
OCRmyPDF,"<!-- SPDX-FileCopyrightText: 2014 Julien Pfefferkorn -->
<!-- SPDX-FileCopyrightText: 2015 James R. Barlow -->
<!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->

<img src=""docs/images/logo.svg"" width=""240"" alt=""OCRmyPDF"">

[![Build Status](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg)](https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml) [![PyPI version][pypi]](https://pypi.org/project/ocrmypdf/) ![Homebrew version][homebrew] ![ReadTheDocs][docs] ![Python versions][pyversions]

[pypi]: https://img.shields.io/pypi/v/ocrmypdf.svg ""PyPI version""
[homebrew]: https://img.shields.io/homebrew/v/ocrmypdf.svg ""Homebrew version""
[docs]: https://readthedocs.org/projects/ocrmypdf/badge/?version=latest ""RTD""
[pyversions]: https://img.shields.io/pypi/pyversions/ocrmypdf ""Supported Python versions""

OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.

```bash
ocrmypdf                      # it's a scriptable comman"
requests-html,"Requests-HTML: HTML Parsing for Humansâ„¢
=======================================

.. image:: https://farm5.staticflickr.com/4695/39152770914_a3ab8af40d_k_d.jpg

.. image:: https://travis-ci.com/psf/requests-html.svg?branch=master
    :target: https://travis-ci.com/psf/requests-html

This library intends to make parsing HTML (e.g. scraping the web) as
simple and intuitive as possible.

When using this library you automatically get:

- **Full JavaScript support**! (Using Chromium, thanks to pyppeteer)
- *CSS Selectors* (a.k.a jQuery-style, thanks to PyQuery).
- *XPath Selectors*, for the faint of heart.
- Mocked user-agent (like a real web browser).
- Automatic following of redirects.
- Connectionâ€“pooling and cookie persistence.
- The Requests experience you know and love, with magical parsing abilities.
- **Async Support**

.. Other nice features include:

    - Markdown export of pages and elements.


Tutorial & Usage
================

Make a GET request to 'python.org', using Requests:"
Llama-Chinese,"<p align=""left"">
    <a href=""README_EN.md"">English</a> ï½œ ä¸­æ–‡
</p>

<h1 align=""center"">
  Llamaä¸­æ–‡ç¤¾åŒº
</h1>
<p align=""center"" width=""100%"">
  <img src=""assets/llama.jpg"" alt=""Llama"" style=""width: 20%; display: block; margin: auto;""></a>
</p>
<p align=""center"">
  <font face=""é»‘ä½“"" color=orange size=""6""> Llama3ä½“éªŒå’Œå¾®è°ƒå·²å¼€æ”¾ï¼Œæœ€å¥½çš„ä¸­æ–‡Llamaå¤§æ¨¡å‹ </font>
</p>

<p align=""center"">
ğŸ¤— <a href=""https://huggingface.co/FlagAlpha"" target=""_blank"">Hugging Face</a> â€¢ ğŸ¤– <a href=""https://www.modelscope.cn/organization/FlagAlpha/"" target=""_blank"">ModelScope</a> â€¢ âœ¡ï¸ <a href=""https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat"" target=""_blank"">WiseModel</a>
</p> 

<p align=""center"">
  <a href=""https://llama.family"">Llama3.1 åœ¨çº¿ä½“éªŒï¼ˆåŒ…å«Llama2ï¼‰ï¼šhttps://llama.family</a>
</p>
<p align=""center"">
  <a href=""https://huggingface.co/FlagAlpha/Atom-7B-Chat"">åŸºäºLlamaçš„å¼€æºä¸­æ–‡é¢„è®­ç»ƒå¤§æ¨¡å‹Atom</a>
</p>

</br></br>


## ğŸ—‚ï¸ ç›®å½•
- [ğŸ“Œ Llamaä¸­æ–‡ç¤¾åŒº](#-llamaä¸­æ–‡ç¤¾åŒº)
  * [ğŸ”¥ ç¤¾åŒºä»‹ç»ï¼šLlamaä¸­æ–‡ç¤¾åŒº](#-ç¤¾åŒºä»‹ç»llamaä¸­æ–‡ç¤¾åŒº)
  * [ğŸ“¢ æœ€æ–°åŠ¨æ€](#-æœ€æ–°åŠ¨æ€)
  * [ğŸ¤— æ¨¡å‹](#-æ¨¡å‹)
    + [ğŸ¤— ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹At"
Swin-Transformer,"# Swin Transformer

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/object-detection-on-coco)](https://paperswithcode.com/sota/object-detection-on-coco?p=swin-transformer-v2-scaling-up-capacity-and)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/instance-segmentation-on-coco)](https://paperswithcode.com/sota/instance-segmentation-on-coco?p=swin-transformer-v2-scaling-up-capacity-and)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/semantic-segmentation-on-ade20k)](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=swin-transformer-v2-scaling-up-capacity-and)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/swin-transformer-v2-scaling-up-capacity-and/action-classification-on-kinetics-400)](https://paperswithcode."
dvc,"ğŸš€ Check out our new product `DataChain <https://github.com/iterative/datachain>`_ (and give it a â­!) if you need to version and process a large number of files. Contact us at support@iterative.ai to discuss commercial solutions and support for AI reproducibility and data management scenarios.

--------------------------

`Website <https://dvc.org>`_
â€¢ `Docs <https://dvc.org/doc>`_
â€¢ `Blog <http://blog.dataversioncontrol.com>`_
â€¢ `Tutorial <https://dvc.org/doc/get-started>`_
â€¢ `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
â€¢ `How DVC works`_
â€¢ `VS Code Extension`_
â€¢ `Installation`_
â€¢ `Contributing`_
â€¢ `Community and Support`_

|CI| |Python Version| |Coverage| |VS Code| |DOI|

|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|

|

**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:

#. **Version** your data and models.
   Store them in your cloud storag"
flash-attention,"# FlashAttention
This repository provides the official implementation of FlashAttention and
FlashAttention-2 from the
following papers.

**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**  
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©  
Paper: https://arxiv.org/abs/2205.14135  
IEEE Spectrum [article](https://spectrum.ieee.org/mlperf-rankings-2022) about our submission to the MLPerf 2.0 benchmark using FlashAttention.
![FlashAttention](assets/flashattn_banner.jpg)

**FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning**  
Tri Dao

Paper: https://tridao.me/publications/flash2/flash2.pdf

![FlashAttention-2](assets/flashattention_logo.png)


## Usage

We've been very happy to see FlashAttention being widely adopted in such a short
time after its release. This [page](https://github.com/Dao-AILab/flash-attention/blob/main/usage.md)
contains a partial list of places where FlashAttention is being used.

FlashAttent"
Qwen,"<p align=""left"">
    <a href=""README_CN.md"">ä¸­æ–‡</a>&nbsp ï½œ &nbspEnglish&nbsp ï½œ &nbsp<a href=""README_JA.md"">æ—¥æœ¬èª</a> ï½œ &nbsp<a href=""README_FR.md"">FranÃ§ais</a> ï½œ &nbsp<a href=""README_ES.md"">EspaÃ±ol</a>
</p>
<br><br>

<p align=""center"">
    <img src=""https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg"" width=""400""/>
<p>
<br>

<p align=""center"">
        ğŸ¤— <a href=""https://huggingface.co/Qwen"">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href=""https://modelscope.cn/organization/qwen"">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href=""https://arxiv.org/abs/2309.16609"">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href=""https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary"">Demo</a>
<br>
<a href=""assets/wechat.png"">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href=""https://discord.gg/CV4E9rpNSD"">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href=""https://dashscope.aliyun.com"">API</a> 
</p>
<br><br>

> [!Important]
> Qwen2 is here! You are welcome to follow [QwenLM/Qwen2](https://github.com/QwenLM/"
speedtest-cli,"speedtest-cli
=============

Command line interface for testing internet bandwidth using
speedtest.net

.. image:: https://img.shields.io/pypi/v/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: Latest Version
.. image:: https://img.shields.io/travis/sivel/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: Travis
.. image:: https://img.shields.io/pypi/l/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: License

Versions
--------

speedtest-cli works with Python 2.4-3.7

.. image:: https://img.shields.io/pypi/pyversions/speedtest-cli.svg
        :target: https://pypi.python.org/pypi/speedtest-cli/
        :alt: Versions

Installation
------------

pip / easy\_install
~~~~~~~~~~~~~~~~~~~

::

    pip install speedtest-cli

or

::

    easy_install speedtest-cli

Github
~~~~~~

::

    pip install git+https://github.com/sivel/speedtest-cli.git

or

::

    git cl"
zhao,"= ä¿ºæ•´ç†çš„ã€Šå¤ªå­å…šå…³ç³»ç½‘ç»œã€‹ =

== ç®€ä»‹ ==

æ­¤é¡¹ç›®åˆ›å»ºäº2016å¹´2æœˆï¼Œä¸“é—¨ç”¨æ¥æ­éœ²å¤©æœçš„æƒè´µï¼ˆä¹Ÿå°±æ˜¯ä¼ è¯´ä¸­çš„â€œèµµå®¶äººâ€ï¼‰ã€‚

ä¿ºæŠŠè¿™å‡ å¹´æ”¶é›†æ•´ç†çš„æ•°æ®å¼€æºåˆ° GitHubï¼Œä¾¿äºå¤šäººåä½œâ€”â€”å¤§ä¼™å„¿ç¾¤ç­–ç¾¤åŠ›ï¼Œä¸€èµ·æ¥æ›å…‰æƒè´µå®¶æ—ã€‚

åˆæ¬¡ä¸Šä¼ çš„æ•°æ®åŒ…æ‹¬ï¼š700å¤šä¸ªæ•°æ®æ–‡ä»¶ï¼ˆ '''å¯¹åº”700å¤šäººï¼Œ130å¤šä¸ªå®¶æ—''' ï¼‰ï¼Œå¦æœ‰200å¤šå¼ å›¾ç‰‡ï¼ˆäººç‰©å¤´åƒï¼‰ã€‚éšç€ä¿ºä¸æ–­å®Œå–„ï¼Œæ•°æ®ä¼šè¶Šæ¥è¶Šå¤šã€‚

å¯¹è¿™ä¸ªé¡¹ç›®ï¼Œä¿ºä¼šã€æŒç»­æ›´æ–°ã€‘ã€‚æ¯”å¦‚æœå»·æ¯æ¬¡æ¢å±Šçš„æ—¶å€™ï¼Œä¿ºéƒ½ä¼šè¡¥å……æ–°çš„ç´ æã€‚

ä¸ºäº†ç¡®ä¿æ•°æ®çš„å¯ä¿¡åº¦ï¼Œä¿ºä¸»è¦å‚è€ƒâ€œç»´åŸºç™¾ç§‘â€ä»¥åŠä¸€äº›å›½é™…æƒå¨åª’ä½“çš„æŠ¥é“ï¼ˆæ¯”å¦‚ã€Šçº½çº¦æ—¶æŠ¥ã€‹ã€ã€Šåå°”è¡—æ—¥ç‰ˆã€‹ã€ã€Šé‡‘èæ—¶æŠ¥ã€‹ç­‰ç­‰ï¼‰ã€‚

å¦å¤–ï¼Œå¯¹äºæŸäº›å®¢è§‚äº‹å®ï¼ˆæ¯”å¦‚ï¼šç”Ÿå’å¹´æœˆã€ç®€å†ã€äº²æˆšå…³ç³»ï¼‰ï¼Œä¿ºä¹Ÿå‚è€ƒäº†å¤©æœæ”¿åºœçš„å®˜æ–¹ç½‘ç«™ï¼Œä»¥åŠå¢™å†…çš„â€œç™¾åº¦ç™¾ç§‘â€ã€‚


== ä¸‹è½½è¯´æ˜ ==

GitHub æä¾›äº†â€œä¸‹è½½æ•´ä¸ªé¡¹ç›®â€çš„åŠŸèƒ½ï¼Œä½†æ˜¯ä¼šæ¯”è¾ƒå¤§ã€‚

å¦‚æœä½ ä»…ä»…æƒ³çœ‹ã€Šå¤ªå­å…šå…³ç³»ç½‘ç»œã€‹è¿™ä»½æ–‡æ¡£ï¼Œåªéœ€åœ¨é¦–é¡µä¸Šæ–¹ç‚¹å‡»è¿›å…¥ '''download''' è¿™ä¸ªç›®å½•ã€‚

è¯¥ç›®å½•ä¸‹æœ‰ '''pdf''' å’Œ '''jpg''' ä¸¤ä¸ªå­ç›®å½•ï¼Œåˆ†åˆ«å­˜æ”¾å¯¹åº”çš„ '''ã€æ–‡ä»¶ç±»å‹ã€‘''' ã€‚ä½ æƒ³è¦çœ‹å“ªä¸€ç§æ–‡ä»¶æ ¼å¼ï¼Œå°±è¿›å…¥å“ªä¸ªå­ç›®å½•é‡Œé¢ã€‚

è¿›å…¥ã€æ–‡ä»¶ç±»å‹ã€‘çš„å­ç›®å½•ä¹‹åï¼Œä¼šçœ‹åˆ°ä¸€ä¸ªæ–‡ä»¶åˆ—è¡¨ï¼ˆç›®å‰æœ‰13ä¸ªæ–‡ä»¶ï¼‰ã€‚å…ˆç‚¹å‡»ä½ æƒ³è¦çš„æŸä¸ªæ–‡ä»¶ï¼Œä¼šè¿›å…¥è¯¥æ–‡ä»¶çš„é¡µé¢ã€‚

ç„¶ååœ¨ã€å³ä¸Šæ–¹ã€‘ä½ ä¼šçœ‹åˆ°ä¸€ä¸ª '''Raw æŒ‰é’®''' ï¼Œåœ¨è¿™ä¸ªæŒ‰é’®ä¸Šç‚¹ã€å³é”®ã€‘ï¼Œåœ¨ã€å³é”®èœå•ã€‘é‡Œé¢é€‰â€œä¿å­˜â€æˆ–â€œå¦å­˜ä¸ºâ€ï¼Œå°±å¯ä»¥æŠŠè¿™ä¸ªæ–‡ä»¶ä¸‹è½½åˆ°ä½ æœ¬æœºã€‚


== å¤šäººåä½œè¯´æ˜ ==

ä¿ºéå¸¸å¸Œæœ›æœ‰æ›´å¤šçš„ç½‘å‹å‚ä¸è¯¥é¡¹ç›®ï¼Œå¤§ä¼™å„¿ä¸€èµ·æ¥å®Œå–„å¤©æœæƒè´µå®¶æ—çš„èµ„æ–™ã€‚

æƒ³è¦å‚ä¸çš„åŒå­¦ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼ï¼š

* åˆ°[https://program-think.blogspot.com/ ä¿ºåšå®¢]ç•™è¨€è¿›è¡Œåé¦ˆï¼Œè¡¥å……ä¿¡æ¯æˆ–åé¦ˆé”™è¯¯ã€‚

* åœ¨[https://github.com/programthink/zhao/issues æœ¬é¡¹ç›®å‘ä¸€ä¸ª issue]ï¼Œè¡¥å……ä¿¡æ¯æˆ–åé¦ˆé”™è¯¯ã€‚

* Fork è¯¥é¡¹ç›®ï¼Œè¿›è¡Œä¿®æ”¹ï¼Œç„¶åå‘ä¿ºå‘ä¸€ä¸ª Pull Request

ï¼ˆåé¢ä¸¤ç§æ–¹å¼ï¼Œä½ éœ€è¦æœ‰ GitHub çš„å¸å·ï¼‰


== æ•°æ®æ ¼å¼è¯´æ˜ ==

æœ¬é¡¹ç›®çš„æ•°æ®æ–‡ä»¶ï¼Œå…¨éƒ¨é‡‡ç”¨[https://zh.wikiped"
reinforcement-learning-an-introduction,"# Reinforcement Learning: An Introduction

[![Build Status](https://travis-ci.org/ShangtongZhang/reinforcement-learning-an-introduction.svg?branch=master)](https://travis-ci.org/ShangtongZhang/reinforcement-learning-an-introduction)

Python replication for Sutton & Barto's book [*Reinforcement Learning: An Introduction (2nd Edition)*](http://incompleteideas.net/book/the-book-2nd.html)

> If you have any confusion about the code or want to report a bug, please open an issue instead of emailing me directly, and unfortunately I do not have exercise answers for the book.

# Contents 

### Chapter 1
1. Tic-Tac-Toe

### Chapter 2
1. [Figure 2.1: An exemplary bandit problem from the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_1.png)
2. [Figure 2.2: Average performance of epsilon-greedy action-value methods on the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-intr"
nltk,"# Natural Language Toolkit (NLTK)
[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk)
![CI](https://github.com/nltk/nltk/actions/workflows/ci.yaml/badge.svg?branch=develop)

NLTK -- the Natural Language Toolkit -- is a suite of open source Python
modules, data sets, and tutorials supporting research and development in Natural
Language Processing. NLTK requires Python version 3.8, 3.9, 3.10, 3.11 or 3.12.

For documentation, please visit [nltk.org](https://www.nltk.org/).


## Contributing

Do you want to contribute to NLTK development? Great!
Please read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.

See also [how to contribute to NLTK](https://www.nltk.org/contribute.html).


## Donate

Have you found the toolkit helpful?  Please support NLTK development by donating
to the project via PayPal, using the link on the NLTK homepage.


## Citing

If you publish work that uses NLTK, please cite the NLTK book, as follows:

    Bird, Steven, Edward Lope"
detr,"**DEâ«¶TR**: End-to-End Object Detection with Transformers
========

[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)

PyTorch training code and pretrained models for **DETR** (**DE**tection **TR**ansformer).
We replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining **42 AP** on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.

![DETR](.github/DETR.png)

**What it is**. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. 
Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image conte"
dgl,"<p align=""center"">
  <img src=""http://data.dgl.ai/asset/logo.jpg"" height=""200"">
</p>

[![Latest Release](https://img.shields.io/github/v/release/dmlc/dgl)](https://github.com/dmlc/dgl/releases)
[![Conda Latest Release](https://anaconda.org/dglteam/dgl/badges/version.svg)](https://anaconda.org/dglteam/dgl)
[![Build Status](https://ci.dgl.ai/buildStatus/icon?job=DGL/master)](https://ci.dgl.ai/job/DGL/job/master/)
[![Benchmark by ASV](http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat)](https://asv.dgl.ai/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)
[![Twitter](https://img.shields.io/twitter/follow/DGLGraph?style=social)](https://twitter.com/GraphDeep)

[Website](https://www.dgl.ai) | [A Blitz Introduction to DGL](https://docs.dgl.ai/tutorials/blitz/index.html) | Documentation ([Latest](https://docs.dgl.ai/en/latest/) | [Stable](https://docs.dgl.ai)) | [Official Examples](examples/README.md) | [Discussion Forum](https://discuss.d"
searx,".. SPDX-License-Identifier: AGPL-3.0-or-later

Searx is no longer maintained. Thank you for your support and all your contributions.

.. figure:: https://raw.githubusercontent.com/searx/searx/master/searx/static/themes/oscar/img/logo_searx_a.png
   :target: https://searx.github.io/searx/
   :alt: searX
   :width: 100%
   :align: center

-------

|searx install|
|searx homepage|
|searx wiki|
|AGPL License|
|Issues|
|commits|
|OpenCollective searx backers|
|OpenCollective searx sponsors|

Privacy-respecting, hackable `metasearch engine`_ / *pronunciation* **sÉœËks**.

.. _metasearch engine: https://en.wikipedia.org/wiki/Metasearch_engine

.. |searx install| image:: https://img.shields.io/badge/-install-blue
   :target: https://searx.github.io/searx/admin/installation.html

.. |searx homepage| image:: https://img.shields.io/badge/-homepage-blue
   :target: https://searx.github.io/searx

.. |searx wiki| image:: https://img.shields.io/badge/-wiki-blue
   :target: https://github.com/searx/sea"
PySimpleGUI,"<p align=""center"">
    <img src=""https://pysimplegui.net/images/big_news_emoji2.png"">
    <br>
    For more information visit <a href=""https://home.PySimpleGUI.com"">PySimpleGUI.com</a>
</p>


##

<p align=""center"">
    <img height=""250"" src=""https://pysimplegui.net/images/logos/Logo_Full_Transparent_Cropped.png"">
    <h2 align=""center"">User Interfaces for Humans<sup>TM</sup></h2>
</p>

# Welcome to PySimpleGUI 5 !!

Do you use PySimpleGUI 4? [Here is what you need to know.](https://docs.pysimplegui.com/en/latest/readme/sunset/)

**PySimpleGUI creates desktop applications easily**, enhancing the tkinter, Qt, WxPython, and Remi frameworks with a much simpler programming interface:

1. PySimpleGUI user interfaces are defined using core Python data types (lists and dictionaries) that are easily understood by beginners.
2. PySimpleGUI event handling changes from a complex callback-based model to a simple message passing one.
3. PySimpleGUI uses simple Python code and has no requirement for "
DB-GPT,"# DB-GPT: Revolutionizing Database Interactions with Private LLM Technology
 
<p align=""left"">
  <img src=""./assets/LOGO.png"" width=""100%"" />
</p>

<div align=""center"">
  <p>
    <a href=""https://github.com/eosphoros-ai/DB-GPT"">
        <img alt=""stars"" src=""https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social"" />
    </a>
    <a href=""https://github.com/eosphoros-ai/DB-GPT"">
        <img alt=""forks"" src=""https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social"" />
    </a>
    <a href=""https://opensource.org/licenses/MIT"">
      <img alt=""License: MIT"" src=""https://img.shields.io/badge/License-MIT-yellow.svg"" />
    </a>
     <a href=""https://github.com/eosphoros-ai/DB-GPT/releases"">
      <img alt=""Release Notes"" src=""https://img.shields.io/github/release/eosphoros-ai/DB-GPT"" />
    </a>
    <a href=""https://github.com/eosphoros-ai/DB-GPT/issues"">
      <img alt=""Open Issues"" src=""https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT"" />
    </a>
  "
ChatGLM3,"# ChatGLM3

<p align=""center"">
ğŸ“„<a href=""https://arxiv.org/pdf/2406.12793"" target=""_blank""> Report </a> â€¢ ğŸ¤— <a href=""https://huggingface.co/THUDM/chatglm3-6b"" target=""_blank"">HF Repo</a> â€¢ ğŸ¤– <a href=""https://modelscope.cn/models/ZhipuAI/chatglm3-6b"" target=""_blank"">ModelScope</a> â€¢ ğŸŸ£ <a href=""https://www.wisemodel.cn/models/ZhipuAI/chatglm3-6b"" target=""_blank"">WiseModel</a> â€¢ ğŸ“” <a href=""https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof"" target=""_blank"">Document</a> â€¢  ğŸ§° <a href=""https://openxlab.org.cn/models/hot/THUDM"" target=""_blank"">OpenXLab</a> â€¢ ğŸ¦ <a href=""https://twitter.com/thukeg"" target=""_blank"">Twitter</a><br>
</p>
<p align=""center"">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=""https://discord.gg/fK2dz4bg"" target=""_blank"">Discord</a> å’Œ <a href=""resources/WECHAT.md"" target=""_blank"">å¾®ä¿¡</a>
</p>
<p align=""center"">
ğŸ“åœ¨ <a href=""https://www.chatglm.cn"">chatglm.cn</a> ä½“éªŒæ›´å¤§è§„æ¨¡çš„ ChatGLM æ¨¡å‹ã€‚
</p>

[Read this in English.](./README_en.md)

ğŸ“” å…³äº`ChatGLM3-6B` æ›´ä¸ºè¯¦ç»†çš„ä½¿ç”¨ä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒ

+ [ChatGLM3 å¼€æ”¾æŠ€æœ¯æ–‡æ¡£](https:/"
SWE-agent,"<p align=""center"">
  <a href=""https://www.swe-agent.com/"">
    <img src=""assets/swe-agent-banner.png"" alt=""swe-agent.com"" />
  </a>
</p>

<p align=""center"">
  <a href=""#enigma""><img src=""https://github.com/user-attachments/assets/70ba3fdf-ee7b-474f-8fdc-de4ffde51eef"" height=""35px""></a>
</p>

<p align=""center"">
  <a href=""https://princeton-nlp.github.io/SWE-agent/""><strong>Documentation</strong></a>&nbsp; | &nbsp;
  <a href=""https://discord.gg/AVEFbBn2rH""><strong>Discord</strong></a>&nbsp; | &nbsp;
  <a href=""https://arxiv.org/abs/2405.15793""><strong>Preprint</strong></a>&nbsp; | &nbsp;
  <a href=""https://arxiv.org/abs/2409.16165""><strong>EnIGMA preprint</strong></a>
</p>

**SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can resolve issues in real GitHub repositories and more.**

On [SWE-bench][], SWE-agent resolves 12.47% of issues of the full test set and 23% of issues of SWE-bench lite.
[SWE-agent EnIGMA][enigma] solves more than **3x more** challenges of the "
impacket,"Impacket
========

[![Latest Version](https://img.shields.io/pypi/v/impacket.svg)](https://pypi.python.org/pypi/impacket/)
[![Build and test Impacket](https://github.com/fortra/impacket/actions/workflows/build_and_test.yml/badge.svg)](https://github.com/fortra/impacket/actions/workflows/build_and_test.yml)

Copyright Fortra, LLC and its affiliated companies. All rights reserved.

Impacket was originally created by [SecureAuth](https://www.secureauth.com/labs/open-source-tools/impacket), and now maintained by Fortra's Core Security.

Impacket is a collection of Python classes for working with network
protocols. Impacket is focused on providing low-level
programmatic access to the packets and for some protocols (e.g.
SMB1-3 and MSRPC) the protocol implementation itself.
Packets can be constructed from scratch, as well as parsed from 
raw data, and the object-oriented API makes it simple to work with 
deep hierarchies of protocols. The library provides a set of tools
as examples of what c"
transferlearning,"[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

<h1 align=""center"">
  <br>
  <img src=""png/logo.jpg"" alt=""Transfer Leanring"" width=""500"">
</h1>

<h4 align=""center"">Everything about Transfer Learning. è¿ç§»å­¦ä¹ .</h4>

<p align=""center"">
  <strong><a href=""#0papers-è®ºæ–‡"">Papers</a></strong> â€¢
  <strong><a href=""#1introduction-and-tutorials-ç®€ä»‹ä¸æ•™ç¨‹"">Tutorials</a></strong> â€¢
  <a href=""#2transfer-learning-areas-and-papers-ç ”ç©¶é¢†åŸŸä¸ç›¸å…³è®ºæ–‡"">Research areas</a> â€¢
  <a href=""#3theory-and-survey-ç†è®ºä¸ç»¼è¿°"">Theory</a> â€¢
  <a href=""#3theory-and-survey-ç†è®ºä¸ç»¼è¿°"">Survey</a> â€¢
  <strong><a href=""https://github.com/jindongwang/transferlearning/tree/master/code"">Code</a></strong> â€¢
  <strong><a href=""#7datasets-and-benchmarks-æ•°æ®é›†ä¸è¯„æµ‹ç»“æœ"">Dataset & benchmark</a></strong>
</p>
<p align=""center"">
  <a href=""#6transfer-learning-thesis-ç¡•åšå£«è®ºæ–‡"">Thesis</a> â€¢
  <a href=""#5transfer-learning-scholars-è‘—åå­¦è€…"">Schola"
labelme,"<h1 align=""center"">
  <img src=""labelme/icons/icon.png""><br/>labelme
</h1>

<h4 align=""center"">
  Image Polygonal Annotation with Python
</h4>

<div align=""center"">
  <a href=""https://pypi.python.org/pypi/labelme""><img src=""https://img.shields.io/pypi/v/labelme.svg""></a>
  <a href=""https://pypi.org/project/labelme""><img src=""https://img.shields.io/pypi/pyversions/labelme.svg""></a>
  <a href=""https://github.com/labelmeai/labelme/actions""><img src=""https://github.com/labelmeai/labelme/workflows/ci/badge.svg?branch=main&event=push""></a>
</div>

<div align=""center"">
  <a href=""#starter-guide""><b>Starter Guide</b></a>
  | <a href=""#installation""><b>Installation</b></a>
  | <a href=""#usage""><b>Usage</b></a>
  | <a href=""#examples""><b>Examples</b></a>
  <!-- | <a href=""https://github.com/labelmeai/labelme/discussions""><b>Community</b></a> -->
  <!-- | <a href=""https://www.youtube.com/playlist?list=PLI6LvFw0iflh3o33YYnVIfOpaO0hc5Dzw""><b>Youtube FAQ</b></a> -->
</div>

<br/>

<div align=""center"
XSStrike,"<h1 align=""center"">
  <br>
  <a href=""https://github.com/s0md3v/XSStrike""><img src=""https://image.ibb.co/cpuYoA/xsstrike-logo.png"" alt=""XSStrike""></a>
  <br>
  XSStrike
  <br>
</h1>

<h4 align=""center"">Advanced XSS Detection Suite</h4>

<p align=""center"">
  <a href=""https://github.com/s0md3v/XSStrike/releases"">
    <img src=""https://img.shields.io/github/release/s0md3v/XSStrike.svg"">
  </a>
  <a href=""https://travis-ci.com/s0md3v/XSStrike"">
    <img src=""https://img.shields.io/travis/com/s0md3v/XSStrike.svg"">
  </a>
  <a href=""https://github.com/s0md3v/XSStrike/issues?q=is%3Aissue+is%3Aclosed"">
      <img src=""https://img.shields.io/github/issues-closed-raw/s0md3v/XSStrike.svg"">
  </a>
</p>

![multi xss](https://image.ibb.co/gOCV5L/Screenshot-2018-11-19-13-33-49.png)

<p align=""center"">
  <a href=""https://github.com/s0md3v/XSStrike/wiki"">XSStrike Wiki</a> â€¢
  <a href=""https://github.com/s0md3v/XSStrike/wiki/Usage"">Usage</a> â€¢
  <a href=""https://github.com/s0md3v/XSStrike/wiki/FAQ"">FAQ<"
AutoEq,"# AutoEq
AutoEq is a tool for automatically equalizing headphones.

Go to **[autoeq.app](https://autoeq.app)** to get started.

This Github repository now mainly serves developers. The contributions of this project are:
* Web application for easily equalize and tweak headphone frequency responses without needing to install anything
* Library for working with (headphone) frequency responses and optimizing parametric equalizers
* [PyPi package](https://pypi.org/project/autoeq/) for installing the library on your projects
* Collection of headphone [measurements](./measurements) as numerical data from
[oratory1990](https://www.reddit.com/r/oratory1990/wiki/index/list_of_presets/),
[crinacle](https://crinacle.com),
[Innerfidelity](https://www.stereophile.com/content/innerfidelity-headphone-measurements),
[Rtings](https://www.rtings.com/headphones/1-5/graph) and legacy
headphone.com measurements (which are not the same as what the company produces today).
* Collection of different headphone "
pyright,"![Pyright](https://github.com/microsoft/pyright/blob/main/docs/img/PyrightLarge.png)

# Static Type Checker for Python

Pyright is a full-featured, standards-based static type checker for Python. It is designed for high performance and can be used with large Python source bases.

Pyright includes both a [command-line tool](https://microsoft.github.io/pyright/#/command-line) and an [extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-pyright.pyright).


## Pyright Playground

Try Pyright in your browser using the [Pyright Playground](https://pyright-play.net/?code=MQAgKgFglgziMEMC2AHANgUxAEw0g9gHYwAuATgiRnBPgO4gDG%2BSBhIGZZ%2BZcjC7AEZZcVRlWzwSlKPzRoAniEFKUCslADmEEgDoAUPtwAzEAmzYAFAA8AXCGNp8lADQgF9x85IBKW-pBAkDIMEgBXMnZrEABqd0NQAAUEGBgoQk0zKTIQdNIBRiwUkBIILBgMZkJJBDJNMKQMQhJg6jC0Ejh0rLIw5qhGjmtClBIoIgNzKwBGNwAiOZ99IA).


## Documentation

Refer to [the documentation](https://microsoft.github.io/pyright) for installation, configuration,"
memray,"<p align=""center"">
<img src=""https://raw.githubusercontent.com/bloomberg/memray/main/docs/_static/images/logo.png"" width=""70%"">
</p>

---

[![OS Linux](https://img.shields.io/badge/OS-Linux-blue)](https://pypi.org/project/memray)
[![OS MacOS](https://img.shields.io/badge/OS-macOS-blue)](https://pypi.org/project/memray)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/memray)](https://pypi.org/project/memray)
[![PyPI - Implementation](https://img.shields.io/pypi/implementation/memray)](https://pypi.org/project/memray)
[![PyPI](https://img.shields.io/pypi/v/memray)](https://pypi.org/project/memray)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/memray)](https://pypistats.org/packages/memray)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/memray.svg)](https://anaconda.org/conda-forge/memray)
[![Tests](https://github.com/bloomberg/memray/actions/workflows/build.yml/badge.svg)](https://github.com/bloomberg/memray/actions/workflows/build.yml)
[![Code Style"
explainshell,"# [explainshell.com](http://www.explainshell.com) - match command-line arguments to their help text

explainshell is a tool (with a web interface) capable of parsing man pages, extracting options and
explaining a given command-line by matching each argument to the relevant help text in the man page.

## How?

explainshell is built from the following components:

1. man page reader which converts a given man page from raw format to html (manpage.py)
2. classifier which goes through every paragraph in the man page and classifies
   it as contains options or not (algo/classifier.py)
3. an options extractor that scans classified paragraphs and looks for options (options.py)
4. a storage backend that saves processed man pages to mongodb (store.py)
5. a matcher that walks the command's AST (parsed by [bashlex](https://github.com/idank/bashlex)) and contextually matches each node
   to the relevant help text (matcher.py)

When querying explainshell, it:

1. parses the query into an AST
2. vis"
wifiphisher,"[![Build Status](https://travis-ci.org/wifiphisher/wifiphisher.svg?branch=master)](https://travis-ci.org/wifiphisher/wifiphisher)
[![Documentation Status](https://readthedocs.org/projects/wifiphisher/badge/?version=latest)](http://wifiphisher.readthedocs.io/en/latest/?badge=latest)
![Python Version](https://img.shields.io/badge/python-3.7-blue.svg)
![License](https://img.shields.io/badge/license-GPL-blue.svg)

<p align=""center""><img src=""https://wifiphisher.github.io/wifiphisher/wifiphisher.png"" /></p>

## About
<a href=""https://wifiphisher.org"">Wifiphisher</a> is a rogue Access Point framework for conducting red team engagements or Wi-Fi security testing. Using Wifiphisher, penetration testers can easily achieve a man-in-the-middle position against wireless clients by performing targeted Wi-Fi association attacks. Wifiphisher can be further used to mount victim-customized web phishing attacks against the connected clients in order to capture credentials (e.g. from third party login pa"
yfinance,"# Download market data from Yahoo! Finance's API

<table border=1 cellpadding=10><tr><td>

#### \*\*\* IMPORTANT LEGAL DISCLAIMER \*\*\*

---

**Yahoo!, Y!Finance, and Yahoo! finance are registered trademarks of
Yahoo, Inc.**

yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It's
an open-source tool that uses Yahoo's publicly available APIs, and is
intended for research and educational purposes.

**You should refer to Yahoo!'s terms of use**
([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm),
[here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and
[here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) **for
details on your rights to use the actual data downloaded. Remember - the
Yahoo! finance API is intended for personal use only.**

</td></tr></table>

---

<a target=""new"" href=""https://pypi.python.org/pypi/yfinance""><img border=0 src=""https://img.shields.io/badge/python-2.7,%203.6+-blue.svg?style=flat"" alt"
httpx,"<p align=""center"">
  <a href=""https://www.python-httpx.org/""><img width=""350"" height=""208"" src=""https://raw.githubusercontent.com/encode/httpx/master/docs/img/butterfly.png"" alt='HTTPX'></a>
</p>

<p align=""center""><strong>HTTPX</strong> <em>- A next-generation HTTP client for Python.</em></p>

<p align=""center"">
<a href=""https://github.com/encode/httpx/actions"">
    <img src=""https://github.com/encode/httpx/workflows/Test%20Suite/badge.svg"" alt=""Test Suite"">
</a>
<a href=""https://pypi.org/project/httpx/"">
    <img src=""https://badge.fury.io/py/httpx.svg"" alt=""Package version"">
</a>
</p>

HTTPX is a fully featured HTTP client library for Python 3. It includes **an integrated
command line client**, has support for both **HTTP/1.1 and HTTP/2**, and provides both **sync
and async APIs**.

---

Install HTTPX using pip:

```shell
$ pip install httpx
```

Now, let's get started:

```pycon
>>> import httpx
>>> r = httpx.get('https://www.example.org/')
>>> r
<Response [200 OK]>
>>> r.status_co"
chatgpt-mirai-qq-bot,"![cover](https://user-images.githubusercontent.com/117586514/230783378-34ddb86a-c8d3-47a6-baa5-86e39200b258.png)

------------------------------------
<p align=""center"">
  <h2 align=""center"">ChatGPT for Bot</h2>
  <p align=""center"">
    ä¸€æ¬¾æ”¯æŒå„ç§ä¸»æµè¯­è¨€æ¨¡å‹çš„èŠå¤©çš„æœºå™¨äººï¼
    <br/>
    <br/>
    <a href=""https://chatgpt-qq.lss233.com/""><strong>Â» æŸ¥çœ‹ä½¿ç”¨æ•™ç¨‹ Â»</strong></a>
    <br/>
  </p>
</p>

<p align=""center"">
  <a href=""https://github.com/lss233/chatgpt-mirai-qq-bot/stargazers""><img src=""https://img.shields.io/github/stars/lss233/chatgpt-mirai-qq-bot?color=E2CDBC&amp;logo=github&amp;style=for-the-badge"" alt=""Github stars""></a>
  <a href=""https://github.com/lss233/chatgpt-mirai-qq-bot/actions/workflows/docker-latest.yml""><img src=""https://img.shields.io/github/actions/workflow/status/lss233/chatgpt-mirai-qq-bot/docker-latest.yml?color=E2CDBC&amp;logo=docker&amp;logoColor=white&amp;style=for-the-badge"" alt=""Docker build latest""></a>
  <a href=""https://hub.docker.com/r/lss233/chatgpt-mirai-qq-bot/""><img "
authentik,"<p align=""center"">
    <img src=""https://goauthentik.io/img/icon_top_brand_colour.svg"" height=""150"" alt=""authentik logo"">
</p>

---

[![Join Discord](https://img.shields.io/discord/809154715984199690?label=Discord&style=for-the-badge)](https://goauthentik.io/discord)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&label=core%20build&style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&label=outpost%20build&style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&label=web%20build&style=for-the-badge)](https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml)
[![Cod"
edgedb,"<p align=""center"">
  <a href=""https://www.edgedb.com"">
    <img src=""https://www.edgedb.com/github_banner.png"">
  </a>
</p>

<div align=""center"">
  <h1>EdgeDB</h1>
  <a href=""https://github.com/edgedb/edgedb"" rel=""nofollow"">
    <img src=""https://img.shields.io/github/stars/edgedb/edgedb"" alt=""Stars"">
  </a>
  <a href=""https://github.com/edgedb/edgedb/actions"">
    <img src=""https://github.com/edgedb/edgedb/workflows/Tests/badge.svg?event=push&branch=master"" />
  </a>
  <a href=""https://github.com/edgedb/edgedb/blob/master/LICENSE"">
    <img alt=""license"" src=""https://img.shields.io/badge/license-Apache%202.0-blue"" />
  </a>
  <a href=""https://discord.gg/umUueND6ag"">
    <img alt=""discord"" src=""https://img.shields.io/discord/841451783728529451?color=5865F2&label=discord&logo=discord&logoColor=8a9095"">
  </a>
  <br />
  <br />
  <a href=""https://www.edgedb.com/docs/guides/quickstart"">Quickstart</a>
  <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
  <a href=""https://www.edgedb.com"">Website</a>
 "
scipy,".. image:: https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg
  :target: https://scipy.org
  :width: 110
  :height: 110
  :align: left 

.. image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
  :target: https://numfocus.org

.. image:: https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads
  :target: https://pypi.org/project/scipy/

.. image:: https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads
  :target: https://anaconda.org/conda-forge/scipy

.. image:: https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg
  :target: https://stackoverflow.com/questions/tagged/scipy

.. image:: https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue.svg
  :target: https://www.nature.com/articles/s41592-019-0686-2

SciPy (pronounced ""Sigh Pie"") is an open-source software for mathematics,
science, and engineering. It includes modules for statistics, optimizat"
EIPs,"# Ethereum Improvement Proposals (EIPs)

> **_ATTENTION_**: The EIPs repository has recently [undergone](https://github.com/ethereum/EIPs/pull/7206) a separation of ERCs and EIPs. ERCs are now accessible at [https://github.com/ethereum/ercs](https://github.com/ethereum/ercs). All new ERCs and updates to existing ones must be directed at this new repository. The editors apologize for this inconvenience.

The goal of the EIP project is to standardize and provide high-quality documentation for Ethereum itself and conventions built upon it. This repository tracks past and ongoing improvements to Ethereum in the form of Ethereum Improvement Proposals (EIPs). [EIP-1](https://eips.ethereum.org/EIPS/eip-1) governs how EIPs are published.

The [status page](https://eips.ethereum.org/) tracks and lists EIPs, which can be divided into the following categories:

- [Core EIPs](https://eips.ethereum.org/core) are improvements to the Ethereum consensus protocol.
- [Networking EIPs](https://eips.ether"
sympy,"# SymPy

[![pypi version](https://img.shields.io/pypi/v/sympy.svg)](https://pypi.python.org/pypi/sympy)
[![Join the chat at https://gitter.im/sympy/sympy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/sympy/sympy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Zenodo Badge](https://zenodo.org/badge/18918/sympy/sympy.svg)](https://zenodo.org/badge/latestdoi/18918/sympy/sympy)
[![Downloads](https://pepy.tech/badge/sympy/month)](https://pepy.tech/project/sympy)
[![GitHub Issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/sympy/sympy/issues)
[![Git Tutorial](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project)
[![Powered by NumFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)
[![Commits since last release](https://img.shields.io/github/commits-since/sympy/symp"
tushare,"TuShare


Tushare Proç‰ˆå·²å‘å¸ƒï¼Œè¯·è®¿é—®æ–°çš„å®˜ç½‘äº†è§£å’ŒæŸ¥è¯¢æ•°æ®æ¥å£ï¼ [https://tushare.pro](https://tushare.pro)

TuShareæ˜¯å®ç°å¯¹è‚¡ç¥¨/æœŸè´§ç­‰é‡‘èæ•°æ®ä»**æ•°æ®é‡‡é›†**ã€**æ¸…æ´—åŠ å·¥** åˆ° **æ•°æ®å­˜å‚¨**è¿‡ç¨‹çš„å·¥å…·ï¼Œæ»¡è¶³é‡‘èé‡åŒ–åˆ†æå¸ˆå’Œå­¦ä¹ æ•°æ®åˆ†æçš„äººåœ¨æ•°æ®è·å–æ–¹é¢çš„éœ€æ±‚ï¼Œå®ƒçš„ç‰¹ç‚¹æ˜¯æ•°æ®è¦†ç›–èŒƒå›´å¹¿ï¼Œæ¥å£è°ƒç”¨ç®€å•,å“åº”å¿«é€Ÿã€‚

![](http://tushare.org/_images/main_pic_min.png)

æ¬¢è¿å…³æ³¨æ‰«æTuShareçš„å¾®ä¿¡å…¬ä¼—å·â€œæŒ–åœ°å…”â€ï¼Œæ›´å¤šèµ„æºå’Œä¿¡æ¯ä¸æ‚¨åˆ†äº«ã€‚å¦å¤–ï¼Œç”±äºtushareå®˜ç½‘åœ¨é‡æ–°è®¾è®¡å’Œå¼€å‘ï¼Œæœ€æ–°æ¥å£çš„ä½¿ç”¨æ–‡æ¡£éƒ½ä¼šåœ¨æŒ–åœ°å…”å…¬ä¼—å·å‘å¸ƒï¼Œæ‰€ä»¥ï¼Œè¯·æ‰«ç å…³æ³¨ï¼Œè°¢è°¢ï¼

![](http://tushare.org/_images/ts.jpg)

QQäº¤æµç¾¤ï¼š

- ä¸€ç¾¤ï¼ˆå·²æ»¡ï¼‰ï¼š14934432
- äºŒç¾¤ï¼ˆä»˜è´¹é«˜çº§ç”¨æˆ·ç¾¤ï¼Œå¯è·å¾—æ›´å¤šæ”¯æŒåŠå‚ä¸åœˆå­æ´»åŠ¨ï¼‰ï¼š658562506
- ä¸‰ç¾¤ï¼ˆå…è´¹ï¼‰ï¼š665480579
- å››ç¾¤ (å…è´¹) ï¼š527416821



Dependencies
=========
python 2.x/3.x   

[pandas](http://pandas.pydata.org/ ""pandas"")


Installation
====

- æ–¹å¼1ï¼špip install tushare
- æ–¹å¼2ï¼špython setup.py install
- æ–¹å¼3ï¼šè®¿é—®[https://pypi.python.org/pypi/tushare/](https://pypi.python.org/pypi/tushare/)ä¸‹è½½å®‰è£…


Upgrade
=======

	pip install tushare --upgrade

Quick Start
======
**Example 1.** è·å–ä¸ªè‚¡å†å²äº¤æ˜“æ•°æ®ï¼ˆåŒ…æ‹¬å‡çº¿æ•°æ®ï¼‰ï¼š

    import tushare as ts

	ts.get_hist_data('600848') #ä¸€æ¬¡æ€§è·å–å…¨éƒ¨æ•°æ®
	å¦å¤–ï¼Œå‚è€ƒget_k_dataå‡½æ•°

ç»“æœæ˜¾ç¤ºï¼š

>"
beets,".. image:: https://img.shields.io/pypi/v/beets.svg
    :target: https://pypi.python.org/pypi/beets

.. image:: https://img.shields.io/codecov/c/github/beetbox/beets.svg
    :target: https://codecov.io/github/beetbox/beets

.. image:: https://github.com/beetbox/beets/workflows/ci/badge.svg?branch=master
    :target: https://github.com/beetbox/beets/actions

.. image:: https://repology.org/badge/tiny-repos/beets.svg
    :target: https://repology.org/project/beets/versions


beets
=====

Beets is the media library management system for obsessive music geeks.

The purpose of beets is to get your music collection right once and for all.
It catalogs your collection, automatically improving its metadata as it goes.
It then provides a bouquet of tools for manipulating and accessing your music.

Here's an example of beets' brainy tag corrector doing its thing::

  $ beet import ~/music/ladytron
  Tagging:
      Ladytron - Witching Hour
  (Similarity: 98.4%)
   * Last One Standing      -> The La"
spiderfoot,"<a href=""https://www.spiderfoot.net/r.php?u=aHR0cHM6Ly93d3cuc3BpZGVyZm9vdC5uZXQv&s=os_gh""><img src=""https://www.spiderfoot.net/wp-content/themes/spiderfoot/img/spiderfoot-wide.png""></a>


[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/smicallef/spiderfoot/master/LICENSE)
[![Python Version](https://img.shields.io/badge/python-3.7+-green)](https://www.python.org)
[![Stable Release](https://img.shields.io/badge/version-4.0-blue.svg)](https://github.com/smicallef/spiderfoot/releases/tag/v4.0)
[![CI status](https://github.com/smicallef/spiderfoot/workflows/Tests/badge.svg)](https://github.com/smicallef/spiderfoot/actions?query=workflow%3A""Tests"")
[![Last Commit](https://img.shields.io/github/last-commit/smicallef/spiderfoot)](https://github.com/smicallef/spiderfoot/commits/master)
[![Codecov](https://codecov.io/github/smicallef/spiderfoot/coverage.svg)](https://codecov.io/github/smicallef/spiderfoot)
[![Twitter Follow](https://img.shields.i"
pre-commit,"[![build status](https://github.com/pre-commit/pre-commit/actions/workflows/main.yml/badge.svg)](https://github.com/pre-commit/pre-commit/actions/workflows/main.yml)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/pre-commit/pre-commit/main.svg)](https://results.pre-commit.ci/latest/github/pre-commit/pre-commit/main)

## pre-commit

A framework for managing and maintaining multi-language pre-commit hooks.

For more information see: https://pre-commit.com/
"
httpbin,"# httpbin(1): HTTP Request & Response Service


A [Kenneth Reitz](http://kennethreitz.org/bitcoin) Project.

![ice cream](http://farm1.staticflickr.com/572/32514669683_4daf2ab7bc_k_d.jpg)

Run locally:
```sh
docker pull kennethreitz/httpbin
docker run -p 80:80 kennethreitz/httpbin
```

See http://httpbin.org for more information.

## Officially Deployed at:

- http://httpbin.org
- https://httpbin.org
- https://hub.docker.com/r/kennethreitz/httpbin/


## SEE ALSO

- http://requestb.in
- http://python-requests.org
- https://grpcb.in/

## Build Status

[![Build Status](https://travis-ci.org/requests/httpbin.svg?branch=master)](https://travis-ci.org/requests/httpbin)
"
Auto_Jobs_Applier_AIHawk,"<div align=""center"">
<img src=""./assets/AIHawk.png"">

<!-- At first glance, the branding and messaging clearly conveys what to expect -->


  <!-- [![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/AIhawkCommunity) -->
 
  [![Gmail](https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:federico.elia.majo@gmail.com)

  # Auto_Jobs_Applier_AIHawk
  ![CI](https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk/actions/workflows/ci.yml/badge.svg)

  #### ğŸ¤–ğŸ” Your AI-powered job search assistant. Automate applications, get personalized recommendations, and land your dream job faster.



<br />

<!-- Message Clarity -->
## ğŸš€ Join the AIHawk Community ğŸš€ 

Connect with like-minded individuals and get the most out of AIHawk.

ğŸ’¡ **Get support:** Ask questions, troubleshoot issues, and find solutions.

ğŸ—£ï¸ **Share knowledge:** Share your experiences, tips, and best practices.

ğŸ¤ **Networ"
PaddleHub,"English | [ç®€ä½“ä¸­æ–‡](README_ch.md)

<p align=""center"">
 <img src=""./docs/imgs/paddlehub_logo.jpg"" align=""middle"" width=""400"" />
<p align=""center"">
<div align=""center"">  
  <h3> <a href=#QuickStart> Quick Start </a> | <a href=""./modules""> Model List </a> | <a href=#demos> Demos </a> </h3>
</div>

------------------------------------------------------------------------------------------

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.6.2+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""""><img src=""https://img.shields.io/pypi/format/paddlehub?color=c77""></a>
    <a href=""https://pypi.org/project/paddlehub/""><img src=""https://img.shields.io/pypi/dm/paddlehub?color=9cf""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleHub/stargazers""><img src=""https://img.shields.io/github/stars/Paddle"
mamba,"# Mamba

![Mamba](assets/selection.png ""Selective State Space"")
> **Mamba: Linear-Time Sequence Modeling with Selective State Spaces**\
> Albert Gu*, Tri Dao*\
> Paper: https://arxiv.org/abs/2312.00752

![Mamba-2](assets/ssd_algorithm.png ""State Space Dual Model"")
> **Transformers are SSMs: Generalized Models and Efficient Algorithms**\
>     **Through Structured State Space Duality**\
> Tri Dao*, Albert Gu*\
> Paper: https://arxiv.org/abs/2405.21060

## About

Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of Transformers.
It is based on the line of progress on [structured state space models](https://github.com/state-spaces/s4),
with an efficient hardware-aware design and implementation in the spirit of [FlashAttention](https://github.com/Dao-AILab/flash-attention).

## Installation

- [Option] `pip install causal-conv1d>=1.4.0`: an efficient implementation o"
openage,"[![openage](/assets/logo/banner.svg)](http://openage.dev)
=========================================================

**openage**: a volunteer project to create a free engine clone of the *Genie Engine* used by *Age of Empires*, *Age of Empires II (HD)* and *Star Wars: Galactic Battlegrounds*, comparable to projects like [OpenMW](https://openmw.org/), [OpenRA](http://openra.net/),  [OpenSAGE](https://github.com/OpenSAGE/OpenSAGE/), [OpenTTD](https://openttd.org/) and [OpenRCT2](https://openrct2.org/).

openage uses the original game assets (such as sounds and graphics), but (for obvious reasons) doesn't ship them.
To play, you require *[any of the original games (AoE1, AoE2)](/doc/media_convert.md)* or their *Definitive Edition* releases.

[![github stars](https://img.shields.io/github/stars/SFTtech/openage.svg)](https://github.com/SFTtech/openage/stargazers)
[![#sfttech on matrix.org](/assets/doc/matrixroom.svg)](https://matrix.to/#/#sfttech:matrix.org)
[![GPL licensed](/assets/doc/lic"
khoj,"<p align=""center""><img src=""src/khoj/interface/web/assets/icons/khoj-logo-sideways-500.png"" width=""230"" alt=""Khoj Logo""></p>

<div align=""center"">

[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)
[![dockerize](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)
[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)
![Discord](https://img.shields.io/discord/1112065956647284756?style=plastic&label=discord)

</div>

<div align=""center"">
<b>The open-source, personal AI for your digital brain</b>
</div>

<br />

<div align=""center"">

[ğŸ“‘ Docs](https://docs.khoj.dev)
<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
[ğŸ® App](https://khoj.dev)
<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
[ğŸ’¬ Discord](https://discord.gg/BDgyabRM6e)
<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
[ğŸ“š Blog](https://blog"
MHDDoS,"<p align=""center""><img src=""https://i.ibb.co/3F6V9JQ/MHDDoS.png"" width=""400px"" height=""150px"" alt=""ddos""></p>

<h1 align=""center"">MHDDoS - DDoS Attack Script With 56 Methods</h1>
<em><h5 align=""center"">(Programming Language - Python 3)</h5></em>

<p align=""center"">
<a href=""#""><img alt=""MH-DDoS forks"" src=""https://img.shields.io/github/forks/MatrixTM/MHDDoS?style=for-the-badge""></a>
<a href=""#""><img alt=""MH-DDoS last commit (main)"" src=""https://img.shields.io/github/last-commit/MatrixTM/MHDDoS/main?color=green&style=for-the-badge""></a>
<a href=""#""><img alt=""MH-DDoS Repo stars"" src=""https://img.shields.io/github/stars/MatrixTM/MHDDoS?style=for-the-badge&color=yellow""></a>
<a href=""#""><img alt=""MH-DDoS License"" src=""https://img.shields.io/github/license/MatrixTM/MHDDoS?color=orange&style=for-the-badge""></a>
<a href=""https://github.com/MatrixTM/MHDDoS/issues""><img alt=""MatrixTM issues"" src=""https://img.shields.io/github/issues/MatrixTM/MHDDoS?color=purple&style=for-the-badge""></a>
  
<p a"
pandas-ai,"# ![PandasAI](assets/logo.png)

[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&style=flat-square)](https://pypi.org/project/pandasai/)
[![CI](https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg)](https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg)
[![CD](https://github.com/gventuri/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/gventuri/pandas-ai/actions/workflows/cd.yml/badge.svg)
[![Coverage](https://codecov.io/gh/gventuri/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/gventuri/pandas-ai)
[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&compact=true)](https://discord.gg/kF7FqH2FwS)
[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab."
PaddleDetection,"ç®€ä½“ä¸­æ–‡ | [English](README_en.md)

<div align=""center"">
<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/48054808/160532560-34cf7a1f-d950-435e-90d2-4b0a679e5119.png"" align=""middle"" width = ""800"" />
</p>

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleDetection/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleDetection?color=ffa""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleDetection/stargazers""><img src=""https://img.shields.io/github/stars/PaddlePaddle/PaddleDetection?color=ccf""></a>
</p>
</div>

## ğŸ’Œç›®å½•
- [ğŸ’Œç›®å½•](#ç›®å½•)
- [ğŸŒˆç®€ä»‹](#ç®€ä»‹)
- [ğŸ“£æœ€æ–°è¿›å±•](#æœ€æ–°è¿›å±•)
- [ğŸ‘«å¼€æºç¤¾åŒº](#å¼€æºç¤¾åŒº)
- [âœ¨ä¸»è¦ç‰¹æ€§](#ä¸»è¦ç‰¹æ€§)
    - [ğŸ§©æ¨¡å—åŒ–è®¾è®¡](#æ¨¡å—åŒ–è®¾è®¡)
    - [ğŸ“±ä¸°å¯Œçš„æ¨¡å‹åº“](#ä¸°å¯Œçš„æ¨¡å‹åº“)
    - [ğŸ—ï¸"
searxng,".. SPDX-License-Identifier: AGPL-3.0-or-later

----

.. figure:: https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng.svg
   :target: https://docs.searxng.org/
   :alt: SearXNG
   :width: 100%
   :align: center

----

Privacy-respecting, hackable `metasearch engine`_

Searx.space_ lists ready-to-use running instances.

A user_, admin_ and developer_ handbook is available on the homepage_.

|SearXNG install|
|SearXNG homepage|
|SearXNG wiki|
|AGPL License|
|Issues|
|commits|
|weblate|
|SearXNG logo|

----

.. _searx.space: https://searx.space
.. _user: https://docs.searxng.org/user
.. _admin: https://docs.searxng.org/admin
.. _developer: https://docs.searxng.org/dev
.. _homepage: https://docs.searxng.org/
.. _metasearch engine: https://en.wikipedia.org/wiki/Metasearch_engine

.. |SearXNG logo| image:: https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng-wordmark.svg
   :target: https://docs.searxng.org/
   :width: 5%

.. |SearXNG install| imag"
redis-py,"# redis-py

The Python interface to the Redis key-value store.

[![CI](https://github.com/redis/redis-py/workflows/CI/badge.svg?branch=master)](https://github.com/redis/redis-py/actions?query=workflow%3ACI+branch%3Amaster)
[![docs](https://readthedocs.org/projects/redis/badge/?version=stable&style=flat)](https://redis-py.readthedocs.io/en/stable/)
[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)
[![pypi](https://badge.fury.io/py/redis.svg)](https://pypi.org/project/redis/)
[![pre-release](https://img.shields.io/github/v/release/redis/redis-py?include_prereleases&label=latest-prerelease)](https://github.com/redis/redis-py/releases)
[![codecov](https://codecov.io/gh/redis/redis-py/branch/master/graph/badge.svg?token=yenl5fzxxr)](https://codecov.io/gh/redis/redis-py)

[Installation](#installation) |  [Usage](#usage) | [Advanced Topics](#advanced-topics) | [Contributing](https://github.com/redis/redis-py/blob/master/CONTRIBUTING.md)

-------------------------"
pygwalker,"[English](README.md) | [EspaÃ±ol](./docs/README.es.md) | [FranÃ§ais](./docs/README.fr.md) | [Deutsch](./docs/README.de.md) | [ä¸­æ–‡](./docs/README.zh.md) | [TÃ¼rkÃ§e](./docs/README.tr.md) | [æ—¥æœ¬èª](./docs/README.ja.md) | [í•œêµ­ì–´](./docs/README.ko.md)

<p align=""center""><a href=""https://github.com/Kanaries/pygwalker""><img width=100% alt="""" src=""https://github.com/Kanaries/pygwalker/assets/22167673/bed8b3db-fda8-43e7-8ad2-71f6afb9dddd"" /></a></p>

<h2 align=""center"">PyGWalker: A Python Library for Exploratory Data Analysis with Visualization</h2>

<p align=""center"">
    <a href=""https://arxiv.org/abs/2406.11637"">
      <img src=""https://img.shields.io/badge/arXiv-2406.11637-b31b1b.svg"" height=""18"" align=""center"">
    </a>
    <a href=""https://badge.fury.io/py/pygwalker"">
        <img src=""https://badge.fury.io/py/pygwalker.svg"" alt=""PyPI version"" height=""18"" align=""center"" />
    </a>
    <a href=""https://mybinder.org/v2/gh/Kanaries/pygwalker/main"">
      <img src=""https://mybinder.org/badge_logo.sv"
fish-speech,"# Fish Speech

<div align=""center"">

**English** | [ç®€ä½“ä¸­æ–‡](README.zh.md) | [Portuguese](README.pt-BR.md) | [æ—¥æœ¬èª](README.ja.md)

</div>

<div>
<a href=""https://www.producthunt.com/posts/fish-speech-1-4?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-fish&#0045;speech&#0045;1&#0045;4"" target=""_blank""><img src=""https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&theme=light"" alt=""Fish&#0032;Speech&#0032;1&#0046;4 - Open&#0045;Source&#0032;Multilingual&#0032;Text&#0045;to&#0045;Speech&#0032;with&#0032;Voice&#0032;Cloning | Product Hunt"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a>

<a href=""https://trendshift.io/repositories/7014"" target=""_blank"">
<img src=""https://trendshift.io/api/badge/repositories/7014"" alt=""fishaudio%2Ffish-speech | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/>
</a>
</div>

<div>
<a target=""_blank"" href=""https://discord.gg/Es5qTB9BcN"">
<img alt=""Discord"" src=""https://img."
pelican,"Pelican |build-status| |pypi-version| |downloads| |repology|
============================================================

Pelican is a static site generator, written in Python_, that allows you to create
web sites by composing text files in formats such as Markdown, reStructuredText, and HTML.

With Pelican, you can create web sites without worrying about databases or server-side programming.
Pelican generates static sites that can be served via any web server or hosting service.

You can perform the following functions with Pelican:

* Compose content in Markdown_ or reStructuredText_ using your editor of choice
* Simple command-line tool (re)generates HTML, CSS, and JS from your source content
* Easy to interface with version control systems and web hooks
* Completely static output is simple to host anywhere


Features
--------

Pelicanâ€™s feature highlights include:

* Chronological content (e.g., articles, blog posts) as well as static pages
* Integration with external services
* S"
OpenCore-Legacy-Patcher,"<div align=""center"">
             <img src=""docs/images/OC-Patcher.png"" alt=""OpenCore Patcher Logo"" width=""256"" />
             <h1>OpenCore Legacy Patcher</h1>
</div>

A Python-based project revolving around [Acidanthera's OpenCorePkg](https://github.com/acidanthera/OpenCorePkg) and [Lilu](https://github.com/acidanthera/Lilu) for both running and unlocking features in macOS on supported and unsupported Macs.

Our project's main goal is to breathe new life into Macs no longer supported by Apple, allowing for the installation and usage of macOS Big Sur and newer on machines as old as 2007.

----------

![GitHub all releases](https://img.shields.io/github/downloads/dortania/OpenCore-Legacy-Patcher/total?color=white&style=plastic) ![GitHub top language](https://img.shields.io/github/languages/top/dortania/OpenCore-Legacy-Patcher?color=4B8BBE&style=plastic) ![Discord](https://img.shields.io/discord/417165963327176704?color=7289da&label=discord&style=plastic)

----------

Noteworthy feature"
litellm,"<h1 align=""center"">
        ğŸš… LiteLLM
    </h1>
    <p align=""center"">
        <p align=""center"">
        <a href=""https://render.com/deploy?repo=https://github.com/BerriAI/litellm"" target=""_blank"" rel=""nofollow""><img src=""https://render.com/images/deploy-to-render-button.svg"" alt=""Deploy to Render""></a>
        <a href=""https://railway.app/template/HLP0Ub?referralCode=jch2ME"">
          <img src=""https://railway.app/button.svg"" alt=""Deploy on Railway"">
        </a>
        </p>
        <p align=""center"">Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        <br>
    </p>
<h4 align=""center""><a href=""https://docs.litellm.ai/docs/simple_proxy"" target=""_blank"">LiteLLM Proxy Server (LLM Gateway)</a> | <a href=""https://docs.litellm.ai/docs/hosted"" target=""_blank""> Hosted Proxy (Preview)</a> | <a href=""https://docs.litellm.ai/docs/enterprise""target=""_blank"">Enterprise Tier</a></h4>
<h4 align=""center"">
    <a href=""https://pypi"
RWKV-LM,"# RWKV: Parallelizable RNN with Transformer-level LM Performance (pronounced as ""RwaKuv"" (rÊŒkuv in IPA), from 4 major params: R W K V)

RWKV homepage: https://www.rwkv.com

**RWKV-5/6 Eagle/Finch paper**: https://arxiv.org/abs/2404.05892

**Awesome RWKV in Vision:** https://github.com/Yaziwel/Awesome-RWKV-in-Vision

RWKV-6 3B Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1

RWKV-6 7B Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2

**RWKV-6 GPT-mode demo code (with comments and explanations)**: https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v5/rwkv_v6_demo.py

RWKV-6 RNN-mode demo: https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v6_demo.py

![MQAR](Research/RWKV-6-MQAR.png)

### HOW TO TEST TRAINING RWKV-5 on MiniPile (1.5G tokens) ###

For reference, use python 3.10, torch 2.3.1+cu121 (or latest), cuda 12.5+, **latest deepspeed**, but **keep pytorch-lightning==1.9.5**

```
pip install torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu121
pi"
dask,"Dask
====

|Build Status| |Coverage| |Doc Status| |Discourse| |Version Status| |NumFOCUS|

Dask is a flexible parallel computing library for analytics.  See
documentation_ for more information.


LICENSE
-------

New BSD. See `License File <https://github.com/dask/dask/blob/main/LICENSE.txt>`__.

.. _documentation: https://dask.org
.. |Build Status| image:: https://github.com/dask/dask/actions/workflows/tests.yml/badge.svg
   :target: https://github.com/dask/dask/actions/workflows/tests.yml
.. |Coverage| image:: https://codecov.io/gh/dask/dask/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/dask/dask/branch/main
   :alt: Coverage status
.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest
   :target: https://dask.org
   :alt: Documentation Status
.. |Discourse| image:: https://img.shields.io/discourse/users?logo=discourse&server=https%3A%2F%2Fdask.discourse.group
   :alt: Discuss Dask-related things and ask for help
   :target: https://dask."
awesome-aws,"<br/>
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png"">
</p>
<br/>

# Awesome AWS [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.

Inspired by the [awesome](https://github.com/sindresorhus/awesome) list.

## The Fiery Meter of AWSome

* Repo with 0100+ Stars: :fire:
* Repo with 0200+ Stars: :fire::fire:
* Repo with 0500+ Stars: :fire::fire::fire:
* Repo with 1000+ Stars: :fire::fire::fire::fire:
* Repo with 2000+ Stars: :fire::fire::fire::fire::fire:

Repos not on `The Fiery Meter of AWSome` can still be awesome, see [A Note on Repo AWSomeness](https://github.com/donnemartin/awesome-aws/blob/master/CONTRIBUTING.md#a-note-on-repo-awsomeness).

### `awesome-aws` Python Module

[![Build Status](https://trav"
seaborn,"<img src=""https://raw.githubusercontent.com/mwaskom/seaborn/master/doc/_static/logo-wide-lightbg.svg""><br>

--------------------------------------

seaborn: statistical data visualization
=======================================

[![PyPI Version](https://img.shields.io/pypi/v/seaborn.svg)](https://pypi.org/project/seaborn/)
[![License](https://img.shields.io/pypi/l/seaborn.svg)](https://github.com/mwaskom/seaborn/blob/master/LICENSE.md)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.03021/status.svg)](https://doi.org/10.21105/joss.03021)
[![Tests](https://github.com/mwaskom/seaborn/workflows/CI/badge.svg)](https://github.com/mwaskom/seaborn/actions)
[![Code Coverage](https://codecov.io/gh/mwaskom/seaborn/branch/master/graph/badge.svg)](https://codecov.io/gh/mwaskom/seaborn)

Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.


Documentation
-------------

Online documentation is available at"
ydata-profiling,"# ydata-profiling

[![Build Status](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml)
[![PyPI download month](https://img.shields.io/pypi/dm/ydata-profiling.svg)](https://pypi.python.org/pypi/ydata-profiling/)
[![](https://pepy.tech/badge/pandas-profiling)](https://pypi.org/project/ydata-profiling/)
[![Code Coverage](https://codecov.io/gh/ydataai/pandas-profiling/branch/master/graph/badge.svg?token=gMptB4YUnF)](https://codecov.io/gh/ydataai/pandas-profiling)
[![Release Version](https://img.shields.io/github/release/ydataai/pandas-profiling.svg)](https://github.com/ydataai/pandas-profiling/releases)
[![Python Version](https://img.shields.io/pypi/pyversions/ydata-profiling)](https://pypi.org/project/ydata-profiling/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)
<img referrerpolicy=""no-referrer-when-"
moviepy,"MoviePy
=======

.. image:: https://badge.fury.io/py/moviepy.svg
    :target: PyPI_
    :alt: MoviePy page on the Python Package Index
.. image:: https://img.shields.io/gitter/room/movie-py/gitter?color=46BC99&logo=gitter
    :target: Gitter_
    :alt: Discuss MoviePy on Gitter
.. image:: https://img.shields.io/github/actions/workflow/status/Zulko/moviepy/test_suite.yml?logo=github
    :target: https://github.com/Zulko/moviepy/actions/workflows/test_suite.yml
    :alt: Build status on gh-actions
.. image:: https://img.shields.io/coveralls/github/Zulko/moviepy/master?logo=coveralls
    :target: https://coveralls.io/github/Zulko/moviepy?branch=master
    :alt: Code coverage from coveralls.io

MoviePy (full documentation_) is a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects. See the gallery_ for some examples of use.

MoviePy can read and write all the most common "
clip-as-service,"<p align=""center"">
<a href=""https://clip-as-service.jina.ai""><img src=""https://github.com/jina-ai/clip-as-service/blob/main/docs/_static/logo-light.svg?raw=true"" alt=""CLIP-as-service logo: The data structure for unstructured data"" width=""200px""></a>
<br><br><br>
</p>


<p align=center>
<a href=""https://pypi.org/project/clip_server/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/clip_server?label=Release&style=flat-square""></a>
<a href=""https://discord.jina.ai""><img src=""https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square""></a>
<a href=""https://codecov.io/gh/jina-ai/clip-as-service""><img alt=""Codecov branch"" src=""https://img.shields.io/codecov/c/github/jina-ai/clip-as-service/main?logo=Codecov&logoColor=white&style=flat-square""></a>
<a href=""https://colab.research.google.com/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb""><img src=""https://img.shields.io/badge/Host-on%20Google%20Colab%20(GPU/TPU)-brightgreen?st"
kotaemon,"# kotaemon

An open-source clean & customizable RAG UI for chatting with your documents. Built with both end users and
developers in mind.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)

[Live Demo](https://huggingface.co/spaces/cin-model/kotaemon-demo) |
[Source Code](https://github.com/Cinnamon/kotaemon)

[User Guide](https://cinnamon.github.io/kotaemon/) |
[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |
[Feedback](https://github.com/Cinnamon/kotaemon/issues)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
<a href=""https://github.com/Cinnamon/kotaemon"" target=""_blank"">
<img src=""https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen"" alt=""docker pull ghcr.io/cinnamon/kotaemon:latest""></a>
![download](https:/"
alphafold,"![header](imgs/header.jpg)

# AlphaFold

This package provides an implementation of the inference pipeline of AlphaFold
v2. For simplicity, we refer to this model as AlphaFold throughout the rest of
this document.

We also provide:

1.  An implementation of AlphaFold-Multimer. This represents a work in progress
    and AlphaFold-Multimer isn't expected to be as stable as our monomer
    AlphaFold system. [Read the guide](#updating-existing-installation) for how
    to upgrade and update code.
2.  The [technical note](docs/technical_note_v2.3.0.md) containing the models
    and inference procedure for an updated AlphaFold v2.3.0.
3.  A [CASP15 baseline](docs/casp15_predictions.zip) set of predictions along
    with documentation of any manual interventions performed.

Any publication that discloses findings arising from using this source code or
the model parameters should [cite](#citing-this-work) the
[AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2) and, if
applicable, the"
Pillow,"<p align=""center"">
    <img width=""248"" height=""250"" src=""https://raw.githubusercontent.com/python-pillow/pillow-logo/main/pillow-logo-248x250.png"" alt=""Pillow logo"">
</p>

# Pillow

## Python Imaging Library (Fork)

Pillow is the friendly PIL fork by [Jeffrey A. Clark and
contributors](https://github.com/python-pillow/Pillow/graphs/contributors).
PIL is the Python Imaging Library by Fredrik Lundh and contributors.
As of 2019, Pillow development is
[supported by Tidelift](https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=readme&utm_campaign=enterprise).

<table>
    <tr>
        <th>docs</th>
        <td>
            <a href=""https://pillow.readthedocs.io/?badge=latest""><img
                alt=""Documentation Status""
                src=""https://readthedocs.org/projects/pillow/badge/?version=latest""></a>
        </td>
    </tr>
    <tr>
        <th>tests</th>
        <td>
            <a href=""https://github.com/python-pillow/Pillow/actions/workflows/li"
routersploit,"# RouterSploit - Exploitation Framework for Embedded Devices

[![Python 3.6](https://img.shields.io/badge/Python-3.6-yellow.svg)](http://www.python.org/download/)
[![Build Status](https://travis-ci.org/threat9/routersploit.svg?branch=master)](https://travis-ci.org/threat9/routersploit)

The RouterSploit Framework is an open-source exploitation framework dedicated to embedded devices.

[![asciicast](https://asciinema.org/a/180370.png)](https://asciinema.org/a/180370)

It consists of various modules that aid penetration testing operations:

* exploits - modules that take advantage of identified vulnerabilities
* creds - modules designed to test credentials against network services
* scanners - modules that check if a target is vulnerable to any exploit
* payloads - modules that are responsible for generating payloads for various architectures and injection points
* generic - modules that perform generic attacks 

# Installation

## Requirements

Required:
* future
* requests
* paramiko
*"
pytube,"<div align=""center"">
  <p>
    <a href=""#""><img src=""https://assets.nickficano.com/gh-pytube.min.svg"" width=""456"" height=""143"" alt=""pytube logo"" /></a>
  </p>
  <p align=""center"">
	<a href=""https://pypi.org/project/pytube/""><img src=""https://img.shields.io/pypi/dm/pytube?style=flat-square"" alt=""pypi""/></a>
	<a href=""https://pytube.io/en/latest/""><img src=""https://readthedocs.org/projects/python-pytube/badge/?version=latest&style=flat-square"" /></a>
	<a href=""https://pypi.org/project/pytube/""><img src=""https://img.shields.io/pypi/v/pytube?style=flat-square"" /></a>
  </p>
</div>

### Actively soliciting contributors!

Have ideas for how pytube can be improved? Feel free to open an issue or a pull request!

# pytube

*pytube* is a genuine, lightweight, dependency-free Python library (and command-line utility) for downloading YouTube videos.

## Documentation

Detailed documentation about the usage of the library can be found at [pytube.io](https://pytube.io). This is recommended for most "
LaTeX-OCR,"# pix2tex - LaTeX OCR

[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces"
buzz,"# Buzz

[Documentation](https://chidiwilliams.github.io/buzz/) | [Buzz Captions on the App Store](https://apps.apple.com/us/app/buzz-captions/id6446018936?mt=12&itsct=apps_box_badge&itscg=30200)

Transcribe and translate audio offline on your personal computer. Powered by
OpenAI's [Whisper](https://github.com/openai/whisper).

![MIT License](https://img.shields.io/badge/license-MIT-green)
[![CI](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg)](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml)
[![codecov](https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP)](https://codecov.io/github/chidiwilliams/buzz)
![GitHub release (latest by date)](https://img.shields.io/github/v/release/chidiwilliams/buzz)
[![Github all releases](https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg)](https://GitHub.com/chidiwilliams/buzz/releases/)

<blockquote>
<p>Buzz is better on the App Store. Get a Mac-native versio"
MiniCPM-V,"<div align=""center"">

<img src=""./assets/minicpmv.png"" width=""300em"" ></img> 

**A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone**

  <strong>[ä¸­æ–‡](./README_zh.md) |
  English</strong>

Join our <a href=""docs/wechat.md"" target=""_blank""> ğŸ’¬ WeChat</a> | View  MiniCPM-V <a href=""docs/best_practice_summary.md"" target=""_blank""> ğŸ“– best practices</a>


<p align=""center"">
  MiniCPM-V 2.6 <a href=""https://huggingface.co/openbmb/MiniCPM-V-2_6"">ğŸ¤—</a> <a href=""https://huggingface.co/spaces/openbmb/MiniCPM-V-2_6"">ğŸ¤–</a> | MiniCPM-Llama3-V 2.5  <a href=""https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5/"">ğŸ¤—</a> <a href=""https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"">ğŸ¤–</a> |
  <a href=https://arxiv.org/abs/2408.01800>MiniCPM-Llama3-V 2.5 Technical Report</a> 
</p>

</div>


**MiniCPM-V** is a series of end-side multimodal LLMs (MLLMs) designed for vision-language understanding. The models take image, video and text as inputs and provide high-quality text outputs."
taipy,"
![Hactoberfestnew](https://github.com/user-attachments/assets/149a5cee-6af1-4d4e-9c43-6fcf82a9b07e)


<div align=""center"">
  <a href=""https://taipy.io?utm_source=github"" target=""_blank"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/Avaiga/taipy/assets/100117126/509bf101-54c2-4321-adaf-a2af63af9682"">
    <img alt=""Taipy"" src=""https://github.com/Avaiga/taipy/assets/100117126/4df8a733-d8d0-4893-acf0-d24ef9e8b58a"" width=""300"" />
  </picture>
  </a>
</div>

<h1 align=""center"">
Build Python Data & AI web applications
</h1>

<div align=""center"">
From simple pilots to production-ready web applications in no time. <br />
No more compromise on performance, customization, and scalability.
</div>

<br />

<div align=""center"">

**Go beyond existing libraries**

</div>

<p align=""left"">
    <br />
    <a href=""https://docs.taipy.io/en/latest/""><strong>ğŸ“š Explore the docs </strong></a>
    <br />
    <a href=""https://discord.com/invite/SJyz2VJGxV""><strong>  "
pyodide,"<div align=""center"">
  <a href=""https://github.com/pyodide/pyodide"">
  <img src=""./docs/_static/img/pyodide-logo-readme.png"" alt=""Pyodide"">
  </a>
</div>

[![NPM Latest Release](https://img.shields.io/npm/v/pyodide)](https://www.npmjs.com/package/pyodide)
[![PyPI Latest Release](https://img.shields.io/pypi/v/pyodide-py.svg)](https://pypi.org/project/pyodide-py/)
[![Build Status](https://circleci.com/gh/pyodide/pyodide.png)](https://circleci.com/gh/pyodide/pyodide)
[![Documentation Status](https://readthedocs.org/projects/pyodide/badge/?version=stable)](https://pyodide.readthedocs.io/?badge=stable)

Pyodide is a Python distribution for the browser and Node.js based on WebAssembly.

## What is Pyodide?

Pyodide is a port of CPython to WebAssembly/[Emscripten](https://emscripten.org/).

Pyodide makes it possible to install and run Python packages in the browser with
[micropip](https://micropip.pyodide.org/). Any pure
Python package with a wheel available on PyPi is supported. Many package"
numpy-100,"## 100 numpy exercises

[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/rougier/numpy-100/notebooks/100%20Numpy%20exercises.ipynb)

This is a collection of numpy exercises from numpy mailing list, stack overflow, and numpy documentation. I've also created some problems myself to reach the 100 limit. The goal of this collection is to offer a quick reference for both old and new users but also to provide a set of exercises for those who teach. For extended exercises, make sure to read [From Python to NumPy](http://www.labri.fr/perso/nrougier/from-python-to-numpy/).

â†’ [Test them on Binder](http://mybinder.org:/repo/rougier/numpy-100/notebooks/100_Numpy_exercises.ipynb)  
â†’ [Read them on GitHub](100_Numpy_exercises.md)  

Note: markdown and ipython notebook are created programmatically from the source data in `source/exercises.ktx`.
To modify the content of these files, please change the text in the source and run the `generators.py` module with a python
interpreter w"
pgcli,"We stand with Ukraine
---------------------

Ukrainian people are fighting for their country. A lot of civilians, women and children, are suffering. Hundreds were killed and injured, and thousands were displaced.

This is an image from my home town, Kharkiv. This place is right in the old city center.

.. image:: screenshots/kharkiv-destroyed.jpg

Picture by @fomenko_ph (Telegram).

Please consider donating or volunteering.

* https://bank.gov.ua/en/
* https://savelife.in.ua/en/donate/
* https://www.comebackalive.in.ua/donate
* https://www.globalgiving.org/projects/ukraine-crisis-relief-fund/
* https://www.savethechildren.org/us/where-we-work/ukraine
* https://www.facebook.com/donate/1137971146948461/
* https://donate.wck.org/give/393234#!/donation/checkout
* https://atlantaforukraine.com/


A REPL for Postgres
-------------------

|Build Status| |CodeCov| |PyPI| |netlify|

This is a postgres client that does auto-completion and syntax highlighting.

Home Page: http://pgcli.com

MySQL "
cookiecutter-django,"# Cookiecutter Django

[![Build Status](https://img.shields.io/github/actions/workflow/status/cookiecutter/cookiecutter-django/ci.yml?branch=master)](https://github.com/cookiecutter/cookiecutter-django/actions/workflows/ci.yml?query=branch%3Amaster)
[![Documentation Status](https://readthedocs.org/projects/cookiecutter-django/badge/?version=latest)](https://cookiecutter-django.readthedocs.io/en/latest/?badge=latest)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/cookiecutter/cookiecutter-django/master.svg)](https://results.pre-commit.ci/latest/github/cookiecutter/cookiecutter-django/master)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

[![Updates](https://pyup.io/repos/github/cookiecutter/cookiecutter-django/shield.svg)](https://pyup.io/repos/github/cookiecutter/cookiecutter-django/)
[![Join our Discord](https://img.shields.io/badge/Discord-cookiecutter-5865F2?style=flat&logo=discord&logoColor=whi"
PaddleNLP,"**ç®€ä½“ä¸­æ–‡**ğŸ€„ | [EnglishğŸŒ](./README_en.md)

<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/1371212/175816733-8ec25eb0-9af3-4380-9218-27c154518258.png"" align=""middle""  width=""500"" />
</p>

------------------------------------------------------------------------------------------

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-dfd.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleNLP/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleNLP?color=ffa""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleNLP/graphs/contributors""><img src=""https://img.shields.io/github/contributors/PaddlePaddle/PaddleNLP?color=9ea""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleNLP/commits""><img src=""https://img"
LivePortrait,"<h1 align=""center"">LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</h1>

<div align='center'>
    <a href='https://github.com/cleardusk' target='_blank'><strong>Jianzhu Guo</strong></a><sup> 1â€ </sup>&emsp;
    <a href='https://github.com/Mystery099' target='_blank'><strong>Dingyun Zhang</strong></a><sup> 1,2</sup>&emsp;
    <a href='https://github.com/KwaiVGI' target='_blank'><strong>Xiaoqiang Liu</strong></a><sup> 1</sup>&emsp;
    <a href='https://github.com/zzzweakman' target='_blank'><strong>Zhizhou Zhong</strong></a><sup> 1,3</sup>&emsp;
    <a href='https://scholar.google.com.hk/citations?user=_8k1ubAAAAAJ' target='_blank'><strong>Yuan Zhang</strong></a><sup> 1</sup>&emsp;
</div>

<div align='center'>
    <a href='https://scholar.google.com/citations?user=P6MraaYAAAAJ' target='_blank'><strong>Pengfei Wan</strong></a><sup> 1</sup>&emsp;
    <a href='https://openreview.net/profile?id=~Di_ZHANG3' target='_blank'><strong>Di Zhang</strong></a><sup> 1<"
pwntools,"# pwntools - CTF toolkit
![pwntools logo](https://github.com/Gallopsled/pwntools/blob/stable/docs/source/logo.png?raw=true)

[![PyPI](https://img.shields.io/pypi/v/pwntools?style=flat)](https://pypi.python.org/pypi/pwntools/)
[![Docs](https://readthedocs.org/projects/pwntools/badge/?version=stable)](https://docs.pwntools.com/)
[![GitHub Workflow Status (dev)](https://img.shields.io/github/actions/workflow/status/Gallopsled/pwntools/ci.yml?branch=dev&logo=GitHub)](https://github.com/Gallopsled/pwntools/actions/workflows/ci.yml?query=branch%3Adev)
[![Coveralls](https://img.shields.io/coveralls/github/Gallopsled/pwntools/dev?logo=coveralls)](https://coveralls.io/github/Gallopsled/pwntools?branch=dev)
[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)
[![Packaging status](https://img.shields.io/repology/repositories/python:pwntools)](https://repology.org/project/python:pwntools/versions)
[![Discord](https://img.shields.io"
pix2code,"# pix2code
*Generating Code from a Graphical User Interface Screenshot*

[![License](http://img.shields.io/badge/license-APACHE2-blue.svg)](LICENSE.txt)

* A video demo of the system can be seen [here](https://youtu.be/pqKeXkhFA3I)
* The paper is available at [https://arxiv.org/abs/1705.07962](https://arxiv.org/abs/1705.07962)
* Official research page: [https://uizard.io/research#pix2code](https://uizard.io/research#pix2code)

## Abstract
Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).

## Citation

```
@article{beltramelli2017pix2code,
  title={pix2code: Generating Code from "
MOSS,"# MOSS
<p align=""center"" width=""100%"">
<a href=""https://txsun1997.github.io/blogs/moss.html"" target=""_blank""><img src=""https://txsun1997.github.io/images/moss.png"" alt=""MOSS"" style=""width: 50%; min-width: 300px; display: block; margin: auto;""></a>
</p>

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg)](https://github.com/OpenLMLab/MOSS/blob/main/LICENSE)
[![Data License](https://img.shields.io/badge/Data%20License-CC%20BY--NC%204.0-blue.svg)](https://github.com/OpenLMLab/MOSS/blob/main/DATA_LICENSE)
[![Model License](https://img.shields.io/badge/Model%20License-GNU%20AGPL%203.0-red.svg)](https://github.com/OpenLMLab/MOSS/blob/main/MODEL_LICENSE)

[[è®ºæ–‡](https://link.springer.com/article/10.1007/s11633-024-1502-8)][[ä¸­æ–‡ç‰ˆ](https://github.com/OpenLMLab/MOSS/blob/main/README.md)] [[English](https://github.com/OpenLMLab/MOSS/blob/main/README_en.md)] [[å®˜æ–¹å¾®ä¿¡ç¾¤](https://github.com/OpenLMLab/MOSS/blob/main/examples/WeChatGroupQR.jpg)]

## ç›®å½•

- [å¼€æºæ¸…å•](#spira"
pytest,".. image:: https://github.com/pytest-dev/pytest/raw/main/doc/en/img/pytest_logo_curves.svg
   :target: https://docs.pytest.org/en/stable/
   :align: center
   :height: 200
   :alt: pytest


------

.. image:: https://img.shields.io/pypi/v/pytest.svg
    :target: https://pypi.org/project/pytest/

.. image:: https://img.shields.io/conda/vn/conda-forge/pytest.svg
    :target: https://anaconda.org/conda-forge/pytest

.. image:: https://img.shields.io/pypi/pyversions/pytest.svg
    :target: https://pypi.org/project/pytest/

.. image:: https://codecov.io/gh/pytest-dev/pytest/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/pytest-dev/pytest
    :alt: Code coverage Status

.. image:: https://github.com/pytest-dev/pytest/actions/workflows/test.yml/badge.svg
    :target: https://github.com/pytest-dev/pytest/actions?query=workflow%3Atest

.. image:: https://results.pre-commit.ci/badge/github/pytest-dev/pytest/main.svg
   :target: https://results.pre-commit.ci/latest/github/pytest-d"
deepface,"# deepface

<div align=""center"">

[![Downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&units=international_system&left_color=grey&right_color=blue&left_text=downloads)](https://pepy.tech/project/deepface)
[![Stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&style=flat&label=%E2%AD%90%20stars)](https://github.com/serengil/deepface/stargazers)
[![License](http://img.shields.io/:license-MIT-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/LICENSE)
[![Tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)
[![DOI](http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat)](https://doi.org/10.17671/gazibtd.1399077)

[![Blog](https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&logo=wordpress)](https://sefiks.com)
[![YouTube](https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&logo=youtube)](https:/"
dirsearch,"<img src=""static/logo.png#gh-light-mode-only"" alt=""dirsearch logo (light)"" width=""675px"">
<img src=""static/logo-dark.png#gh-dark-mode-only"" alt=""dirsearch logo (dark)"" width=""675px"">

dirsearch - Web path discovery
=========

![Build](https://img.shields.io/badge/Built%20with-Python-Blue)
![License](https://img.shields.io/badge/license-GNU_General_Public_License-_red.svg)
![Stars](https://img.shields.io/github/stars/maurosoria/dirsearch.svg)
[![Release](https://img.shields.io/github/release/maurosoria/dirsearch.svg)](https://github.com/maurosoria/dirsearch/releases)
[![Sponsors](https://img.shields.io/github/sponsors/maurosoria)](https://github.com/sponsors/maurosoria)
[![Discord](https://img.shields.io/discord/992276296669339678.svg?logo=discord)](https://discord.gg/2N22ZdAJRj)
[![Twitter](https://img.shields.io/twitter/follow/_dirsearch?label=Follow)](https://twitter.com/_dirsearch)


> An advanced web path brute-forcer

**dirsearch** is being actively developed by [@maurosoria](http"
activitywatch,"<img title=""ActivityWatch"" src=""https://activitywatch.net/img/banner.png"" align=""center"">

<p align=""center"">
  <b>Records what you do</b> so that you can <i>know how you've spent your time</i>.
  <br>
  All in a secure way where <i>you control the data</i>.
</p>

<p align=""center"">
  <a href=""https://twitter.com/ActivityWatchIt"">
    <img title=""Twitter follow"" src=""https://img.shields.io/twitter/follow/ActivityWatchIt.svg?style=social&label=Follow""/>
  </a>
  <a href=""https://github.com/ActivityWatch/activitywatch"">
    <img title=""Star on GitHub"" src=""https://img.shields.io/github/stars/ActivityWatch/activitywatch.svg?style=social&label=Star"">
  </a>

  <br>

  <b>
    <a href=""https://activitywatch.net/"">Website</a>
    â€” <a href=""https://forum.activitywatch.net/"">Forum</a>
    â€” <a href=""https://docs.activitywatch.net"">Documentation</a>
    â€” <a href=""https://github.com/ActivityWatch/activitywatch/releases"">Releases</a>
  </b>

  <br>

  <b>
    <a href=""https://activitywatch.net/"
tiktoken,"# â³ tiktoken

tiktoken is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokeniser for use with
OpenAI's models.

```python
import tiktoken
enc = tiktoken.get_encoding(""o200k_base"")
assert enc.decode(enc.encode(""hello world"")) == ""hello world""

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model(""gpt-4o"")
```

The open source version of `tiktoken` can be installed from PyPI:
```
pip install tiktoken
```

The tokeniser API is documented in `tiktoken/core.py`.

Example code using `tiktoken` can be found in the
[OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb).


## Performance

`tiktoken` is between 3-6x faster than a comparable open source tokeniser:

![image](https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg)

Performance measured on 1GB of text using the GPT-2 tokeniser, using `GPT2TokenizerFast` from
`tokenizers==0.13.2`, `trans"
Zappa,"Project moved to https://github.com/zappa/Zappa. Versions after 0.52.0 are published from there. Thank you Rich Jones for all your work on creating Zappa and maintaining it for years!
"
neural-enhance,"Neural Enhance
==============

.. image:: docs/OldStation_example.gif

**Example #1** â€” Old Station: `view comparison <http://enhance.nucl.ai/w/0f5177f4-9ce6-11e6-992c-c86000be451f/view>`_ in 24-bit HD, `original photo <https://flic.kr/p/oYhbBv>`_ CC-BY-SA @siv-athens.

----

`As seen on TV! <https://www.youtube.com/watch?v=LhF_56SxrGk>`_ What if you could increase the resolution of your photos using technology from CSI laboratories? Thanks to deep learning and ``#NeuralEnhance``, it's now possible to train a neural network to zoom in to your images at 2x or even 4x.  You'll get even better results by increasing the number of neurons or training with a dataset similar to your low resolution image.

The catch? The neural network is hallucinating details based on its training from example images. It's not reconstructing your photo exactly as it would have been if it was HD. That's only possible in Hollywood â€” but using deep learning as ""Creative AI"" works and it is just as cool!  Here's "
fashion-mnist,"# Fashion-MNIST

[![GitHub stars](https://img.shields.io/github/stars/zalandoresearch/fashion-mnist.svg?style=flat&label=Star)](https://github.com/zalandoresearch/fashion-mnist/)
[![Gitter](https://badges.gitter.im/zalandoresearch/fashion-mnist.svg)](https://gitter.im/fashion-mnist/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link)
[![Readme-CN](https://img.shields.io/badge/README-ä¸­æ–‡-green.svg)](README.zh-CN.md)
[![Readme-JA](https://img.shields.io/badge/README-æ—¥æœ¬èª-green.svg)](README.ja.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Year-In-Review](https://img.shields.io/badge/%F0%9F%8E%82-Year%20in%20Review-orange.svg)](https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/)

<details><summary>Table of Contents</summary><p>

* [Why we made Fashion-MNIST](#why-we-made-fashion-mnist)
* [Get the Data](#get-the-data)
* [Usage](#usage)
* [Benchmark](#benchmark)
* [Visualization](#visualization"
walle-web,"![](https://raw.github.com/meolu/walle-web/master/screenshot/logo.jpg)

Walle 2.0 - [å®˜æ–¹ä¸»é¡µ](https://www.walle-web.io)
=========================
<p align=""left"">
    <a href='https://travis-ci.org/meolu/walle-web'><img src='https://travis-ci.org/meolu/walle-web.svg?branch=master' alt=""Build Status""></a>  
    <a href='https://gitter.im/meolu/walle-web'><img src='https://badges.gitter.im/Join%20Chat.svg'></a>
</p>

**å­—èŠ‚è·³åŠ¨å†…æ¨**ï¼šToB Lark æ‹›è˜å¤§æ•°æ®ç ”å‘ã€æ•°æ®åˆ†æå¸ˆï¼Œæœºä¼šæä½³ï¼Œè¯·å‹¿é”™è¿‡ã€‚è¯·å„ä½æœ‹å‹æ‰©æ•£ä¸‹æœ‰éœ€è¦çš„åŒå­¦ï¼Œ[ç›´è¾¾å†…æ¨é“¾æ¥](https://job.toutiao.com/referral/pc/position/detail/?token=MTsxNTcxMTA2MDM0NTkyOzY3MDQwNDI5MDQ2MTQzMDczMzY7NjcxODk1MDE2MDEzMjQ3NTE0OQ%3D%3D)ï¼Œå¸®åŠ©å†…æ¨ï¼Œå›å­æˆäººä¹‹ç¾ï¼Œè°¢è°¢ã€‚

åŠŸèƒ½å¼ºå¤§ï¼Œä¸”å…è´¹å¼€æºçš„`walle-web ç“¦åŠ›`ç»ˆäºæ›´æ–°`2.0.0`äº†ï¼ï¼ï¼

walle è®©ç”¨æˆ·ä»£ç å‘å¸ƒç»ˆäºå¯ä»¥ä¸åªèƒ½é€‰æ‹© jenkinsï¼æ”¯æŒå„ç§webä»£ç å‘å¸ƒï¼Œphpã€javaã€pythonã€goç­‰ä»£ç çš„å‘å¸ƒã€å›æ»šå¯ä»¥é€šè¿‡webæ¥ä¸€é”®å®Œæˆã€‚walle ä¸€ä¸ªå¯è‡ªç”±é…ç½®é¡¹ç›®ï¼Œæ›´äººæ€§åŒ–ï¼Œé«˜é¢œå€¼ï¼Œæ”¯æŒgitã€å¤šç”¨æˆ·ã€å¤šè¯­è¨€ã€å¤šé¡¹ç›®ã€å¤šç¯å¢ƒåŒæ—¶éƒ¨ç½²çš„å¼€æºä¸Šçº¿éƒ¨ç½²ç³»ç»Ÿã€‚

`2.0.0` å ç”¨äº†æˆ‘å‡ ä¹æ‰€æœ‰ä¸šä½™æ—¶é—´ï¼Œç²¾åŠ›ä¸é‡‘é’±ä»˜å‡ºæ¢å„ä½ä½¿ç”¨æ”¶ç›Šï¼Œæœ›å„ä½å–œæ¬¢ä¸åé¡ºæ‰‹ `star` ä»¥ç¤ºæ”¯æŒï¼Œé¡¹ç›®æ›´å¥½äº¦åé¦ˆäºˆä½ ã€‚ç›®å‰ `2.0.0` å·²ç»å‘å¸ƒï¼Œè¯·ä¿æŒå…³æ³¨ï¼Œæˆ‘ä¼šåœ¨å…¬ä¼—å·æ›´æ–°ï¼ˆåœ¨æœ€ä¸‹é¢ï¼‰ã€‚  


æœ‰æ¨å¹¿èµ„æºï¼ˆå¼€æºæ–‡ç« æ¨èã€å¤§ä¼šåˆ†äº«ï¼‰çš„åŒå­¦ï¼Œè¯·å¾®ä¿¡è”ç³»æˆ‘ï¼Œå¼ºçƒˆéœ€è¦å¸®åŠ©ã€‚å¦å¤–ï¼Œè€ç‰ˆæœ¬å·²è¿ç§»åˆ° [walle 1.x](ht"
abu,"![](./img/head.png)

### ç´¢å¼•

| å†…å®¹ | ä½ç½® | 
| ------| ------ | 
| é˜¿å¸ƒé‡åŒ–ç³»ç»Ÿæºä»£ç  | abupyç›®å½• |
| é˜¿å¸ƒé‡åŒ–ä½¿ç”¨æ•™ç¨‹ | abupy_lectureç›®å½• |
| é˜¿å¸ƒé‡åŒ–éç¼–ç¨‹ç•Œé¢æ“ä½œ | abupy_uiç›®å½• |
| ã€Šé‡åŒ–äº¤æ˜“ä¹‹è·¯ã€‹ç¤ºä¾‹ä»£ç  | ipythonï¼pythonç›®å½•| 
| ã€Šæœºå™¨å­¦ä¹ ä¹‹è·¯ã€‹ç¤ºä¾‹ä»£ç  | https://github.com/maxmon/abu_ml | 


###  ğŸ† [è§ˆå™¨è®¿é—®ç½‘å€: https://www.abuquant.com](https://www.abuquant.com)

* ğŸ‡¨ğŸ‡³ [ä¸Šè¯æŒ‡æ•°å‘¨æŠ¥ç¤ºä¾‹é‡åŒ–åˆ†æ:](https://www.abuquant.com/abu_context/output_cn_week_2023-09-27/report/sh000001/index.html)
* ğŸ‡¨ğŸ‡³ [ä¸Šè¯æŒ‡æ•°æ—¥æŠ¥ç¤ºä¾‹é‡åŒ–åˆ†æ:](https://www.abuquant.com/abu_context/output_cn_day_2023-09-27-10-20-44/report/sh000001/index.html)

* ğŸš© [æ·±è¯æˆæŒ‡å‘¨æŠ¥ç¤ºä¾‹é‡åŒ–åˆ†æ:](https://www.abuquant.com/abu_context/output_cn_week_2023-09-27/report/sz399001/index.html)
* ğŸš© [æ·±è¯æˆæŒ‡æ—¥æŠ¥ç¤ºä¾‹é‡åŒ–åˆ†æ:](https://www.abuquant.com/abu_context/output_cn_day_2023-10-08-14-39-39/report/sz399001/index.html)

* ğŸ‡ºğŸ‡¸ [é“ç¼æ–¯å‘¨æŠ¥ç¤ºä¾‹é‡åŒ–åˆ†æ:](https://www.abuquant.com/abu_context/output_us_week_2023-09-27/report/us.DJI/index.html)
* ğŸ‡ºğŸ‡¸ [é“ç¼æ–¯æ—¥æŠ¥ç¤ºä¾‹é‡åŒ–åˆ†æ:](https://www.abuquant.com/abu_context/output_us_day_2023-09-27-09-39-11/report/us.DJI/index.html)

* ğŸš©"
fail2ban,"                         __      _ _ ___ _               
                        / _|__ _(_) |_  ) |__  __ _ _ _  
                       |  _/ _` | | |/ /| '_ \/ _` | ' \ 
                       |_| \__,_|_|_/___|_.__/\__,_|_||_|
                       v1.1.0.dev1            20??/??/??

## Fail2Ban: ban hosts that cause multiple authentication errors

Fail2Ban scans log files like `/var/log/auth.log` and bans IP addresses conducting
too many failed login attempts. It does this by updating system firewall rules
to reject new connections from those IP addresses, for a configurable amount
of time. Fail2Ban comes out-of-the-box ready to read many standard log files,
such as those for sshd and Apache, and is easily configured to read any log
file of your choosing, for any error you wish.

Though Fail2Ban is able to reduce the rate of incorrect authentication
attempts, it cannot eliminate the risk presented by weak authentication.
Set up services to use only two factor, or public/private a"
developer,"# ğŸ£ smol developer

<a href=""https://app.e2b.dev/agent/smol-developer"" target=""_blank"" rel=""noopener noreferrer"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://app.e2b.dev/api/badge_light"">
  <img alt=""Deploy agent on e2b button"" src=""https://app.e2b.dev/api/badge""/>
</picture>
</a>
<a href=""https://github.com/modal-labs/devlooper""><img src=""https://github.com/smol-ai/developer/assets/6764957/6af16d37-2494-4722-b3a2-6fc91c005451""></img>
</a>
<a href=""https://twitter.com/morph_labs/status/1689321673151979536""><img src=""https://avatars.githubusercontent.com/u/136536927?s=40&v=4"" alt=""Morph""></img> Morph
</a>

***Human-centric & Coherent Whole Program Synthesis*** aka your own personal junior developer

> [Build the thing that builds the thing!](https://twitter.com/swyx/status/1657578738345979905) a `smol dev` for every dev in every situation

This is a ""junior developer"" agent (aka `smol dev`) that either:

1. scaffolds an entire codebase out for you once you g"
synapse,"=========================================================================
Synapse |support| |development| |documentation| |license| |pypi| |python|
=========================================================================

Synapse is now actively maintained at `element-hq/synapse <https://github.com/element-hq/synapse>`_
=================================================================================================

Synapse is an open-source `Matrix <https://matrix.org/>`_ homeserver developed
from 2019 through 2023 as part of the Matrix.org Foundation. The Matrix.org
Foundation is not able to resource maintenance of Synapse and it
`continues to be developed by Element <https://github.com/element-hq/synapse>`_;
additionally you have the choice of `other Matrix homeservers <https://matrix.org/ecosystem/servers/>`_.

See `The future of Synapse and Dendrite <https://matrix.org/blog/2023/11/06/future-of-synapse-dendrite/>`_
blog post for more information.

==============================="
Nuitka,".. image:: https://img.shields.io/pypi/pyversions/Nuitka.svg
   :target: https://pypi.org/project/Nuitka

.. image:: https://badge.fury.io/py/Nuitka.svg
   :target: https://pypi.org/project/Nuitka

.. image:: https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg
   :target: CODE_OF_CONDUCT.md

####################
 Nuitka User Manual
####################

This document is the recommended first read when you start using
**Nuitka**. On this page, you will learn more about **Nuitka**
fundamentals, such as license type, use cases, requirements, and
credits.

.. contents:: Table of Contents
   :depth: 1
   :local:
   :class: page-toc

Nuitka is **the** Python compiler. It is written in Python. It is a
seamless replacement or extension to the Python interpreter and compiles
**every** construct that Python 2 (2.6, 2.7) and Python 3 (3.4 - 3.12)
have, when itself run with that Python version.

It then executes uncompiled code and compiled code together in an
extremely c"
Chinese-Word-Vectors,"# Chinese Word Vectors ä¸­æ–‡è¯å‘é‡
[ä¸­æ–‡](https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md)

This project provides 100+ Chinese Word Vectors (embeddings) trained with different **representations** (dense and sparse), **context features** (word, ngram, character, and more), and **corpora**. One can easily obtain pre-trained vectors with different properties and use them for downstream tasks. 

Moreover, we provide a Chinese analogical reasoning dataset **CA8** and an evaluation toolkit for users to evaluate the quality of their word vectors.

## Reference
Please cite the paper, if using these embeddings and CA8 dataset.

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href=""http://aclweb.org/anthology/P18-2023""><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.

```
@InProceedings{P18-2023,
  author =  ""Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong"",
  tit"
pyinstaller,"PyInstaller Overview
====================

.. image:: https://img.shields.io/pypi/v/pyinstaller
   :alt: PyPI
   :target: https://pypi.org/project/pyinstaller
.. image:: https://img.shields.io/pypi/pyversions/pyinstaller
   :alt: PyPI - Python Version
   :target: https://pypi.org/project/pyinstaller
.. image:: https://img.shields.io/readthedocs/pyinstaller/stable
   :alt: Read the Docs (version)
   :target: https://pyinstaller.org
.. image:: https://img.shields.io/pypi/dm/pyinstaller
   :alt: PyPI - Downloads
   :target: https://pypistats.org/packages/pyinstaller


PyInstaller bundles a Python application and all its dependencies into a single
package. The user can run the packaged app without installing a Python
interpreter or any modules.

:Documentation: https://pyinstaller.org/
:Code:          https://github.com/pyinstaller/pyinstaller

PyInstaller reads a Python script written by you. It analyzes your code
to discover every other module and library your script needs in order to
ex"
schedule,"`schedule <https://schedule.readthedocs.io/>`__
===============================================


.. image:: https://github.com/dbader/schedule/workflows/Tests/badge.svg
        :target: https://github.com/dbader/schedule/actions?query=workflow%3ATests+branch%3Amaster

.. image:: https://coveralls.io/repos/dbader/schedule/badge.svg?branch=master
        :target: https://coveralls.io/r/dbader/schedule

.. image:: https://img.shields.io/pypi/v/schedule.svg
        :target: https://pypi.python.org/pypi/schedule

Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.

- A simple to use API for scheduling jobs, made for humans.
- In-process scheduler for periodic jobs. No extra processes needed!
- Very lightweight and no external dependencies.
- Excellent test coverage.
- Tested on Python and 3.7, 3.8, 3.9, 3.10, 3.11, 3.12

Usage
-----

.. code-block:: bash

    $ pip install schedule

.. code-block:: python

    import schedule"
tutorials,"<p align=""center"">
    <a href=""https://mofanpy.com/tutorials/"" target=""_blank"">
    <img width=""60%"" src=""%E7%89%87%E5%A4%B4.png"" style=""max-width:100%;"">
    </a>
</p>


<br>

æˆ‘æ˜¯ å‘¨æ²«å‡¡, [è«çƒ¦Python](https://mofanpy.com/) åªæ˜¯è°éŸ³, æˆ‘å–œæ¬¢åˆ¶ä½œ,
åˆ†äº«æ‰€å­¦çš„ä¸œè¥¿, æ‰€ä»¥ä½ èƒ½åœ¨è¿™é‡Œæ‰¾åˆ°å¾ˆå¤šæœ‰ç”¨çš„ä¸œè¥¿, å°‘èµ°å¼¯è·¯. ä½ èƒ½åœ¨[è¿™é‡Œ](https://mofanpy.com/about/)æ‰¾åˆ°å…³äºæˆ‘çš„æ‰€æœ‰ä¸œè¥¿.

## è¿™ä¸ª Python tutorial çš„ä¸€äº›å†…å®¹:

* [Python åŸºç¡€](https://mofanpy.com/tutorials/python-basic/)
  * [åŸºç¡€](https://mofanpy.com/tutorials/python-basic/basic/)
  * [å¤šçº¿ç¨‹ threading](https://mofanpy.com/tutorials/python-basic/threading/)
  * [å¤šè¿›ç¨‹ multiprocessing](https://mofanpy.com/tutorials/python-basic/multiprocessing/)
  * [ç®€å•çª—å£ tkinter](https://mofanpy.com/tutorials/python-basic/tkinter/)
* [æœºå™¨å­¦ä¹ ](https://mofanpy.com/tutorials/machine-learning/)
  * [æœ‰è¶£çš„æœºå™¨å­¦ä¹ ](https://mofanpy.com/tutorials/machine-learning/ML-intro/)
  * [å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/)
  * [è¿›åŒ–ç®—æ³• (Evolutionary Algorithm) å¦‚é—ä¼ ç®—æ³•ç­‰](https://mofanpy.com/tut"
MemGPT,"# Letta (previously MemGPT)
[![Discord](https://img.shields.io/discord/1161736243340640419?label=Discord&logo=discord&logoColor=5865F2&style=flat-square&color=5865F2)](https://discord.gg/letta)
[![Twitter Follow](https://img.shields.io/badge/follow-%40Letta_AI-1DA1F2?style=flat-square&logo=x&logoColor=white)](https://twitter.com/Letta_AI)
[![arxiv 2310.08560](https://img.shields.io/badge/arXiv-2310.08560-B31B1B?logo=arxiv&style=flat-square)](https://arxiv.org/abs/2310.08560)

> [!NOTE]
> **Looking for MemGPT?**
>
> The MemGPT package and Docker image have been renamed to `letta` to clarify the distinction between **MemGPT agents** and the API server / runtime that runs LLM agents as *services*.
>
> You use the **Letta framework** to create **MemGPT agents**. Read more about the relationship between MemGPT and Letta [here](https://www.letta.com/blog/memgpt-and-letta).

See [documentation](https://docs.letta.com/introduction) for setup and usage.

## How to Get Involved
* **Contribute to"
allennlp,"<div align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/allenai/allennlp/main/docs/img/allennlp-logo-dark.png"" width=""400""/>
    <p>
    An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.
    </p>
    <hr/>
</div>
<p align=""center"">
    <a href=""https://github.com/allenai/allennlp/actions"">
        <img alt=""CI"" src=""https://github.com/allenai/allennlp/workflows/CI/badge.svg?event=push&branch=main"">
    </a>
    <a href=""https://pypi.org/project/allennlp/"">
        <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/allennlp"">
    </a>
    <a href=""https://github.com/allenai/allennlp/blob/main/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/github/license/allenai/allennlp.svg?color=blue&cachedrop"">
    </a>
    <a href=""https://codecov.io/gh/allenai/allennlp"">
        <img alt=""Codecov"" src=""https://codecov.io/gh/allenai/allennlp/branch/main/graph/badge.s"
SadTalker,"<div align=""center"">

<img src='https://user-images.githubusercontent.com/4397546/229094115-862c747e-7397-4b54-ba4a-bd368bfe2e0f.png' width='500px'/>


<!--<h2> ğŸ˜­ SadTalkerï¼š <span style=""font-size:12px"">Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation </span> </h2> -->

  <a href='https://arxiv.org/abs/2211.12194'><img src='https://img.shields.io/badge/ArXiv-PDF-red'></a> &nbsp; <a href='https://sadtalker.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp; [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb) &nbsp; [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/vinthony/SadTalker) &nbsp; [![sd webui-colab](https://img.shields.io/badge/Automatic1111-Colab-green)](https://colab.research.google.com/github/camenduru/s"
sshuttle,"sshuttle: where transparent proxy meets VPN meets ssh
=====================================================

As far as I know, sshuttle is the only program that solves the following
common case:

- Your client machine (or router) is Linux, FreeBSD, MacOS or Windows.

- You have access to a remote network via ssh.

- You don't necessarily have admin access on the remote network.

- The remote network has no VPN, or only stupid/complex VPN
  protocols (IPsec, PPTP, etc). Or maybe you *are* the
  admin and you just got frustrated with the awful state of
  VPN tools.

- You don't want to create an ssh port forward for every
  single host/port on the remote network.

- You hate openssh's port forwarding because it's randomly
  slow and/or stupid.

- You can't use openssh's PermitTunnel feature because
  it's disabled by default on openssh servers; plus it does
  TCP-over-TCP, which has `terrible performance`_.

.. _terrible performance: https://sshuttle.readthedocs.io/en/stable/how-it-works"
apprise,"![Apprise Logo](https://raw.githubusercontent.com/caronc/apprise/master/apprise/assets/themes/default/apprise-logo.png)

<hr/>

**apÂ·prise** / *verb*<br/>
To inform or tell (someone). To make one aware of something.
<hr/>

*Apprise* allows you to send a notification to *almost* all of the most popular *notification* services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc.

* One notification library to rule them all.
* A common and intuitive notification syntax.
* Supports the handling of images and attachments (_to the notification services that will accept them_).
* It's incredibly lightweight.
* Amazing response times because all messages sent asynchronously.

Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the "
tvm,"<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- ""License""); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<img src=https://raw.githubusercontent.com/apache/tvm-site/main/images/logo/tvm-logo-small.png width=128"
owasp-mastg,"<img width=""180px"" align=""right"" style=""float: right;"" src=""cover.png"">

# OWASP Mobile Application Security Testing Guide (MASTG)

[![OWASP Flagship](https://img.shields.io/badge/owasp-flagship%20project-48A646.svg)](https://owasp.org/projects/)
[![Creative Commons License](https://img.shields.io/github/license/OWASP/owasp-mastg)](https://creativecommons.org/licenses/by-sa/4.0/ ""CC BY-SA 4.0"")

[![Document Build](https://github.com/OWASP/owasp-mastg/workflows/Documents%20Build/badge.svg)](https://github.com/OWASP/owasp-mastg/actions?query=workflow%3A%22Document+Build%22)
[![Markdown Linter](https://github.com/OWASP/owasp-mastg/workflows/Markdown%20Linter/badge.svg)](https://github.com/OWASP/owasp-mastg/actions?query=workflow%3A%22Markdown+Linter%22)
[![URL Checker](https://github.com/OWASP/owasp-mastg/workflows/URL%20Checker/badge.svg)](https://github.com/OWASP/owasp-mastg/actions?query=workflow%3A%22URL+Checker%22)

This is the official GitHub Repository of the OWASP Mobile Applicati"
MinerU,"<div align=""center"" xmlns=""http://www.w3.org/1999/html"">
<!-- logo -->
<p align=""center"">
  <img src=""docs/images/MinerU-logo.png"" width=""300px"" style=""vertical-align:middle;"">
</p>

<!-- icon -->

[![stars](https://img.shields.io/github/stars/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![forks](https://img.shields.io/github/forks/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![open issues](https://img.shields.io/github/issues-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![PyPI version](https://badge.fury.io/py/magic-pdf.svg)](https://badge.fury.io/py/magic-pdf)
[![Downloads](https://static.pepy.tech/badge/magic-pdf)](https://pepy.tech/project/magic-pdf)
[![Downloads](https://static.pepy.tech/badge/magic-pdf/month)](https://pepy.tech/project/magic-pdf)

[![OpenDataLab](https://img.shiel"
scalene,"![scalene](https://github.com/plasma-umass/scalene/raw/master/docs/scalene-icon-white.png)

# Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals

by [Emery Berger](https://emeryberger.com), [Sam Stern](https://samstern.me/), and [Juan Altmayer Pizzorno](https://github.com/jaltmayerpizzorno).

[![Scalene community Slack](https://github.com/plasma-umass/scalene/raw/master/docs/images/slack-logo.png)](https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg)[Scalene community Slack](https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg)

[![PyPI Latest Release](https://img.shields.io/pypi/v/scalene.svg)](https://pypi.org/project/scalene/)[![Anaconda-Server Badge](https://img.shields.io/conda/v/conda-forge/scalene)](https://anaconda.org/conda-forge/scalene) [![Downloads](https://static.pepy.tech/badge/scalene)](https://pepy.tech/project/scalene)[![Anaconda downloads](https://img"
NeMo,"[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)
[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)
[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolki"
playwright-python,"# ğŸ­ [Playwright](https://playwright.dev) for Python [![PyPI version](https://badge.fury.io/py/playwright.svg)](https://pypi.python.org/pypi/playwright/) [![Anaconda version](https://img.shields.io/conda/v/microsoft/playwright)](https://anaconda.org/Microsoft/playwright) [![Join Discord](https://img.shields.io/badge/join-discord-infomational)](https://aka.ms/playwright/discord)

Playwright is a Python library to automate [Chromium](https://www.chromium.org/Home), [Firefox](https://www.mozilla.org/en-US/firefox/new/) and [WebKit](https://webkit.org/) browsers with a single API. Playwright delivers automation that is **ever-green**, **capable**, **reliable** and **fast**. [See how Playwright is better](https://playwright.dev/python).

|          | Linux | macOS | Windows |
|   :---   | :---: | :---: | :---:   |
| Chromium <!-- GEN:chromium-version -->129.0.6668.29<!-- GEN:stop --> | âœ… | âœ… | âœ… |
| WebKit <!-- GEN:webkit-version -->18.0<!-- GEN:stop --> | âœ… | âœ… | âœ… |
| Firefox <!-- GEN:fire"
shap-e,"# Shap-E

This is the official code and model release for [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463).

 * See [Usage](#usage) for guidance on how to use this repository.
 * See [Samples](#samples) for examples of what our text-conditional model can generate.

# Samples

Here are some highlighted samples from our text-conditional model. For random samples on selected prompts, see [samples.md](samples.md).

<table>
    <tbody>
        <tr>
            <td align=""center"">
                <img src=""samples/a_chair_that_looks_like_an_avocado/2.gif"" alt=""A chair that looks like an avocado"">
            </td>
            <td align=""center"">
                <img src=""samples/an_airplane_that_looks_like_a_banana/3.gif"" alt=""An airplane that looks like a banana"">
            </td align=""center"">
            <td align=""center"">
                <img src=""samples/a_spaceship/0.gif"" alt=""A spaceship"">
            </td>
        </tr>
        <tr>
        "
whisperX,"<h1 align=""center"">WhisperX</h1>

<p align=""center"">
  <a href=""https://github.com/m-bain/whisperX/stargazers"">
    <img src=""https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&colorB=orange&logo=github""
         alt=""GitHub stars"">
  </a>
  <a href=""https://github.com/m-bain/whisperX/issues"">
        <img src=""https://img.shields.io/github/issues/m-bain/whisperx.svg""
             alt=""GitHub issues"">
  </a>
  <a href=""https://github.com/m-bain/whisperX/blob/master/LICENSE"">
        <img src=""https://img.shields.io/github/license/m-bain/whisperX.svg""
             alt=""GitHub license"">
  </a>
  <a href=""https://arxiv.org/abs/2303.00747"">
        <img src=""http://img.shields.io/badge/Arxiv-2303.00747-B31B1B.svg""
             alt=""ArXiv paper"">
  </a>
  <a href=""https://twitter.com/intent/tweet?text=&url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX"">
  <img src=""https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social"" alt=""Twitter"">
  </a>  "
faster-whisper,"[![CI](https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg)](https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI) [![PyPI version](https://badge.fury.io/py/faster-whisper.svg)](https://badge.fury.io/py/faster-whisper)

# Faster Whisper transcription with CTranslate2

**faster-whisper** is a reimplementation of OpenAI's Whisper model using [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models.

This implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.

## Benchmark

### Whisper

For reference, here's the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7tTptBo9I) of audio using different implementations:

* [openai/whisper](https://github.com/openai/whisper)@[6dea21fd](http"
storm,"<p align=""center"">
  <img src=""assets/logo.svg"" style=""width: 25%; height: auto;"">
</p>

# STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking

<p align=""center"">
| <a href=""http://storm.genie.stanford.edu""><b>Research preview</b></a> | <a href=""https://arxiv.org/abs/2402.14207""><b>STORM Paper</b></a>| <a href=""https://www.arxiv.org/abs/2408.15232""><b>Co-STORM Paper</b></a>  | <a href=""https://storm-project.stanford.edu/""><b>Website</b></a> |
</p>

**Latest News** ğŸ”¥

- [2024/09] Co-STORM codebase is now released and integrated into `knowledge-storm` python package v1.0.0. Run `pip install knowledge-storm --upgrade` to check it out.

- [2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! [Co-STORM Paper](https://www.arxiv.org/abs/2408.15232) has been accepted to EMNLP 2024 main conference.

- [2024/07] You can now install our package with `pip install knowledge-storm`!
- [2024/07] We add `Vecto"
QAnything,"<div align=""center"">

  <a href=""https://github.com/netease-youdao/QAnything"">
    <!-- Please provide path to your logo here -->
    <img src=""docs/images/qanything_logo.png"" alt=""Logo"" width=""800"">
  </a>

# **Q**uestion and **A**nswer based on **Anything**

<p align=""center"">
  <a href=""./README.md"">English</a> |
  <a href=""./README_zh.md"">ç®€ä½“ä¸­æ–‡</a>
</p>

</div>

<div align=""center"">

<a href=""https://qanything.ai""><img src=""https://img.shields.io/badge/try%20online-qanything.ai-purple""></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://read.youdao.com#/home""><img src=""https://img.shields.io/badge/try%20online-read.youdao.com-purple""></a>
&nbsp;&nbsp;&nbsp;&nbsp;

<a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-AGPL--3.0-yellow""></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://github.com/netease-youdao/QAnything/pulls""><img src=""https://img.shields.io/badge/PRs-welcome-red""></a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://twitter.com/YDopensource""><img src=""https://img.shi"
gallery-dl,"==========
gallery-dl
==========

*gallery-dl* is a command-line program
to download image galleries and collections
from several image hosting sites
(see `Supported Sites <docs/supportedsites.md>`__).
It is a cross-platform tool
with many `configuration options <https://gdl-org.github.io/docs/configuration.html>`__
and powerful `filenaming capabilities <https://gdl-org.github.io/docs/formatting.html>`__.


|pypi| |build|

.. contents::


Dependencies
============

- Python_ 3.4+
- Requests_

Optional
--------

- yt-dlp_ or youtube-dl_: HLS/DASH video downloads, ``ytdl`` integration
- FFmpeg_: Pixiv Ugoira conversion
- mkvmerge_: Accurate Ugoira frame timecodes
- PySocks_: SOCKS proxy support
- brotli_ or brotlicffi_: Brotli compression support
- zstandard_: Zstandard compression support
- PyYAML_: YAML configuration file support
- toml_: TOML configuration file support for Python<3.11
- SecretStorage_: GNOME keyring passwords for ``--cookies-from-browser``


Installation
============
"
mycli,"# mycli

[![Build Status](https://github.com/dbcli/mycli/workflows/mycli/badge.svg)](https://github.com/dbcli/mycli/actions?query=workflow%3Amycli)

A command line client for MySQL that can do auto-completion and syntax highlighting.

HomePage: [http://mycli.net](http://mycli.net)
Documentation: [http://mycli.net/docs](http://mycli.net/docs)

![Completion](screenshots/tables.png)
![CompletionGif](screenshots/main.gif)

Postgres Equivalent: [http://pgcli.com](http://pgcli.com)

Quick Start
-----------

If you already know how to install python packages, then you can install it via pip:

You might need sudo on linux.

```
$ pip install -U mycli
```

or

```
$ brew update && brew install mycli  # Only on macOS
```

or

```
$ sudo apt-get install mycli # Only on debian or ubuntu
```

### Usage

    $ mycli --help
    Usage: mycli [OPTIONS] [DATABASE]

      A MySQL terminal client with auto-completion and syntax highlighting.

      Examples:
        - mycli my_database
        - mycli -u "
External-Attention-pytorch,"
<img src=""./FightingCVimg/LOGO.gif"" height=""200"" width=""400""/>

ç®€ä½“ä¸­æ–‡ | [English](./README_EN.md)

# FightingCV ä»£ç åº“ï¼Œ åŒ…å« [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)

![](https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen)
![](https://img.shields.io/badge/python->=v3.0-blue)
![](https://img.shields.io/badge/pytorch->=v1.4-red)

<!--
-------
*If this project is helpful to you, welcome to give a ***star***.* 

*Don't forget to ***follow*** me to learn about project updates.*

-->




Helloï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°é©¬ğŸš€ğŸš€ğŸš€

***For å°ç™½ï¼ˆLike Meï¼‰ï¼š***
æœ€è¿‘åœ¨è¯»è®ºæ–‡çš„æ—¶å€™ä¼šå‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œæœ‰æ—¶å€™è®ºæ–‡æ ¸å¿ƒæ€æƒ³éå¸¸ç®€å•ï¼Œæ ¸å¿ƒä»£ç å¯èƒ½ä¹Ÿå°±åå‡ è¡Œã€‚ä½†æ˜¯æ‰“å¼€ä½œè€…releaseçš„æºç æ—¶ï¼Œå´å‘ç°æå‡ºçš„æ¨¡å—åµŒå…¥åˆ°åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡æ¡†æ¶ä¸­ï¼Œå¯¼è‡´ä»£ç æ¯”è¾ƒå†—ä½™ï¼Œå¯¹äºç‰¹å®šä»»åŠ¡æ¡†æ¶ä¸ç†Ÿæ‚‰çš„æˆ‘ï¼Œ**å¾ˆéš¾æ‰¾åˆ°æ ¸å¿ƒä»£ç **ï¼Œå¯¼è‡´åœ¨è®ºæ–‡å’Œç½‘ç»œæ€æƒ³çš„ç†è§£ä¸Šä¼šæœ‰ä¸€å®šå›°éš¾ã€‚

***For è¿›é˜¶è€…ï¼ˆLike Youï¼‰ï¼š***
å¦‚æœæŠŠConvã€FCã€RNNè¿™äº›åŸºæœ¬å•å…ƒçœ‹åšå°çš„Legoç§¯æœ¨ï¼ŒæŠŠTransformerã€ResNetè¿™äº›ç»“æ„çœ‹æˆå·²ç»æ­å¥½çš„LegoåŸå ¡ã€‚é‚£ä¹ˆæœ¬é¡¹ç›®æä¾›çš„æ¨¡å—å°±æ˜¯ä¸€ä¸ªä¸ªå…·æœ‰å®Œæ•´è¯­ä¹‰ä¿¡æ¯çš„Legoç»„ä»¶ã€‚**è®©ç§‘ç ”å·¥ä½œè€…ä»¬é¿å…åå¤é€ è½®å­**ï¼Œåªéœ€æ€è€ƒå¦‚ä½•åˆ©ç”¨è¿™äº›â€œLegoç»„ä»¶â€ï¼Œæ­å»ºå‡ºæ›´å¤šç»šçƒ‚å¤šå½©çš„ä½œå“ã€‚

***F"
deep_learning_object_detection,"# deep learning object detection
A paper list of object detection using deep learning. I wrote this page with reference to [this survey paper](https://arxiv.org/pdf/1809.02165v1.pdf) and searching and searching.. 

*Last updated: 2020/09/22*

#### Update log
*2018/9/18* - update all of recent papers and make some diagram about history of object detection using deep learning. 
*2018/9/26* - update codes of papers. (official and unofficial)  
*2018/october* - update 5 papers and performance table.  
*2018/november* - update 9 papers.  
*2018/december* - update 8 papers and and performance table and add new diagram(**2019 version!!**).  
*2019/january* - update 4 papers and and add commonly used datasets.  
*2019/february* - update 3 papers.  
*2019/march* - update figure and code links.  
*2019/april* - remove author's names and update ICLR 2019 & CVPR 2019 papers.  
*2019/may* - update CVPR 2019 papers.  
*2019/june* - update CVPR 2019 papers and dataset paper.  
*2019/july* - update BM"
CustomTkinter,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""./documentation_images/CustomTkinter_logo_dark.png"">
    <img src=""./documentation_images/CustomTkinter_logo_light.png"">
  </picture>
</p>

<div align=""center"">

![PyPI](https://img.shields.io/pypi/v/customtkinter)
![PyPI - Downloads](https://img.shields.io/pypi/dm/customtkinter?color=green&label=downloads)
![Downloads last 6 month](https://static.pepy.tech/personalized-badge/customtkinter?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads%20last%206%20month)
![PyPI - License](https://img.shields.io/badge/license-MIT-blue)
![LOC](https://tokei.rs/b1/github/tomschimansky/customtkinter?category=lines)

</div>

---

<div align=""center"">
<a href=""https://www.paypal.com/donate/?hosted_button_id=LK5QAZYRN2R2A""><img src=""documentation_images/paypal_donate_button.png"" width=170 alt=""Paypal donation button""></a>

<a></a>

| Massive Thanks to all the People who Donat"
Open-Sora-Plan,"

<h1 align=""left""> <a href="""">Open-Sora Plan</a></h1>

This project aims to create a simple and scalable repo, to reproduce [Sora](https://openai.com/sora) (OpenAI, but we prefer to call it ""ClosedAI"" ). We wish the open-source community can contribute to this project. Pull requests are welcome! The current code supports complete training and inference using the Huawei Ascend AI computing system. Models trained on Huawei Ascend can also output video quality comparable to industry standards.

æœ¬é¡¹ç›®å¸Œæœ›é€šè¿‡å¼€æºç¤¾åŒºçš„åŠ›é‡å¤ç°Soraï¼Œç”±åŒ—å¤§-å…”å±•AIGCè”åˆå®éªŒå®¤å…±åŒå‘èµ·ï¼Œå½“å‰ç‰ˆæœ¬ç¦»ç›®æ ‡å·®è·ä»ç„¶è¾ƒå¤§ï¼Œä»éœ€æŒç»­å®Œå–„å’Œå¿«é€Ÿè¿­ä»£ï¼Œæ¬¢è¿Pull requestï¼ç›®å‰ä»£ç åŒæ—¶æ”¯æŒä½¿ç”¨å›½äº§AIè®¡ç®—ç³»ç»Ÿï¼ˆåä¸ºæ˜‡è…¾ï¼‰è¿›è¡Œå®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†ã€‚åŸºäºæ˜‡è…¾è®­ç»ƒå‡ºçš„æ¨¡å‹ï¼Œä¹Ÿå¯è¾“å‡ºæŒå¹³ä¸šç•Œçš„è§†é¢‘è´¨é‡ã€‚

<h5 align=""left"">

[![slack badge](https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp)](https://discord.gg/FkFm5M2J)
[![WeChat badge](https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-green?logo=wechat&amp)](https://github.com/PKU-YuanGroup/Open-Sora-Plan/issues/53#issuecomment-1987226516)
[![Twitter](https://img.shields.io/badge/-Twitter@LinBin46984-b"
h2ogpt,"# h2oGPT

Turn â˜… into â­ (top-right corner) if you like the project!

Query and summarize your documents or just chat with local private GPT LLMs using h2oGPT, an Apache V2 open-source project.

Check out a long CoT Open-o1 open ğŸ“strawberryğŸ“ project: https://github.com/pseudotensor/open-strawberry

## Live Demo

[![img-small.png](docs/img-small.png) Gradio Demo](https://gpt.h2o.ai/)

[![img-small.png](docs/img-small.png) OpenWebUI Demo](https://gpt-docs.h2o.ai/)

## Video Demo

https://github.com/h2oai/h2ogpt/assets/2249614/2f805035-2c85-42fb-807f-fd0bca79abc6

[![img-small.png](docs/img-small.png) YouTube 4K Video](https://www.youtube.com/watch?v=_iktbj4obAI)

## Features

- **Private** offline database of any documents [(PDFs, Excel, Word, Images, Video Frames, YouTube, Audio, Code, Text, MarkDown, etc.)](docs/README_LangChain.md#supported-datatypes)
  - **Persistent** database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)
"
gorilla,"# Gorilla: Large Language Model Connected with Massive APIs [[Project Website](https://shishirpatil.github.io/gorilla/)]


<img src=""https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png"" width=50% height=50%>

**ğŸš’  GoEx: A Runtime for executing LLM generated actions like code & API calls** GoEx presents â€œundoâ€ and â€œdamage confinementâ€ abstractions for mitigating the risk of unintended actions taken in LLM-powered systems. [Release blog](https://gorilla.cs.berkeley.edu/blogs/10_gorilla_exec_engine.html) [Paper](https://arxiv.org/abs/2404.06921).

**ğŸ‰ Berkeley Function Calling Leaderboard** How do models stack up for function calling? :dart: Releasing the [Berkeley Function Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard). Read more in our [Release Blog](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html). 

**:trophy: Gorilla OpenFunctions v2** Sets new SoTA for open-source LLMs :muscle: On-par with GPT-4 :raised_hand"
dagster,"<div align=""center"">
  <!-- Note: Do not try adding the dark mode version here with the `picture` element, it will break formatting in PyPI -->
  <a target=""_blank"" href=""https://dagster.io"" style=""background:none"">
    <img alt=""dagster logo"" src=""https://raw.githubusercontent.com/dagster-io/dagster/master/.github/dagster-readme-header.svg"" width=""auto"" height=""100%"">
  </a>
  <a target=""_blank"" href=""https://github.com/dagster-io/dagster"" style=""background:none"">
    <img src=""https://img.shields.io/github/stars/dagster-io/dagster?labelColor=4F43DD&color=163B36&logo=github"">
  </a>
  <a target=""_blank"" href=""https://github.com/dagster-io/dagster/blob/master/LICENSE"" style=""background:none"">
    <img src=""https://img.shields.io/badge/License-Apache_2.0-blue.svg?label=license&labelColor=4F43DD&color=163B36"">
  </a>
  <a target=""_blank"" href=""https://pypi.org/project/dagster/"" style=""background:none"">
    <img src=""https://img.shields.io/pypi/v/dagster?labelColor=4F43DD&color=163B36"">
 "
phidata,"<h1 align=""center"">
  phidata
</h1>

<h3 align=""center"">
Build AI Assistants with memory, knowledge and tools
</h3>

![image](https://github.com/phidatahq/phidata/assets/22579644/295187f6-ac9d-41e0-abdb-38e3291ad1d1)

## What is phidata?

**Phidata is a framework for building Autonomous Assistants** (aka Agents) that have long-term memory, contextual knowledge and the ability to take actions using function calling.

Use phidata to turn any LLM into an AI Assistant that can:
- **Search the web** using DuckDuckGo, Google etc.
- **Analyze data** using SQL, DuckDb, etc.
- **Conduct research** and generate reports.
- **Answer questions** from PDFs, APIs, etc.
- **Write scripts** for movies, books, etc.
- **Summarize** articles, videos, etc.
- **Perform tasks** like sending emails, querying databases, etc.
- **And much more...**

## Why phidata?

**Problem:** We need to turn general-purpose LLMs into specialized assistants for our use-case.

**Solution:** Extend LLMs with memory, knowledge a"
theZoo,"# theZoo - A Live Malware Repository

[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=round)](https://github.com/ytisf/theZoo/issues)
[![HitCount](http://hits.dwyl.com/ytisf/theZoo.svg)](http://hits.dwyl.com/ytisf/theZoo)
[![GitHub stars](https://img.shields.io/github/stars/ytisf/theZoo.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/ytisf/theZoo/stargazers/)
[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)

![Logo](https://github.com/ytisf/theZoo/raw/gh-pages/MalDB-Logo-Thumb.png)

theZoo is a project created to make the possibility of malware analysis open and available to the public. Since we have found out that almost all versions of malware are very hard to come by in a way which will allow analysis, we have decided to gather all of them for you in an accessible and safe way.
theZoo was born by Yuval tisf Nativ and is now maintained by Shahak Shalev.

**theZ"
theHarvester,"![theHarvester](https://github.com/laramies/theHarvester/blob/master/theHarvester-logo.webp)

![TheHarvester CI](https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg) ![TheHarvester Docker Image CI](https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg)
[![Rawsec's CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg)](https://inventory.raw.pm/)

What is this?
-------------
theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red<br>
team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine<br>
a domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using<br>
multiple public resources that include:<br>

Passive modules:
----------------
* anubis: Anubis-DB - https://github.com/jonluca/anubis

* bevigil: Clou"
FastPhotoStyle,"[![License CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC4.0-blue.svg)](https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/LICENSE.md)
![Python 2.7](https://img.shields.io/badge/python-2.7-green.svg)
![Python 3.5](https://img.shields.io/badge/python-3.5-green.svg)

## FastPhotoStyle

### License
Copyright (C) 2018 NVIDIA Corporation.  All rights reserved.
Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

<img src=""https://raw.githubusercontent.com/NVIDIA/FastPhotoStyle/master/teaser.png"" width=""800"" title=""Teaser results""> 


### What's new
 
 | Date     | News |
 |----------|--------------|
 |2018-07-25| Migrate to pytorch 0.4.0. For pytorch 0.3.0 user, check out [FastPhotoStyle for pytorch 0.3.0](https://github.com/NVIDIA/FastPhotoStyle/releases/tag/f33e07f). |
 |          | Add a [tutorial](TUTORIAL.md) showing 3 ways of using the FastPhotoStyle algorithm.|
 |2018-07-10| Our paper is accepted by the ECCV "
ludwig,"<p align=""center"">
  <a href=""https://ludwig.ai"">
    <img src=""https://github.com/ludwig-ai/ludwig-docs/raw/master/docs/images/ludwig_hero_smaller.jpg"" height=""150"">
  </a>
</p>

<div align=""center"">

_Declarative deep learning framework built for scale and efficiency._

[![PyPI version](https://badge.fury.io/py/ludwig.svg)](https://badge.fury.io/py/ludwig)
[![Discord](https://dcbadge.vercel.app/api/server/CBgdrGnZjy?style=flat&theme=discord-inverted)](https://discord.gg/CBgdrGnZjy)
[![DockerHub](https://img.shields.io/docker/pulls/ludwigai/ludwig.svg)](https://hub.docker.com/r/ludwigai)
[![Downloads](https://pepy.tech/badge/ludwig)](https://pepy.tech/project/ludwig)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ludwig-ai/ludwig/blob/master/LICENSE)
[![X](https://img.shields.io/twitter/follow/ludwig_ai.svg?style=social&logo=twitter)](https://twitter.com/ludwig_ai)

</div>

> \[!IMPORTANT\]
> Our community has moved to [Discord](https://dis"
peewee,".. image:: https://media.charlesleifer.com/blog/photos/peewee3-logo.png

peewee
======

Peewee is a simple and small ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use.

* a small, expressive ORM
* python 2.7+ and 3.4+
* supports sqlite, mysql, mariadb, postgresql and cockroachdb
* tons of `extensions <http://docs.peewee-orm.com/en/latest/peewee/playhouse.html>`_

New to peewee? These may help:

* `Quickstart <http://docs.peewee-orm.com/en/latest/peewee/quickstart.html#quickstart>`_
* `Example twitter app <http://docs.peewee-orm.com/en/latest/peewee/example.html>`_
* `Using peewee interactively <http://docs.peewee-orm.com/en/latest/peewee/interactive.html>`_
* `Models and fields <http://docs.peewee-orm.com/en/latest/peewee/models.html>`_
* `Querying <http://docs.peewee-orm.com/en/latest/peewee/querying.html>`_
* `Relationships and joins <http://docs.peewee-orm.com/en/latest/peewee/relationships.html>`_

Examples
--------

Defining models is similar "
Meshroom,"# ![Meshroom - 3D Reconstruction Software](/docs/logo/banner-meshroom.png)

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2997/badge)](https://bestpractices.coreinfrastructure.org/projects/2997)

Meshroom is a free, open-source 3D Reconstruction Software based on the [AliceVision](https://github.com/alicevision/AliceVision) Photogrammetric Computer Vision framework.

Learn more details about the pipeline on [AliceVision website](http://alicevision.github.io).

See [results of the pipeline on sketchfab](http://sketchfab.com/AliceVision).

Continuous integration:
* Windows: [![Build status](https://ci.appveyor.com/api/projects/status/25sd7lfr3v0rnvni/branch/develop?svg=true)](https://ci.appveyor.com/project/AliceVision/meshroom/branch/develop)
* Linux: [![Build Status](https://travis-ci.org/alicevision/meshroom.svg?branch=develop)](https://travis-ci.org/alicevision/meshroom)


## Photogrammetry

Photogrammetry is the science of making measurements from phot"
DALLE2-pytorch,"<img src=""./dalle2.png"" width=""450px""></img>

## DALL-E 2 - Pytorch

Implementation of <a href=""https://openai.com/dall-e-2/"">DALL-E 2</a>, OpenAI's updated text-to-image synthesis neural network, in Pytorch.

<a href=""https://youtu.be/RJwPN4qNi_Y?t=555"">Yannic Kilcher summary</a> | <a href=""https://www.youtube.com/watch?v=F1X4fHzF4mQ"">AssemblyAI explainer</a>

The main novelty seems to be an extra layer of indirection with the prior network (whether it is an autoregressive transformer or a diffusion network), which predicts an image embedding based on the text embedding from CLIP. Specifically, this repository will only build out the diffusion prior network, as it is the best performing variant (but which incidentally involves a causal transformer as the denoising network ğŸ˜‚)

This model is SOTA for text-to-image for now.

Please join <a href=""https://discord.gg/xBPBXfcFHd""><img alt=""Join us on Discord"" src=""https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&lo"
ml-engineering,"# Machine Learning Engineering Open Book

This is an open collection of methodologies, tools and step by step instructions to help with successful training of large language models and multi-modal models.

This is a technical material suitable for LLM/VLM training engineers and operators. That is the content here contains lots of scripts and copy-n-paste commands to enable you to quickly address your needs.

This repo is an ongoing brain dump of my experiences training Large Language Models (LLM) (and VLMs); a lot of the know-how I acquired while training the open-source [BLOOM-176B](https://huggingface.co/bigscience/bloom) model in 2022 and [IDEFICS-80B](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) multi-modal model in 2023. Currently, I'm working on developing/training open-source Retrieval Augmented Generation (RAG) models at [Contextual.AI](https://contextual.ai/).

I've been compiling this information mostly for myself so that I could quickly find solutions I have al"
gdb-dashboard,"# GDB dashboard

GDB dashboard is a standalone `.gdbinit` file written using the [Python API][] that enables a modular interface showing relevant information about the program being debugged. Its main goal is to reduce the number of GDB commands needed to inspect the status of current program thus allowing the developer to primarily focus on the control flow.

![Screenshot](https://raw.githubusercontent.com/wiki/cyrus-and/gdb-dashboard/Screenshot.png)

[Python API]: https://sourceware.org/gdb/onlinedocs/gdb/Python-API.html

## Quickstart

Just place [`.gdbinit`][] in your home directory, for example with:

```
wget -P ~ https://github.com/cyrus-and/gdb-dashboard/raw/master/.gdbinit
```

Optionally install [Pygments][] to enable syntax highlighting:

```
pip install pygments
```

Then debug as usual, the dashboard will appear automatically every time the inferior program stops.

Keep in mind that no GDB command has been redefined, instead all the features are available via the main `das"
borg,"This is borg2!
--------------

Please note that this is the README for borg2 / master branch.

For the stable version's docs, please see there:

https://borgbackup.readthedocs.io/en/stable/

Borg2 is currently in beta testing and might get major and/or
breaking changes between beta releases (and there is no beta to
next-beta upgrade code, so you will have to delete and re-create repos).

Thus, **DO NOT USE BORG2 FOR YOUR PRODUCTION BACKUPS!** Please help with
testing it, but set it up *additionally* to your production backups.

TODO: the screencasts need a remake using borg2, see there:

https://github.com/borgbackup/borg/issues/6303


What is BorgBackup?
-------------------

BorgBackup (short: Borg) is a deduplicating backup program.
Optionally, it supports compression and authenticated encryption.

The main goal of Borg is to provide an efficient and secure way to back up data.
The data deduplication technique used makes Borg suitable for daily backups
since only changes are stored.
"
Statistical-Learning-Method_Code,"## ã€å¹¿å‘Šã€‘æ¯æ—¥Arxivï¼ˆä¸­æ–‡ç‰ˆï¼‰
æ¯æ—¥Arxivï¼ˆä¸­æ–‡ç‰ˆï¼‰ç«‹å¿—paper**æ±‰åŒ–**ï¼Œç›®å‰ç¿»è¯‘ç›®å‰æ¶µç›–**æ ‡é¢˜**å’Œ**æ‘˜è¦**ï¼ŒAIå­¦ç§‘è¿‘æœŸæ”¯æŒè®ºæ–‡**å…¨æ–‡æ±‰åŒ–**

ä¸€å¤©é˜…è¯»ç™¾ç¯‡paperä¸æ˜¯æ¢¦ï¼

é“¾æ¥ï¼š [å­¦æœ¯å··å­(xueshuxiangzi.com)](https://www.xueshuxiangzi.com/)


å‰è¨€
====

åŠ›æ±‚æ¯è¡Œä»£ç éƒ½æœ‰æ³¨é‡Šï¼Œé‡è¦éƒ¨åˆ†æ³¨æ˜å…¬å¼æ¥æºã€‚å…·ä½“ä¼šè¿½æ±‚ä¸‹æ–¹è¿™æ ·çš„ä»£ç ï¼Œå­¦ä¹ è€…å¯ä»¥ç…§ç€å…¬å¼çœ‹ç¨‹åºï¼Œè®©ä»£ç æœ‰æ®å¯æŸ¥ã€‚

![image](https://github.com/Dod-o/Statistical-Learning-Method_Code/blob/master/CodePic.png)

    
å¦‚æœæ—¶é—´å……æ²›çš„è¯ï¼Œå¯èƒ½ä¼šè¯•ç€ç»™æ¯ä¸€ç« å†™ä¸€ç¯‡åšå®¢ã€‚å…ˆæ”¾ä¸ªåšå®¢é“¾æ¥å§ï¼š[ä¼ é€é—¨](http://www.pkudodo.com/)ã€‚    

##### æ³¨ï¼šå…¶ä¸­Mnistæ•°æ®é›†å·²è½¬æ¢ä¸ºcsvæ ¼å¼ï¼Œç”±äºä½“ç§¯ä¸º107Mè¶…è¿‡é™åˆ¶ï¼Œæ”¹ä¸ºå‹ç¼©åŒ…å½¢å¼ã€‚ä¸‹è½½ååŠ¡å¿…å…ˆå°†Mnistæ–‡ä»¶å†…å‹ç¼©åŒ…ç›´æ¥è§£å‹ã€‚  

### ã€Updatesã€‘
**ä¹¦ç±å‡ºç‰ˆ**ï¼šç›®å‰å·²ä¸**äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾**ç­¾è®¢åˆåŒï¼Œæœªæ¥å°†ç»“åˆè¯¥repoæ•´ç†å‡ºç‰ˆæœºå™¨å­¦ä¹ å®è·µç›¸å…³ä¹¦ç±ã€‚åŒæ—¶ä¼šåœ¨bookåˆ†æ”¯ä¸­å¯¹ä»£ç è¿›è¡Œé‡æ„ï¼Œæ¬¢è¿åœ¨issueä¸­æå»ºè®®ï¼åŒæ—¶issueä¸­ç°æœ‰çš„é—®é¢˜ä¹Ÿä¼šè€ƒè™‘è¿›å»ã€‚ï¼ˆFeb 12 2022ï¼‰

**çº¿ä¸‹åŸ¹è®­**ï¼šå¥³æœ‹å‹è®¡åˆ’è¿‘æœŸå¼€åŠ**ML/MLP/CVçº¿ä¸‹åŸ¹è®­ç­**ï¼Œåœ°ç‚¹**åŒ—ä¸Šå¹¿æ·±æ­**ï¼Œç›®æ ‡å„æ–¹å‘**å¿«é€Ÿå…¥é—¨**ï¼Œæ­£åœ¨ç­¹å¤‡ã€‚è¿™é‡Œå¸®å¥¹æ‰“ä¸ªå¹¿å‘Šï¼Œå¯ä»¥æ·»åŠ å¾®ä¿¡15324951814ï¼ˆå¤‡æ³¨çº¿ä¸‹åŸ¹è®­ï¼‰ã€‚æœ¬äººä¹Ÿä¼šè¢«æ‹‰è¿‡å»ä¹‰åŠ¡è¯„ä¼°è¯¾ç¨‹è´¨é‡ã€‚ã€‚ã€‚ï¼ˆFeb 12 2022ï¼‰

**æ— ç›‘ç£éƒ¨åˆ†æ›´æ–°**ï¼šéƒ¨åˆ†**æ— ç›‘ç£**ç®—æ³•å·²æ›´æ–°ï¼ï¼ï¼ è¯¥éƒ¨åˆ†ç”±[Harold-Ran](https://github.com/Harold-Ran)æä¾›ï¼Œåœ¨æ­¤æ„Ÿè°¢ï¼ æœ‰å…¶ä»–ç®—æ³•è¡¥å……çš„åŒå­¦ä¹Ÿæ¬¢è¿æ·»åŠ æˆ‘å¾®ä¿¡å¹¶prï¼ï¼ˆJan 27 2021ï¼‰
       
å®ç°
======

## ç›‘ç£éƒ¨åˆ†

### ç¬¬äºŒç«  æ„ŸçŸ¥æœºï¼š
åšå®¢ï¼š[ç»Ÿè®¡å­¦ä¹ æ–¹æ³•|æ„ŸçŸ¥æœºåŸç†å‰–æåŠå®ç°](http://www.pkudodo.com/2018/11/18/1-4/)      
å®ç°ï¼š[perceptron/"
PaddleSpeech,"([ç®€ä½“ä¸­æ–‡](./README_cn.md)|English)
<p align=""center"">
  <img src=""./docs/images/PaddleSpeech_logo.png"" />
</p>

<p align=""center"">
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache%202-red.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/releases""><img src=""https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa""></a>
    <a href=""support os""><img src=""https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg""></a>
    <a href=""""><img src=""https://img.shields.io/badge/python-3.7+-aff.svg""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors""><img src=""https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/commits""><img src=""https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af""></a>
    <a href=""https://github.com/PaddlePaddle/PaddleSpeech/issues""><img src=""https://img."
flet,"# Flet

<img src=""media/logo/flet-logo.svg"" width=""50%""/>

[![Build status](https://ci.appveyor.com/api/projects/status/xwablctxslvey576/branch/main?svg=true)](https://ci.appveyor.com/project/flet-dev/flet/branch/main)

Flet is a framework that enables you to easily build real-time web, mobile, and desktop apps in your favorite language and securely share them with your team. No frontend experience is required.

### âš¡From idea to app in minutes

An internal tool or a dashboard for your team, weekend project, data entry form, kiosk app, or high-fidelity prototype - Flet is an ideal framework to quickly hack great-looking interactive apps to serve a group of users.

### ğŸ“ Simple architecture

No more complex architecture with JavaScript frontend, REST API backend, database, cache, etc. With Flet you just write a monolith stateful app in Python only and get multi-user, real-time Single-Page Application (SPA).

### ğŸ”‹Batteries included

To start developing with Flet, you just need your favo"
stylegan2,"## StyleGAN2 &mdash; Official TensorFlow Implementation

![Teaser image](./docs/stylegan2-teaser-1024x256.png)

**Analyzing and Improving the Image Quality of StyleGAN**<br>
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila<br>

Paper: http://arxiv.org/abs/1912.04958<br>
Video: https://youtu.be/c-NJtV9Jvp0<br>

Abstract: *The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it po"
vanna,"

| GitHub | PyPI | Documentation |
| ------ | ---- | ------------- |
| [![GitHub](https://img.shields.io/badge/GitHub-vanna-blue?logo=github)](https://github.com/vanna-ai/vanna) | [![PyPI](https://img.shields.io/pypi/v/vanna?logo=pypi)](https://pypi.org/project/vanna/) | [![Documentation](https://img.shields.io/badge/Documentation-vanna-blue?logo=read-the-docs)](https://vanna.ai/docs/) |

# Vanna
Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality.

https://github.com/vanna-ai/vanna/assets/7146154/1901f47a-515d-4982-af50-f12761a3b2ce

![vanna-quadrants](https://github.com/vanna-ai/vanna/assets/7146154/1c7c88ba-c144-4ecf-a028-cf5ba7344ca2)

## How Vanna works

![Screen Recording 2024-01-24 at 11 21 37â€¯AM](https://github.com/vanna-ai/vanna/assets/7146154/1d2718ad-12a8-4a76-afa2-c61754462f93)


Vanna works in two easy steps - train a RAG ""model"" on your data, and then ask questions which will return SQL q"
Photon,"
<h1 align=""center"">
  <br>
  <a href=""https://github.com/s0md3v/Photon""><img src=""https://image.ibb.co/h5OZAK/photonsmall.png"" alt=""Photon""></a>
  <br>
  Photon
  <br>
</h1>

<h4 align=""center"">Incredibly fast crawler designed for OSINT.</h4>

<p align=""center"">
  <a href=""https://github.com/s0md3v/Photon/releases"">
    <img src=""https://img.shields.io/github/release/s0md3v/Photon.svg"">
  </a>
  <a href=""https://pypi.org/project/photon/"">
    <img src=""https://img.shields.io/badge/pypi-@photon-red.svg?style=style=flat-square""
         alt=""pypi"">
  </a>
  <a href=""https://github.com/s0md3v/Photon/issues?q=is%3Aissue+is%3Aclosed"">
      <img src=""https://img.shields.io/github/issues-closed-raw/s0md3v/Photon.svg"">
  </a>
  <a href=""https://travis-ci.com/s0md3v/Photon"">
    <img src=""https://img.shields.io/travis/com/s0md3v/Photon.svg"">
  </a>
</p>

Met a CAPTCHA? Try [CapSolver](https://www.capsolver.com/?utm_source=github&utm_medium=repo&utm_campaign=scraping&utm_term=photon) solving s"
InstantID,"<div align=""center"">
<h1>InstantID: Zero-shot Identity-Preserving Generation in Seconds</h1>

[**Qixun Wang**](https://github.com/wangqixun)<sup>12</sup> Â· [**Xu Bai**](https://huggingface.co/baymin0220)<sup>12</sup> Â· [**Haofan Wang**](https://haofanwang.github.io/)<sup>12*</sup> Â· [**Zekui Qin**](https://github.com/ZekuiQin)<sup>12</sup> Â· [**Anthony Chen**](https://antonioo-c.github.io/)<sup>123</sup>

Huaxia Li<sup>2</sup> Â· Xu Tang<sup>2</sup> Â· Yao Hu<sup>2</sup>

<sup>1</sup>InstantX Team Â· <sup>2</sup>Xiaohongshu Inc Â· <sup>3</sup>Peking University

<sup>*</sup>corresponding authors

<a href='https://instantid.github.io/'><img src='https://img.shields.io/badge/Project-Page-green'></a>
<a href='https://arxiv.org/abs/2401.07519'><img src='https://img.shields.io/badge/Technique-Report-red'></a>
<a href='https://huggingface.co/papers/2401.07519'><img src='https://img.shields.io/static/v1?label=Paper&message=Huggingface&color=orange'></a> 
[![GitHub](https://img.shields.io/github/st"
fast-style-transfer,"## Fast Style Transfer in [TensorFlow](https://github.com/tensorflow/tensorflow)

Add styles from famous paintings to any photo in a fraction of a second! [You can even style videos!](#video-stylization)

<p align = 'center'>
<img src = 'examples/style/udnie.jpg' height = '246px'>
<img src = 'examples/content/stata.jpg' height = '246px'>
<a href = 'examples/results/stata_udnie.jpg'><img src = 'examples/results/stata_udnie_header.jpg' width = '627px'></a>
</p>
<p align = 'center'>
It takes 100ms on a 2015 Titan X to style the MIT Stata Center (1024Ã—680) like Udnie, by Francis Picabia.
</p>

Our implementation is based off of a combination of Gatys' [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576), Johnson's [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](http://cs.stanford.edu/people/jcjohns/eccv16/), and Ulyanov's [Instance Normalization](https://arxiv.org/abs/1607.08022). 

### Sponsorship
Please consider sponsoring my work on this project"
chinese-xinhua,"# chinese-xinhua

ä¸­åæ–°åå­—å…¸æ•°æ®åº“å’Œ API ã€‚æ”¶å½•åŒ…æ‹¬ 14032 æ¡æ­‡åè¯­ï¼Œ16142 ä¸ªæ±‰å­—ï¼Œ264434 ä¸ªè¯è¯­ï¼Œ31648 ä¸ªæˆè¯­ã€‚

## Project Structure

```
chinese-xinhua/
|
+- data/ <-- æ•°æ®æ–‡ä»¶å¤¹
|  |
|  +- idiom.json <-- æˆè¯­
|  |
|  +- word.json <-- æ±‰å­—
|  |
|  +- xiehouyu.json <-- æ­‡åè¯­
|  |
|  +- ci.json <-- è¯è¯­
```

## Database Introduction

### æˆè¯­ (idiom.json)

```json
[
    {
        ""derivation"": ""è¯­å‡ºã€Šæ³•åç»Â·æ³•å¸ˆåŠŸå¾·å“ã€‹ä¸‹è‡³é˜¿é¼»åœ°ç‹±ã€‚â€"",
        ""example"": ""ä½†ä¹Ÿæœ‰å°‘æ•°æ„å¿—è–„å¼±çš„â€¦â€¦é€æ­¥ä¸Šå½“ï¼Œç»ˆè‡³å •å…¥ï½ã€‚â˜…ã€Šä¸Šé¥¶é›†ä¸­è¥Â·ç‚¼ç‹±æ‚è®°ã€‹"",
        ""explanation"": ""é˜¿é¼»æ¢µè¯­çš„è¯‘éŸ³ï¼Œæ„è¯‘ä¸ºæ— é—´â€ï¼Œå³ç—›è‹¦æ— æœ‰é—´æ–­ä¹‹æ„ã€‚å¸¸ç”¨æ¥æ¯”å–»é»‘æš—çš„ç¤¾ä¼šå’Œä¸¥é…·çš„ç‰¢ç‹±ã€‚åˆæ¯”å–»æ— æ³•æ‘†è„±çš„æå…¶ç—›è‹¦çš„å¢ƒåœ°ã€‚"",
        ""pinyin"": ""Ä bÃ­ dÃ¬ yÃ¹"",
        ""word"": ""é˜¿é¼»åœ°ç‹±"",
        ""abbreviation"": ""abdy""
    },
    ...
]
```

### è¯è¯­ (ci.json)

```json
[
    { 
        ""ci"": ""å®¸çº¶"", 
        ""explanation"": ""1.å¸ç‹çš„è¯ä¹¦ï¹‘åˆ¶ä»¤ã€‚"" 
    },
    ...
]
```

### æ±‰å­— (word.json)

```json
[
    {
        ""word"": ""å—„"",
        ""oldword"": ""å—„"",
        ""strokes"": ""13"",
        ""pinyin"": ""Ã¡"",
        ""radicals"": ""å£"",
        ""explanation"": ""å—„ã€ˆå¹ã€‰\n\n åŒå•Šâ€ã€‚è¡¨ç¤ºçœæ‚Ÿæˆ–æƒŠå¥‡\n\n å—„!éš¾é“è¿™é‡Œæ˜¯æ²¡æœ‰åœ°æ–¹å®˜çš„ä¹ˆ?--å®‹Â·ä½šåã€Šæ–°ç¼–äº”ä»£å²å¹³è¯ã€‹\n\n å—„Ã¡å¹è¯ã€‚åœ¨å¥é¦–ï¼Œã€ˆè¡¨ã€‰ç–‘é—®æˆ–å"
urh,"![URH image](https://raw.githubusercontent.com/jopohl/urh/master/data/icons/banner.png)

[![CI](https://github.com/jopohl/urh/actions/workflows/ci.yml/badge.svg)](https://github.com/jopohl/urh/actions/workflows/ci.yml)
[![Code style: black](https://img.shields.io/badge/code%20style-black-black)](https://github.com/psf/black)
[![PyPI version](https://badge.fury.io/py/urh.svg)](https://badge.fury.io/py/urh)
[![Packaging status](https://repology.org/badge/tiny-repos/urh.svg)](https://repology.org/project/urh/versions)
 [![Blackhat Arsenal 2017](https://rawgit.com/toolswatch/badges/master/arsenal/usa/2017.svg)](http://www.toolswatch.org/2017/06/the-black-hat-arsenal-usa-2017-phenomenal-line-up-announced/)
 [![Blackhat Arsenal 2018](https://rawgit.com/toolswatch/badges/master/arsenal/europe/2018.svg)](http://www.toolswatch.org/2018/09/black-hat-arsenal-europe-2018-lineup-announced/)


The Universal Radio Hacker (URH) is a complete suite for wireless protocol investigation with native suppor"
chia-blockchain,"# chia-blockchain

[![Chia Network logo][logo-chia]][link-chia]

| Releases                                                                                                                                        | Repo Stats                                                                                                                                                                                                           | Socials                                                                                                                                                                                   |
| ----------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------"
dolly,"# Dolly

Databricksâ€™ [Dolly](https://huggingface.co/databricks/dolly-v2-12b) is an instruction-following large language model trained on the Databricks machine learning platform
that is licensed for commercial use. Based on `pythia-12b`, Dolly is trained on ~15k instruction/response fine tuning records
[`databricks-dolly-15k`](https://huggingface.co/datasets/databricks/databricks-dolly-15k) generated
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. `dolly-v2-12b` is not a state-of-the-art model, but does exhibit surprisingly
high quality instruction following behavior not characteristic of the foundation model on which it is based.

Databricks is committed to ensuring that every organization and individual benefits from the transformative power of artificial intelligence. The Dolly model family represents our first steps along this journey, and weâ€™"
DALL-E,"# Overview

[[Blog]](https://openai.com/blog/dall-e/) [[Paper]](https://arxiv.org/abs/2102.12092) [[Model Card]](model_card.md) [[Usage]](notebooks/usage.ipynb)

This is the official PyTorch package for the discrete VAE used for DALLÂ·E. The transformer used to generate the images from the text is not part of this code release.

# Installation

Before running [the example notebook](notebooks/usage.ipynb), you will need to install the package using

	pip install DALL-E
"
social-engineer-toolkit,"# The Social-Engineer Toolkit (SET)
* Copyright :copyright: 2020
* Written by: David Kennedy (ReL1K) @HackingDave 
* Company: [TrustedSec](https://www.trustedsec.com)

<br/>

## Description
The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC â€“ an information security consulting firm located in Cleveland, Ohio.

DISCLAIMER: This is *only* for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period.
Please read the LICENSE under readme/LICENSE for the licensing of SET. 

#### Supported platforms:
* Linux
* Mac OS X (experimental)

# Installation

## Install via requirements.txt

```bash
pip3 install -r requirements.txt
python3 setup.py 
```

## Install SET
=======
#### Mac OS X
You will need to use a virtual environment for the Python instal"
opensnitch,"<p align=""center"">
  <small>Join the project community on our server!</small>
  <br/><br/>
  <a href=""https://discord.gg/https://discord.gg/btZpkp45gQ"" target=""_blank"" title=""Join our community!"">
    <img src=""https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ""/>
  </a>
</p>
<hr/>

<p align=""center"">
  <img alt=""opensnitch"" src=""https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png"" height=""160"" />
  <p align=""center"">
    <img src=""https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"" />
    <a href=""https://github.com/evilsocket/opensnitch/releases/latest""><img alt=""Release"" src=""https://img.shields.io/github/release/evilsocket/opensnitch.svg?style=flat-square""></a>
    <a href=""https://github.com/evilsocket/opensnitch/blob/master/LICENSE.md""><img alt=""Software License"" src=""https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square""></a>
    <a href=""https://goreportcard.com/report/github.c"
faceai,"[English Doc](README_en.md)
# åŠŸèƒ½ #

1. äººè„¸æ£€æµ‹ã€è¯†åˆ«ï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
2. è½®å»“æ ‡è¯†
3. å¤´åƒåˆæˆï¼ˆç»™äººæˆ´å¸½å­ï¼‰
4. æ•°å­—åŒ–å¦†ï¼ˆç”»å£çº¢ã€çœ‰æ¯›ã€çœ¼ç›ç­‰ï¼‰
5. æ€§åˆ«è¯†åˆ«
6. è¡¨æƒ…è¯†åˆ«ï¼ˆç”Ÿæ°”ã€åŒæ¶ã€ææƒ§ã€å¼€å¿ƒã€éš¾è¿‡ã€æƒŠå–œã€å¹³é™ç­‰ä¸ƒç§æƒ…ç»ªï¼‰
7. è§†é¢‘å¯¹è±¡æå–
8. å›¾ç‰‡ä¿®å¤ï¼ˆå¯ç”¨äºæ°´å°å»é™¤ï¼‰
9. å›¾ç‰‡è‡ªåŠ¨ä¸Šè‰²
10. çœ¼åŠ¨è¿½è¸ªï¼ˆå¾…å®Œå–„ï¼‰
11. æ¢è„¸ï¼ˆå¾…å®Œå–„ï¼‰

**æŸ¥çœ‹åŠŸèƒ½é¢„è§ˆâ†“â†“â†“**

# å¼€å‘ç¯å¢ƒ #

- Windows 10ï¼ˆx64ï¼‰
- Python 3.6.4
- OpenCV 3.4.1
- Dlib 19.8.1
- face_recognition 1.2.2
- keras 2.1.6
- tensorflow 1.8.0
- Tesseract OCR 4.0.0-beta.1


# æ•™ç¨‹ #

[OpenCVç¯å¢ƒæ­å»º](doc/settingup.md)

[Tesseract OCRæ–‡å­—è¯†åˆ«](doc/tesseractOCR.md)

[å›¾ç‰‡äººè„¸æ£€æµ‹ï¼ˆOpenCVç‰ˆï¼‰](doc/detectionOpenCV.md)

[å›¾ç‰‡äººè„¸æ£€æµ‹ï¼ˆDlibç‰ˆï¼‰](doc/detectionDlib.md)

[è§†é¢‘äººè„¸æ£€æµ‹ï¼ˆOpenCVç‰ˆï¼‰](doc/videoOpenCV.md)

[è§†é¢‘äººè„¸æ£€æµ‹ï¼ˆDlibç‰ˆï¼‰](doc/videoDlib.md)

[è„¸éƒ¨è½®å»“ç»˜åˆ¶](doc/faceRecognitionOutline.md)

[æ•°å­—åŒ–å¦†](doc/faceRecognitionMakeup.md)

[è§†é¢‘äººè„¸è¯†åˆ«](doc/faceRecognition.md)

[å¤´åƒç‰¹æ•ˆåˆæˆ](doc/compose.md)

[æ€§åˆ«è¯†åˆ«](doc/gender.md)

[è¡¨æƒ…è¯†åˆ«](doc/emotion.md)

[è§†é¢‘å¯¹è±¡æå–](https://github.com/vipstone/faceai/blob/master/doc/hsv-opencv.md)

[å›¾ç‰‡ä¿®å¤](https://github.com/vipstone/faceai/blob/master/doc/inpaint.md)


# å…¶ä»–æ•™ç¨‹ #

[Ubuntu apt-getå’Œpipæºæ›´æ¢](doc/ubuntuChan"
MLAlgorithms,"# Machine learning algorithms
A collection of minimal and clean implementations of machine learning algorithms.

### Why?
This project is targeting people who want to learn internals of ml algorithms or implement them from scratch.  
The code is much easier to follow than the optimized libraries and easier to play with.  
All algorithms are implemented in Python, using numpy, scipy and autograd.  

### Implemented:
* [Deep learning (MLP, CNN, RNN, LSTM)](mla/neuralnet)
* [Linear regression, logistic regression](mla/linear_models.py)
* [Random Forests](mla/ensemble/random_forest.py)
* [Support vector machine (SVM) with kernels (Linear, Poly, RBF)](mla/svm)
* [K-Means](mla/kmeans.py)
* [Gaussian Mixture Model](mla/gaussian_mixture.py)
* [K-nearest neighbors](mla/knn.py)
* [Naive bayes](mla/naive_bayes.py)
* [Principal component analysis (PCA)](mla/pca.py)
* [Factorization machines](mla/fm.py)
* [Restricted Boltzmann machine (RBM)](mla/rbm.py)
* [t-Distributed Stochastic Neighbor Embeddin"
binwalk,"# *** Binwalk v2 Deprecation Notice ***

### This version of Binwalk is largely unmaintained.

### [Binwalkv3](https://github.com/ReFirmLabs/binwalk/tree/binwalkv3) is currently under active development.
### While still experimental, we recommend trying it out; issues/bug reports welcome! :)
### A fork of Binwalk v2 is still actively maintained by [OSPG](https://github.com/OSPG/binwalk).

## 
[![Build Status](https://travis-ci.org/ReFirmLabs/binwalk.svg?branch=master)](https://travis-ci.org/ReFirmLabs/binwalk)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/ReFirmLabs/binwalk/graphs/commit-activity)
[![GitHub license](https://img.shields.io/github/license/ReFirmLabs/binwalk.svg)](https://github.com/ReFirmLabs/binwalk/blob/master/LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/badges/shields.svg?style=social&label=Stars)](https://github.com/ReFirmLabs/binwalk/stargazers)

Binwalk is a fast, easy to use tool for analyzing, rever"
python-mastery,"# Advanced Python Mastery

A course by David Beazley (https://www.dabeaz.com)  
Copyright (C) 2007-2024  

## Synopsis

An exercise-driven course on Advanced Python Programming that was
battle-tested several hundred times on the corporate-training circuit
for more than a decade.  Written by David Beazley, author of the
[Python Cookbook, 3rd Edition](https://www.dabeaz.com/cookbook.html) (O'Reilly) and 
[Python Distilled](https://www.dabeaz.com/python-distilled/index.html)
(Addison-Wesley).  Released under a Creative Commons license.  Free of
ads, tracking, pop-ups, newsletters, and AI.

Everything in this course should work with the latest version of
Python, but be aware that the course primarily targets the feature set
of Python 3.6.  As such, certain modern features don't get coverage. 
Honestly, this shouldn't affect you much unless you're trying to write code
that's freakishly clever.

## Target Audience 

This course is for Python programmers who want to move beyond 
short scripts"
prowler,"<p align=""center"">
  <img align=""center"" src=""https://github.com/prowler-cloud/prowler/blob/master/docs/img/prowler-logo-black.png#gh-light-mode-only"" width=""50%"" height=""50%"">
  <img align=""center"" src=""https://github.com/prowler-cloud/prowler/blob/master/docs/img/prowler-logo-white.png#gh-dark-mode-only"" width=""50%"" height=""50%"">
</p>
<p align=""center"">
  <b><i>Prowler SaaS </b> and <b>Prowler Open Source</b> are as dynamic and adaptable as the environment theyâ€™re meant to protect. Trusted by the leaders in security.
</p>
<p align=""center"">
<b>Learn more at <a href=""https://prowler.com"">prowler.com</i></b>
</p>

<p align=""center"">
<a href=""https://join.slack.com/t/prowler-workspace/shared_invite/zt-1hix76xsl-2uq222JIXrC7Q8It~9ZNog""><img width=""30"" height=""30"" alt=""Prowler community on Slack"" src=""https://github.com/prowler-cloud/prowler/assets/38561120/3c8b4ec5-6849-41a5-b5e1-52bbb94af73a""></a>
  <br>
  <a href=""https://join.slack.com/t/prowler-workspace/shared_invite/zt-2oinmgmw6-cl"
chalice,"===========
AWS Chalice
===========

.. image:: https://badges.gitter.im/awslabs/chalice.svg
   :target: https://gitter.im/awslabs/chalice?utm_source=badge&utm_medium=badge
   :alt: Gitter
.. image:: https://readthedocs.org/projects/chalice/badge/?version=latest
   :target: http://aws.github.io/chalice/?badge=latest
   :alt: Documentation Status


.. image:: https://aws.github.io/chalice/_images/chalice-logo-whitespace.png
   :target: https://aws.github.io/chalice/
   :alt: Chalice Logo


Chalice is a framework for writing serverless apps in python. It allows
you to quickly create and deploy applications that use AWS Lambda.  It provides:

* A command line tool for creating, deploying, and managing your app
* A decorator based API for integrating with Amazon API Gateway, Amazon S3,
  Amazon SNS, Amazon SQS, and other AWS services.
* Automatic IAM policy generation


You can create Rest APIs:

.. code-block:: python

    from chalice import Chalice

    app = Chalice(app_name=""helloworl"
scientific-visualization-book,"## Scientific Visualization: Python + Matplotlib
**Nicolas P. Rougier, Bordeaux, November 2021.**  

<img src=""images/book.png"" width=""25%"" alt=""Front cover"" align=""left""/>

The Python scientific visualisation landscape is huge. It is composed of a myriad of tools, ranging from the most versatile and widely used down to the more specialised and confidential. Some of these tools are community based while others are developed by companies. Some are made specifically for the web, others are for the desktop only, some deal with 3D and large data, while others target flawless 2D rendering. In this landscape, Matplotlib has a very special place. It is a versatile and powerful library that allows you to design very high quality figures, suitable for scientific publishing. It also offers a simple and intuitive interface as well as an object oriented architecture that allows you to tweak anything within a figure. Finally, it can be used as a regular graphic library in order to design nonâ€scient"
scapy,"<!-- start_ppi_description -->

# <img src=""https://github.com/secdev/scapy/raw/master/doc/scapy/graphics/scapy_logo.png"" width=""64"" valign=""middle"" alt=""Scapy"" />&nbsp;&nbsp; Scapy

[![Scapy unit tests](https://github.com/secdev/scapy/actions/workflows/unittests.yml/badge.svg?branch=master&event=push)](https://github.com/secdev/scapy/actions/workflows/unittests.yml?query=event%3Apush) <!-- ignore_ppi -->
[![AppVeyor Build status](https://ci.appveyor.com/api/projects/status/os03daotfja0wtp7/branch/master?svg=true)](https://ci.appveyor.com/project/secdev/scapy/branch/master) <!-- ignore_ppi -->
[![Codecov Status](https://codecov.io/gh/secdev/scapy/branch/master/graph/badge.svg)](https://codecov.io/gh/secdev/scapy) <!-- ignore_ppi -->
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/30ee6772bb264a689a2604f5cdb0437b)](https://www.codacy.com/app/secdev/scapy) <!-- ignore_ppi -->
[![PyPI Version](https://img.shields.io/pypi/v/scapy.svg)](https://pypi.python.org/pypi/scapy/)
[![Li"
optuna,"<div align=""center""><img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" width=""800""/></div>

# Optuna: A hyperparameter optimization framework

[![Python](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)](https://www.python.org)
[![pypi](https://img.shields.io/pypi/v/optuna.svg)](https://pypi.python.org/pypi/optuna)
[![conda](https://img.shields.io/conda/vn/conda-forge/optuna.svg)](https://anaconda.org/conda-forge/optuna)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna)
[![Read the Docs](https://readthedocs.org/projects/optuna/badge/?version=stable)](https://optuna.readthedocs.io/en/stable/)
[![Codecov](https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg)](https://codecov.io/gh/optuna/optuna)

:link: [**Website**](https://optuna.org/)
| :page_with_curl: [**Docs**](https://optuna.readthedocs.io/en/stable/)
| :gear: [**"
awesome-chatgpt-zh,"# ğŸ¤– ChatGPT ä¸­æ–‡æŒ‡å— ğŸ¤–

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 
[![Code License](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/yzfly/awesome-chatgpt-zh/blob/main/LICENSE)
[![slack badge](https://img.shields.io/badge/Telegrem-join-blueviolet?logo=telegrem&amp)](https://t.me/AwesomeChatGPT)

[GitHub æŒç»­æ›´æ–°ï¼Œæ¬¢è¿å…³æ³¨ï¼Œæ¬¢è¿ star ~](https://github.com/yzfly/awesome-chatgpt-zh)

[ä¸ºæ–¹ä¾¿å›½å†…è®¿é—®, GitLab é•œåƒåŒæ­¥æ›´æ–°~](https://gitlab.com/awesomeai/awesome-chatgpt-zh)


ChatGPT ä¸­æ–‡æŒ‡å—é¡¹ç›®æ—¨åœ¨å¸®åŠ©ä¸­æ–‡ç”¨æˆ·äº†è§£å’Œä½¿ç”¨ChatGPTã€‚æˆ‘ä»¬æ”¶é›†äº†å„ç§å…è´¹å’Œä»˜è´¹çš„ChatGPTèµ„æºï¼Œä»¥åŠå¦‚ä½•æ›´æœ‰æ•ˆåœ°ä½¿ç”¨ä¸­æ–‡ä¸ ChatGPT è¿›è¡Œäº¤æµçš„æ–¹æ³•ã€‚æˆ‘ä»¬æ”¶é›†äº†æ”¶é›†äº†ChatGPTåº”ç”¨å¼€å‘çš„å„ç§ç›¸å…³èµ„æºï¼Œä¹Ÿæ”¶é›†äº†åŸºäº ChatGPTèƒ½åŠ›å¼€å‘çš„ç”Ÿäº§åŠ›å·¥å…·ã€‚åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°ä¸°å¯Œçš„ ChatGPTå·¥å…·ã€åº”ç”¨å’Œç¤ºä¾‹ã€‚

- [ğŸ¤– ChatGPT ä¸­æ–‡æŒ‡å— ğŸ¤–](#-chatgpt-ä¸­æ–‡æŒ‡å—-)
  - [ä»€ä¹ˆæ˜¯ ChatGPT ?](#ä»€ä¹ˆæ˜¯-chatgpt-)
  - [ChatGPT ä½¿ç”¨é€”å¾„](#chatgpt-ä½¿ç”¨é€”å¾„)
  - [ä¸ ChatGPT é«˜æ•ˆå¯¹è¯ï¼Ÿâ€”â€”Promptå·¥ç¨‹æŒ‡å—](#ä¸-chatgpt-é«˜æ•ˆå¯¹è¯promptå·¥ç¨‹æŒ‡å—)
  - [OpenAI GPTs æŒ‡å—](#openai-gpts-æŒ‡å—)
  - [ChatGPT é¡¶çº§çˆ†æ¬¾å¼€æºé¡¹ç›®(10K+ Stars)](#chatgpt-é¡¶çº§çˆ†æ¬¾å¼€æºé¡¹ç›®10k-stars)
  - [ChatGPT åº”ç”¨](#chatgpt-åº”ç”¨)
  - [ChatGPT æ’ä»¶](#chatgpt-æ’ä»¶)
  - [Chat"
howdoi,"<p align=""center"">
    <a href=""https://pypi.python.org/pypi/howdoi"">
        <img src=""https://www.dropbox.com/s/dk13iy2uoufdwr7/HowDoIcolor512.png?raw=1"" alt=""Sherlock, your neighborhood command-line sloth sleuth"" />
    </a>
</p>
<h1 align=""center"">howdoi</h1>
<h2 align=""center"">Instant coding answers via the command line</h2>
<p align=""center""><strong>âš¡ Never open your browser to look for help again âš¡</strong></p>

<p align=""center"">
    <a href=""https://github.com/gleitz/howdoi/actions?query=workflow%3A%22Python+CI%22""><img src=""https://img.shields.io/github/actions/workflow/status/gleitz/howdoi/python.yml?style=plastic&color=78dce8"" alt=""build status""></a>
    <a href=""https://pepy.tech/project/howdoi""><img src=""https://img.shields.io/pypi/dm/howdoi?style=plastic&color=ab9df2&maxAge=86400&label=downloads&query=%24.total_downloads&url=https%3A%2F%2Fapi.pepy.tech%2Fapi%2Fprojects%2Fhowdoi"" alt=""downloads""></a>
    <a href=""https://pypi.python.org/pypi/howdoi""><img src=""https://img."
fsociety,"# Fsociety Hacking Tools Pack

[![Python2.7](https://img.shields.io/badge/Python-2.7-green.svg?style=flat-square)](https://www.python.org/downloads/release/python-2714/) 
![OS](https://img.shields.io/badge/Tested%20On-Linux%20|%20OSX%20|%20Windows%20|%20Android-yellowgreen.svg?style=flat-square) 
![Docker](https://img.shields.io/docker/automated/jrottenberg/ffmpeg.svg?style=flat-square) 
[![License](https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square)](https://github.com/Manisso/fsociety/blob/master/LICENSE)

A Penetration Testing Framework, you will have every script that a hacker needs. Works with Python 2. For a Python 3 version see our updated version at [`fsociety-team/fsociety`](https://github.com/fsociety-team/fsociety).

## Fsociety Contains All Tools Used in Mr. Robot Series

[![Mr. Robot](http://nikolaskama.me/content/images/2016/07/mr-robot-1.gif)](https://wikipedia.org/wiki/Mr._Robot)

## Menu

- Information Gathering
- Password Attacks
- Wireless Testing
- "
english-words,"List Of English Words
=============

A text file containing over 466k English words.

While searching for a list of english words (for an auto-complete tutorial)
I found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to [https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable](https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable) (archived).

No idea why infochimps put the word list inside an excel (.xls) file.

I pulled out the words into a simple new-line-delimited text file.
Which is more useful when building apps or importing into databases etc.

Copyright still belongs to them.

Files you may be interested in:

-  [words.txt](words.txt) contains all words.
-  [words_alpha.txt](words_alpha.txt) contains only [[:alpha:]] words (words that only have letters, no numbers or symbols). If you want a quick solution choose this"
AnimatedDrawings,"# Animated Drawings

![Sequence 02](https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif)


This repo contains an implementation of the algorithm described in the paper, [A Method for Animating Children's Drawings of the Human Figure](https://dl.acm.org/doi/10.1145/3592788).

In addition, this repo aims to be a useful creative tool in its own right, allowing you to flexibly create animations starring your own drawn characters. If you do create something fun with this, let us know! Use hashtag **#FAIRAnimatedDrawings**, or tag me on twitter: [@hjessmith](https://twitter.com/hjessmith/).

Project website: [http://www.fairanimateddrawings.com](http://www.fairanimateddrawings.com)

Video overview of [Animated Drawings OS Project](https://www.youtube.com/watch?v=WsMUKQLVsOI)


## Installation
*This project has been tested with macOS Ventura 13.2.1 and Ubuntu 18.04. If you're installing on another operating system, you may encounter issues.*

W"
ParlAI,"<p align=""center"">
 <img width=""70%"" src=""docs/source/\_static/img/parlai.png"" />
</p>

<p align=""center"">
   <a href=""https://github.com/facebookresearch/ParlAI/blob/main/LICENSE"">
    <img src=""https://img.shields.io/badge/license-MIT-blue.svg"" alt=""CircleCI"" />
  </a>
   <a href=""https://pypi.org/project/parlai/"">
    <img src=""https://img.shields.io/pypi/v/parlai?color=blue&label=release"" alt=""CircleCI"" />
  </a>
    <a href=""https://circleci.com/gh/facebookresearch/ParlAI/tree/main"">
    <img src=""https://img.shields.io/circleci/build/github/facebookresearch/ParlAI/main"" alt=""Coverage"" />
  </a>
    <a href=""https://codecov.io/gh/facebookresearch/ParlAI"">
    <img src=""https://img.shields.io/codecov/c/github/facebookresearch/ParlAI"" alt=""GitHub contributors"" />
  </a>
    <a href=""https://img.shields.io/github/contributors/facebookresearch/ParlAI"">
    <img src=""https://img.shields.io/github/contributors/facebookresearch/ParlAI""/>
  </a>
    <a href=""https://twitter.com/parlai_par"
tweepy,"Tweepy: Twitter for Python!
======

[![PyPI Version](https://img.shields.io/pypi/v/tweepy?label=PyPI)](https://pypi.org/project/tweepy/)
[![Python Versions](https://img.shields.io/pypi/pyversions/tweepy?label=Python)](https://pypi.org/project/tweepy/)
[![DOI](https://zenodo.org/badge/244025.svg)](https://zenodo.org/badge/latestdoi/244025)

[![Documentation Status](https://readthedocs.org/projects/tweepy/badge/?version=latest)](https://tweepy.readthedocs.io/en/latest/)
[![Test Status](https://github.com/tweepy/tweepy/workflows/Test/badge.svg)](https://github.com/tweepy/tweepy/actions?query=workflow%3ATest)
[![Coverage Status](https://img.shields.io/coveralls/tweepy/tweepy/master.svg?style=flat)](https://coveralls.io/github/tweepy/tweepy?branch=master)

[![Discord Server](https://discord.com/api/guilds/432685901596852224/embed.png)](https://discord.gg/bJvqnhg)

Installation
------------

The easiest way to install the latest version from PyPI is by using
[pip](https://pip.pypa.io/):

   "
github-trends,"# GitHub Trends

## SPECIAL: GitHub Wrapped

Check out your GitHub Wrapped at `githubwrapped.io`!

![github-wrapped](https://github.com/avgupta456/github-trends/assets/16708871/bf9406a4-6a49-4dbf-8f60-af221bb84bd6)

---

## What is GitHub Trends

GitHub Trends dives deep into the GitHub API to bring you exciting and impactful metrics about your code contributions. Generate insights on lines written by language, repository, and time. Easily embed dynamic images into your GitHub profile to share your statistics with the world. Check out some of the examples below:

<a href=""https://githubtrends.io"">
  <img align=""center"" src=""https://api.githubtrends.io/user/svg/avgupta456/langs?time_range=one_year&include_private=True&loc_metric=changed"" />
</a>
<a href=""https://githubtrends.io"">
  <img align=""center"" src=""https://api.githubtrends.io/user/svg/avgupta456/repos?time_range=one_year&include_private=True&group=private&loc_metric=changed"" />
</a>

## Quickstart

First, visit `https://api.gith"
MaxKB,"[English](README_EN.md) | [ä¸­æ–‡](README.md)

<p align=""center""><img src= ""https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf"" alt=""MaxKB"" width=""300"" /></p>
<h3 align=""center"">åŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œ RAG çš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ</h3>
<p align=""center""><a href=""https://trendshift.io/repositories/9113"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/9113"" alt=""1Panel-dev%2FMaxKB | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a></p>
<p align=""center"">
  <a href=""https://www.gnu.org/licenses/gpl-3.0.html#license-text""><img src=""https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF"" alt=""License: GPL v3""></a>
  <a href=""https://app.codacy.com/gh/1Panel-dev/maxkb?utm_source=github.com&utm_medium=referral&utm_content=1Panel-dev/maxkb&utm_campaign=Badge_Grade_Dashboard""><img src=""https://app.codacy.com/project/badge/Grade/da67574fd82b473992781d1386b937ef"" alt=""Codacy""></a>
  <a href=""https://github.com/1Panel-dev/maxkb/"
LoRA,"# LoRA: Low-Rank Adaptation of Large Language Models

This repo contains the source code of the Python package `loralib` and several examples of how to integrate it with PyTorch models, such as those in Hugging Face.
We only support PyTorch for now.
See our paper for a detailed description of LoRA.

**LoRA: Low-Rank Adaptation of Large Language Models** <br>
*Edward J. Hu\*, Yelong Shen\*, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen* <br>
Paper: https://arxiv.org/abs/2106.09685 <br>
Video explainer: https://www.youtube.com/watch?v=DhRoTONcyZE <br>

*Update 2/2023: LoRA is now supported by the [State-of-the-art Parameter-Efficient Fine-Tuning (PEFT)](https://github.com/huggingface/peft) library by Hugging Face.*

LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights.
This vastly reduces the storage requirement for large language models adapted to specific tasks and enables ef"
magic-animate,"<!-- # magic-edit.github.io -->

<p align=""center"">

  <h2 align=""center"">MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</h2>
  <p align=""center"">
    <a href=""https://scholar.google.com/citations?user=-4iADzMAAAAJ&hl=en""><strong>Zhongcong Xu</strong></a>
    Â·
    <a href=""http://jeff95.me/""><strong>Jianfeng Zhang</strong></a>
    Â·
    <a href=""https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ&hl=en""><strong>Jun Hao Liew</strong></a>
    Â·
    <a href=""https://hanshuyan.github.io/""><strong>Hanshu Yan</strong></a>
    Â·
    <a href=""https://scholar.google.com/citations?user=stQQf7wAAAAJ&hl=en""><strong>Jia-Wei Liu</strong></a>
    Â·
    <a href=""https://zhangchenxu528.github.io/""><strong>Chenxu Zhang</strong></a>
    Â·
    <a href=""https://sites.google.com/site/jshfeng/home""><strong>Jiashi Feng</strong></a>
    Â·
    <a href=""https://sites.google.com/view/showlab""><strong>Mike Zheng Shou</strong></a>
    <br>
    <br>
        <a href=""https://ar"
amazing-qr,"# Amazing-QR

[![former name](https://img.shields.io/badge/old%20name-MyQR-yellow)](https://pypi.org/project/myqr/) [![PyPI - Downloads](https://img.shields.io/pypi/dm/myqr?label=downloads@myqr)](https://pypi.org/project/myqr/) [![](https://img.shields.io/badge/language-Python-blue)](https://www.python.org/) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/amzqr?logo=python&logoColor=ffffff&label=Python&labelColor=blue&color=ffffff) [![PyPI](https://img.shields.io/pypi/v/amzqr?logo=pypi&logoColor=ffffff&label=PyPI&labelColor=blue)](https://pypi.org/project/amzqr/) ![PyPI - Wheel](https://img.shields.io/pypi/wheel/amzqr) [![PyPI - Downloads](https://img.shields.io/pypi/dm/amzqr)](https://pypi.org/project/amzqr/) [![PyPI - License](https://img.shields.io/pypi/l/amzqr)](https://github.com/x-hw/amazing-qr/blob/master/LICENSE.md) ![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/x-hw/amazing-qr) ![macos](https://img.shields.io/badge/-macOS-black?l"
Wav2Lip,"# **Wav2Lip**: *Accurately Lip-syncing Videos In The Wild* 
### Wav2Lip is hosted for free at [Sync Labs](https://sync.so/)
Are you looking to integrate this into a product? We have a turn-key hosted API with new and improved lip-syncing models here: https://sync.so/
For any other commercial / enterprise requests, please contact us at pavan@synclabs.so and prady@sync.so
To reach out to the authors directly you can reach us at prajwal@synclabs.so, rudrabha@sync.so.
This code is part of the paper: _A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild_ published at ACM Multimedia 2020. 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs2)](https://paperswithcode.com/sota/lip-sync-on-lrs2?p=a-lip-sync-expert-is-all-you-need-for-speech)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs3)]("
AnimateDiff,"# AnimateDiff

This repository is the official implementation of [AnimateDiff](https://arxiv.org/abs/2307.04725) [ICLR2024 Spotlight].
It is a plug-and-play module turning most community text-to-image models into animation generators, without the need of additional training.

**[AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725)** 
</br>
[Yuwei Guo](https://guoyww.github.io/),
[Ceyuan Yangâœ](https://ceyuan.me/),
[Anyi Rao](https://anyirao.com/),
[Zhengyang Liang](https://maxleung99.github.io/),
[Yaohui Wang](https://wyhsirius.github.io/),
[Yu Qiao](https://scholar.google.com.hk/citations?user=gFtI-8QAAAAJ),
[Maneesh Agrawala](https://graphics.stanford.edu/~maneesh/),
[Dahua Lin](http://dahua.site),
[Bo Dai](https://daibo.info)
(âœCorresponding Author)  
[![arXiv](https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg)](https://arxiv.org/abs/2307.04725)
[![Project Page](https://img.shields.io/badge/Project-We"
mvt,"<p align=""center"">
     <img src=""https://docs.mvt.re/en/latest/mvt.png"" width=""200"" />
</p>

# Mobile Verification Toolkit

[![](https://img.shields.io/pypi/v/mvt)](https://pypi.org/project/mvt/)
[![Documentation Status](https://readthedocs.org/projects/mvt/badge/?version=latest)](https://docs.mvt.re/en/latest/?badge=latest)
[![CI](https://github.com/mvt-project/mvt/actions/workflows/python-package.yml/badge.svg)](https://github.com/mvt-project/mvt/actions/workflows/python-package.yml)
[![Downloads](https://pepy.tech/badge/mvt)](https://pepy.tech/project/mvt)

Mobile Verification Toolkit (MVT) is a collection of utilities to simplify and automate the process of gathering forensic traces helpful to identify a potential compromise of Android and iOS devices.

It has been developed and released by the [Amnesty International Security Lab](https://securitylab.amnesty.org) in July 2021 in the context of the [Pegasus Project](https://forbiddenstories.org/about-the-pegasus-project/) along wit"
stanford-tensorflow-tutorials,"[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Join the https://gitter.im/stanford-tensorflow-tutorials](https://badges.gitter.im/tflearn/tflearn.svg)](https://gitter.im/stanford-tensorflow-tutorials)

# stanford-tensorflow-tutorials
This repository contains code examples for the course CS 20: TensorFlow for Deep Learning Research. <br>
It will be updated as the class progresses. <br>
Detailed syllabus and lecture notes can be found [here](http://cs20.stanford.edu).<br>
For this course, I use python3.6 and TensorFlow 1.4.1.

For the code and notes of the previous year's course, please see the folder 2017 and the website https://web.stanford.edu/class/cs20si/2017

For setup instruction and the list of dependencies, please see the setup folder of this repository."
danswer,"<!-- DANSWER_METADATA={""link"": ""https://github.com/danswer-ai/danswer/blob/main/README.md""} -->

<h2 align=""center"">
<a href=""https://www.danswer.ai/""> <img width=""50%"" src=""https://github.com/danswer-owners/danswer/blob/1fabd9372d66cd54238847197c33f091a724803b/DanswerWithName.png?raw=true)"" /></a>
</h2>

<p align=""center"">
<p align=""center"">Open Source Gen-AI Chat + Unified Search.</p>

<p align=""center"">
<a href=""https://docs.danswer.dev/"" target=""_blank"">
    <img src=""https://img.shields.io/badge/docs-view-blue"" alt=""Documentation"">
</a>
<a href=""https://join.slack.com/t/danswer/shared_invite/zt-2lcmqw703-071hBuZBfNEOGUsLa5PXvQ"" target=""_blank"">
    <img src=""https://img.shields.io/badge/slack-join-blue.svg?logo=slack"" alt=""Slack"">
</a>
<a href=""https://discord.gg/TDJ59cGV2X"" target=""_blank"">
    <img src=""https://img.shields.io/badge/discord-join-blue.svg?logo=discord&logoColor=white"" alt=""Discord"">
</a>
<a href=""https://github.com/danswer-ai/danswer/blob/main/README.md"" target=""_"
pytorch-grad-cam,"[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Build Status](https://github.com/jacobgil/pytorch-grad-cam/workflows/Tests/badge.svg)
[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=Monthly%20Downloads)](https://pepy.tech/project/grad-cam)
[![Downloads](https://static.pepy.tech/personalized-badge/grad-cam?period=total&units=international_system&left_color=black&right_color=blue&left_text=Total%20Downloads)](https://pepy.tech/project/grad-cam)

# Advanced AI explainability for PyTorch

`pip install grad-cam`

Documentation with advanced tutorials: [https://jacobgil.github.io/pytorch-gradcam-book](https://jacobgil.github.io/pytorch-gradcam-book)


This is a package with state of the art methods for Explainable AI for computer vision.
This can be used for diagnosing model predictions, either in production or while
devel"
jinja,"# Jinja

Jinja is a fast, expressive, extensible templating engine. Special
placeholders in the template allow writing code similar to Python
syntax. Then the template is passed data to render the final document.

It includes:

-   Template inheritance and inclusion.
-   Define and import macros within templates.
-   HTML templates can use autoescaping to prevent XSS from untrusted
    user input.
-   A sandboxed environment can safely render untrusted templates.
-   AsyncIO support for generating templates and calling async
    functions.
-   I18N support with Babel.
-   Templates are compiled to optimized Python code just-in-time and
    cached, or can be compiled ahead-of-time.
-   Exceptions point to the correct line in templates to make debugging
    easier.
-   Extensible filters, tests, functions, and even syntax.

Jinja's philosophy is that while application logic belongs in Python if
possible, it shouldn't make the template designer's job difficult by
restricting functionality"
HivisionIDPhotos,"<div align=""center"">

<img alt=""hivision_logo"" src=""assets/hivision_logo.png"" width=120 height=120>
<h1>HivisionIDPhoto</h1>

[English](README_EN.md) / ä¸­æ–‡ / [æ—¥æœ¬èª](README_JP.md) / [í•œêµ­ì–´](README_KO.md)

[![][release-shield]][release-link]
[![][dockerhub-shield]][dockerhub-link]
[![][github-stars-shield]][github-stars-link]
[![][github-issues-shield]][github-issues-link]
[![][github-contributors-shield]][github-contributors-link]
[![][github-forks-shield]][github-forks-link]
[![][license-shield]][license-link]  
[![][wechat-shield]][wechat-link]
[![][spaces-shield]][spaces-link]
[![][swanhub-demo-shield]][swanhub-demo-link]
[![][modelscope-shield]][modelscope-link]

[![][trendshift-shield]][trendshift-link]
[![][hellogithub-shield]][hellogithub-link]

<img src=""assets/demoImage.jpg"" width=900>

ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„[å¾®ä¿¡ç¾¤][wechat-link]

</div>

> **ç›¸å…³é¡¹ç›®**ï¼š
>
> - [SwanLab](https://github.com/SwanHubX/SwanLab)ï¼šè®­ç»ƒäººåƒæŠ å›¾æ¨¡å‹å…¨ç¨‹ç”¨å®ƒæ¥åˆ†æå’Œç›‘æ§ï¼Œä»¥åŠå’Œå®éªŒå®¤åŒå­¦åä½œäº¤æµï¼Œå¤§å¹…æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚


<br>

# ç›®å½•

- [æœ€è¿‘æ›´æ–°](#-æœ€è¿‘æ›´æ–°)
- [é¡¹ç›®ç®€ä»‹](#-é¡¹ç›®ç®€ä»‹)
- [ç¤¾åŒº]("
psutil,"|  |downloads| |stars| |forks| |contributors| |coverage|
|  |version| |py-versions| |packages| |license|
|  |github-actions-wheels|  |github-actions-bsd| |appveyor| |doc| |twitter| |tidelift|

.. |downloads| image:: https://img.shields.io/pypi/dm/psutil.svg
    :target: https://pepy.tech/project/psutil
    :alt: Downloads

.. |stars| image:: https://img.shields.io/github/stars/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/stargazers
    :alt: Github stars

.. |forks| image:: https://img.shields.io/github/forks/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/network/members
    :alt: Github forks

.. |contributors| image:: https://img.shields.io/github/contributors/giampaolo/psutil.svg
    :target: https://github.com/giampaolo/psutil/graphs/contributors
    :alt: Contributors

.. |github-actions-wheels| image:: https://img.shields.io/github/actions/workflow/status/giampaolo/psutil/.github/workflows/build.yml.svg?label=Linux%2C%20macOS%2C%20W"
pyautogui,"PyAutoGUI
=========

PyAutoGUI is a  cross-platform GUI automation Python module for human beings. Used to programmatically control the mouse & keyboard.

`pip install pyautogui`

Full documentation available at https://pyautogui.readthedocs.org

Simplified Chinese documentation available at https://github.com/asweigart/pyautogui/blob/master/docs/simplified-chinese.ipynb

Source code available at https://github.com/asweigart/pyautogui

If you need help installing Python, visit https://installpython3.com/

Dependencies
============

PyAutoGUI supports Python 2 and 3. If you are installing PyAutoGUI from PyPI using pip:

Windows has no dependencies. The Win32 extensions do not need to be installed.

macOS needs the pyobjc-core and pyobjc module installed (in that order).

Linux needs the python3-xlib (or python-xlib for Python 2) module installed.

Pillow needs to be installed, and on Linux you may need to install additional libraries to make sure Pillow's PNG/JPEG works correctly. See:
"
q,"[![Build and Package](https://github.com/harelba/q/workflows/BuildAndPackage/badge.svg?branch=master)](https://github.com/harelba/q/actions?query=branch%3Amaster)

# q - Text as Data
q's purpose is to bring SQL expressive power to the Linux command line and to provide easy access to text as actual data.

q allows the following:

* Performing SQL-like statements directly on tabular text data, auto-caching the data in order to accelerate additional querying on the same file. 
* Performing SQL statements directly on multi-file sqlite3 databases, without having to merge them or load them into memory

The following table shows the impact of using caching:

|    Rows   | Columns | File Size | Query time without caching | Query time with caching | Speed Improvement |
|:---------:|:-------:|:---------:|:--------------------------:|:-----------------------:|:-----------------:|
| 5,000,000 |   100   |   4.8GB   |    4 minutes, 47 seconds   |       1.92 seconds      |        x149       |
| 1,000"
yolov3,"<div align=""center"">
  <p>
    <a align=""center"" href=""https://ultralytics.com/yolov3"" target=""_blank"">
      <img width=""100%"" src=""https://raw.githubusercontent.com/ultralytics/assets/main/yolov3/banner-yolov3.png""></a>
  </p>

[ä¸­æ–‡](https://docs.ultralytics.com/zh) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko) | [æ—¥æœ¬èª](https://docs.ultralytics.com/ja) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [FranÃ§ais](https://docs.ultralytics.com/fr) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar)

<div>
    <a href=""https://github.com/ultralytics/yolov3/actions/workflows/ci-testing.yml""><img src=""https://github.com/ultralytics/yolov3/actions/workflows/ci-testing.yml/badge.svg"" alt=""YOLOv3 CI""></a>
    <a href=""https://zenodo.org/badge/latestdoi/264818686""><img src=""https://zen"
pipx,"<p align=""center"">
<a href=""https://pipx.pypa.io"">
<img align=""center"" src=""https://github.com/pypa/pipx/raw/main/logo.svg"" width=""200""/>
</a>
</p>

# pipx â€” Install and Run Python Applications in Isolated Environments

<p align=""center"">
<a href=""https://github.com/pypa/pipx/raw/main/pipx_demo.gif"">
<img src=""https://github.com/pypa/pipx/raw/main/pipx_demo.gif""/>
</a>
</p>

<p align=""center"">
<a href=""https://github.com/pypa/pipx/actions"">
<img src=""https://github.com/pypa/pipx/workflows/tests/badge.svg?branch=main"" alt=""image"" /></a> <a href=""https://badge.fury.io/py/pipx""><img src=""https://badge.fury.io/py/pipx.svg"" alt=""PyPI version""></a> <a href=""https://badge.fury.io/py/pipx""><img src=""https://static.pepy.tech/badge/pipx""></a>

</p>

**Documentation**: <https://pipx.pypa.io>

**Source Code**: <https://github.com/pypa/pipx>

_For comparison to other tools including pipsi, see
[Comparison to Other Tools](https://pipx.pypa.io/stable/comparisons/)._

## Install pipx

> [!WARNING]
>
>"
eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee,"# eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
![eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee](https://img.shields.io/badge/eeeeee-eeeeee-eeeeee.svg)
eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"
django-cms,"##########
django CMS
##########
.. image:: https://img.shields.io/pypi/v/django-cms.svg
    :target: https://pypi.python.org/pypi/django-cms/
.. image:: https://img.shields.io/badge/wheel-yes-green.svg
    :target: https://pypi.python.org/pypi/django-cms/
.. image:: https://img.shields.io/pypi/l/django-cms.svg
    :target: https://pypi.python.org/pypi/django-cms/
.. image:: https://codeclimate.com/github/divio/django-cms/badges/gpa.svg
   :target: https://codeclimate.com/github/divio/django-cms
   :alt: Code Climate

Open source enterprise content management system based on the Django framework and backed by the non-profit django CMS Association (`Sponsor us! <https://www.django-cms.org/en/memberships/>`_).

*******************************************
Contribute to this project and win rewards
*******************************************

Because django CMS is a community-driven project, we welcome everyone to `get involved in the project <https://www.django-cms.org/en/contribute/>`_. "
maigret,"# Maigret

<p align=""center"">
  <p align=""center"">
    <a href=""https://pypi.org/project/maigret/"">
      <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/maigret?style=flat-square"">
    </a>
    <a href=""https://pypi.org/project/maigret/"">
      <img alt=""PyPI - Downloads"" src=""https://img.shields.io/pypi/dw/maigret?style=flat-square"">
    </a>
    <a href=""https://pypi.org/project/maigret/"">
      <img alt=""Views"" src=""https://komarev.com/ghpvc/?username=maigret&color=brightgreen&label=views&style=flat-square"">
    </a>
  </p>
  <p align=""center"">
    <img src=""https://raw.githubusercontent.com/soxoj/maigret/main/static/maigret.png"" height=""200""/>
  </p>
</p>

<i>The Commissioner Jules Maigret is a fictional French police detective, created by Georges Simenon. His investigation method is based on understanding the personality of different people and their interactions.</i>

<b>ğŸ‘‰ğŸ‘‰ğŸ‘‰ [Online Telegram bot](https://t.me/osint_maigret_bot)</b>

## About

**Maigret** collects a dossier on"
MoneyPrinter,"# MoneyPrinter ğŸ’¸

Automate the creation of YouTube Shorts, simply by providing a video topic to talk about.

<a href=""https://trendshift.io/repositories/7545"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/7545"" alt=""FujiwaraChoki%2FMoneyPrinter | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

> **Important** Please make sure you look through existing/closed issues before opening your own. If it's just a question, please join our [discord](https://dsc.gg/fuji-community) and ask there.

> **ğŸ¥** Watch the video on [YouTube](https://youtu.be/mkZsaDA2JnA?si=pNne3MnluRVkWQbE).

Check out the instructions for the local version [here](Local.md).

## FAQ ğŸ¤”

### How do I get the TikTok session ID?

You can obtain your TikTok session ID by logging into TikTok in your browser and copying the value of the `sessionid` cookie.

### My ImageMagick binary is not being detected

Make sure you set your path to the ImageMagick binary correctly in th"
Megatron-LM,"<div align=""center"">

Megatron-LM & Megatron-Core
===========================
<h4>GPU optimized techniques for training transformer models at-scale</h4>

[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html)
[![version](https://img.shields.io/badge/release-0.5.0-green)](./setup.py)
[![license](https://img.shields.io/badge/license-OpenBSD-blue)](./LICENSE)

<div align=""left"">

# Latest News

- **[2024/7]** Megatron-Core v0.7 improves scalability and training resiliency and adds support for multimodal training ([blog](https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-megatron-core-functionalities/)). 
- **[2024/6]** Megatron-Core added supports for Mamba-based models. Check out our paper [An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887) and [code example](https://github.com/NVIDIA/Megatron-LM/tree/ssm/ex"
bpytop,"# ![bpytop](https://github.com/aristocratos/bpytop/raw/master/Imgs/logo.png)

<a href=""https://repology.org/project/bpytop/versions"">
    <img src=""https://repology.org/badge/vertical-allrepos/bpytop.svg"" alt=""Packaging status"" align=""right"">
</a>

![Linux](https://img.shields.io/badge/-Linux-grey?logo=linux)
![OSX](https://img.shields.io/badge/-OSX-black?logo=apple)
![FreeBSD](https://img.shields.io/badge/-FreeBSD-red?logo=freebsd)
![Usage](https://img.shields.io/badge/Usage-System%20resource%20monitor-yellow)
![Python](https://img.shields.io/badge/Python-v3.7%5E-green?logo=python)
![bpytop_version](https://img.shields.io/github/v/tag/aristocratos/bpytop?label=version)
[![pypi_version](https://img.shields.io/pypi/v/bpytop?label=pypi)](https://pypi.org/project/bpytop)
[![Test Status](https://img.shields.io/github/workflow/status/aristocratos/bpytop/Testing?label=tests)](https://github.com/aristocratos/bpytop/actions?query=workflow%3Atesting)
[![Donate](https://img.shields.io/badge/-Don"
pyvideotrans,"ç®€ä½“ä¸­æ–‡ | [English](docs/EN/README_EN.md) | [pt-BR](docs/pt-BR/README_pt-BR.md) | [Italian](docs/IT/README_IT.md) | [Spanish](docs/ES/README_ES.md) / [æåŠ©](docs/about.md) / [Discord](https://discord.gg/y9gUweVCCJ) / å¾®ä¿¡å…¬ä¼—å·ï¼š`pyvideotrans`

# è§†é¢‘ç¿»è¯‘é…éŸ³å·¥å…·

è¿™æ˜¯ä¸€ä¸ªè§†é¢‘ç¿»è¯‘é…éŸ³å·¥å…·ï¼Œå¯å°†ä¸€ç§è¯­è¨€çš„è§†é¢‘ç¿»è¯‘ä¸ºæŒ‡å®šè¯­è¨€çš„è§†é¢‘ï¼Œè‡ªåŠ¨ç”Ÿæˆå’Œæ·»åŠ è¯¥è¯­è¨€çš„å­—å¹•å’Œé…éŸ³ã€‚å¹¶æ”¯æŒAPIè°ƒç”¨


è¯­éŸ³è¯†åˆ«æ”¯æŒ `faster-whisper`å’Œ`openai-whisper`æœ¬åœ°ç¦»çº¿æ¨¡å‹ åŠ `OpenAI SpeechToText API`  `GoogleSpeech` `é˜¿é‡Œä¸­æ–‡è¯­éŸ³è¯†åˆ«æ¨¡å‹`å’Œè±†åŒ…æ¨¡å‹ï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰è¯­éŸ³è¯†åˆ«api.

æ–‡å­—ç¿»è¯‘æ”¯æŒ `å¾®è½¯ç¿»è¯‘|Googleç¿»è¯‘|ç™¾åº¦ç¿»è¯‘|è…¾è®¯ç¿»è¯‘|ChatGPT|AzureAI|Gemini|DeepL|DeepLX|å­—èŠ‚ç«å±±|ç¦»çº¿ç¿»è¯‘OTT`

æ–‡å­—åˆæˆè¯­éŸ³æ”¯æŒ `Microsoft Edge tts` `Google tts` `Azure AI TTS` `Openai TTS` `Elevenlabs TTS` `è‡ªå®šä¹‰TTSæœåŠ¡å™¨api` `GPT-SoVITS` [clone-voice](https://github.com/jianchang512/clone-voice)  [ChatTTS-ui](https://github.com/jianchang512/ChatTTS-ui)  [Fish TTS](https://github.com/fishaudio/fish-speech)  [CosyVoice](https://github.com/FunAudioLLM/CosyVoice)

å…è®¸ä¿ç•™èƒŒæ™¯ä¼´å¥éŸ³ä¹ç­‰(åŸºäºuvr5)

æ”¯æŒçš„è¯­è¨€ï¼šä¸­æ–‡ç®€ç¹ã€è‹±è¯­ã€éŸ©è¯­ã€æ—¥è¯­ã€ä¿„è¯­ã€æ³•è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­ã€è¥¿ç­ç‰™è¯­ã€è‘¡è„ç‰™è¯­ã€è¶Šå—è¯­ã€æ³°å›½è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€åœŸè€³å…¶è¯­ã€åŒˆç‰™åˆ©è¯­ã€å°åº¦è¯­ã€ä¹Œå…‹å…°è¯­ã€å“ˆè¨å…‹è¯­ã€å°å°¼è¯­ã€é©¬æ¥è¯­ã€æ·å…‹è¯­ã€æ³¢å…°è¯­ã€è·å…°è¯­ã€ç‘å…¸è¯­


> **[èµåŠ©å•†]**
>
"
word_cloud,"[![licence](http://img.shields.io/badge/licence-MIT-blue.svg?style=flat)](https://github.com/amueller/word_cloud/blob/master/LICENSE)
[![DOI](https://zenodo.org/badge/21369/amueller/word_cloud.svg)](https://zenodo.org/badge/latestdoi/21369/amueller/word_cloud)


word_cloud
==========

A little word cloud generator in Python. Read more about it on the [blog
post][blog-post] or the [website][website].

The code is tested against Python 3.7, 3.8, 3.9, 3.10, 3.11, 3.12.

## Installation

If you are using pip:

    pip install wordcloud

If you are using conda, you can install from the `conda-forge` channel:

    conda install -c conda-forge wordcloud


#### Installation notes

wordcloud depends on `numpy`, `pillow`, and `matplotlib`.

If there are no wheels available for your version of python, installing the
package requires having a C compiler set up. Before installing a compiler, report
an issue describing the version of python and operating system being used.


## Examples

Check out ["
starlette,"<p align=""center"">
  <a href=""https://www.starlette.io/""><img width=""420px"" src=""https://raw.githubusercontent.com/encode/starlette/master/docs/img/starlette.svg"" alt='starlette'></a>
</p>
<p align=""center"">
    <em>âœ¨ The little ASGI framework that shines. âœ¨</em>
</p>

---

[![Build Status](https://github.com/encode/starlette/workflows/Test%20Suite/badge.svg)](https://github.com/encode/starlette/actions)
[![Package version](https://badge.fury.io/py/starlette.svg)](https://pypi.python.org/pypi/starlette)
[![Supported Python Version](https://img.shields.io/pypi/pyversions/starlette.svg?color=%2334D058)](https://pypi.org/project/starlette)

---

**Documentation**: <a href=""https://www.starlette.io/"" target=""_blank"">https://www.starlette.io</a>

**Source Code**: <a href=""https://github.com/encode/starlette"" target=""_blank"">https://github.com/encode/starlette</a>

---

# Starlette

Starlette is a lightweight [ASGI][asgi] framework/toolkit,
which is ideal for building async web services in P"
LLMSurvey,"# LLMSurvey


> A collection of papers and resources related to Large Language Models. 
>
> The organization of papers refers to our survey [**""A Survey of Large Language Models""**](https://arxiv.org/abs/2303.18223). [![Paper page](https://huggingface.co/datasets/huggingface/badges/raw/main/paper-page-sm-dark.svg)](https://huggingface.co/papers/2303.18223)
>
> Please let us know if you find out a mistake or have any suggestions by e-mail: batmanfly@gmail.com
>
> (we suggest ccing another email francis_kun_zhou@163.com meanwhile, in case of any unsuccessful delivery issue.)
>
>
> If you find our survey useful for your research, please cite the following paper:

```
@article{LLMSurvey,
    title={A Survey of Large Language Models},
    author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao "
the-algorithm-ml,"This project open sources some of the ML models used at Twitter.

Currently these are:

1. The ""For You"" Heavy Ranker (projects/home/recap).

2. TwHIN embeddings (projects/twhin) https://arxiv.org/abs/2202.05387


This project can be run inside a python virtualenv. We have only tried this on Linux machines and because we use torchrec it works best with an Nvidia GPU. To setup run

`./images/init_venv.sh` (Linux only).

The READMEs of each project contain instructions about how to run each project.
"
thumbor,"<h4 align=""center"">Join <a href=""https://github.com/thumbor/thumbor-bootcamp"">thumbor-bootcamp</a> for a learning and contribution experience with â¤ï¸ and ğŸ¤— from the thumbor team</h4>

<p align=""center"">
  <a href=""http://www.thumbor.org"">
    <img title=""thumbor"" alt=""thumbor"" src=""https://github.com/thumbor/thumbor/blob/readme/docs/thumbor-logo.png?raw=true"" />
  </a>
</p>

<h3 align=""center"">
Crop, resize, transform and much more, all on-demand and AI Powered
</h3>

<p align=""center"">
  <img src='https://github.com/thumbor/thumbor/workflows/build/badge.svg' />
  <a href='https://coveralls.io/github/thumbor/thumbor?branch=master' target='_blank'>
    <img src='https://coveralls.io/repos/thumbor/thumbor/badge.svg?branch=master&service=github'/>
  </a>
  <a href='https://codeclimate.com/github/thumbor/thumbor' target='_blank'>
    <img src='https://codeclimate.com/github/thumbor/thumbor/badges/gpa.svg'/>
  </a>
  <a href='https://pypi.python.org/pypi/thumbor' target='_blank'>
    <img s"
video2x,"<p align=""center"">
   <img src=""https://user-images.githubusercontent.com/21986859/102733190-872a7880-4334-11eb-8e9e-0ca747f130b1.png""/>
   </br>
   <img src=""https://img.shields.io/github/v/release/k4yt3x/video2x?style=flat-square""/>
   <img src=""https://img.shields.io/github/actions/workflow/status/k4yt3x/video2x/ci.yml?label=CI&style=flat-square""/>
   <img src=""https://img.shields.io/github/downloads/k4yt3x/video2x/total?style=flat-square""/>
   <img src=""https://img.shields.io/github/license/k4yt3x/video2x?style=flat-square""/>
   <img src=""https://img.shields.io/badge/dynamic/json?color=%23e85b46&label=Patreon&query=data.attributes.patron_count&suffix=%20patrons&url=https%3A%2F%2Fwww.patreon.com%2Fapi%2Fcampaigns%2F4507807&style=flat-square""/>
</p>

## [ğŸ’¬ Telegram Discussion Group](https://t.me/video2x)

Join our Telegram discussion group to ask any questions you have about Video2X, chat directly with the developers, or discuss about upscaling technologies and the future of Video2X "
statsmodels,".. image:: docs/source/images/statsmodels-logo-v2-horizontal.svg
  :alt: Statsmodels logo

|PyPI Version| |Conda Version| |License| |Azure CI Build Status|
|Codecov Coverage| |Coveralls Coverage| |PyPI downloads| |Conda downloads|

About statsmodels
=================

statsmodels is a Python package that provides a complement to scipy for
statistical computations including descriptive statistics and estimation
and inference for statistical models.


Documentation
=============

The documentation for the latest release is at

https://www.statsmodels.org/stable/

The documentation for the development version is at

https://www.statsmodels.org/dev/

Recent improvements are highlighted in the release notes

https://www.statsmodels.org/stable/release/

Backups of documentation are available at https://statsmodels.github.io/stable/
and https://statsmodels.github.io/dev/.


Main Features
=============

* Linear regression models:

  - Ordinary least squares
  - Generalized least squares
  - W"
spinningup,"**Status:** Maintenance (expect bug fixes and minor updates)

Welcome to Spinning Up in Deep RL! 
==================================

This is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).

For the unfamiliar: [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with [deep learning](http://ufldl.stanford.edu/tutorial/).

This module contains a variety of helpful resources, including:

- a short [introduction](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) to RL terminology, kinds of algorithms, and basic theory,
- an [essay](https://spinningup.openai.com/en/latest/spinningup/spinningup.html) about how to grow into an RL research role,
- a [curated list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) of important papers "
litgpt,"<div align=""center"">


# âš¡ LitGPT

**20+ high-performance LLMs with recipes to pretrain, finetune, and deploy at scale.**

<pre>
âœ… From scratch implementations     âœ… No abstractions    âœ… Beginner friendly   
âœ… Flash attention                  âœ… FSDP               âœ… LoRA, QLoRA, Adapter
âœ… Reduce GPU memory (fp4/8/16/32)  âœ… 1-1000+ GPUs/TPUs  âœ… 20+ LLMs            
</pre>


---


![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)
![cpu-tests](https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg) [![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/lit-stablelm/blob/master/LICENSE) [![Discord](https://img.shields.io/discord/1077906959069626439)](https://discord.gg/VptPCZkGNa)

<p align=""center"">
  <a href=""#quick-start"">Quick start</a> â€¢
  <a href=""#choose-from-20-llms"">Models</a> â€¢
  <a href=""#finetune-an-llm"">Finetune</a> â€¢ 
  <a href=""#deploy-an-llm"">Deploy</a> â€¢    
  "
AudioGPT,"# AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head

[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2304.12995)
[![GitHub Stars](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social)](https://github.com/AIGC-Audio/AudioGPT)
![visitors](https://visitor-badge.glitch.me/badge?page_id=AIGC-Audio.AudioGPT)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/AIGC-Audio/AudioGPT)


We provide our implementation and pretrained models as open source in this repository.


## Get Started

Please refer to [run.md](run.md)


## Capabilities

Here we list the capability of AudioGPT at this time. More supported models and tasks are coming soon. For prompt examples, refer to [asset](assets/README.md).

Currently not every model has repository.
### Speech
|            Task            |   Supported Foundation Models   | Status |
|:--------------------------:|:-"
visdom,"

<h3 align=""center"">
    <br/>
    <img src=""https://user-images.githubusercontent.com/19650074/198746195-574bb828-026f-41cb-82a9-250fcbc4e090.png"" width=""300"" alt=""Logo""/><br/><br/>
    Creating, organizing & sharing visualizations of live, rich data. Supports <a href=""https://pypi.org/project/visdom/"">Python</a>.
</h3>


<p align=""center""> Jump To: <a href=""#setup"">Setup</a>, <a href=""#usage"">Usage</a>, <a href=""#api"">API</a>, <a href=""#customizing-visdom"">Customizing</a>, <a href=""#contributing"">Contributing</a>, <a href=""#license"">License</a>
</p>


<p align=""center"">
    <a href=""https://github.com/fossasia/visdom/releases""><img src=""https://img.shields.io/github/v/release/fossasia/visdom?colorA=363a4f&colorB=a6da95&style=for-the-badge""/></a>
    <a href=""https://pypi.org/project/visdom""><img src=""https://img.shields.io/pypi/dd/visdom?colorA=363a4f&colorB=156df1&style=for-the-badge""></a>
    <a href=""https://github.com/fossasia/visdom/commits""><img src=""https://img.shields.io/git"
EverydayWechat,![python_vesion](https://img.shields.io/badge/Python-3.5%2B-green.svg)   [![itchat_vesion](https://img.shields.io/badge/Itchat-1.3.10-brightgreen.svg)](https://github.com/littlecodersh/ItChat)   [![codebeat badge](https://codebeat.co/badges/0953014f-dbd3-41f4-bacd-60018e7d5065)](https://codebeat.co/projects/github-com-sfyc23-everydaywechat-master)   [![Codacy Badge](https://api.codacy.com/project/badge/Grade/a278078ba9a14e22bd86740b0807a78e)](https://www.codacy.com/app/sfyc23/EverydayWechat?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=sfyc23/EverydayWechat&amp;utm_campaign=Badge_Grade)   [![MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/sfyc23/EverydayWechat/blob/master/LICENSE)               [![weibo](https://img.shields.io/badge/weibo-@sfyc23-red.svg)](https://www.weibo.com/sfyc23)  [![GitHub issues](https://img.shields.io/github/issues/sfyc23/EverydayWechat.svg)](https://github.com/sfyc23/EverydayWechat/issues)  [![GitHub contributors](h
streamlink,"<h1 align=""center""><a href=""https://streamlink.github.io/"">Streamlink<br><img height=""150"" alt=""Streamlink"" src=""https://raw.githubusercontent.com/streamlink/streamlink/master/icon.svg""></a></h1>

<p align=""center"">
  <a href=""https://streamlink.github.io/install.html""><img alt=""Supported Python versions"" src=""https://img.shields.io/pypi/pyversions/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://streamlink.github.io/changelog.html""><img alt=""Latest release"" src=""https://img.shields.io/github/release/streamlink/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://github.com/streamlink/streamlink""><img alt=""License"" src=""https://img.shields.io/github/license/streamlink/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://github.com/streamlink/streamlink/issues""><img alt=""Open issues"" src=""https://img.shields.io/github/issues/streamlink/streamlink.svg?style=flat-square&maxAge=86400""></a>
  <a href=""https://github.com/streamlink/st"
eat_tensorflow2_in_30_days,"# How to eat TensorFlow2 in 30 days ?ğŸ”¥ğŸ”¥

Click here for [Chinese Versionï¼ˆä¸­æ–‡ç‰ˆï¼‰](#30å¤©åƒæ‰é‚£åª-tensorflow2)

**ã€Š10å¤©åƒæ‰é‚£åªpysparkã€‹**
* ğŸš€ githubé¡¹ç›®åœ°å€: https://github.com/lyhue1991/eat_pyspark_in_10_days
* ğŸ³ å’Œé²¸ä¸“æ åœ°å€: https://www.heywhale.com/home/column/5fe6aa955e24ed00302304e0 ã€ä»£ç å¯ç›´æ¥forkåäº‘ç«¯è¿è¡Œï¼Œæ— éœ€é…ç½®ç¯å¢ƒã€‘


**ã€Š20å¤©åƒæ‰é‚£åªPytorchã€‹**
* ğŸš€ githubé¡¹ç›®åœ°å€: https://github.com/lyhue1991/eat_pytorch_in_20_days
* ğŸ³ å’Œé²¸ä¸“æ åœ°å€: https://www.heywhale.com/home/column/5f2ac5d8af3980002cb1bc08 ã€ä»£ç å¯ç›´æ¥forkåäº‘ç«¯è¿è¡Œï¼Œæ— éœ€é…ç½®ç¯å¢ƒã€‘


**ã€Š30å¤©åƒæ‰é‚£åªTensorFlow2ã€‹**
* ğŸš€ githubé¡¹ç›®åœ°å€: https://github.com/lyhue1991/eat_tensorflow2_in_30_days
* ğŸ³ å’Œé²¸ä¸“æ åœ°å€: https://www.heywhale.com/home/column/5d8ef3c3037db3002d3aa3a0 ã€ä»£ç å¯ç›´æ¥forkåäº‘ç«¯è¿è¡Œï¼Œæ— éœ€é…ç½®ç¯å¢ƒã€‘

**æé€Ÿé€šé“** 
*  ğŸš€ å…¬ä¼—å· â€œ**ç®—æ³•ç¾é£Ÿå±‹**â€ åå°å›å¤æš—å·ï¼š""**åƒè´§æ¥äº†**""
*  ğŸ˜‹ è·å–ä»¥ä¸Š3å¥—æ•™ç¨‹çš„jupyter notebook æºç æ–‡ä»¶ä»¥åŠå…¨éƒ¨æ•°æ®é›†çš„ç™¾åº¦äº‘ç›˜ä¸‹è½½é“¾æ¥ã€‚
*   https://mp.weixin.qq.com/s/ymLtH5BqlWAkpOmCLQOYxw 


### 1. TensorFlow2 ğŸ or PytorchğŸ”¥

Conclusion first: 

**For the engineers, priority goes to TensorFlow2.**

**For the students and researchersï¼Œfirst choice should be"
surya,"# Surya

Surya is a document OCR toolkit that does:

- OCR in 90+ languages that benchmarks favorably vs cloud services
- Line-level text detection in any language
- Layout analysis (table, image, header, etc detection)
- Reading order detection

It works on a range of documents (see [usage](#usage) and [benchmarks](#benchmarks) for more details).

|                            Detection                             |                                   OCR                                   |
|:----------------------------------------------------------------:|:-----------------------------------------------------------------------:|
|  ![New York Times Article Detection](static/images/excerpt.png)  |  ![New York Times Article Recognition](static/images/excerpt_text.png)  |

|                               Layout                               |                               Reading Order                                |
|:------------------------------------------------------------------:|:"
ffmpeg-python,"# ffmpeg-python: Python bindings for FFmpeg

[![CI][ci-badge]][ci]

[ci-badge]: https://github.com/kkroening/ffmpeg-python/actions/workflows/ci.yml/badge.svg
[ci]: https://github.com/kkroening/ffmpeg-python/actions/workflows/ci.yml

<img src=""https://raw.githubusercontent.com/kkroening/ffmpeg-python/master/doc/formula.png"" alt=""ffmpeg-python logo"" width=""60%"" />

## Overview

There are tons of Python FFmpeg wrappers out there but they seem to lack complex filter support.  `ffmpeg-python` works well for simple as well as complex signal graphs.


## Quickstart

Flip a video horizontally:
```python
import ffmpeg
stream = ffmpeg.input('input.mp4')
stream = ffmpeg.hflip(stream)
stream = ffmpeg.output(stream, 'output.mp4')
ffmpeg.run(stream)
```

Or if you prefer a fluent interface:
```python
import ffmpeg
(
    ffmpeg
    .input('input.mp4')
    .hflip()
    .output('output.mp4')
    .run()
)
```

## [API reference](https://kkroening.github.io/ffmpeg-python/)

## Complex filter graphs
FFmpe"
Theano,"============================================================================================================
MILA has stopped developing Theano: https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ

The PyMC developers have forked Theano to a new project called PyTensor that is being actively developed: https://github.com/pymc-devs/pytensor
============================================================================================================
"
neural-doodle,"Neural Doodle
=============

.. image:: docs/Workflow.gif

Use a deep neural network to borrow the skills of real artists and turn your two-bit doodles into masterpieces! This project is an implementation of `Semantic Style Transfer <http://arxiv.org/abs/1603.01768>`_ (Champandard, 2016), based on the `Neural Patches <http://arxiv.org/abs/1601.04589>`_ algorithm (Li, 2016). Read more about the motivation in this `in-depth article <https://nucl.ai/blog/neural-doodles/>`_ and watch this `workflow video <https://www.youtube.com/watch?v=fu2fzx4w3mI>`_ for inspiration.

The ``doodle.py`` script generates a new image by using one, two, three or four images as inputs depending what you're trying to do: the original style and its annotation, and a target content image (optional) with its annotation (a.k.a. your doodle). The algorithm extracts annotated patches from the style image, and incrementally transfers them over to the target image based on how closely they match.

**NOTE**: Making a ``"
open_clip,"# OpenCLIP

[[Paper]](https://arxiv.org/abs/2212.07143) [[Citations]](#citing) [[Clip Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb) [[Coca Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)
[![pypi](https://img.shields.io/pypi/v/open_clip_torch.svg)](https://pypi.python.org/pypi/open_clip_torch)

Welcome to an open source implementation of OpenAI's [CLIP](https://arxiv.org/abs/2103.00020) (Contrastive Language-Image Pre-training).

Using this codebase, we have trained several models on a variety of data sources and compute budgets, ranging from [small-scale experiments](docs/LOW_ACC.md) to larger runs including models trained on datasets such as [LAION-400M](https://arxiv.org/abs/2111.02114), [LAION-2B](https://arxiv.org/abs/2210.08402) and [DataComp-1B](https://arxiv.org/abs/2304.14108).
Many of our models and their scaling properties a"
node-gyp,"# `node-gyp` - Node.js native addon build tool

[![Build Status](https://github.com/nodejs/node-gyp/workflows/Tests/badge.svg?branch=main)](https://github.com/nodejs/node-gyp/actions?query=workflow%3ATests+branch%3Amain)
![npm](https://img.shields.io/npm/dm/node-gyp)

`node-gyp` is a cross-platform command-line tool written in Node.js for
compiling native addon modules for Node.js. It contains a vendored copy of the
[gyp-next](https://github.com/nodejs/gyp-next) project that was previously used
by the Chromium team and extended to support the development of Node.js native
addons.

Note that `node-gyp` is _not_ used to build Node.js itself.

All current and LTS target versions of Node.js are supported. Depending on what version of Node.js is actually installed on your system
`node-gyp` downloads the necessary development files or headers for the target version. List of stable Node.js versions can be found on [Node.js website](https://nodejs.org/en/about/previous-releases).

## Features
"
kedro,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-light.png"">
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-dark.png"">
    <img src=""https://raw.githubusercontent.com/kedro-org/kedro/main/.github/demo-light.png"" alt=""Kedro"">
  </picture>
</p>

[![Python version](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue.svg)](https://pypi.org/project/kedro/)
[![PyPI version](https://badge.fury.io/py/kedro.svg)](https://pypi.org/project/kedro/)
[![Conda version](https://img.shields.io/conda/vn/conda-forge/kedro.svg)](https://anaconda.org/conda-forge/kedro)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/kedro-org/kedro/blob/main/LICENSE.md)
[![Slack Organisation](https://img.shields.io/badge/slack-chat-blueviolet.svg?label=K"
great_expectations,"[![Python Versions](https://img.shields.io/pypi/pyversions/great_expectations.svg)](https://pypi.python.org/pypi/great_expectations)
[![PyPI](https://img.shields.io/pypi/v/great_expectations)](https://pypi.org/project/great-expectations/#history)
[![PyPI Downloads](https://img.shields.io/pypi/dm/great-expectations)](https://pypistats.org/packages/great-expectations)
[![Build Status](https://img.shields.io/azure-devops/build/great-expectations/bedaf2c2-4c4a-4b37-87b0-3877190e71f5/1)](https://dev.azure.com/great-expectations/great_expectations/_build/latest?definitionId=1&branchName=develop)
[![pre-commit.ci Status](https://results.pre-commit.ci/badge/github/great-expectations/great_expectations/develop.svg)](https://results.pre-commit.ci/latest/github/great-expectations/great_expectations/develop)
[![codecov](https://codecov.io/gh/great-expectations/great_expectations/graph/badge.svg?token=rbHxgTxYTs)](https://codecov.io/gh/great-expectations/great_expectations)
[![DOI](https://zenodo.o"
practical-python,"# Welcome!

When I first learned Python nearly 27 years ago, I was immediately
struck by how I could productively apply it to all sorts of messy work
projects. Fast-forward a decade and I found myself teaching others the
same fun.  The result of that teaching is this course--A no-nonsense
treatment of Python that has been actively taught to more than 400
in-person groups since 2007.  Traders, systems admins, astronomers,
tinkerers, and even a few hundred rocket scientists who used Python to
help land a rover on Mars--they've all taken this course. Now, I'm
pleased to make it available under a Creative Commons license--completely
free of spam, signups, and other nonsense. Enjoy!

[GitHub Pages](https://dabeaz-course.github.io/practical-python) | [GitHub Repo](https://github.com/dabeaz-course/practical-python).

--David Beazley ([https://dabeaz.com](https://dabeaz.com)), [@dabeaz](https://mastodon.social/@dabeaz)

(P.S. This course is about Python. If you want a Python course that's abou"
kornia,"<div align=""center"">
<p align=""center"">
  <img width=""55%"" src=""https://github.com/kornia/data/raw/main/kornia_banner_pixie.png"" />
</p>

---

English | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

<!-- prettier-ignore -->
<a href=""https://kornia.readthedocs.io"">Docs</a> â€¢
<a href=""https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/nbs/hello_world_tutorial.ipynb"">Try it Now</a> â€¢
<a href=""https://kornia.github.io/tutorials/"">Tutorials</a> â€¢
<a href=""https://github.com/kornia/kornia-examples"">Examples</a> â€¢
<a href=""https://kornia.github.io//kornia-blog"">Blog</a> â€¢
<a href=""https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-CnydWe5fmvkcktIeRFGCEQ"">Community</a>

[![PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/kornia)
[![Downloads](https://static.pepy.tech/badge/kornia)](https://pepy.tech/project/kornia)
[![Slack](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=white)](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu"
numba,"*****
Numba
*****

.. image:: https://badges.gitter.im/numba/numba.svg
   :target: https://gitter.im/numba/numba?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge
   :alt: Gitter

.. image:: https://img.shields.io/badge/discuss-on%20discourse-blue
   :target: https://numba.discourse.group/
   :alt: Discourse

.. image:: https://zenodo.org/badge/3659275.svg
   :target: https://zenodo.org/badge/latestdoi/3659275
   :alt: Zenodo DOI

.. image:: https://img.shields.io/pypi/v/numba.svg
   :target: https://pypi.python.org/pypi/numba/
   :alt: PyPI

.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.numba?branchName=main
    :target: https://dev.azure.com/numba/numba/_build/latest?definitionId=1?branchName=main
    :alt: Azure Pipelines

A Just-In-Time Compiler for Numerical Functions in Python
#########################################################

Numba is an open source, NumPy-aware optimizing compiler for Python sponsored
by Anaconda, Inc.  It uses the LLVM com"
rq,"RQ (_Redis Queue_) is a simple Python library for queueing jobs and processing
them in the background with workers.  It is backed by Redis and it is designed
to have a low barrier to entry.  It should be integrated in your web stack
easily.

RQ requires Redis >= 3.0.0.

[![Build status](https://github.com/rq/rq/workflows/Test%20rq/badge.svg)](https://github.com/rq/rq/actions?query=workflow%3A%22Test+rq%22)
[![PyPI](https://img.shields.io/pypi/pyversions/rq.svg)](https://pypi.python.org/pypi/rq)
[![Coverage](https://codecov.io/gh/rq/rq/branch/master/graph/badge.svg)](https://codecov.io/gh/rq/rq)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


Full documentation can be found [here][d].


## Support RQ

If you find RQ useful, please consider supporting this project via [Tidelift](https://tidelift.com/subscription/pkg/pypi-rq?utm_source=pypi-rq&utm_medium=referral&utm_campaign=readme).


## Getting started

First, run a Red"
musicbox,"# NetEase-MusicBox

**æ„Ÿè°¢ä¸º MusicBox çš„å¼€å‘ä»˜å‡ºè¿‡åŠªåŠ›çš„[æ¯ä¸€ä¸ªäºº](https://github.com/darknessomi/musicbox/graphs/contributors)ï¼**

é«˜å“è´¨ç½‘æ˜“äº‘éŸ³ä¹å‘½ä»¤è¡Œç‰ˆæœ¬ï¼Œç®€æ´ä¼˜é›…ï¼Œä¸èˆ¬é¡ºæ»‘ï¼ŒåŸºäº Python ç¼–å†™ã€‚

[![Software License](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)
[![versions](https://img.shields.io/pypi/v/NetEase-MusicBox.svg)](https://pypi.org/project/NetEase-MusicBox/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/NetEase-MusicBox.svg)](https://pypi.org/project/NetEase-MusicBox/)

## Demo

[![NetEase-MusicBox-GIF](https://qfile.aobeef.cn/3abba3b8a3994ee3d5cd.gif)](https://pypi.org/project/NetEase-MusicBox/)

## åŠŸèƒ½ç‰¹æ€§

1. 320kbps çš„é«˜å“è´¨éŸ³ä¹
2. æ­Œæ›²ï¼Œè‰ºæœ¯å®¶ï¼Œä¸“è¾‘æ£€ç´¢
3. ç½‘æ˜“ 22 ä¸ªæ­Œæ›²æ’è¡Œæ¦œ
4. ç½‘æ˜“æ–°ç¢Ÿæ¨è
5. ç½‘æ˜“ç²¾é€‰æ­Œå•
6. ç½‘æ˜“ä¸»æ’­ç”µå°
7. ç§äººæ­Œå•ï¼Œæ¯æ—¥æ¨è
8. éšå¿ƒæ‰“ç¢Ÿ
9. æœ¬åœ°æ”¶è—ï¼Œéšæ—¶åŠ  â¤
10. æ’­æ”¾è¿›åº¦åŠæ’­æ”¾æ¨¡å¼æ˜¾ç¤º
11. ç°åœ¨æ’­æ”¾åŠæ¡Œé¢æ­Œè¯æ˜¾ç¤º
12. æ­Œæ›²è¯„è®ºæ˜¾ç¤º
13. ä¸€é”®è¿›å…¥æ­Œæ›²ä¸“è¾‘
14. å®šæ—¶é€€å‡º
15. Vimer å¼å¿«æ·é”®è®©æ“ä½œä¸èˆ¬é¡ºæ»‘
16. å¯ä½¿ç”¨æ•°å­—å¿«æ·é”®
17. å¯ä½¿ç”¨è‡ªå®šä¹‰å…¨å±€å¿«æ·é”®
18. å¯¹å½“å‰æ­Œå•åˆ—è¡¨è¿›è¡Œæœ¬åœ°æ¨¡ç³Šæœç´¢

### é”®ç›˜å¿«æ·é”®

æœ‰ num + å­—æ ·çš„å¿«æ·é”®å¯ä»¥ç”¨æ•°å­—ä¿®é¥°ï¼ŒæŒ‰é”®é¡ºåºä¸ºå…ˆè¾“å…¥æ•°å­—å†é”®å…¥è¢«ä¿®é¥°çš„é”®ï¼Œå³ num + åçš„å¿«æ·é”®ã€‚

| Key                                   | Effect       "
OpenLLM,"# ğŸ¦¾ OpenLLM: Self-Hosting LLMs Made Easy

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202-green.svg)](https://github.com/bentoml/OpenLLM/blob/main/LICENSE)
[![Releases](https://img.shields.io/pypi/v/openllm.svg?logo=pypi&label=PyPI&logoColor=gold)](https://pypi.org/project/openllm)
[![CI](https://results.pre-commit.ci/badge/github/bentoml/OpenLLM/main.svg)](https://results.pre-commit.ci/latest/github/bentoml/OpenLLM/main)
[![X](https://badgen.net/badge/icon/@bentomlai/000000?icon=twitter&label=Follow)](https://twitter.com/bentomlai)
[![Community](https://badgen.net/badge/icon/Community/562f5d?icon=slack&label=Join)](https://l.bentoml.com/join-slack)

OpenLLM allows developers to run **anyÂ open-source LLMs** (Llama 3.1, Qwen2, Phi3 and [more](#supported-models)) or **custom models**Â asÂ **OpenAI-compatible APIs** withÂ a single command. It features a [built-in chat UI](#chat-ui), state-of-the-art inference backends, and a simplified workflow for creating enterprise"
modin,"<p align=""center""><a href=""https://modin.readthedocs.io""><img width=77% alt="""" src=""https://github.com/modin-project/modin/raw/7c009c747caa90554607e30b9ac2bd1b190b8c7d/docs/img/MODIN_ver2_hrz.png?raw=true""></a></p>
<h2 align=""center"">Scale your pandas workflows by changing one line of code</h2>

<div align=""center"">

| <h3>Dev Community & Support</h3> | <h3>Forums</h3> | <h3>Socials</h3> | <h3>Docs</h3> |
|:---: | :---: | :---: | :---: |
| [![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/modin-project/shared_invite/zt-yvk5hr3b-f08p_ulbuRWsAfg9rMY3uA) | [![Stack Overflow](https://img.shields.io/badge/-Stackoverflow-FE7A16?style=for-the-badge&logo=stack-overflow&logoColor=white)](https://stackoverflow.com/questions/tagged/modin) | <a href=""https://twitter.com/modin_project""><img alt=""Twitter Follow"" src=""https://img.shields.io/twitter/follow/modin_project?style=social"" height=28 align=""center""></a> | <a href=""ht"
Telethon,"Telethon
========
.. epigraph::

  â­ï¸ Thanks **everyone** who has starred the project, it means a lot!

|logo| **Telethon** is an asyncio_ **Python 3**
MTProto_ library to interact with Telegram_'s API
as a user or through a bot account (bot API alternative).

.. important::

    If you have code using Telethon before its 1.0 version, you must
    read `Compatibility and Convenience`_ to learn how to migrate.
    As with any third-party library for Telegram, be careful not to
    break `Telegram's ToS`_ or `Telegram can ban the account`_.

What is this?
-------------

Telegram is a popular messaging application. This library is meant
to make it easy for you to write Python programs that can interact
with Telegram. Think of it as a wrapper that has already done the
heavy job for you, so you can focus on developing an application.


Installing
----------

.. code-block:: sh

  pip3 install telethon


Creating a client
-----------------

.. code-block:: python

    from telethon import Te"
gunicorn,"Gunicorn
--------

.. image:: https://img.shields.io/pypi/v/gunicorn.svg?style=flat
    :alt: PyPI version
    :target: https://pypi.python.org/pypi/gunicorn

.. image:: https://img.shields.io/pypi/pyversions/gunicorn.svg
    :alt: Supported Python versions
    :target: https://pypi.python.org/pypi/gunicorn

.. image:: https://github.com/benoitc/gunicorn/actions/workflows/tox.yml/badge.svg
    :alt: Build Status
    :target: https://github.com/benoitc/gunicorn/actions/workflows/tox.yml

.. image:: https://github.com/benoitc/gunicorn/actions/workflows/lint.yml/badge.svg
    :alt: Lint Status
    :target: https://github.com/benoitc/gunicorn/actions/workflows/lint.yml

Gunicorn 'Green Unicorn' is a Python WSGI HTTP Server for UNIX. It's a pre-fork
worker model ported from Ruby's Unicorn_ project. The Gunicorn server is broadly
compatible with various web frameworks, simply implemented, light on server
resource usage, and fairly speedy.

Feel free to join us in `#gunicorn`_ on `Libera.chat"
sonnet,"![Sonnet](https://sonnet.dev/images/sonnet_logo.png)

# Sonnet

[**Documentation**](https://sonnet.readthedocs.io/) | [**Examples**](#examples)

Sonnet is a library built on top of [TensorFlow 2](https://www.tensorflow.org/)
designed to provide simple, composable abstractions for machine learning
research.

# Introduction

Sonnet has been designed and built by researchers at DeepMind. It can be used to
construct neural networks for many different purposes (un/supervised learning,
reinforcement learning, ...). We find it is a successful abstraction for our
organization, you might too!

More specifically, Sonnet provides a simple but powerful programming model
centered around a single concept: `snt.Module`. Modules can hold references to
parameters, other modules and methods that apply some function on the user
input. Sonnet ships with many predefined modules (e.g. `snt.Linear`,
`snt.Conv2D`, `snt.BatchNorm`) and some predefined networks of modules (e.g.
`snt.nets.MLP`) but users are als"
termtosvg,"**Note: As of June 2020 I do not have time to maintain termtosvg anymore and this repository is now read-only.**

# termtosvg
termtosvg is a Unix terminal recorder written in Python that renders your command
line sessions as standalone SVG animations.

![Example](./docs/examples/awesome_window_frame_powershell.svg)

* [Gallery of examples](https://nbedos.github.io/termtosvg/pages/examples.html)
* [Gallery of templates](https://nbedos.github.io/termtosvg/pages/templates.html)

## Features
* Produce lightweight and clean looking animations or still frames embeddable on a project page
* Custom color themes, terminal UI and animation controls via user-defined [SVG templates](man/termtosvg-templates.md)
* Rendering of recordings in asciicast format made with asciinema
    
## Installation
termtosvg is compatible with Linux, macOS and BSD OSes, requires Python >= 3.5 and can be installed as follows using pip:
```shell
# Create virtualenv named '.venv'
python3 -m venv .venv
# Activate virtual"
Sublist3r,"## About Sublist3r 

Sublist3r is a python tool designed to enumerate subdomains of websites using OSINT. It helps penetration testers and bug hunters collect and gather subdomains for the domain they are targeting. Sublist3r enumerates subdomains using many search engines such as Google, Yahoo, Bing, Baidu and Ask. Sublist3r also enumerates subdomains using Netcraft, Virustotal, ThreatCrowd, DNSdumpster and ReverseDNS.

[subbrute](https://github.com/TheRook/subbrute) was integrated with Sublist3r to increase the possibility of finding more subdomains using bruteforce with an improved wordlist. The credit goes to TheRook who is the author of subbrute.

## Screenshots

![Sublist3r](http://www.secgeek.net/images/Sublist3r.png ""Sublist3r in action"")


## Installation

```
git clone https://github.com/aboul3la/Sublist3r.git
```

## Recommended Python Version:

Sublist3r currently supports **Python 2** and **Python 3**.

* The recommended version for Python 2 is **2.7.x**
* The recommended "
ddddocr,"

# DdddOcr å¸¦å¸¦å¼Ÿå¼ŸOCRé€šç”¨éªŒè¯ç ç¦»çº¿æœ¬åœ°è¯†åˆ«SDKå…è´¹å¼€æºç‰ˆ

DdddOcrï¼Œå…¶ç”± [æœ¬ä½œè€…](https://github.com/sml2h3) ä¸ [kerlomz](https://github.com/kerlomz) å…±åŒåˆä½œå®Œæˆï¼Œé€šè¿‡å¤§æ‰¹é‡ç”Ÿæˆéšæœºæ•°æ®åè¿›è¡Œæ·±åº¦ç½‘ç»œè®­ç»ƒï¼Œæœ¬èº«å¹¶éé’ˆå¯¹ä»»ä½•ä¸€å®¶éªŒè¯ç å‚å•†è€Œåˆ¶ä½œï¼Œæœ¬åº“ä½¿ç”¨æ•ˆæœå®Œå…¨é ç„å­¦ï¼Œå¯èƒ½å¯ä»¥è¯†åˆ«ï¼Œå¯èƒ½ä¸èƒ½è¯†åˆ«ã€‚

DdddOcrã€æœ€ç®€ä¾èµ–çš„ç†å¿µï¼Œå°½é‡å‡å°‘ç”¨æˆ·çš„é…ç½®å’Œä½¿ç”¨æˆæœ¬ï¼Œå¸Œæœ›ç»™æ¯ä¸€ä½æµ‹è¯•è€…å¸¦æ¥èˆ’é€‚çš„ä½“éªŒ

é¡¹ç›®åœ°å€ï¼š [ç‚¹æˆ‘ä¼ é€](https://github.com/sml2h3/ddddocr) 

### è‡ªè¥GPTèšåˆå¹³å°ï¼š [ç‚¹æˆ‘ä¼ é€](https://juxiangyun.com/)

<!-- PROJECT SHIELDS -->

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]

<!-- PROJECT LOGO -->
<br />

<p align=""center"">
  <a href=""https://github.com/shaojintian/Best_README_template/"">
    <img src=""https://cdn.wenanzhe.com/img/logo.png!/crop/700x500a400a500"" alt=""Logo"">
  </a>
  <p align=""center"">
    ä¸€ä¸ªå®¹æ˜“ä½¿ç”¨çš„é€šç”¨éªŒè¯ç è¯†åˆ«pythonåº“
    <br />
    <a href=""https://github.com/shaojintian/Best_README_template""><strong>æ¢ç´¢æœ¬é¡¹ç›®çš„æ–‡æ¡£ Â»</strong></a>
    <br />
    <br />
    Â·
    <a href=""ht"
mlcourse.ai,"<div align=""center"">

![ODS stickers](https://github.com/Yorko/mlcourse.ai/blob/main/img/ods_stickers.jpg)

**[mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course**

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-green)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
[![Slack](https://img.shields.io/badge/slack-ods.ai-orange)](https://opendatascience.slack.com/archives/C91N8TL83/p1567408586359500)
[![Donate](https://img.shields.io/badge/support-patreon-red)](https://www.patreon.com/ods_mlcourse)
[![Donate](https://img.shields.io/badge/support-ko--fi-red)](https://ko-fi.com/mlcourse_ai)

</div>

[mlcourse.ai](https://mlcourse.ai) is an open Machine Learning course by [OpenDataScience (ods.ai)](https://ods.ai/), led by [Yury Kashnitsky (yorko)](https://yorko.github.io/). Having both a Ph.D. degree in applied math and a Kaggle Competitions Master tier, Yury aimed at designing an ML course with a perfect balance between theory and "
robotframework,"Robot Framework
===============

.. contents::
   :local:

Introduction
------------

`Robot Framework <http://robotframework.org>`_ |r| is a generic open source
automation framework for acceptance testing, acceptance test driven
development (ATDD), and robotic process automation (RPA). It has simple plain
text syntax and it can be extended easily with generic and custom libraries.

Robot Framework is operating system and application independent. It is
implemented using `Python <http://python.org>`_ which is also the primary
language to extend it. The framework has a rich ecosystem around it consisting
of various generic libraries and tools that are developed as separate projects.
For more information about Robot Framework and the ecosystem, see
http://robotframework.org.

Robot Framework project is hosted on GitHub_ where you can find source code,
an issue tracker, and some further documentation. Downloads are hosted on PyPI_.

Robot Framework development is sponsored by non-profit `R"
tpot,"Master status: [![Master Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=master)](https://travis-ci.com/EpistasisLab/tpot)
[![Master Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/master?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=master)
[![Master Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=master)](https://coveralls.io/github/EpistasisLab/tpot?branch=master)

Development status: [![Development Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=development)](https://travis-ci.com/EpistasisLab/tpot/branches)
[![Development Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/development?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=development)
[![Development Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=development)](https://cov"
dbt-core,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/dbt-labs/dbt-core/fa1ea14ddfb1d5ae319d5141844910dd53ab2834/etc/dbt-core.svg"" alt=""dbt logo"" width=""750""/>
</p>
<p align=""center"">
  <a href=""https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml"">
    <img src=""https://github.com/dbt-labs/dbt-core/actions/workflows/main.yml/badge.svg?event=push"" alt=""CI Badge""/>
  </a>
</p>

**[dbt](https://www.getdbt.com/)** enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.

![architecture](https://github.com/dbt-labs/dbt-core/blob/202cb7e51e218c7b29eb3b11ad058bd56b7739de/etc/dbt-transform.png)

## Understanding dbt

Analysts using dbt can transform their data by simply writing select statements, while dbt handles turning these statements into tables and views in a data warehouse.

These select statements, or ""models"", form a dbt project. Models frequently build on top of one another â€“ dbt mak"
undetected-chromedriver,"# undetected_chromedriver #

https://github.com/ultrafunkamsterdam/undetected-chromedriver


Optimized Selenium Chromedriver patch which does not trigger anti-bot services like Distill Network / Imperva / DataDome / Botprotect.io
Automatically downloads the driver binary and patches it.

* Tested until current chrome beta versions
* Works also on Brave Browser and many other Chromium based browsers, but you need to know what you're doing and needs some tweaking.
* Python 3.6++**


## Installation ##

```
pip install undetected-chromedriver
```
or , if you're feeling adventurous, install directly via github

```
pip install git+https://www.github.com/ultrafunkamsterdam/undetected-chromedriver@master     # replace @master with @branchname for other branches
```


- - -
## Message for all ##
I will be putting limits on the issue tracker. It has beeen abused too long.  
any good news?  
Yes, i've opened [Undetected-Discussions](https://github.com/ultrafunkamste"
qutebrowser,"// SPDX-License-Identifier: GPL-3.0-or-later

// If you are reading this in plaintext or on PyPi:
//
// A rendered version is available at:
// https://github.com/qutebrowser/qutebrowser/blob/main/README.asciidoc

qutebrowser
===========

// QUTE_WEB_HIDE
image:qutebrowser/icons/qutebrowser-64x64.png[qutebrowser logo] *A keyboard-driven, vim-like browser based on Python and Qt.*

image:https://github.com/qutebrowser/qutebrowser/workflows/CI/badge.svg[""Build Status"", link=""https://github.com/qutebrowser/qutebrowser/actions?query=workflow%3ACI""]
image:https://codecov.io/github/qutebrowser/qutebrowser/coverage.svg?branch=main[""coverage badge"",link=""https://codecov.io/github/qutebrowser/qutebrowser?branch=main""]

link:https://www.qutebrowser.org[website] | link:https://blog.qutebrowser.org[blog] | https://github.com/qutebrowser/qutebrowser/blob/main/doc/faq.asciidoc[FAQ] | https://www.qutebrowser.org/doc/contributing.html[contributing] | link:https://github.com/qutebrowser/qutebrowser/relea"
tflearn,"[![Build Status](https://travis-ci.org/tflearn/tflearn.svg?branch=master)](https://travis-ci.org/tflearn/tflearn)
[![PyPI version](https://badge.fury.io/py/tflearn.svg)](https://badge.fury.io/py/tflearn)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Join the chat at https://gitter.im/einsteinsci/betterbeginnings](https://badges.gitter.im/tflearn/tflearn.svg)](https://gitter.im/tflearn/tflearn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

# TFLearn: Deep learning library featuring a higher-level API for TensorFlow.

TFlearn is a modular and transparent deep learning library built on top of Tensorflow.  It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.

TFLearn features include:

- Easy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.
- Fast prototyping t"
Python,"
# æ¬¢è¿å…³æ³¨æˆ‘çš„å¾®ä¿¡å…¬ä¼—å·ã€æ™ºèƒ½åˆ¶é€ ç¤¾åŒºã€‘

## å·¦æ‰‹ä»£ç ï¼Œå³æ‰‹åˆ¶é€ ï¼Œåˆ†äº«æ™ºèƒ½åˆ¶é€ ç›¸å…³æŠ€æœ¯å’Œä¸šåŠ¡ï¼ŒåŒ…æ‹¬ Python, C#, æ•°æ®åº“ï¼Œå·¥ä¸šå¤§æ•°æ®ã€ç‰©è”ç½‘æŠ€æœ¯åŠMES/ERP/SAPç­‰ç³»ç»Ÿã€‚

## å¯ä»¥é€šè¿‡å¾®ä¿¡å…¬ä¼—å·åŠ æˆ‘å¥½å‹

![äºŒç»´ç ](qrcode.jpg)

# å†…å®¹åˆ—è¡¨

## [Pythonå¾®ä¿¡å…¬ä¼—å·å¼€å‘](https://github.com/injetlee/Python/tree/master/wechat)

- ### Python å¾®ä¿¡å…¬ä¼—å·å¼€å‘â€”å°ç™½ç¯‡(ä¸€)

- ### Python å…¬ä¼—å·å¼€å‘â€”é¢œå€¼æ£€æµ‹

## [Python çˆ¬è™«å…¥é—¨åˆé›†](https://github.com/injetlee/Python/tree/master/%E7%88%AC%E8%99%AB%E9%9B%86%E5%90%88)

- ### Python çˆ¬è™«å…¥é—¨(ä¸€)â€”â€”çˆ¬å–ç³—äº‹ç™¾ç§‘

- ### Python çˆ¬è™«å…¥é—¨(äºŒ)â€”â€”çˆ¬å–å¦¹å­å›¾

- ### Python çˆ¬è™«â€”â€”Python å²—ä½åˆ†ææŠ¥å‘Š

- ### Python çˆ¬è™«åˆ©å™¨â€”â€”Seleniumä»‹ç»

- ### Python çˆ¬è™«â€”â€” æŠ–éŸ³ App è§†é¢‘æŠ“åŒ…çˆ¬å–

## [Python é»‘é­”æ³•](https://github.com/injetlee/Python/tree/master/Python%20%E9%BB%91%E9%AD%94%E6%B3%95)

- ### Python è¿œç¨‹å…³æœº

## SQL æ•°æ®åº“

- [1 å°æ—¶ SQL æé€Ÿå…¥é—¨ï¼ˆä¸€ï¼‰](https://mp.weixin.qq.com/s/Lx4B349OlD49ihJPnB6YiA)
- [1 å°æ—¶ SQL æé€Ÿå…¥é—¨ï¼ˆäºŒï¼‰](https://mp.weixin.qq.com/s/D-CEtGYomne5kV_Ji4lodA)
- [1 å°æ—¶ SQL æé€Ÿå…¥é—¨ï¼ˆä¸‰ï¼‰](https://mp.weixin.qq.com/s/7aJqrhCNcvnt2gO3p5P50Q)
- [SQL é«˜çº§æŸ¥è¯¢â€”â€”ï¼ˆå±‚æ¬¡åŒ–æŸ¥è¯¢ï¼Œé€’å½’ï¼‰](https://mp.weixin.qq.com/s/R9Yldd-5AK4ObRA9Lfbz-Q)
- [GROUP BYé«˜çº§æŸ¥è¯¢,ROLLUPï¼ŒCUBEï¼ŒGROUPPINGè¯¦è§£]("
Chinese-BERT-wwm,"# [Chinese-LLaMA-Alpaca-2 v1.0ç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)å·²æ­£å¼å‘å¸ƒï¼

[**ä¸­æ–‡è¯´æ˜**](https://github.com/ymcui/Chinese-BERT-wwm/) | [**English**](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md)

<p align=""center"">
    <br>
    <img src=""./pics/banner.png"" width=""500""/>
    <br>
</p>
<p align=""center"">
    <a href=""https://github.com/ymcui/Chinese-BERT-wwm/blob/master/LICENSE"">
        <img alt=""GitHub"" src=""https://img.shields.io/github/license/ymcui/Chinese-BERT-wwm.svg?color=blue&style=flat-square"">
    </a>
</p>

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPre-trained Language Modelsï¼‰å·²æˆä¸ºéå¸¸é‡è¦çš„åŸºç¡€æŠ€æœ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›ä¸­æ–‡ä¿¡æ¯å¤„ç†çš„ç ”ç©¶å‘å±•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†åŸºäºå…¨è¯æ©ç ï¼ˆWhole Word Maskingï¼‰æŠ€æœ¯çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹BERT-wwmï¼Œä»¥åŠä¸æ­¤æŠ€æœ¯å¯†åˆ‡ç›¸å…³çš„æ¨¡å‹ï¼šBERT-wwm-extï¼ŒRoBERTa-wwm-extï¼ŒRoBERTa-wwm-ext-large, RBT3, RBTL3ç­‰ã€‚  

- **[Pre-Training with Whole Word Masking for Chinese BERT](https://ieeexplore.ieee.org/document/9599397)**  
- *Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang*
- Published in *IEEE/ACM Transactions on Audio, Speech, and"
Osintgram,"# Osintgram ğŸ”ğŸ“¸

[![version-1.3](https://img.shields.io/badge/version-1.3-green)](https://github.com/Datalux/Osintgram/releases/tag/1.3)
[![GPLv3](https://img.shields.io/badge/license-GPLv3-blue)](https://img.shields.io/badge/license-GPLv3-blue)
[![Python3](https://img.shields.io/badge/language-Python3-red)](https://img.shields.io/badge/language-Python3-red)
[![Telegram](https://img.shields.io/badge/Telegram-Channel-blue.svg)](https://t.me/osintgram)
[![Docker](https://img.shields.io/badge/Docker-Supported-blue)](https://img.shields.io/badge/Docker-Supported-blue)

Osintgram is an **OSINT** tool on Instagram to collect, analyze, and run reconnaissance.

<p align=""center"">
<img align=""center"" src="".img/carbon.png"" width=""900"">
</p>

Disclaimer: **FOR EDUCATIONAL PURPOSE ONLY! The contributors do not assume any responsibility for the use of this tool.**

Warning: It is advisable to **not** use your own/primary account when using this tool.

## Tools and Commands ğŸ§°

Osintgram offers an int"
yolov10,"# [YOLOv10: Real-Time End-to-End Object Detection](https://arxiv.org/abs/2405.14458)


Official PyTorch implementation of **YOLOv10**.

<p align=""center"">
  <img src=""figures/latency.svg"" width=48%>
  <img src=""figures/params.svg"" width=48%> <br>
  Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs.
</p>

[YOLOv10: Real-Time End-to-End Object Detection](https://arxiv.org/abs/2405.14458).\
Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding\
[![arXiv](https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg)](https://arxiv.org/abs/2405.14458) <a href=""https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb#scrollTo=SaKTSzSWnG7s""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingf"
falcon,".. image:: https://raw.githubusercontent.com/falconry/falcon/master/logo/banner.jpg
   :align: center
   :alt: Falcon logo
   :target: https://falconframework.org/
   :width: 100 %

|Build Status| |Docs| |codecov.io|

The Falcon Web Framework
========================

`Falcon <https://falconframework.org>`__ is a minimalist ASGI/WSGI framework for
building mission-critical REST APIs and microservices, with a focus on
reliability, correctness, and performance at scale.

When it comes to building HTTP APIs, other frameworks weigh you down with tons
of dependencies and unnecessary abstractions. Falcon cuts to the chase with a
clean design that embraces HTTP and the REST architectural style.

Falcon apps work with any `WSGI <https://www.python.org/dev/peps/pep-3333/>`_
or `ASGI <https://asgi.readthedocs.io/en/latest/>`_ server, and run like a
champ under CPython 3.8+ and PyPy 3.8+.

Quick Links
-----------

* `Read the docs <https://falcon.readthedocs.io/en/stable>`_
  (`FAQ <https://falco"
pifuhd,"# [PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization (CVPR 2020)](https://shunsukesaito.github.io/PIFuHD/)

[![report](https://img.shields.io/badge/arxiv-report-red)](https://arxiv.org/pdf/2004.00452.pdf) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing)

News:
* \[2020/06/15\] Demo with Google Colab (incl. visualization) is available! Please check out [#pifuhd on Twitter](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) for many results tested by users!

This repository contains a pytorch implementation of ""Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization"".

![Teaser Image](https://shunsukesaito.github.io/PIFuHD/resources/images/pifuhd.gif)

This codebase provides: 
- test code
- visualization code

See our [blog post](https://ai.facebook.com/blog/facebook-research-at-cvp"
docker-android,"
<p align=""center"">
  <img id=""header"" src=""./images/logo_docker-android.png"" />
</p>

[![Paypal Donate](https://img.shields.io/badge/paypal-donate-blue.svg)](http://paypal.me/budtmo) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![codecov](https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg)](https://codecov.io/gh/budtmo/docker-android) [![Join the chat at https://gitter.im/budtmo/docker-android](https://badges.gitter.im/budtmo/docker-android.svg)](https://gitter.im/budtmo/docker-android?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![GitHub release](https://img.shields.io/github/release/budtmo/docker-android.svg)](https://github.com/budtmo/docker-android/releases)

Docker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).

Advantages of using this"
StreamDiffusion,"# StreamDiffusion

[English](./README.md) | [æ—¥æœ¬èª](./README-ja.md) | [í•œêµ­ì–´](./README-ko.md)

<p align=""center"">
  <img src=""./assets/demo_07.gif"" width=90%>
  <img src=""./assets/demo_09.gif"" width=90%>
</p>

# StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation

**Authors:** [Akio Kodaira](https://www.linkedin.com/in/akio-kodaira-1a7b98252/), [Chenfeng Xu](https://www.chenfengx.com/), Toshiki Hazama, [Takanori Yoshimoto](https://twitter.com/__ramu0e__), [Kohei Ohno](https://www.linkedin.com/in/kohei--ohno/), [Shogo Mitsuhori](https://me.ddpn.world/), [Soichi Sugano](https://twitter.com/toni_nimono), [Hanying Cho](https://twitter.com/hanyingcl), [Zhijian Liu](https://zhijianliu.com/), [Kurt Keutzer](https://scholar.google.com/citations?hl=en&user=ID9QePIAAAAJ)

StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation technique"
pip,"pip - The Python Package Installer
==================================

.. |pypi-version| image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.org/project/pip/
   :alt: PyPI

.. |python-versions| image:: https://img.shields.io/pypi/pyversions/pip
   :target: https://pypi.org/project/pip
   :alt: PyPI - Python Version

.. |docs-badge| image:: https://readthedocs.org/projects/pip/badge/?version=latest
   :target: https://pip.pypa.io/en/latest
   :alt: Documentation

|pypi-version| |python-versions| |docs-badge|

pip is the `package installer`_ for Python. You can use pip to install packages from the `Python Package Index`_ and other indexes.

Please take a look at our documentation for how to install and use pip:

* `Installation`_
* `Usage`_

We release updates regularly, with a new version every 3 months. Find more details in our documentation:

* `Release notes`_
* `Release process`_

If you find bugs, need help, or want to talk to the developers, please use our maili"
LaZagne,"
__The LaZagne Project !!!__
==

Description
----
The __LaZagne project__ is an open source application used to __retrieve lots of passwords__ stored on a local computer. 
Each software stores its passwords using different techniques (plaintext, APIs, custom algorithms, databases, etc.). This tool has been developed for the purpose of finding these passwords for the most commonly-used software. 

<p align=""center""><img src=""https://user-images.githubusercontent.com/10668373/43320585-3e34c124-91a9-11e8-9ebc-d8eabafd8ac5.png"" alt=""The LaZagne project""></p>

This project has been added to [pupy](https://github.com/n1nj4sec/pupy/) as a post-exploitation module. Python code will be interpreted in memory without touching the disk and it works on Windows and Linux host.

Standalones
----
Standalones are now available here: https://github.com/AlessandroZ/LaZagne/releases/

Installation
----
```
pip install -r requirements.txt
```

Usage
----
* Launch all modules
```
laZagne.exe all
```

* Laun"
sqlalchemy,"SQLAlchemy
==========

|PyPI| |Python| |Downloads|

.. |PyPI| image:: https://img.shields.io/pypi/v/sqlalchemy
    :target: https://pypi.org/project/sqlalchemy
    :alt: PyPI

.. |Python| image:: https://img.shields.io/pypi/pyversions/sqlalchemy
    :target: https://pypi.org/project/sqlalchemy
    :alt: PyPI - Python Version

.. |Downloads| image:: https://static.pepy.tech/badge/sqlalchemy/month
    :target: https://pepy.tech/project/sqlalchemy
    :alt: PyPI - Downloads


The Python SQL Toolkit and Object Relational Mapper

Introduction
-------------

SQLAlchemy is the Python SQL toolkit and Object Relational Mapper
that gives application developers the full power and
flexibility of SQL. SQLAlchemy provides a full suite
of well known enterprise-level persistence patterns,
designed for efficient and high-performing database
access, adapted into a simple and Pythonic domain
language.

Major SQLAlchemy features include:

* An industrial strength ORM, built
  from the core on the identity"
PySyft,"<div align=""left""> <a href=""https://pypi.org/project/syft/""><img src=""https://static.pepy.tech/badge/pysyft"" /></a> <a href=""https://pypi.org/project/syft/""><img src=""https://badge.fury.io/py/syft.svg"" /></a> <a href=""https://hub.docker.com/u/openmined""><img src=""https://img.shields.io/badge/docker-images-blue?logo=docker"" /></a> <a href=""https://github.com/OpenMined/PySyft/actions/workflows/nightlies.yml""><img src=""https://github.com/OpenMined/PySyft/actions/workflows/nightlies.yml/badge.svg?branch=dev"" /></a> <a href=""https://join.slack.com/t/openmined/shared_invite/zt-2hxwk07i9-HO7u5C7XOgou4Z62VU78zA/""><img src=""https://img.shields.io/badge/chat-on%20slack-purple?logo=slack"" /></a> <a href=""https://docs.openmined.org/en/latest/index.html""><img src=""https://img.shields.io/badge/read-docs-yellow?logo=mdbook"" /></a>
<br /><br /></div>

<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""docs/img/Syft-Logo-Light.svg"">
  <img alt=""Syft Logo"" src=""docs/img/Syft-Logo.svg"" widt"
segmentation_models.pytorch,"<div align=""center"">
 
![logo](https://i.ibb.co/dc1XdhT/Segmentation-Models-V2-Side-1-1.png)  
**Python library with Neural Networks for Image  
Segmentation based on [PyTorch](https://pytorch.org/).**  

[![Generic badge](https://img.shields.io/badge/License-MIT-<COLOR>.svg?style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/blob/main/LICENSE) 
[![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/qubvel/segmentation_models.pytorch/tests.yml?branch=main&style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/actions/workflows/tests.yml) 
[![Read the Docs](https://img.shields.io/readthedocs/smp?style=for-the-badge&logo=readthedocs&logoColor=white)](https://smp.readthedocs.io/en/latest/) 
<br>
[![PyPI](https://img.shields.io/pypi/v/segmentation-models-pytorch?color=blue&style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/segmentation-models-pytorch/) 
[![PyPI - Downloads](https://img.shi"
doccano,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/doccano/doccano/master/docs/images/logo/doccano.png"">
</div>

# doccano

[![Codacy Badge](https://app.codacy.com/project/badge/Grade/35ac8625a2bc4eddbff23dbc61bc6abb)](https://www.codacy.com/gh/doccano/doccano/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=doccano/doccano&amp;utm_campaign=Badge_Grade)
[![doccano CI](https://github.com/doccano/doccano/actions/workflows/ci.yml/badge.svg)](https://github.com/doccano/doccano/actions/workflows/ci.yml)

doccano is an open-source text annotation tool for humans. It provides annotation features for text classification, sequence labeling, and sequence to sequence tasks. You can create labeled data for sentiment analysis, named entity recognition, text summarization, and so on. Just create a project, upload data, and start annotating. You can build a dataset in hours.

## Demo

Try the [annotation demo](http://doccano.herokuapp.com).

![Demo image](https:"
cleanlab,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/cleanlab_logo_open_source.png"" width=60% height=60%>
</p>

<div align=""center"">
<a href=""https://pypi.org/pypi/cleanlab/"" target=""_blank""><img src=""https://img.shields.io/pypi/v/cleanlab.svg"" alt=""pypi_versions""></a>
<a href=""https://pypi.org/pypi/cleanlab/"" target=""_blank""><img src=""https://img.shields.io/badge/python-3.8%2B-blue"" alt=""py_versions""></a>
<a href=""https://app.codecov.io/gh/cleanlab/cleanlab"" target=""_blank""><img src=""https://codecov.io/gh/cleanlab/cleanlab/branch/master/graph/badge.svg"" alt=""coverage""></a>
<a href=""https://github.com/cleanlab/cleanlab/stargazers/"" target=""_blank""><img src=""https://img.shields.io/github/stars/cleanlab/cleanlab?style=social&maxAge=2592000"" alt=""Github Stars""></a>
<a href=""https://cleanlab.ai/slack"" target=""_blank""><img src=""https://img.shields.io/static/v1?logo=slack&style=flat&color=white&label=slack&message=join"" alt=""Slack Community""></a>
<"
django-allauth,"
==========================
Welcome to django-allauth!
==========================

.. image:: https://codeberg.org/allauth/allauth.org/raw/commit/da3b56390e1b18eaec09b05cd89dfa7812212dfc/content/news/2024/04/website-redesign/logo-light.png
   :target: https://allauth.org
   :align: right
   :alt: django-allauth logo
   :width: 250px


.. |ci| image:: https://img.shields.io/github/actions/workflow/status/pennersr/django-allauth/ci.yml.svg
   :target: https://github.com/pennersr/django-allauth/actions
.. |pypi| image:: https://img.shields.io/pypi/v/django-allauth
   :target: https://pypi.python.org/pypi/django-allauth
.. |cov| image:: https://img.shields.io/coverallsCoverage/github/pennersr/django-allauth
   :alt: Coverage Status
   :target: https://coveralls.io/r/pennersr/django-allauth
.. |btc| image:: https://img.shields.io/badge/bitcoin-donate-yellow
   :target: https://blockchain.info/address/1AJXuBMPHkaDCNX2rwAy34bGgs7hmrePEr
.. |liberapay| image:: https://img.shields.io/liberapay/"
datasette,"<img src=""https://datasette.io/static/datasette-logo.svg"" alt=""Datasette"">

[![PyPI](https://img.shields.io/pypi/v/datasette.svg)](https://pypi.org/project/datasette/)
[![Changelog](https://img.shields.io/github/v/release/simonw/datasette?label=changelog)](https://docs.datasette.io/en/latest/changelog.html)
[![Python 3.x](https://img.shields.io/pypi/pyversions/datasette.svg?logo=python&logoColor=white)](https://pypi.org/project/datasette/)
[![Tests](https://github.com/simonw/datasette/workflows/Test/badge.svg)](https://github.com/simonw/datasette/actions?query=workflow%3ATest)
[![Documentation Status](https://readthedocs.org/projects/datasette/badge/?version=latest)](https://docs.datasette.io/en/latest/?badge=latest)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/datasette/blob/main/LICENSE)
[![docker: datasette](https://img.shields.io/badge/docker-datasette-blue)](https://hub.docker.com/r/datasetteproject/datasette)
[![discord](https"
PythonPark,"è¿™é‡Œæ˜¯å­¦ä¹  Python çš„ä¹å›­ï¼Œ**ä¿å§†çº§æ•™ç¨‹**ï¼šAIå®éªŒå®¤ã€å®è—è§†é¢‘ã€æ•°æ®ç»“æ„ã€å­¦ä¹ æŒ‡å—ã€æœºå™¨å­¦ä¹ å®æˆ˜ã€æ·±åº¦å­¦ä¹ å®æˆ˜ã€PythonåŸºç¡€ã€ç½‘ç»œçˆ¬è™«ã€å¤§å‚é¢ç»ã€ç¨‹åºäººç”Ÿã€èµ„æºåˆ†äº«ã€‚**æˆ‘ä¼šé€æ¸å®Œå–„å®ƒï¼ŒæŒç»­è¾“å‡ºä¸­ï¼**

åŸåˆ›æ–‡ç« æ¯å‘¨æœ€å°‘ä¸¤ç¯‡ï¼Œ**åç»­æœ€æ–°æ–‡ç« **ä¼šåœ¨[ã€å…¬ä¼—å·ã€‘](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)é¦–å‘ï¼Œè§†é¢‘[ã€Bç«™ã€‘](https://space.bilibili.com/331507846)é¦–å‘ï¼Œå¤§å®¶å¯ä»¥åŠ æˆ‘[ã€å¾®ä¿¡ã€‘](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)è¿›**äº¤æµç¾¤**ï¼ŒæŠ€æœ¯äº¤æµæˆ–ææ„è§éƒ½å¯ä»¥ï¼Œæ¬¢è¿**Star**ï¼

<!--!* [In English](https://github.com/Jack-Cherish/PythonPark/blob/master/README_en.md ""In English"")<br>-->

[ã€æ€ç»´å¯¼å›¾ã€‘](#æ€ç»´å¯¼å›¾)è§æ–‡æœ«~

<p align=""center"">
    <a href=""https://github.com/Jack-Cherish/PythonPark"" target=""_blank"">
        <!--<img src=""http://photos.cuijiahua.com/github/logo.png"" width=""200"" height=""200""/>-->
        <!--<img src=""https://ftp.bmp.ovh/imgs/2021/01/cb3d04cb065fcdad.png"" width=""200"" height=""200""/>-->
        <img src=""https://raw.githubusercontent.com/Jack-Cherish/PythonPark/master/images/logo.png"" width=""200"" height=""200""/>
    </a>
</p>

<p align=""center"">
  <a href=""https://cuijiahua.com/wp-content/uploads/"
cython,"Welcome to Cython!
==================

Cython is a Python compiler that makes writing C extensions for
Python as easy as Python itself.  Cython is based on Pyrex,
but supports more cutting edge functionality and optimizations.

Cython translates Python code to C/C++ code, but additionally supports calling
C functions and declaring C types on variables and class attributes.
This allows the compiler to generate very efficient C code from Cython code.

This makes Cython the ideal language for wrapping external C libraries, and
for fast C modules that speed up the execution of Python code.

* Official website: https://cython.org/
* Documentation: https://docs.cython.org/
* Github repository: https://github.com/cython/cython
* Wiki: https://github.com/cython/cython/wiki

Cython has `about 30 million downloads <https://pypistats.org/packages/cython>`_
per month on PyPI.  You can **support the Cython project** via
`Github Sponsors <https://github.com/users/scoder/sponsorship>`_ or
`Tidelift <"
ChatRWKV,"# ChatRWKV (pronounced as ""RwaKuv"" (rÊŒkuv in IPA), from 4 major params: R W K V)

RWKV homepage: https://www.rwkv.com

## Please check https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py first

ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :)

Our latest version is **RWKV-6** https://arxiv.org/abs/2404.05892 (Preview models: https://huggingface.co/BlinkDL/temp )

**RWKV-6 3B** Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1

**RWKV-6 7B** Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2

![RWKV-v5-benchmark-1](RWKV-v5-benchmark-1.png)

**RWKV-LM main repo**: https://github.com/BlinkDL/RWKV-LM (explanation, fine-tuning, training, etc.)

Chat Demo for developers: https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py

## RWKV Discord: https://discord.gg/bDSBUMe"
whoogle-search,"![Whoogle Search](docs/banner.png)

[![Latest Release](https://img.shields.io/github/v/release/benbusby/whoogle-search)](https://github.com/benbusby/shoogle/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![tests](https://github.com/benbusby/whoogle-search/actions/workflows/tests.yml/badge.svg)](https://github.com/benbusby/whoogle-search/actions/workflows/tests.yml)
[![buildx](https://github.com/benbusby/whoogle-search/actions/workflows/buildx.yml/badge.svg)](https://github.com/benbusby/whoogle-search/actions/workflows/buildx.yml)
[![codebeat badge](https://codebeat.co/badges/e96cada2-fb6f-4528-8285-7d72abd74e8d)](https://codebeat.co/projects/github-com-benbusby-shoogle-master)
[![Docker Pulls](https://img.shields.io/docker/pulls/benbusby/whoogle-search)](https://hub.docker.com/r/benbusby/whoogle-search)

<table>
  <tr>
    <td><a href=""https://sr.ht/~benbusby/whoogle-search"">SourceHut</a></td>
    <td><a href=""http"
kohya_ss,"# Kohya's GUI

This repository primarily provides a Gradio GUI for [Kohya's Stable Diffusion trainers](https://github.com/kohya-ss/sd-scripts). However, support for Linux OS is also offered through community contributions. macOS support is not optimal at the moment but might work if the conditions are favorable.

The GUI allows you to set the training parameters and generate and run the required CLI commands to train the model.

## Table of Contents

- [Kohya's GUI](#kohyas-gui)
  - [Table of Contents](#table-of-contents)
  - [ğŸ¦’ Colab](#-colab)
  - [Installation](#installation)
    - [Windows](#windows)
      - [Windows Pre-requirements](#windows-pre-requirements)
      - [Setup Windows](#setup-windows)
      - [Optional: CUDNN 8.9.6.50](#optional-cudnn-89650)
    - [Linux and macOS](#linux-and-macos)
      - [Linux Pre-requirements](#linux-pre-requirements)
      - [Setup Linux](#setup-linux)
      - [Install Location](#install-location)
    - [Runpod](#runpod)
      - [Manual install"
shell_gpt,"# ShellGPT
A command-line productivity tool powered by AI large language models (LLM). This command-line tool offers streamlined generation of **shell commands, code snippets, documentation**, eliminating the need for external resources (like Google search). Supports Linux, macOS, Windows and compatible with all major Shells like PowerShell, CMD, Bash, Zsh, etc.

https://github.com/TheR1D/shell_gpt/assets/16740832/9197283c-db6a-4b46-bfea-3eb776dd9093

## Installation
```shell
pip install shell-gpt
```
By default, ShellGPT uses OpenAI's API and GPT-4 model. You'll need an API key, you can generate one [here](https://beta.openai.com/account/api-keys). You will be prompted for your key which will then be stored in `~/.config/shell_gpt/.sgptrc`. OpenAI API is not free of charge, please refer to the [OpenAI pricing](https://openai.com/pricing) for more information.

> [!TIP]
> Alternatively, you can use locally hosted open source models which are available for free. To use local models, you"
trl,"<div style=""text-align: center"">
<img src=""https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_banner_dark.png"">
</div>

# TRL - Transformer Reinforcement Learning

<h3 align=""center"">
    <p>Full stack library to post-train large language models.</p>
</h3>

<p align=""center"">
    <a href=""https://github.com/huggingface/trl/blob/main/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/github/license/huggingface/trl.svg?color=blue"">
    </a>
    <a href=""https://huggingface.co/docs/trl/index"">
        <img alt=""Documentation"" src=""https://img.shields.io/website/http/huggingface.co/docs/trl/index.svg?down_color=red&down_message=offline&up_message=online"">
    </a>
    <a href=""https://github.com/huggingface/trl/releases"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/huggingface/trl.svg"">
    </a>
</p>


## What is it?

TRL is a library to post-train LLMs and diffusion models with methods such as Supervis"
coursera-dl,"# Coursera Downloader

[![Build Status](https://travis-ci.org/coursera-dl/coursera-dl.svg?branch=master)](https://travis-ci.org/coursera-dl/coursera-dl)
[![Build status](https://ci.appveyor.com/api/projects/status/3hru0ycv5fbny5k8/branch/master?svg=true)](https://ci.appveyor.com/project/balta2ar/coursera-dl/branch/master)
[![Coverage Status](https://coveralls.io/repos/coursera-dl/coursera-dl/badge.svg)](https://coveralls.io/r/coursera-dl/coursera-dl)
[![Latest version on PyPI](https://img.shields.io/pypi/v/coursera-dl.svg)](https://pypi.python.org/pypi/coursera-dl)
[![Code Climate](https://codeclimate.com/github/coursera-dl/coursera-dl/badges/gpa.svg)](https://codeclimate.com/github/coursera-dl/coursera-dl)

<!-- TOC -->

- [Coursera Downloader](#coursera-downloader)
- [Introduction](#introduction)
- [Features](#features)
- [Disclaimer](#disclaimer)
- [Installation instructions](#installation-instructions)
    - [Recommended installation method for all Operating Systems](#recommended-i"
YOLOX,"<div align=""center""><img src=""assets/logo.png"" width=""350""></div>
<img src=""assets/demo.png"" >

## Introduction
YOLOX is an anchor-free version of YOLO, with a simpler design but better performance! It aims to bridge the gap between research and industrial communities.
For more details, please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).

This repo is an implementation of PyTorch version YOLOX, there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).

<img src=""assets/git_fig.png"" width=""1000"" >

## Updates!!
* ã€2023/02/28ã€‘ We support assignment visualization tool, see doc [here](./docs/assignment_visualization.md).
* ã€2022/04/14ã€‘ We support jit compile op.
* ã€2021/08/19ã€‘ We optimize the training process with **2x** faster training and **~1%** higher performance! See [notes](docs/updates_note.md) for more details.
* ã€2021/08/05ã€‘ We release [MegEngine version YOLOX](https://github.com/MegEngine/YOLOX).
* ã€2021/07/28ã€‘ We fix the fatal error of "
serverless-application-model,"# AWS SAM transform

[![Tests](https://github.com/aws/serverless-application-model/actions/workflows/build.yml/badge.svg)](https://github.com/aws/serverless-application-model/actions/workflows/build.yml)
[![Update schema](https://github.com/aws/serverless-application-model/actions/workflows/schema.yml/badge.svg)](https://github.com/aws/serverless-application-model/actions/workflows/schema.yml)
[![PyPI](https://img.shields.io/pypi/v/aws-sam-translator?label=PyPI)](https://pypi.org/project/aws-sam-translator/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/aws-sam-translator?label=Python)](https://pypi.org/project/aws-sam-translator/)
[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/aws/serverless-application-model.git)

The [AWS Serverless Application Model](https://aws.amazon.com/serverless/sam/) (AWS SAM) transform is a [AWS CloudFormation macro](https://docs.aws.amazon.com/AWS"
wait-for-it,"# wait-for-it

`wait-for-it.sh` is a pure bash script that will wait on the availability of a
host and TCP port.  It is useful for synchronizing the spin-up of
interdependent services, such as linked docker containers.  Since it is a pure
bash script, it does not have any external dependencies.

## Usage

```text
wait-for-it.sh host:port [-s] [-t timeout] [-- command args]
-h HOST | --host=HOST       Host or IP under test
-p PORT | --port=PORT       TCP port under test
                            Alternatively, you specify the host and port as host:port
-s | --strict               Only execute subcommand if the test succeeds
-q | --quiet                Don't output any status messages
-t TIMEOUT | --timeout=TIMEOUT
                            Timeout in seconds, zero for no timeout
-- COMMAND ARGS             Execute command with args after the test finishes
```

## Examples

For example, let's test to see if we can access port 80 on `www.google.com`,
and if it is available, echo the m"
nerfstudio,"<p align=""center"">
    <!-- community badges -->
    <a href=""https://discord.gg/uMbNqcraFc""><img src=""https://dcbadge.vercel.app/api/server/uMbNqcraFc?style=plastic""/></a>
    <!-- doc badges -->
    <a href='https://docs.nerf.studio/'>
        <img src='https://readthedocs.com/projects/plenoptix-nerfstudio/badge/?version=latest' alt='Documentation Status' /></a>
    <!-- pi package badge -->
    <a href=""https://badge.fury.io/py/nerfstudio""><img src=""https://badge.fury.io/py/nerfstudio.svg"" alt=""PyPI version""></a>
    <!-- code check badges -->
    <a href='https://github.com/nerfstudio-project/nerfstudio/actions/workflows/core_code_checks.yml'>
        <img src='https://github.com/nerfstudio-project/nerfstudio/actions/workflows/core_code_checks.yml/badge.svg' alt='Test Status' /></a>
    <!-- license badge -->
    <a href=""https://github.com/nerfstudio-project/nerfstudio/blob/master/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/badge/License-Apache_2.0-blue.svg""><"
Douyin-Bot,"# å¦‚ä½•åœ¨æŠ–éŸ³ä¸Šæ‰¾åˆ°æ¼‚äº®å°å§å§----æŠ–éŸ³æœºå™¨äºº

[![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badge/) [![MIT Licence](https://badges.frapsoft.com/os/mit/mit.svg?v=103)](https://opensource.org/licenses/mit-license.php)      

æœ€è¿‘æ²‰è¿·äºæŠ–éŸ³æ— æ³•è‡ªæ‹”ï¼Œå¸¸å¸¸èŠ±å¥½å‡ ä¸ªå°æ—¶åœ¨æŠ–éŸ³**æ¼‚äº®å°å§å§**èº«ä¸Šã€‚

æœ¬ç€**é«˜æ•ˆã€ç›´æ¥**åœ°æ‰¾åˆ°æ¼‚äº®å°å§å§çš„æ ¸å¿ƒæ€æƒ³ï¼Œæˆ‘ç”¨ Python + ADB åšäº†ä¸€ä¸ª Python æŠ–éŸ³æœºå™¨äºº Douyin-Botã€‚

<img src=""./screenshot/demo.gif"" title=""Logo""  width=""300""> <img src=""./screenshot/auto_reply.gif"" title=""Logo""  width=""300"">
    
##  ç‰¹æ€§

- [x] **è‡ªåŠ¨ç¿»é¡µ**
- [x] **é¢œå€¼æ£€æµ‹**
- [x] **äººè„¸è¯†åˆ«**
- [x] **è‡ªåŠ¨ç‚¹èµ**
- [x] **è‡ªåŠ¨å…³æ³¨**
- [x] éšæœºé˜² Ban
- [x] **è‡ªåŠ¨è¯„è®º**

## åŸç†

- æ‰“å¼€ã€ŠæŠ–éŸ³çŸ­è§†é¢‘ã€‹APPï¼Œè¿›å…¥ä¸»ç•Œé¢
- è·å–æ‰‹æœºæˆªå›¾ï¼Œå¹¶å¯¹æˆªå›¾è¿›è¡Œå‹ç¼© (Size < 1MB)ï¼›
- è¯·æ±‚ [äººè„¸è¯†åˆ« API](http://ai.qq.com/)ï¼›
- è§£æè¿”å›çš„äººè„¸ Json ä¿¡æ¯ï¼Œå¯¹äººè„¸æ£€æµ‹åˆ‡å‰²ï¼›
- å½“é¢œå€¼å¤§äºé—¨é™å€¼ `BEAUTY_THRESHOLD`æ—¶ï¼Œç‚¹èµå¹¶å…³æ³¨ï¼›
- ä¸‹ä¸€é¡µï¼Œè¿”å›ç¬¬ä¸€æ­¥ï¼›

## ä½¿ç”¨æ•™ç¨‹

- Pythonç‰ˆæœ¬ï¼š3.0åŠä»¥ä¸Š
- ç›¸å…³è½¯ä»¶å·¥å…·å®‰è£…å’Œä½¿ç”¨æ­¥éª¤è¯·å‚è€ƒ [wechat_jump_game](https://github.com/wangshub/wechat_jump_game) å’Œ [Android æ“ä½œæ­¥éª¤](https://github.com/wangshub/wechat_jump_game/wiki/Android-%E5%92%8"
maskrcnn-benchmark,"# Faster R-CNN and Mask R-CNN in PyTorch 1.0

**maskrcnn-benchmark has been deprecated. Please see [detectron2](https://github.com/facebookresearch/detectron2), which includes implementations for all models in maskrcnn-benchmark**

This project aims at providing the necessary building blocks for easily
creating detection and segmentation models using PyTorch 1.0.

![alt text](demo/demo_e2e_mask_rcnn_X_101_32x8d_FPN_1x.png ""from http://cocodataset.org/#explore?id=345434"")

## Highlights
- **PyTorch 1.0:** RPN, Faster R-CNN and Mask R-CNN implementations that matches or exceeds Detectron accuracies
- **Very fast**: up to **2x** faster than [Detectron](https://github.com/facebookresearch/Detectron) and **30%** faster than [mmdetection](https://github.com/open-mmlab/mmdetection) during training. See [MODEL_ZOO.md](MODEL_ZOO.md) for more details.
- **Memory efficient:** uses roughly 500MB less GPU memory than mmdetection during training
- **Multi-GPU training and inference**
- **Mixed preci"
portia,"Portia
======

Portia is a tool that allows you to visually scrape websites without any programming knowledge required. With Portia you can annotate a web page to identify the data you wish to extract, and Portia will understand based on these annotations how to scrape data from similar pages.

# Running Portia

The easiest way to run Portia is using [Docker]:

You can run Portia using Docker & official Portia-image by running:

    docker run -v ~/portia_projects:/app/data/projects:rw -p 9001:9001 scrapinghub/portia

You can also set up a local instance with [Docker-compose] by cloning this repo & running from the root of the folder:

    docker-compose up

For more detailed instructions, and alternatives to using Docker, see the [Installation] docs.

# Documentation

Documentation can be found from [Read the docs]. Source files can be found in the ``docs`` directory.

[Docker]: https://www.docker.com/
[Docker-compose]:https://docs.docker.com/compose
[Installation]: http://portia.read"
python-prompt-toolkit,"Python Prompt Toolkit
=====================

|AppVeyor|  |PyPI|  |RTD|  |License|  |Codecov|

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/logo_400px.png

``prompt_toolkit`` *is a library for building powerful interactive command line applications in Python.*

Read the `documentation on readthedocs
<http://python-prompt-toolkit.readthedocs.io/en/stable/>`_.


Gallery
*******

`ptpython <http://github.com/prompt-toolkit/ptpython/>`_ is an interactive
Python Shell, build on top of ``prompt_toolkit``.

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/ptpython.png

`More examples <https://python-prompt-toolkit.readthedocs.io/en/stable/pages/gallery.html>`_


prompt_toolkit features
***********************

``prompt_toolkit`` could be a replacement for `GNU readline
<https://tiswww.case.edu/php/chet/readline/rltop.html>`_, but it can be much
more than that.

Some features:

- **Pure Python**.
- Syntax hi"
youtube-dl-gui,"[![Donations Badge](https://yourdonation.rocks/images/badge.svg)](https://mrs0m30n3.github.io/youtube-dl-gui/donate.html)

# youtube-dlG
A cross platform front-end GUI of the popular [youtube-dl](https://rg3.github.io/youtube-dl/) media downloader written in wxPython. [Supported sites](https://rg3.github.io/youtube-dl/supportedsites.html)

## Screenshots
![youtube-dl-gui main window](https://raw.githubusercontent.com/MrS0m30n3/youtube-dl-gui/gh-pages/images/ydlg_ui.gif)

## Requirements
* [Python 2.7.3+](https://www.python.org/downloads)
* [wxPython 3](https://wxpython.org/download.php)
* [TwoDict](https://pypi.python.org/pypi/twodict)
* [GNU gettext](https://www.gnu.org/software/gettext/) (to build the package)
* [FFmpeg](https://ffmpeg.org/download.html) (optional, to post process video files)

## Downloads
* [Source (.zip)](https://github.com/MrS0m30n3/youtube-dl-gui/archive/0.4.zip)
* [Source (.tar.gz)](https://github.com/MrS0m30n3/youtube-dl-gui/archive/0.4.tar.gz)
* [PyPi](https:"
promptflow,"# Prompt flow

[![Python package](https://img.shields.io/pypi/v/promptflow)](https://pypi.org/project/promptflow/)
[![Python](https://img.shields.io/pypi/pyversions/promptflow.svg?maxAge=2592000)](https://pypi.python.org/pypi/promptflow/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/promptflow)](https://pypi.org/project/promptflow/)
[![CLI](https://img.shields.io/badge/CLI-reference-blue)](https://microsoft.github.io/promptflow/reference/pf-command-reference.html)
[![vsc extension](https://img.shields.io/visual-studio-marketplace/i/prompt-flow.prompt-flow?logo=Visual%20Studio&label=Extension%20)](https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow)

[![Doc](https://img.shields.io/badge/Doc-online-green)](https://microsoft.github.io/promptflow/index.html)
[![Issue](https://img.shields.io/github/issues/microsoft/promptflow)](https://github.com/microsoft/promptflow/issues/new/choose)
[![Discussions](https://img.shields.io/github/discussions/microsoft/promptf"
altair,"# Vega-Altair <a href=""https://altair-viz.github.io/""><img align=""right"" src=""https://altair-viz.github.io/_static/altair-logo-light.png"" height=""50""></img></a>

[![github actions](https://github.com/vega/altair/workflows/build/badge.svg)](https://github.com/vega/altair/actions?query=workflow%3Abuild)
[![typedlib_mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://www.mypy-lang.org)
[![JOSS Paper](https://joss.theoj.org/papers/10.21105/joss.01057/status.svg)](https://joss.theoj.org/papers/10.21105/joss.01057)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/altair)](https://pypi.org/project/altair)

**Vega-Altair** is a declarative statistical visualization library for Python. With Vega-Altair, you can spend more time understanding your data and its meaning. Vega-Altair's
API is simple, friendly and consistent and built on top of the powerful
[Vega-Lite](https://github.com/vega/vega-lite) JSON specification. This elegant
simplicity produces beautiful and effective visual"
hallo,"<h1 align='center'>Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation</h1>

<div align='center'>
    <a href='https://github.com/xumingw' target='_blank'>Mingwang Xu</a><sup>1*</sup>&emsp;
    <a href='https://github.com/crystallee-ai' target='_blank'>Hui Li</a><sup>1*</sup>&emsp;
    <a href='https://github.com/subazinga' target='_blank'>Qingkun Su</a><sup>1*</sup>&emsp;
    <a href='https://github.com/NinoNeumann' target='_blank'>Hanlin Shang</a><sup>1</sup>&emsp;
    <a href='https://github.com/AricGamma' target='_blank'>Liwei Zhang</a><sup>1</sup>&emsp;
    <a href='https://github.com/cnexah' target='_blank'>Ce Liu</a><sup>3</sup>&emsp;
</div>
<div align='center'>
    <a href='https://jingdongwang2017.github.io/' target='_blank'>Jingdong Wang</a><sup>2</sup>&emsp;
    <a href='https://yoyo000.github.io/' target='_blank'>Yao Yao</a><sup>4</sup>&emsp;
    <a href='https://sites.google.com/site/zhusiyucs/home' target='_blank'>Siyu Zhu</a><sup>1</sup>&emsp;
"
fuzzywuzzy,"
## This project has been renamed and moved to https://github.com/seatgeek/thefuzz


**[TheFuzz](https://github.com/seatgeek/thefuzz)** version 0.19.0 correlates with this project's 0.18.0 version with `thefuzz` replacing all instances of this project's name.

PRs and issues here will need to be resubmitted to **[TheFuzz](https://github.com/seatgeek/thefuzz)**"
WizardLM,"## WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions

<p style=""font-size:50px;"" align=""center"">
ğŸ  <a href=""https://wizardlm.github.io/"" target=""_blank"">Home Page</a> </p>
<p align=""center"">
    
<p align=""center"">
ğŸ¤— <a href=""https://huggingface.co/WizardLMTeam"" target=""_blank"">HF Repo</a> â€¢ ğŸ¦ <a href=""https://twitter.com/WizardLM_AI"" target=""_blank"">Twitter</a> â€¢ ğŸ“ƒ <a href=""https://arxiv.org/abs/2304.12244"" target=""_blank"">[WizardLM] @ICLR2024</a>  â€¢ ğŸ“ƒ <a href=""https://arxiv.org/abs/2306.08568"" target=""_blank"">[WizardCoder] @ICLR2024</a>    â€¢ ğŸ“ƒ <a href=""https://arxiv.org/abs/2308.09583"" target=""_blank"">[WizardMath]</a> <br>
</p>
<p align=""center"">
    ğŸ‘‹ Join our <a href=""https://discord.gg/VZjjHtWrKs"" target=""_blank"">Discord</a>
</p>

<p align=""center"" width=""100%"">
<a ><img src=""imgs/WizardLM.png"" alt=""WizardLM"" style=""width: 20%; min-width: 300px; display: block; margin: auto;""></a>
</p>

[![Code License](https://img.shields.io/badge/Code%20Licens"
Keras-GAN,"<p align=""center"">
    <img src=""assets/keras_gan.png"" width=""480""\>
</p>

**This repository has gone stale as I unfortunately do not have the time to maintain it anymore. If you would like to continue the development of it as a collaborator send me an email at eriklindernoren@gmail.com.**

## Keras-GAN
Collection of Keras implementations of Generative Adversarial Networks (GANs) suggested in research papers. These models are in some cases simplified versions of the ones ultimately described in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GAN varieties to implement are very welcomed.

<b>See also:</b> [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)

## Table of Contents
  * [Installation](#installation)
  * [Implementations](#implementations)
    + [Auxiliary Classifier GAN](#ac-gan)
    + [Adversarial Autoencoder](#adversarial-autoencoder)
    + [Bidirectional"
cupy,"<div align=""center""><img src=""https://raw.githubusercontent.com/cupy/cupy/main/docs/image/cupy_logo_1000px.png"" width=""400""/></div>

# CuPy : NumPy & SciPy for GPU

[![pypi](https://img.shields.io/pypi/v/cupy)](https://pypi.python.org/pypi/cupy)
[![Conda](https://img.shields.io/badge/conda--forge-cupy-blue)](https://anaconda.org/conda-forge/cupy)
[![GitHub license](https://img.shields.io/github/license/cupy/cupy)](https://github.com/cupy/cupy)
[![Matrix](https://img.shields.io/matrix/cupy_community:gitter.im?server_fqdn=matrix.org)](https://gitter.im/cupy/community)
[![Twitter](https://img.shields.io/twitter/follow/CuPy_Team?label=%40CuPy_Team)](https://twitter.com/CuPy_Team)
[![Medium](https://img.shields.io/badge/Medium-CuPy-teal)](https://medium.com/cupy-team)

[**Website**](https://cupy.dev/)
| [**Install**](https://docs.cupy.dev/en/stable/install.html)
| [**Tutorial**](https://docs.cupy.dev/en/stable/user_guide/basic.html)
| [**Examples**](https://github.com/cupy/cupy/tree/main/ex"
FlexiGen,"# FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU [[paper](https://arxiv.org/abs/2303.06865)]

FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows **high-throughput** generation by IO-efficient offloading, compression, and **large effective batch sizes**.

## Motivation

In recent years, large language models (LLMs) have shown great performance across a 
wide range of tasks. Increasingly, LLMs have been applied not only to interactive 
applications (such as chat), but also to many ""back-of-house"" tasks.
These tasks include benchmarking, information extraction, data wrangling, and form processing.

One key characteristic of these applications is that they are **throughput-oriented**: they require
running LLM inferences over millions of tokens in batches, e.g., all the private documents in a company's
corpus, or all the tasks in the [HELM](https://crfm.stanford.edu/helm/latest/) "
autokeras,"<p align=""center"">
  <img width=""500"" alt=""logo"" src=""https://autokeras.com/img/row_red.svg""/>
</p>

[![](https://github.com/keras-team/autokeras/workflows/Tests/badge.svg?branch=master)](https://github.com/keras-team/autokeras/actions?query=workflow%3ATests+branch%3Amaster)
[![codecov](https://codecov.io/gh/keras-team/autokeras/branch/master/graph/badge.svg)](https://codecov.io/gh/keras-team/autokeras)
[![PyPI version](https://badge.fury.io/py/autokeras.svg)](https://badge.fury.io/py/autokeras)
[![Python](https://img.shields.io/badge/python-v3.8.0+-success.svg)](https://www.python.org/downloads/)
[![Tensorflow](https://img.shields.io/badge/tensorflow-v2.8.0+-success.svg)](https://www.tensorflow.org/versions)
[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/keras-team/autokeras/issues)

Official Website: [autokeras.com](https://autokeras.com)

##
AutoKeras: An AutoML system based on Keras.
It is developed by <a"
chisel,"<a href=""https://opensource.facebook.com/support-ukraine"">
  <img src=""https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB"" alt=""Support Ukraine - Help Provide Humanitarian Aid to Ukraine."" />
</a>

# Chisel
`Chisel` is a collection of `LLDB` commands to assist in the debugging of iOS apps.

[[Installation](#installation) &bull; [Commands](#commands) &bull; [Custom Commands](#custom-commands) &bull; [Development Workflow](#development-workflow) [Contributing](#contributing) &bull; [License](#license)]

For a comprehensive overview of LLDB, and how Chisel complements it, read Ari Grant's [Dancing in the Debugger â€” A Waltz with LLDB](http://www.objc.io/issue-19/lldb-debugging.html) in issue 19 of [objc.io](http://www.objc.io/).

## Installation

```shell
brew update
brew install chisel
```

if `.lldbinit` file doesn't exist you can create it & open it by tapping on the terminal

 ```shell
 touch .lldbinit
 open .lldbinit
```

Then add the following line to yo"
TextBlob,"
TextBlob: Simplified Text Processing
====================================

.. image:: https://badgen.net/pypi/v/TextBlob
    :target: https://pypi.org/project/textblob/
    :alt: Latest version

.. image:: https://github.com/sloria/TextBlob/actions/workflows/build-release.yml/badge.svg
    :target: https://github.com/sloria/TextBlob/actions/workflows/build-release.yml
    :alt: Build status


Homepage: `https://textblob.readthedocs.io/ <https://textblob.readthedocs.io/>`_

`TextBlob` is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, and more.


.. code-block:: python

    from textblob import TextBlob

    text = """"""
    The titular threat of The Blob has always struck me as the ultimate movie
    monster: an insatiably hungry, amoeba-like mass able to penetrate
    virtually any safeguard, capable of--as a d"
petals,"<p align=""center"">
    <img src=""https://i.imgur.com/7eR7Pan.png"" width=""400""><br>
    Run large language models at home, BitTorrent-style.<br>
    Fine-tuning and inference <a href=""https://github.com/bigscience-workshop/petals#benchmarks"">up to 10x faster</a> than offloading
    <br><br>
    <a href=""https://pypi.org/project/petals/""><img src=""https://img.shields.io/pypi/v/petals.svg?color=green""></a>
    <a href=""https://discord.gg/tfHfe8B34k""><img src=""https://img.shields.io/discord/865254854262652969?label=discord&logo=discord&logoColor=white""></a>
    <br>
</p>

Generate text with distributed **Llama 3.1** (up to 405B), **Mixtral** (8x22B), **Falcon** (40B+) or **BLOOM** (176B) and fineâ€‘tune them for your own tasks &mdash; right from your desktop computer or Google Colab:

```python
from transformers import AutoTokenizer
from petals import AutoDistributedModelForCausalLM

# Choose any model available at https://health.petals.dev
model_name = ""meta-llama/Meta-Llama-3.1-405B-Instru"
minbpe,"# minbpe

Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is ""byte-level"" because it runs on UTF-8 encoded strings.

This algorithm was popularized for LLMs by the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the associated GPT-2 [code release](https://github.com/openai/gpt-2) from OpenAI. [Sennrich et al. 2015](https://arxiv.org/abs/1508.07909) is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.

There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:

1. [minbpe/base.py](minbpe/base.py):"
claude-engineer,"# ğŸ¤– Claude Engineer

Claude Engineer is an advanced interactive command-line interface (CLI) that harnesses the power of Anthropic's Claude 3 and Claude 3.5 models to assist with a wide range of software development tasks. This tool seamlessly combines the capabilities of state-of-the-art large language models with practical file system operations, web search functionality, intelligent code analysis, and execution capabilities.

## NEW

TTS using 11labs WebSockets and audio streaming.
Type
```
11labs on
```
to use TTS and 11labs off to return to regualr mode.

Voice mode ğŸ—£ï¸: Now you can talk to the Engineer directly without even touching your keyboard.

Type
```
voice
```
to enter voice mode.

Say ""exit voice mode"" to return to regular text.

If you want to use your voice and 11 labs at the same time, first activate 11labs then type voice to use your voice. 

Prompt caching. Make sure you udpate your Anthropic python package before running the script.
```
pip install --upgrade anthropi"
paramiko,"|version| |python| |license| |ci| |coverage|

.. |version| image:: https://img.shields.io/pypi/v/paramiko
    :target: https://pypi.org/project/paramiko/
    :alt: PyPI - Package Version
.. |python| image:: https://img.shields.io/pypi/pyversions/paramiko
    :target: https://pypi.org/project/paramiko/
    :alt: PyPI - Python Version
.. |license| image:: https://img.shields.io/pypi/l/paramiko
    :target: https://github.com/paramiko/paramiko/blob/main/LICENSE
    :alt: PyPI - License
.. |ci| image:: https://img.shields.io/circleci/build/github/paramiko/paramiko/main
    :target: https://app.circleci.com/pipelines/github/paramiko/paramiko
    :alt: CircleCI
.. |coverage| image:: https://img.shields.io/codecov/c/gh/paramiko/paramiko
    :target: https://app.codecov.io/gh/paramiko/paramiko
    :alt: Codecov

Welcome to Paramiko!
====================

Paramiko is a pure-Python [#]_ (3.6+) implementation of the SSHv2 protocol
[#]_, providing both client and server functionality. It provides "
fluentui-emoji,"# Fluent Emoji

Fluent Emoji are a collection of familiar, friendly, and modern emoji from Microsoft.

![Fluent Emoji](art/readme_banner.webp)

## Contact

Please feel free to [open a GitHub issue](https://github.com/microsoft/fluentui-emoji/issues/new) and assign to the following points of contact with questions or requests.

- Jason Custer([@jasoncuster](https://github.com/jasoncuster)) / Spencer Nelson([@spencer-nelson](https://github.com/spencer-nelson)) - Design

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.
"
akshare,"**æ¬¢è¿åŠ å…¥ä¸“æ³¨äºè´¢ç»æ•°æ®å’Œé‡åŒ–æŠ•èµ„çš„çŸ¥è¯†ç¤¾åŒºï¼Œè¯·ç‚¹å‡»[äº†è§£æ›´å¤š](https://akshare.akfamily.xyz/learn.html)**

**ç›¸å…³è§†é¢‘æ•™ç¨‹å·²ç»å‘å¸ƒï¼šã€ŠAKShare-åˆé˜¶-ä½¿ç”¨æ•™å­¦ã€‹ã€ã€ŠAKShare-åˆé˜¶-å®æˆ˜åº”ç”¨ã€‹ã€ã€ŠAKShare-æºç è§£æã€‹ã€ã€Šå¼€æºé¡¹ç›®å·¡ç¤¼ã€‹**ï¼Œè¯¦æƒ…è¯·è®¿é—®[è¯¾ç¨‹](https://app3rqjh1z21630.h5.xiaoeknow.com)æŸ¥çœ‹æ›´å¤šè¯¾ç¨‹ä¿¡æ¯ï¼

**AKQuant é‡åŒ–æ•™ç¨‹è¯·è®¿é—®ï¼š[åˆ©ç”¨ PyBroker è¿›è¡Œé‡åŒ–æŠ•èµ„](https://akquant.akfamily.xyz/)**

**æœ¬æ¬¡å‘å¸ƒ [AKTools](https://github.com/akfamily/aktools) ä½œä¸º AKShare çš„ HTTP API ç‰ˆæœ¬ï¼Œ
çªç ´ Python è¯­è¨€çš„é™åˆ¶ï¼Œæ¬¢è¿å„ä½å°ä¼™ä¼´è¯•ç”¨å¹¶æå‡ºæ›´å¥½çš„æ„è§æˆ–å»ºè®®ï¼
ç‚¹å‡» [AKTools](https://github.com/akfamily/aktools) æŸ¥çœ‹ä½¿ç”¨æŒ‡å—ã€‚å¦å¤–æä¾› [awesome-data](https://github.com/akfamily/awesome-data) æ–¹ä¾¿å„ä½å°ä¼™ä¼´æŸ¥è¯¢å„ç§æ•°æ®æºã€‚**

![AKShare Logo](https://github.com/akfamily/akshare/blob/main/assets/images/akshare_logo.jpg)

[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/akshare.svg)](https://pypi.org/project/akshare/)
[![PyPI](https://img.shields.io/pypi/v/akshare.svg)](https://pypi.org/project/akshare/)
[![Downloads](https://pepy.tech/badge/akshare)](https://pepy.tech/project/akshare)
[![Documentation Status](https://readthedocs.org/projects/akshare/badge/?ver"
pretrained-models.pytorch,"# Pretrained models for Pytorch (Work in progress)

The goal of this repo is:

- to help to reproduce research papers results (transfer learning setups for instance),
- to access pretrained ConvNets with a unique interface/API inspired by torchvision.

<a href=""https://travis-ci.org/Cadene/pretrained-models.pytorch""><img src=""https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master""/></a>

News:
- 27/10/2018: Fix compatibility issues, Add tests, Add travis
- 04/06/2018: [PolyNet](https://github.com/CUHK-MMLAB/polynet) and [PNASNet-5-Large](https://arxiv.org/abs/1712.00559) thanks to [Alex Parinov](https://github.com/creafz)
- 16/04/2018: [SE-ResNet* and SE-ResNeXt*](https://github.com/hujie-frank/SENet) thanks to [Alex Parinov](https://github.com/creafz)
- 09/04/2018: [SENet154](https://github.com/hujie-frank/SENet) thanks to [Alex Parinov](https://github.com/creafz)
- 22/03/2018: CaffeResNet101 (good for localization with FasterRCNN)
- 21/03/2018: NASNet Mobile tha"
Tkinter-Designer,"<a href=""https://www.ultrahuman.com/ring/buy/?referral=1pauci"" target=""_blank""><img src=""https://github.com/user-attachments/assets/3a3cf7e8-ebf2-48df-9e6d-5db67756e976"" alt=""Ultrahuman Ring Air Discount Code 2024""></a>

<p align=""center"">
  <img width=""200"" src=""https://user-images.githubusercontent.com/42001064/120057695-b1f6c680-c062-11eb-96d5-2c43d05f9018.png"" alt=""logo"">
  <h1 align=""center"" style=""margin: 0 auto 0 auto;"">Tkinter Designer</h1>
  <h4 align=""center"" style=""margin: 0 auto 0 auto;"">Drag & Drop GUI Creator</h4>


<p align=""center"">
  <img src=""https://img.shields.io/github/last-commit/ParthJadhav/Tkinter-Designer"">
  <img src=""https://img.shields.io/github/contributors/ParthJadhav/Tkinter-Designer"">
  <img src=""https://img.shields.io/github/issues/ParthJadhav/Tkinter-Designer?label=issues"">
  <img src=""https://img.shields.io/github/stars/ParthJadhav/Tkinter-Designer"">
</p>

<p align=""center"">
<a href=""https://www.producthunt.com/posts/tkinter-designer?utm_source=badge-"
awesome-scala,"<!--- This file is automatically generated. Do not edit directly. -->
Awesome Scala [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
=============

A community driven list of useful Scala libraries, frameworks and software. This is not a catalog of all the libraries, just a starting point for your explorations. Inspired by [awesome-python](https://github.com/vinta/awesome-python). Other amazingly awesome lists can be found in the [awesome-awesomeness](https://github.com/bayandin/awesome-awesomeness) list.

Also awesome is [Scaladex](https://index.scala-lang.org/), the searchable, tagged, and centralized index of Scala libraries.

Projects with over 500 stargazers are in bold.

## Contributing

Your contributions are always welcome! Please submit a pull request or create an issue to add a new framework, library or software to the list. Do not submit a project that hasnâ€™t been updat"
OpenChatKit,"# OpenChatKit

OpenChatKit provides a powerful, open-source base to create both specialized and general purpose models for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between [Together](https://www.together.xyz/), [LAION](https://laion.ai), and [Ontocord.ai](https://ontocord.ai). 

In this repo, you'll find code for:
- Training GPT-NeoXT-Chat-Base-20B, a 20B parameter chat model (see [docs/GPT-NeoXT-Chat-Base-20B.md](docs/GPT-NeoXT-Chat-Base-20B.md))
- Fine-tuning Llama-2-7B-32K-beta, a 7B parameter long context model
- Training Pythia-Chat-Base-7B, a 7B parameter chat model
- Testing inference using either of the chat models
- Augmenting the model with additional context from a retrieval index

# Contents

- [Getting Started](#getting-started)
  * ["
Pytorch-UNet,"# U-Net: Semantic segmentation with PyTorch
<a href=""#""><img src=""https://img.shields.io/github/actions/workflow/status/milesial/PyTorch-UNet/main.yml?logo=github&style=for-the-badge"" /></a>
<a href=""https://hub.docker.com/r/milesial/unet""><img src=""https://img.shields.io/badge/docker%20image-available-blue?logo=Docker&style=for-the-badge"" /></a>
<a href=""https://pytorch.org/""><img src=""https://img.shields.io/badge/PyTorch-v1.13+-red.svg?logo=PyTorch&style=for-the-badge"" /></a>
<a href=""#""><img src=""https://img.shields.io/badge/python-v3.6+-blue.svg?logo=python&style=for-the-badge"" /></a>

![input and output for a random image in the test dataset](https://i.imgur.com/GD8FcB7.png)


Customized implementation of the [U-Net](https://arxiv.org/abs/1505.04597) in PyTorch for Kaggle's [Carvana Image Masking Challenge](https://www.kaggle.com/c/carvana-image-masking-challenge) from high definition images.

- [Quick start](#quick-start)
  - [Without Docker](#without-docker)
  - [With Docker](#w"
boto3,"===============================
Boto3 - The AWS SDK for Python
===============================

|Version| |Python| |License|

Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for
Python, which allows Python developers to write software that makes use
of services like Amazon S3 and Amazon EC2. You can find the latest, most
up to date, documentation at our `doc site`_, including a list of
services that are supported.

Boto3 is maintained and published by `Amazon Web Services`_.

Boto (pronounced boh-toh) was named after the fresh water dolphin native to the Amazon river. The name was chosen by the author of the original Boto library, Mitch Garnaat, as a reference to the company.

Notices
-------

On 2023-12-13, support for Python 3.7 ended for Boto3. This follows the
Python Software Foundation `end of support <https://peps.python.org/pep-0537/#lifespan>`__
for the runtime which occurred on 2023-06-27.
For more information, see this `blog post <https://aws.amazon.com/"
Machine-Learning,"# Machine-Learning
* [In English](https://github.com/Jack-Cherish/Machine-Learning/blob/master/README-eng.md ""æ‚¬åœæ˜¾ç¤º"")<br>

åŸåˆ›æ–‡ç« æ¯å‘¨æœ€å°‘ä¸¤ç¯‡ï¼Œ**åç»­æœ€æ–°æ–‡ç« **ä¼šåœ¨[ã€å…¬ä¼—å·ã€‘](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)é¦–å‘ï¼Œè§†é¢‘[ã€Bç«™ã€‘](https://space.bilibili.com/331507846)é¦–å‘ï¼Œå¤§å®¶å¯ä»¥åŠ æˆ‘[ã€å¾®ä¿¡ã€‘](https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg)è¿›**äº¤æµç¾¤**ï¼ŒæŠ€æœ¯äº¤æµæˆ–ææ„è§éƒ½å¯ä»¥ï¼Œæ¬¢è¿**Star**ï¼

<p align=""center"">
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/weChat-å¾®ä¿¡ç¾¤-blue.svg"" alt=""å¾®ä¿¡ç¾¤""></a>
  <a href=""https://cuijiahua.com/wp-content/uploads/2020/05/gzh-w.jpg"" target=""_blank""><img src=""https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Jack%20Cui-lightgrey.svg"" alt=""å…¬ä¼—å·""></a>
  <a href=""https://space.bilibili.com/331507846""><img src=""https://img.shields.io/badge/bilibili-å“”å“©å“”å“©-critical"" alt=""Bç«™""></a>
  <a href=""https://www.zhihu.com/people/Jack--Cui"" target=""_blank""><img src=""https://img.shields.io/badge/zhihu-çŸ¥ä¹-informational"
http-prompt,"HTTP Prompt
===========

|PyPI| |Docs| |Build| |Coverage| |Discord|

HTTP Prompt is an interactive command-line HTTP client featuring autocomplete
and syntax highlighting, built on HTTPie_ and prompt_toolkit_.

|Asciinema|


Links
-----

* Home: https://http-prompt.com
* Documentation: https://docs.http-prompt.com
* Code: https://github.com/httpie/http-prompt
* Chat: https://httpie.io/chat


.. |PyPI| image:: https://img.shields.io/pypi/v/http-prompt.svg
    :target: https://pypi.python.org/pypi/http-prompt

.. |Docs| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat
    :target: http://docs.http-prompt.com/en/latest/?badge=latest

.. |Build| image:: https://github.com/httpie/http-prompt/workflows/Build/badge.svg
    :target: https://github.com/httpie/http-prompt/actions

.. |Coverage| image:: https://coveralls.io/repos/github/eliangcs/http-prompt/badge.svg?branch=master
    :target: https://coveralls.io/github/eliangcs/http-prompt?branch=master

.. |Discord| "
icecream,"<h1 align=""center"">
  <img src=""logo.svg"" width=""220px"" height=""370px"" alt=""icecream"">
</h1>

<p align=""center"">
  <a href=""https://pypi.python.org/pypi/icecream""><img src=""https://badge.fury.io/py/icecream.svg""></a>
  <a href=""https://github.com/gruns/icecream/actions/workflows/ci.yml""><img src=""https://github.com/gruns/icecream/actions/workflows/ci.yml/badge.svg""></a>
  <a href=""http://unlicense.org/""><img src=""https://img.shields.io/pypi/l/icecream.svg""></a>
  <a href=""https://pypi.python.org/pypi/icecream""><img src=""https://img.shields.io/pypi/pyversions/icecream.svg""></a>
</p>


### IceCream â€” Never use print() to debug again

Do you ever use `print()` or `log()` to debug your code? Of course you
do. IceCream, or `ic` for short, makes print debugging a little sweeter.

`ic()` is like `print()`, but better:

  1. It prints both expressions/variable names and their values.
  2. It's 60% faster to type.
  3. Data structures are pretty printed.
  4. Output is syntax highlighted.
  5. "
byob,"![Banner](https://github.com/malwaredllc/byob/blob/master/byob/static/byob_logo_black.svg)

[![license](https://img.shields.io/badge/license-GPL-brightgreen.svg)](https://github.com/malwaredllc/byob/blob/master/LICENSE)
[![version](https://img.shields.io/badge/version-2.0-blue.svg)](https://github.com/malwaredllc/byob)
[![Coverage Status](https://coveralls.io/repos/github/malwaredllc/byob/badge.svg)](https://coveralls.io/github/malwaredllc/byob)
<img alt=""Discord"" src=""https://img.shields.io/discord/709150520446550097""/>
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=BYOB%20(Post-Exploitation%20Framework)&url=https://github.com/malwaredllc/byob&via=malwaredllc&hashtags=byob,python,security,github)


**Questions?** Check out the [docs](https://github.com/malwaredllc/byob/wiki) or join our [Discord support server](https://discord.gg/8FsSrw7)

__Disclaimer__: This project should be used for authorized testing or educat"
wandb,"<p align=""center"">
  <img src=""./assets/logo-dark.svg#gh-dark-mode-only"" width=""600"" alt=""Weights & Biases"" />
  <img src=""./assets/logo-light.svg#gh-light-mode-only"" width=""600"" alt=""Weights & Biases"" />
</p>

<p align=""center"">
<a href=""https://pypi.python.org/pypi/wandb""><img src=""https://img.shields.io/pypi/v/wandb"" /></a>
<a href=""https://anaconda.org/conda-forge/wandb""><img src=""https://img.shields.io/conda/vn/conda-forge/wandb"" /></a>
<a href=""https://circleci.com/gh/wandb/wandb""><img src=""https://img.shields.io/circleci/build/github/wandb/wandb/main"" /></a>
<a href=""https://codecov.io/gh/wandb/wandb""><img src=""https://img.shields.io/codecov/c/gh/wandb/wandb"" /></a>
</p>
<p align='center'>
<a href=""https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" /></a>
</p>

Use W&B to build better models faster. Track and visualize all the pieces of your machin"
nicegui,"<a href=""https://nicegui.io/#about"">
  <img src=""https://raw.githubusercontent.com/zauberzeug/nicegui/main/screenshot.png""
    width=""200"" align=""right"" alt=""Try online!"" />
</a>

# NiceGUI

NiceGUI is an easy-to-use, Python-based UI framework, which shows up in your web browser.
You can create buttons, dialogs, Markdown, 3D scenes, plots and much more.

It is great for micro web apps, dashboards, robotics projects, smart home solutions and similar use cases.
You can also use it in development, for example when tweaking/configuring a machine learning algorithm or tuning motor controllers.

NiceGUI is available as [PyPI package](https://pypi.org/project/nicegui/), [Docker image](https://hub.docker.com/r/zauberzeug/nicegui) and on [conda-forge](https://anaconda.org/conda-forge/nicegui) as well as [GitHub](https://github.com/zauberzeug/nicegui).

[![PyPI](https://img.shields.io/pypi/v/nicegui?color=dark-green)](https://pypi.org/project/nicegui/)
[![PyPI downloads](https://img.shields.io/p"
sshuttle,"sshuttle: where transparent proxy meets VPN meets ssh
=====================================================

As far as I know, sshuttle is the only program that solves the following
common case:

- Your client machine (or router) is Linux, FreeBSD, or MacOS.

- You have access to a remote network via ssh.

- You don't necessarily have admin access on the remote network.

- The remote network has no VPN, or only stupid/complex VPN
  protocols (IPsec, PPTP, etc). Or maybe you *are* the
  admin and you just got frustrated with the awful state of
  VPN tools.

- You don't want to create an ssh port forward for every
  single host/port on the remote network.

- You hate openssh's port forwarding because it's randomly
  slow and/or stupid.

- You can't use openssh's PermitTunnel feature because
  it's disabled by default on openssh servers; plus it does
  TCP-over-TCP, which has terrible performance (see below).


Obtaining sshuttle
------------------

- From PyPI::

      pip install sshutt"
yolov9,"# YOLOv9

Implementation of paper - [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)

[![arxiv.org](http://img.shields.io/badge/cs.CV-arXiv%3A2402.13616-B31B1B.svg)](https://arxiv.org/abs/2402.13616)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/kadirnar/Yolov9)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/merve/yolov9)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb)
[![OpenCV](https://img.shields.io/badge/OpenCV-BlogPost-black?logo=opencv&labelColor=blue&color=black)](https://learnopencv.com/yolov9-advancing-the-yolo-legacy/)

<div align=""center"">
    <a href=""./"">
        <img src=""./figure/performance.png"" w"
hamulete,"
<!--
</a><img align=""right"" src=""https://fastly.jsdelivr.net/gh/hoochanlon/w3-goto-world/W3UnitTest/mof2.PNG"" width=""250 "" height=""250"" /></a><a><img align=""right"" src=""https://fastly.jsdelivr.net/gh/hoochanlon/w3-goto-world/W3UnitTest/mof1.PNG"" width=""250 "" height=""250"" />

[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE) [![996.icu](https://img.shields.io/badge/link-996.icu-red.svg)](https://996.icu) 

-->

<!--![å†²å‡ºä½ çš„çª—å£](https://fastly.jsdelivr.net/gh/hoochanlon/w3-goto-world/W3UnitTest/ccndck.png)-->

<h1 align=""center""> å…è´£å£°æ˜ </h1>

<p align=""center""> <a href=""https://github.com/hoochanlon/hamulete/blob/master/README_JP.md""> æ—¥æœ¬èª </a> | <a href=""https://github.com/hoochanlon/hamulete/blob/master/README.md""> ä¸­æ–‡ </a>  </p>

è¯¥åº“å¹¶ä¸æä¾›ä»»ä½•èµ„æ–™ï¼Œæ‰€æœ‰å†…å®¹å‡ä¸ºæ¬è¿ã€‚ä»…é¢å‘æµ·å¤–åäººåŠç¤¾ç§‘ç ”ç©¶è€…ï¼Œåˆ‡å‹¿ç”¨äºå…¶ä»–ç”¨é€”ï¼å¤§é™†ç”¨æˆ·è¯·è‡ªè§‰å…³é—­ï¼Œå¹¶åœ¨24å°æ—¶ä¹‹å†…åˆ æ‰ä¸æœ¬é¡¹ç›®ç›¸å…³çš„ä¸€åˆ‡å†…å®¹ã€‚å¦åˆ™å‡ºç°ä¸€åˆ‡é—®é¢˜ï¼Œé¡¹ç›®ä½œè€…æ¦‚ä¸è´Ÿè´£ï¼<br>

# ***åŸä¸­æ‘å“ˆå§†é›·ç‰¹ï¼ˆã€Šä¿¡æ¯ç½‘ç»œç¤¾ç§‘æ•´åˆèµ„æºåº“ã€‹ï¼‰***

<a href=""https://ndltd.ncl.edu.tw"" target=""_blank"">
<img "
Reinforcement-learning-with-tensorflow,"<p align=""center"">
    <a href=""https://www.youtube.com/watch?v=pieI7rOXELI&list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba"" target=""_blank"">
    <img width=""60%"" src=""https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/RL_cover.jpg"" style=""max-width:100%;"">
    </a>
</p>


<br>

# Reinforcement Learning Methods and Tutorials

In these tutorials for reinforcement learning, it covers from the basic RL algorithms to advanced algorithms developed recent years.

**If you speak Chinese, visit [è«çƒ¦ Python](https://mofanpy.com) or my [Youtube channel](https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg) for more.**

**As many requests about making these tutorials available in English, please find them in this playlist:** ([https://www.youtube.com/playlist?list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba](https://www.youtube.com/playlist?list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba))

# Table of Contents

* Tutorials
    * [Simple entry example](contents/1_command_line_reinforcement"
hacktricks,"# HackTricks

<figure><img src="".gitbook/assets/hacktricks.gif"" alt=""""><figcaption></figcaption></figure>

_Hacktricks logos & motion design by_ [_@ppiernacho_](https://www.instagram.com/ppieranacho/)_._

{% hint style=""success"" %}
**Welcome to the wiki where you will find each hacking trick/technique/whatever I have learnt from CTFs, real life apps, reading researches, and news.**
{% endhint %}

To get started follow this page where you will find the **typical flow** that **you should follow when pentesting** one or more **machines:**

{% content-ref url=""generic-methodologies-and-resources/pentesting-methodology.md"" %}
[pentesting-methodology.md](generic-methodologies-and-resources/pentesting-methodology.md)
{% endcontent-ref %}

## Corporate Sponsors

### [STM Cyber](https://www.stmcyber.com)

<figure><img src="".gitbook/assets/stm (1).png"" alt=""""><figcaption></figcaption></figure>

[**STM Cyber**](https://www.stmcyber.com) is a great cybersecurity company whose slogan is **HACK THE "
30-seconds-of-python,"> **IMPORTANT NOTICE:**
>
> As of May, 2023, all 30-seconds content repositories have been merged into [30-seconds-of-code](https://github.com/30-seconds/30-seconds-of-code).
>
> Please watch, star and follow relevant activity there.

[![Logo](/logo.png)](https://30secondsofcode.org/python/p/1)

# 30 seconds of code

> Short Python code snippets for all your development needs

* Visit [our website](https://30secondsofcode.org) to view our snippet collection.
* Use the [Search page](https://30secondsofcode.org/search) to find snippets that suit your needs. You can search by name, tag, language or using a snippet's description. Just start typing a term and see what comes up.
* Browse the [Python Snippet collection](https://30secondsofcode.org/python/p/1) to see all the snippets in this project or click individual tags at the top of the same page to narrow down your search to a specific tag.
* Click on each snippet card to view the whole snippet, including code, explanation and examples.
"
Mailpile,"# Welcome to Mailpile! #

**IMPORTANT NOTE**

Development on this codebase has halted, until the
[Python3 rewrite](https://community.mailpile.is/t/a-very-uninformative-progress-update-mailpile-2/785)
has completed.

Apologies to those who have unanswered, out-standing pull requests and
issues. ğŸ˜¢ Your efforts are appreciated!

If you rely on this code and have your own branch which you actively
maintain, let us know: we would be happy to link to it.

If you need to run Mailpile v1 to access legacy data, consider using
our [legacy Docker images](https://github.com/mailpile/Mailpile-v1-Docker).


------------------------------------------------------------------------

## Introduction (Obsolete) ##

Mailpile (<https://www.mailpile.is/>) is a modern, fast web-mail client
with user-friendly encryption and privacy features. The development of
Mailpile is funded by
[a large community of backers](https://www.mailpile.is/#community)
and all code related to the project is and will be released un"
text-generation-inference,"<div align=""center"">

<a href=""https://www.youtube.com/watch?v=jlMAX2Oaht0"">
  <img width=560 width=315 alt=""Making TGI deployment optimal"" src=""https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png"">
</a>

# Text Generation Inference

<a href=""https://github.com/huggingface/text-generation-inference"">
  <img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social"">
</a>
<a href=""https://huggingface.github.io/text-generation-inference"">
  <img alt=""Swagger API documentation"" src=""https://img.shields.io/badge/API-Swagger-informational"">
</a>

A Rust, Python and gRPC server for text generation inference. Used in production at [Hugging Face](https://huggingface.co)
to power Hugging Chat, the Inference API and Inference Endpoint.

</div>

## Table of contents

  - [Get Started](#get-started)
    - [Docker](#docker)
    - [API documentation](#api-documentation)
    - [Using a private or gated model](#using-a-"
stable-baselines3,"<!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) -->
![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)
[![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master)
[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


# Stable Baselines3

<img src=""docs/\_static/img/logo.png"" align=""right"" width=""40%""/>

Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).

You can read a detailed presentation"
pydub,"# Pydub [![Build Status](https://travis-ci.org/jiaaro/pydub.svg?branch=master)](https://travis-ci.org/jiaaro/pydub) [![Build status](https://ci.appveyor.com/api/projects/status/gy1ucp9o5khq7fqi/branch/master?svg=true)](https://ci.appveyor.com/project/jiaaro/pydub/branch/master)

Pydub lets you do stuff to audio in a way that isn't stupid.

**Stuff you might be looking for**:
 - [Installing Pydub](https://github.com/jiaaro/pydub#installation)
 - [API Documentation](https://github.com/jiaaro/pydub/blob/master/API.markdown)
 - [Dependencies](https://github.com/jiaaro/pydub#dependencies)
 - [Playback](https://github.com/jiaaro/pydub#playback)
 - [Setting up ffmpeg](https://github.com/jiaaro/pydub#getting-ffmpeg-set-up)
 - [Questions/Bugs](https://github.com/jiaaro/pydub#bugs--questions)
 

##  Quickstart

Open a WAV file

```python
from pydub import AudioSegment

song = AudioSegment.from_wav(""never_gonna_give_you_up.wav"")
```

...or a mp3

```python
song = AudioSegment.from_mp3(""never_gonn"
nougat,"<div align=""center"">
<h1>Nougat: Neural Optical Understanding for Academic Documents</h1>

[![Paper](https://img.shields.io/badge/Paper-arxiv.2308.13418-white)](https://arxiv.org/abs/2308.13418)
[![GitHub](https://img.shields.io/github/license/facebookresearch/nougat)](https://github.com/facebookresearch/nougat)
[![PyPI](https://img.shields.io/pypi/v/nougat-ocr?logo=pypi)](https://pypi.org/project/nougat-ocr)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Community%20Space-blue)](https://huggingface.co/spaces/ysharma/nougat)

</div>

This is the official repository for Nougat, the academic document PDF parser that understands LaTeX math and tables.

Project page: https://facebookresearch.github.io/nougat/

## Install

From pip:
``"
Douyin_TikTok_Download_API,"<div align=""center"">
<a href=""https://douyin.wtf/"" alt=""logo"" ><img src=""https://raw.githubusercontent.com/Evil0ctal/Douyin_TikTok_Download_API/main/logo/logo192.png"" width=""120""/></a>
</div>
<h1 align=""center"">Douyin_TikTok_Download_API(æŠ–éŸ³/TikTok API)</h1>

<div align=""center"">

[English](./README.en.md) | [ç®€ä½“ä¸­æ–‡](./README.md)

ğŸš€ã€ŒDouyin_TikTok_Download_APIã€æ˜¯ä¸€ä¸ªå¼€ç®±å³ç”¨çš„é«˜æ€§èƒ½å¼‚æ­¥[æŠ–éŸ³](https://www.douyin.com)|[TikTok](https://www.tiktok.com)|[Bilibili](https://www.bilibili.com)æ•°æ®çˆ¬å–å·¥å…·ï¼Œæ”¯æŒAPIè°ƒç”¨ï¼Œåœ¨çº¿æ‰¹é‡è§£æåŠä¸‹è½½ã€‚

[![GitHub license](https://img.shields.io/github/license/Evil0ctal/Douyin_TikTok_Download_API?style=flat-square)](LICENSE)
[![Release Version](https://img.shields.io/github/v/release/Evil0ctal/Douyin_TikTok_Download_API?style=flat-square)](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/releases/latest)
[![GitHub Star](https://img.shields.io/github/stars/Evil0ctal/Douyin_TikTok_Download_API?style=flat-square)](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/stargazers)
[![GitHub For"
attention-is-all-you-need-pytorch,"# Attention is all you need: A Pytorch Implementation

This is a PyTorch implementation of the Transformer model in ""[Attention is All You Need](https://arxiv.org/abs/1706.03762)"" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). 


A novel sequence to sequence framework utilizes the **self-attention mechanism**, instead of Convolution operation or Recurrent structure, and achieve the state-of-the-art performance on **WMT 2014 English-to-German translation task**. (2017/06/12)

> The official Tensorflow Implementation can be found in: [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py).

> To learn more about self-attention mechanism, you could read ""[A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)"".

<p align=""center"">
<img src=""http://imgur.com/1krF2R6.png"" width=""250"">
</p>


The project support t"
txtai,"<p align=""center"">
    <img src=""https://raw.githubusercontent.com/neuml/txtai/master/logo.png""/>
</p>

<p align=""center"">
    <b>All-in-one embeddings database</b>
</p>

<p align=""center"">
    <a href=""https://github.com/neuml/txtai/releases"">
        <img src=""https://img.shields.io/github/release/neuml/txtai.svg?style=flat&color=success"" alt=""Version""/>
    </a>
    <a href=""https://github.com/neuml/txtai"">
        <img src=""https://img.shields.io/github/last-commit/neuml/txtai.svg?style=flat&color=blue"" alt=""GitHub last commit""/>
    </a>
    <a href=""https://github.com/neuml/txtai/issues"">
        <img src=""https://img.shields.io/github/issues/neuml/txtai.svg?style=flat&color=success"" alt=""GitHub issues""/>
    </a>
    <a href=""https://join.slack.com/t/txtai/shared_invite/zt-1cagya4yf-DQeuZbd~aMwH5pckBU4vPg"">
        <img src=""https://img.shields.io/badge/slack-join-blue?style=flat&logo=slack&logocolor=white"" alt=""Join Slack""/>
    </a>
    <a href=""https://github.com/neuml/txtai/"
hello-git,"# Hello Git & GitHub La queria tanto

[![Git](https://img.shields.io/badge/Git-2.37+-f14e32?style=for-the-badge&logo=git&logoColor=white&labelColor=101010)](https://git-scm.com/)
[![GitHub](https://img.shields.io/badge/GitHub-Web-blue?style=for-the-badge&logo=github&logoColor=white&labelColor=101010)](https://github.com/)

## Curso completo de 5 horas y 45 lecciones para aprender a trabajar con Git & GitHub desde cero y para principiantes

![](./Media/header.jpg)

### Proyecto realizado durante emisiones en directo desde [Twitch](https://twitch.tv/mouredev)

> ##### Si consideras Ãºtil el curso, apÃ³yalo haciendo ""â˜… Star"" en el repositorio. Â¡Gracias!

## Lo que aprenderÃ¡s

- Git desde su historia y fundamentos
- Conceptos principales y flujo de trabajo
- Manejo de terminal
- InstalaciÃ³n y configuraciÃ³n
- MÃ¡s de 25 comandos de Git
- GitHub desde cero
- ConfiguraciÃ³n y autenticaciÃ³n
- IntegraciÃ³n de Git con GitHub
- Flujo colaborativo
- Herramientas destacadas
- Ejemplos prÃ¡cticos

Y mucho"
pattern,"Pattern
=======

[![Build Status](http://img.shields.io/travis/clips/pattern/master.svg?style=flat)](https://travis-ci.org/clips/pattern/branches)
[![Coverage](https://img.shields.io/coveralls/clips/pattern/master.svg?style=flat)](https://coveralls.io/github/clips/pattern?branch=master)
[![PyPi version](http://img.shields.io/pypi/v/pattern.svg?style=flat)](https://pypi.python.org/pypi/pattern)
[![License](https://img.shields.io/badge/License-BSD%203--Clause-green.svg?style=flat)](https://github.com/clips/pattern/blob/master/LICENSE.txt)

Pattern is a web mining module for Python. It has tools for:

 * Data Mining: web services (Google, Twitter, Wikipedia), web crawler, HTML DOM parser
 * Natural Language Processing: part-of-speech taggers, n-gram search, sentiment analysis, WordNet
 * Machine Learning: vector space model, clustering, classification (KNN, SVM, Perceptron)
 * Network Analysis: graph centrality and visualization.

It is well documented, thoroughly tested with 350+ unit te"
pytorch3d,"<img src=""https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/.github/pytorch3dlogo.png"" width=""900""/>

[![CircleCI](https://circleci.com/gh/facebookresearch/pytorch3d.svg?style=svg)](https://circleci.com/gh/facebookresearch/pytorch3d)
[![Anaconda-Server Badge](https://anaconda.org/pytorch3d/pytorch3d/badges/version.svg)](https://anaconda.org/pytorch3d/pytorch3d)

# Introduction

PyTorch3D provides efficient, reusable components for 3D Computer Vision research with [PyTorch](https://pytorch.org).

Key features include:

- Data structure for storing and manipulating triangle meshes
- Efficient operations on triangle meshes (projective transformations, graph convolution, sampling, loss functions)
- A differentiable mesh renderer
- Implicitron, see [its README](projects/implicitron_trainer), a framework for new-view synthesis via implicit representations. ([blog post](https://ai.facebook.com/blog/implicitron-a-new-modular-extensible-framework-for-neural-implicit-representati"
so-vits-svc-fork,"# SoftVC VITS Singing Voice Conversion Fork

[ç®€ä½“ä¸­æ–‡](README_zh_CN.md)

<p align=""center"">
  <a href=""https://github.com/voicepaw/so-vits-svc-fork/actions/workflows/ci.yml?query=branch%3Amain"">
    <img src=""https://img.shields.io/github/actions/workflow/status/voicepaw/so-vits-svc-fork/ci.yml?branch=main&label=CI&logo=github&style=flat-square"" alt=""CI Status"" >
  </a>
  <a href=""https://so-vits-svc-fork.readthedocs.io"">
    <img src=""https://img.shields.io/readthedocs/so-vits-svc-fork.svg?logo=read-the-docs&logoColor=fff&style=flat-square"" alt=""Documentation Status"">
  </a>
  <a href=""https://codecov.io/gh/voicepaw/so-vits-svc-fork"">
    <img src=""https://img.shields.io/codecov/c/github/voicepaw/so-vits-svc-fork.svg?logo=codecov&logoColor=fff&style=flat-square"" alt=""Test coverage percentage"">
  </a>
</p>
<p align=""center"">
  <a href=""https://python-poetry.org/"">
    <img src=""https://img.shields.io/badge/packaging-poetry-299bd7?style=flat-square&logo=data:image/png;base64,iVBORw0KGgoAAA"
arrow,"Arrow: Better dates & times for Python
======================================

.. start-inclusion-marker-do-not-remove

.. image:: https://github.com/arrow-py/arrow/workflows/tests/badge.svg?branch=master
   :alt: Build Status
   :target: https://github.com/arrow-py/arrow/actions?query=workflow%3Atests+branch%3Amaster

.. image:: https://codecov.io/gh/arrow-py/arrow/branch/master/graph/badge.svg
   :alt: Coverage
   :target: https://codecov.io/gh/arrow-py/arrow

.. image:: https://img.shields.io/pypi/v/arrow.svg
   :alt: PyPI Version
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/pypi/pyversions/arrow.svg
   :alt: Supported Python Versions
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/pypi/l/arrow.svg
   :alt: License
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :alt: Code Style: Black
   :target: https://github.com/psf/black


**Arrow** is "
awesome-math,"# Awesome Math [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome mathematics resources.

All resources are freely available except those with a ğŸ’² icon.

# Contents

<!-- START_TOC -->

* [Contents](#contents)
* [General Resources](#general-resources)
    * [Learning Platforms](#learning-platforms)
    * [Learn to Learn](#learn-to-learn)
    * [Youtube Series](#youtube-series)
    * [Tools](#tools)
    * [Questions and Answers](#questions-and-answers)
    * [Encyclopedia](#encyclopedia)
    * [Books](#books)
    * [Magazines](#magazines)
    * [Blogs](#blogs)
    * [Meetings and Conferences](#meetings-and-conferences)
    * [Misc](#misc)
* [Branches of Mathematics](#branches-of-mathematics)
    * [Foundations of Mathematics](#foundations-of-mathematics)
        * [Transition To Pure Rigour Math](#transition-to-pure-rigour-math)
        * [Set Theory](#set-"
pymc,".. image:: https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg
    :height: 100px
    :alt: PyMC logo
    :align: center

|Build Status| |Coverage| |NumFOCUS_badge| |Binder| |Dockerhub| |DOIzenodo| |Conda Downloads|

PyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling
focusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI)
algorithms. Its flexibility and extensibility make it applicable to a
large suite of problems.

Check out the `PyMC overview <https://docs.pymc.io/en/latest/learn/core_notebooks/pymc_overview.html>`__,  or
one of `the many examples <https://www.pymc.io/projects/examples/en/latest/gallery.html>`__!
For questions on PyMC, head on over to our `PyMC Discourse <https://discourse.pymc.io/>`__ forum.

Features
========

-  Intuitive model specification syntax, for example, ``x ~ N(0,1)``
   translates to ``x = Normal('x',0,1)``
-  **Powerful sampling algorithms**, such as the `No U-Turn
   Sampler <ht"
hydra,"<p align=""center""><img src=""https://raw.githubusercontent.com/facebookresearch/hydra/main/website/static/img/Hydra-Readme-logo2.svg"" alt=""logo"" width=""70%"" /></p>

<p align=""center"">
  <a href=""https://pypi.org/project/hydra-core/"">
    <img src=""https://img.shields.io/pypi/v/hydra-core"" alt=""PyPI"" />
  </a>
  <a href=""https://circleci.com/gh/facebookresearch/hydra"">
    <img src=""https://img.shields.io/circleci/build/github/facebookresearch/hydra?token=af199cd2deca9e70e53776f9ded96284b10687e9"" alt=""CircleCI"" />
  </a>
  <a href=""#"">
    <img src=""https://img.shields.io/pypi/l/hydra-core"" alt=""PyPI - License"" />
  </a>
  <a href=""#"">
    <img src=""https://img.shields.io/pypi/pyversions/hydra-core"" alt=""PyPI - Python Version"" />
  </a>
  <a href=""https://www.pepy.tech/projects/hydra-core?versions=0.11.*&versions=1.0.*&versions=1.1.*&versions=1.2.*&versions=1.3.*&versions=1.4.*"">
    <img src=""https://pepy.tech/badge/hydra-core/month"" alt=""Downloads"" />
  </a>
  <a href=""https://github.c"
self-operating-computer,"<h1 align=""center"">Self-Operating Computer Framework</h1>

<p align=""center"">
  <strong>A framework to enable multimodal models to operate a computer.</strong>
</p>
<p align=""center"">
  Using the same inputs and outputs as a human operator, the model views the screen and decides on a series of mouse and keyboard actions to reach an objective. 
</p>

<div align=""center"">
  <img src=""https://github.com/OthersideAI/self-operating-computer/blob/main/readme/self-operating-computer.png"" width=""750""  style=""margin: 10px;""/>
</div>

<!--
:rotating_light: **OUTAGE NOTIFICATION: gpt-4o**
**This model is currently experiencing an outage so the self-operating computer may not work as expected.**
-->


## Key Features
- **Compatibility**: Designed for various multimodal models.
- **Integration**: Currently integrated with **GPT-4o, Gemini Pro Vision, Claude 3 and LLaVa.**
- **Future Plans**: Support for additional models.

## Ongoing Development
At [HyperwriteAI](https://www.hyperwriteai.com/), we "
opendrop,"# OpenDrop: an Open Source AirDrop Implementation

[![Release](https://img.shields.io/pypi/v/opendrop?color=%23EC6500&label=release)](https://pypi.org/project/opendrop/)
[![Language grade](https://img.shields.io/lgtm/grade/python/github/seemoo-lab/opendrop?label=code%20quality)](https://lgtm.com/projects/g/seemoo-lab/opendrop/context:python)

*OpenDrop* is a command-line tool that allows sharing files between devices directly over Wi-Fi. Its unique feature is that it is protocol-compatible with Apple AirDrop which allows to share files with Apple devices running iOS and macOS. 
~~Currently (and probably also for the foreseeable future), OpenDrop only supports sending to Apple devices that are discoverable by *everybody* as the default *contacts only* mode requires [Apple-signed certificates](https://www.apple.com/certificateauthority/pdf/Apple_AAI_CPS_v6.1.pdf).~~
We support contacts-only devices by using extracted AirDrop credentials (keys and certificates) from macOS via our [keychai"
bisheng,"<img src=""https://dataelem.com/bs/face.png"" alt=""Bisheng banner"">

<p align=""center"">
    <a href=""https://dataelem.feishu.cn/wiki/ZxW6wZyAJicX4WkG0NqcWsbynde""><img src=""https://img.shields.io/badge/docs-Wiki-brightgreen""></a>
    <img src=""https://img.shields.io/github/license/dataelement/bisheng"" alt=""license""/>
    <img src=""https://img.shields.io/docker/pulls/dataelement/bisheng-frontend"" alt=""docker-pull-count"" />
    <a href=""""><img src=""https://img.shields.io/github/last-commit/dataelement/bisheng""></a>
    <a href=""https://star-history.com/#dataelement/bisheng&Timeline""><img src=""https://img.shields.io/github/stars/dataelement/bisheng?color=yellow""></a> 
</p>
<p align=""center"">
  <a href=""./README_CN.md"">ç®€ä½“ä¸­æ–‡</a> |
  <a href=""./README.md"">English</a> |
  <a href=""./README_JPN.md"">æ—¥æœ¬èª</a>
</p>

<p align=""center"">
  <a href=""https://trendshift.io/repositories/717"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/717"" alt=""dataelement%2Fbisheng | Trendshift"" "
data-science-from-scratch,"Data Science from Scratch
=========================

Here's all the code and examples from the second edition of my book _Data Science from Scratch_. They require at least Python 3.6.

(If you're looking for the code and examples from the first edition, that's in the `first-edition` folder.)

If you want to use the code, you should be able to clone the repo and just do things like

```
In [1]: from scratch.linear_algebra import dot

In [2]: dot([1, 2, 3], [4, 5, 6])
Out[2]: 32
```

and so on and so forth.

Two notes:

1. In order to use the library like this, you need to be in the root directory (that is, the directory that contains the `scratch` folder). If you are in the `scratch` directory itself, the imports won't work.

2. It's possible that it will just work. It's also possible that you may need to add the root directory to your `PYTHONPATH`, if you are on Linux or OSX this is as simple as 

```
export PYTHONPATH=/path/to/where/you/cloned/this/repo
```

(substituting in the real "
speechbrain,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/speechbrain/speechbrain/develop/docs/images/speechbrain-logo.svg"" alt=""SpeechBrain Logo""/>
</p>

[![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&size=40&duration=7000&pause=1000&random=false&width=1200&height=100&lines=Simplify+Conversational+AI+Development)](https://git.io/typing-svg)


| ğŸ“˜ [Tutorials](https://speechbrain.readthedocs.io) | ğŸŒ [Website](https://speechbrain.github.io/) | ğŸ“š [Documentation](https://speechbrain.readthedocs.io/en/latest/index.html) | ğŸ¤ [Contributing](https://speechbrain.readthedocs.io/en/latest/contributing.html) | ğŸ¤— [HuggingFace](https://huggingface.co/speechbrain) | â–¶ï¸ [YouTube](https://www.youtube.com/@SpeechBrainProject) | ğŸ¦ [X](https://twitter.com/SpeechBrain1) |

![GitHub Repo stars](https://img.shields.io/github/stars/speechbrain/speechbrain?style=social) *Please, help our community project. Star on GitHub!*

**Exciting News (January, 2024):** Discover what is new in "
Book4_Power-of-Matrix,"ã€Šç»Ÿè®¡è‡³ç®€ã€‹äº”æŠ˜å…¥å£ï¼š
https://zhuanlan.zhihu.com/p/634253719
<br>
ã€Šæ•°å­¦è¦ç´ ã€‹äº”æŠ˜å…¥å£ï¼š
https://zhuanlan.zhihu.com/p/620243026
<br>
ã€ŠçŸ©é˜µåŠ›é‡ã€‹äº”æŠ˜å…¥å£ï¼š
https://zhuanlan.zhihu.com/p/634253719

çœ‹ä¸ªäººæƒ…å†µï¼Œå¼€æºèµ„æºï¼Œæ°¸ä¹…æœ‰æ•ˆå“ˆã€‚

çº é”™å¤šçš„åŒå­¦ä¼šå¾—åˆ°èµ ä¹¦ï¼Œä»¥ç¤ºæ„Ÿè°¢ã€‚
"
PaddleSeg,"ç®€ä½“ä¸­æ–‡ | [English](README_EN.md)

<div align=""center"">

<p align=""center"">
  <img src=""./docs/images/paddleseg_logo.png"" align=""middle"" width = ""500"" />
</p>

**é£æ¡¨é«˜æ€§èƒ½å›¾åƒåˆ†å‰²å¼€å‘å¥—ä»¶ï¼Œç«¯åˆ°ç«¯å®Œæˆä»è®­ç»ƒåˆ°éƒ¨ç½²çš„å…¨æµç¨‹å›¾åƒåˆ†å‰²åº”ç”¨ã€‚**


[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)
[![Version](https://img.shields.io/github/release/PaddlePaddle/PaddleSeg.svg)](https://github.com/PaddlePaddle/PaddleSeg/releases)
![python version](https://img.shields.io/badge/python-3.6+-orange.svg)
![support os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-yellow.svg)
![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleSeg?color=ccf)
</div>

<div align=""center"">
<img src=""https://github.com/shiyutang/files/blob/9590ea6bfc36139982ce75b00d3b9f26713934dd/teasor.gif""  width = ""800"" />  
</div>

## <img src=""./docs/images/seg_news_icon.png"" width=""20""/> æœ€æ–°åŠ¨æ€
- [2024-06-27] **ğŸ’¥ é£æ¡¨ä½ä»£ç å¼€å‘å·¥å…· PaddleX 3.0 é‡ç£…æ›´æ–°ï¼**
  - ä¸°å¯Œçš„æ¨¡å‹äº§çº¿ï¼šç²¾é€‰ 68 ä¸ªä¼˜è´¨é£æ¡¨æ¨¡å‹ï¼Œæ¶µç›–å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€OCRã€æ–‡æœ¬å›¾åƒç‰ˆé¢åˆ†æã€æ—¶åºåˆ†æç­‰ä»»åŠ¡åœºæ™¯ï¼›
  - ä½ä»£ç å¼€å‘èŒƒå¼ï¼šæ”¯æŒå•æ¨¡å‹"
chinese-dos-games,"# ğŸ® ä¸­æ–‡ DOS æ¸¸æˆ

ç½‘å€ï¼š https://dos.lol


ä¸­æ–‡ DOS æ¸¸æˆåˆé›†ï¼Œç›®å‰å…±æœ‰ 1898 æ¬¾æ¸¸æˆã€‚

## ä¸‹è½½æ¸¸æˆæ–‡ä»¶

åœ¨æ ¹ç›®å½•ä¸‹è¿è¡Œ Python 3 è„šæœ¬

``` python
python download_data.py
```

è‹¥ä¸‹è½½å‡ºé”™è¯·å‚è§ [Issue #26](https://github.com/rwv/chinese-dos-games/issues/26)

## æ¸¸æˆåˆ—è¡¨

å‚è§ https://dos.lol/games

## IPFS

IPNS Hash: [`k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2`](https://ipfs.io/ipns/k2k4r8oyknzob8jjqpj6toer4dw3jc6srsbqlbsalktnw1fopb7iyqd2)

## ç½‘ç«™æºä»£ç 

è¯·å‚è§ [rwv/chinese-dos-games-web: ğŸŒ Source code of https://dos.zczc.cz](https://github.com/rwv/chinese-dos-games-web)

## ç‰ˆæƒé—®é¢˜

æœ¬äººæ˜ç™½æ­¤é¡¹ç›®å­˜åœ¨ç‰ˆæƒä¸Šçš„ä¾µæƒï¼Œå¦‚ç‰ˆæƒæ–¹ä»‹æ„çš„è¯ï¼Œè¯·è”ç³» [chinese.dos.games@outlook.com](mailto:chinese.dos.games@outlook.com)ï¼Œæœ¬äººå°†ç«‹åˆ»åˆ é™¤æœ‰å…³æ–‡ä»¶ã€‚

## Contributing

æ¬¢è¿æ [Issue](https://github.com/rwv/chinese-dos-games/issues) å’Œ [Pull request](https://github.com/rwv/chinese-dos-games/pulls) æ¥å¢åŠ æ–°çš„æ¸¸æˆ!

PR å…·ä½“å‚è§ [CONTRIBUTING.md](https://github.com/rwv/chinese-dos-games/blob/master/CONTRIBUTING.md)

## Credits

* [dreamlayers/em-dosbox: An Emscripten port of DOSBox](https://github.com/dreamla"
vid2vid,"<img src='imgs/teaser.gif' align=""right"" width=360>

<br><br><br><br>

# vid2vid
### [Project](https://tcwang0509.github.io/vid2vid/) | [YouTube(short)](https://youtu.be/5zlcXTCpQqM) | [YouTube(full)](https://youtu.be/GrP_aOSXt5U) | [arXiv](https://arxiv.org/abs/1808.06601) | [Paper(full)](https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf)

Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). <br><br>
[Video-to-Video Synthesis](https://tcwang0509.github.io/vid2vid/)  
 [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyul"
ImageAI,"# ImageAI (v3.0.3)



[![Build Status](https://travis-ci.com/OlafenwaMoses/ImageAI.svg?branch=master)](https://travis-ci.com/OlafenwaMoses/ImageAI)  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/OlafenwaMoses/ImageAI/blob/master/LICENSE) [![PyPI version](https://badge.fury.io/py/imageai.svg)](https://badge.fury.io/py/imageai)   [![Downloads](https://pepy.tech/badge/imageai/month)](https://pepy.tech/project/imageai) [![Downloads](https://pepy.tech/badge/imageai/week)](https://pepy.tech/project/imageai)

An open-source python library built to empower developers to build applications and systems with self-contained Deep Learning and Computer Vision capabilities using simple and few lines of code.
 
 If you will like to sponsor this project, kindly visit the <strong>[Github sponsor page](https://github.com/sponsors/OlafenwaMoses)</strong>.
 
 
## ---------------------------------------------------
## Introducing Jarvis and TheiaEngine.

We the cr"
google-images-download,"Google Images Download
######################

Python Script for 'searching' and 'downloading' hundreds of Google images to the local hard disk!

Documentation
=============

* `Documentation Homepage <https://google-images-download.readthedocs.io/en/latest/index.html>`__
* `Installation <https://google-images-download.readthedocs.io/en/latest/installation.html>`__
* `Input arguments <https://google-images-download.readthedocs.io/en/latest/arguments.html>`__
* `Examples and Code Samples <https://google-images-download.readthedocs.io/en/latest/examples.html#>`__


Disclaimer
==========

This program lets you download tons of images from Google.
Please do not download or use any image that violates its copyright terms.
Google Images is a search engine that merely indexes images and allows you to find them.
It does NOT produce its own images and, as such, it doesn't own copyright on any of them.
The original creators of the images own the copyrights.

Images published in the United States"
instaloader,".. image:: https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png

.. badges-start

|pypi| |pyversion| |license| |aur| |contributors| |downloads|

.. |pypi| image:: https://img.shields.io/pypi/v/instaloader.svg
   :alt: Instaloader PyPI Project Page
   :target: https://pypi.org/project/instaloader/

.. |license| image:: https://img.shields.io/github/license/instaloader/instaloader.svg
   :alt: MIT License
   :target: https://github.com/instaloader/instaloader/blob/master/LICENSE

.. |pyversion| image:: https://img.shields.io/pypi/pyversions/instaloader.svg
   :alt: Supported Python Versions

.. |contributors| image:: https://img.shields.io/github/contributors/instaloader/instaloader.svg
   :alt: Contributor Count
   :target: https://github.com/instaloader/instaloader/graphs/contributors

.. |aur| image:: https://img.shields.io/aur/version/instaloader.svg
   :alt: Arch User Repository Package
   :target: https://aur.archlinux.org/packages/instaloader/

.."
flasky,"Flasky
======

This repository contains the source code examples for the second edition of my O'Reilly book [Flask Web Development](http://www.flaskbook.com).

The commits and tags in this repository were carefully created to match the sequence in which concepts are presented in the book. Please read the section titled ""How to Work with the Example Code"" in the book's preface for instructions.

For Readers of the First Edition of the Book
--------------------------------------------

The code examples for the first edition of the book were moved to a different repository: [https://github.com/miguelgrinberg/flasky-first-edition](https://github.com/miguelgrinberg/flasky-first-edition).
"
RobustVideoMatting,"# Robust Video Matting (RVM)

![Teaser](/documentation/image/teaser.gif)

<p align=""center"">English | <a href=""README_zh_Hans.md"">ä¸­æ–‡</a></p>

Official repository for the paper [Robust High-Resolution Video Matting with Temporal Guidance](https://peterl1n.github.io/RobustVideoMatting/). RVM is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network to process videos with temporal memory. RVM can perform matting in real-time on any videos without additional inputs. It achieves **4K 76FPS** and **HD 104FPS** on an Nvidia GTX 1080 Ti GPU. The project was developed at [ByteDance Inc.](https://www.bytedance.com/)

<br>

## News

* [Nov 03 2021] Fixed a bug in [train.py](https://github.com/PeterL1n/RobustVideoMatting/commit/48effc91576a9e0e7a8519f3da687c0d3522045f).
* [Sep 16 2021] Code is re-released under GPL-3.0 license.
* [Aug 25 2021] Source code and pretrained models are published."
U-2-Net,"<p align=""center"">
  <img width=""320"" height=""320"" src=""figures/U2Net_Logo.png"">
  
  <h1 align=""center"">U<sup>2</sup>-Net: U Square Net</h1>
    
</p>

This is the official repo for our paper **U<sup>2</sup>-Net(U square net)** published in Pattern Recognition 2020:

## [U<sup>2</sup>-Net: Going Deeper with Nested U-Structure for Salient Object Detection](https://arxiv.org/pdf/2005.09007.pdf)
[Xuebin Qin](https://xuebinqin.github.io/), [Zichen Zhang](https://webdocs.cs.ualberta.ca/~zichen2/), [Chenyang Huang](https://chenyangh.com/), [Masood Dehghan](https://sites.google.com/view/masooddehghan), [Osmar R. Zaiane](http://webdocs.cs.ualberta.ca/~zaiane/) and [Martin Jagersand](https://webdocs.cs.ualberta.ca/~jag/)


__Contact__: xuebin[at]ualberta[dot]ca

## Updates !!!

** (2022-Aug.-24) ** We are glad to announce that our U<sup>2</sup>-Net published in Pattern Recognition has been awarded the 2020 Pattern Recognition BEST PAPER AWARD !!!
![u2net-best-paper](figures/u2net-best-paper.jp"
pyro,"<!--
Copyright Contributors to the Pyro project.

SPDX-License-Identifier: Apache-2.0
-->

<div align=""center"">
  <a href=""http://pyro.ai""> <img width=""220px"" height=""220px"" src=""docs/source/_static/img/pyro_logo_with_text.png""></a>
</div>

-----------------------------------------

[![Build Status](https://github.com/pyro-ppl/pyro/workflows/CI/badge.svg)](https://github.com/pyro-ppl/pyro/actions)
[![Coverage Status](https://coveralls.io/repos/github/pyro-ppl/pyro/badge.svg?branch=dev)](https://coveralls.io/github/pyro-ppl/pyro?branch=dev)
[![Latest Version](https://badge.fury.io/py/pyro-ppl.svg)](https://pypi.python.org/pypi/pyro-ppl)
[![Documentation Status](https://readthedocs.org/projects/pyro-ppl/badge/?version=dev)](http://pyro-ppl.readthedocs.io/en/stable/?badge=dev)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3056/badge)](https://bestpractices.coreinfrastructure.org/projects/3056)

[Getting Started](http://pyro.ai/examples) |
[Documentation](htt"
awesome-honeypots,"# Awesome Honeypots [![Awesome Honeypots](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome honeypots, plus related components and much more, divided into categories such as Web, services, and others, with a focus on free and open source projects.

There is no pre-established order of items in each category, the order is for contribution. If you want to contribute, please read the [guide](CONTRIBUTING.md).

Discover more awesome lists at [sindresorhus/awesome](https://github.com/sindresorhus/awesome).

# Contents

- [Awesome Honeypots ![Awesome Honeypots](https://github.com/sindresorhus/awesome)](#awesome-honeypots-)
- [Contents](#contents)
  - [Related Lists](#related-lists)
  - [Honeypots](#honeypots)
  - [Honeyd Tools](#honeyd-tools)
  - [Network and Artifact Analysis](#network-and-artifact-analysis)
  - [Data Tools](#data-tools)
  - [Guides](#guides)

## Related "
pyod,"Python Outlier Detection (PyOD)
===============================

**Deployment & Documentation & Stats & License**

|badge_pypi| |badge_anaconda| |badge_docs| |badge_stars| |badge_forks| |badge_downloads| |badge_testing| |badge_coverage| |badge_maintainability| |badge_license| |badge_benchmark|

.. |badge_pypi| image:: https://img.shields.io/pypi/v/pyod.svg?color=brightgreen
   :target: https://pypi.org/project/pyod/
   :alt: PyPI version

.. |badge_anaconda| image:: https://anaconda.org/conda-forge/pyod/badges/version.svg
   :target: https://anaconda.org/conda-forge/pyod
   :alt: Anaconda version

.. |badge_docs| image:: https://readthedocs.org/projects/pyod/badge/?version=latest
   :target: https://pyod.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation status

.. |badge_stars| image:: https://img.shields.io/github/stars/yzhao062/pyod.svg
   :target: https://github.com/yzhao062/pyod/stargazers
   :alt: GitHub stars

.. |badge_forks| image:: https://img.shields.io/github/for"
FreeAskInternet,"# FreeAskInternet

## ğŸ‰ğŸ‰ğŸ‰ Yeah we have a logo now! ğŸ‰ğŸ‰ğŸ‰

![lgoo](./doc/logo-20240412.png)

> Running www.perplexity.ai like app complete FREE, LOCAL, PRIVATE and NO GPU NEED on any computer
> [!IMPORTANT]  
> **If you are unable to use this project normally, it is most likely due to issues with your internet connection or your IP, you need free internet connection to use this project normally. å¦‚æœæ‚¨æ— æ³•æ­£å¸¸ä½¿ç”¨æ­¤é¡¹ç›®ï¼Œå¾ˆå¯èƒ½æ˜¯ç”±äºæ‚¨çš„ IP å­˜åœ¨é—®é¢˜ï¼Œæˆ–è€…ä½ ä¸èƒ½è‡ªç”±è®¿é—®äº’è”ç½‘ã€‚**

## What is FreeAskInternet

FreeAskInternet is a completely free, private and locally running search aggregator & answer generate using LLM, Without GPU needed. The user can ask a question and the system will use searxng to make a multi engine search and combine the search result to the ChatGPT3.5 LLM and generate the answer based on search results. All process running locally and  No GPU or OpenAI or Google API keys are needed.

## Features

- ğŸˆšï¸ Completely FREE (no need for any API keys)
- ğŸ’» Completely LOCAL (no GPU need, any computer can run )
- ğŸ” "
LibreTranslate,"# LibreTranslate

[Try it online!](https://libretranslate.com) | [API Docs](https://libretranslate.com/docs) | [Community Forum](https://community.libretranslate.com/)

[![Python versions](https://img.shields.io/pypi/pyversions/libretranslate)](https://pypi.org/project/libretranslate) [![Run tests](https://github.com/LibreTranslate/LibreTranslate/workflows/Run%20tests/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions?query=workflow%3A%22Run+tests%22) [![Build and Publish Docker Image](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml) [![Publish package](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml) [![Awesome Humane Tech](https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/"
supervisor,"Supervisor
==========

Supervisor is a client/server system that allows its users to
control a number of processes on UNIX-like operating systems.

Supported Platforms
-------------------

Supervisor has been tested and is known to run on Linux (Ubuntu), Mac OS X
(10.4, 10.5, 10.6), and Solaris (10 for Intel) and FreeBSD 6.1.  It will
likely work fine on most UNIX systems.

Supervisor will not run at all under any version of Windows.

Supervisor is intended to work on Python 3 version 3.4 or later
and on Python 2 version 2.7.

Documentation
-------------

You can view the current Supervisor documentation online `in HTML format
<http://supervisord.org/>`_ .  This is where you should go for detailed
installation and configuration documentation.

Reporting Bugs and Viewing the Source Repository
------------------------------------------------

Please report bugs in the `GitHub issue tracker
<https://github.com/Supervisor/supervisor/issues>`_.

You can view the source repository for superv"
WARP-Clash-API,"# WARP Clash API

![GitHub License](https://img.shields.io/github/license/vvbbnn00/WARP-Clash-API)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/67ca8d105fb947eca6204230ba3ac09b)](https://app.codacy.com/gh/vvbbnn00/WARP-Clash-API/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)
![GitHub Repo stars](https://img.shields.io/github/stars/vvbbnn00/WARP-Clash-API?style=flat)

ä¸­æ–‡ | [English](./README_en.md)

> **Warning**
>
> æœ¬é¡¹ç›®æ˜¯å®Œå…¨éå•†ä¸šé¡¹ç›®ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•ç”¨é€”ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚

## ğŸ¤” è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

è¯¥é¡¹ç›®å¯ä»¥è®©ä½ é€šè¿‡è®¢é˜…çš„æ–¹å¼ä½¿ç”¨`WARP+`ï¼Œæ”¯æŒ`Clash`ã€`Shadowrocket`ç­‰å®¢æˆ·ç«¯ã€‚é¡¹ç›®å†…ç½®äº†
åˆ·å–`WARP+`æµé‡çš„åŠŸèƒ½ï¼Œå¯ä»¥è®©ä½ çš„`WARP+`æµé‡ä¸å†å—é™åˆ¶ï¼ˆæ¯`18`ç§’å¯è·å¾—`1GB`æµé‡ï¼‰ï¼ŒåŒæ—¶ï¼Œ
é…å¤‡äº†`IP`é€‰ä¼˜åŠŸèƒ½ã€‚æ”¯æŒ`Docker compose`
ä¸€é”®éƒ¨ç½²ï¼Œæ— éœ€é¢å¤–æ“ä½œï¼Œå³å¯äº«å—ä½ è‡ªå·±çš„`WARP+`ç§
æœ‰é«˜é€ŸèŠ‚ç‚¹ï¼

## ğŸ’¡ ç‰¹è‰²åŠŸèƒ½

- ğŸ’» æ”¯æŒ`Clash`ã€`Surge`ã€`Shadowrocket`ç­‰å®¢æˆ·ç«¯
- ğŸ”‘ æ”¯æŒè®¾ç½®æ‚¨è‡ªå·±çš„`LicenseKey`
- ğŸŒ æ”¯æŒ`IP`é€‰ä¼˜
- ğŸ‹ æ”¯æŒ`Docker compose`ä¸€é”®éƒ¨ç½²
- ğŸ“• å…¨è‡ªåŠ¨åˆ·å–`WARP+`æµé‡ï¼Œè¯·æ±‚ç»è¿‡ä»£ç†ï¼Œé˜²å°`IP`
- â“ æ¯æ¬¡æ›´æ–°è®¢é˜…éšæœºèŠ‚ç‚¹ï¼Œè®©ä½ ä½“éªŒæŠ½å¡çš„ä¹è¶£

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

### 1. å®‰è£…`Docker`å’Œ`Docker compose`

- `Docker`
  å®‰è£…æ•™ç¨‹ï¼š[https://docs.docker.com/engine/install/](https://doc"
microk8s,"<img src=""docs/images/MicroK8s-logo-RGB-2022.png"" width=""400px;"" />

[![](https://github.com/canonical/microk8s/actions/workflows/build-snap.yml/badge.svg)](https://github.com/canonical/microk8s/actions/workflows/build-snap.yml)
[![](https://snapcraft.io/microk8s/badge.svg)](https://snapcraft.io/microk8s)
![](https://img.shields.io/badge/Kubernetes-1.30-326de6.svg)

<img src=""/docs/images/certified_kubernetes_color-222x300.png"" align=""right"" width=""200px"">

## The smallest, fastest Kubernetes

Single-package fully conformant lightweight Kubernetes that works on [42
flavours of Linux](https://snapcraft.io/microk8s). Perfect for:

- Developer workstations
- IoT
- Edge
- CI/CD

 > Canonical might have assembled the easiest way to provision a single node Kubernetes cluster - [Kelsey Hightower](https://twitter.com/kelseyhightower/status/1120834594138406912)

## Why MicroK8s?

- **Small**. Developers want the smallest K8s for laptop and workstation
  development.  MicroK8s provides a standal"
xformers,"<img src=""./docs/assets/logo.png"" width=800>

![Install with conda](https://anaconda.org/xformers/xformers/badges/installer/conda.svg)
![Downloads](https://anaconda.org/xformers/xformers/badges/downloads.svg)
![License](https://anaconda.org/xformers/xformers/badges/license.svg)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/xformers/blob/main/docs/source/xformers_mingpt.ipynb)
<br/><!--
![PyPI](https://img.shields.io/pypi/v/xformers)
![PyPI - License](https://img.shields.io/pypi/l/xformers)
[![Documentation Status](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)
-->
[![CircleCI](https://circleci.com/gh/facebookresearch/xformers.svg?style=shield)](https://app.circleci.com/pipelines/github/facebookresearch/xformers/)
[![Codecov](https://codecov.io/gh/facebookresearch/xformers/"
uvicorn,"<p align=""center"">
  <img width=""320"" height=""320"" src=""https://raw.githubusercontent.com/tomchristie/uvicorn/master/docs/uvicorn.png"" alt='uvicorn'>
</p>

<p align=""center"">
<em>An ASGI web server, for Python.</em>
</p>

---

[![Build Status](https://github.com/encode/uvicorn/workflows/Test%20Suite/badge.svg)](https://github.com/encode/uvicorn/actions)
[![Package version](https://badge.fury.io/py/uvicorn.svg)](https://pypi.python.org/pypi/uvicorn)
[![Supported Python Version](https://img.shields.io/pypi/pyversions/uvicorn.svg?color=%2334D058)](https://pypi.org/project/uvicorn)

**Documentation**: [https://www.uvicorn.org](https://www.uvicorn.org)

---

Uvicorn is an ASGI web server implementation for Python.

Until recently Python has lacked a minimal low-level server/application interface for
async frameworks. The [ASGI specification][asgi] fills this gap, and means we're now able to
start building a common set of tooling usable across all async frameworks.

Uvicorn supports HTTP/1.1"
bottle,".. image:: http://bottlepy.org/docs/dev/_static/logo_nav.png
  :target: http://bottlepy.org/
  :alt: Bottle Logo
  :align: right

.. image:: https://github.com/bottlepy/bottle/workflows/Tests/badge.svg
    :target: https://github.com/bottlepy/bottle/workflows/Tests
    :alt: Tests Status

.. image:: https://img.shields.io/pypi/v/bottle.svg
    :target: https://pypi.python.org/pypi/bottle/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/l/bottle.svg
    :target: https://pypi.python.org/pypi/bottle/
    :alt: License

.. _Python: https://python.org/
.. _mako: https://www.makotemplates.org/
.. _cheetah: https://www.cheetahtemplate.org/
.. _jinja2: https://jinja.palletsprojects.com/

.. _WSGI: https://peps.python.org/pep-3333/
.. _gunicorn: https://gunicorn.org/
.. _paste: https://pythonpaste.readthedocs.io/
.. _cheroot: https://cheroot.cherrypy.dev/

============================
Bottle: Python Web Framework
============================

Bottle is a fast, simple and lightw"
einops,"
<!--
<a href='http://arogozhnikov.github.io/images/einops/einops_video.mp4' >
<div align=""center"">
  <img src=""http://arogozhnikov.github.io/images/einops/einops_video.gif"" alt=""einops package examples"" />
  <br>
  <small><a href='http://arogozhnikov.github.io/images/einops/einops_video.mp4'>This video in high quality (mp4)</a></small>
  <br><br>
</div>
</a>
-->

<!-- this link magically rendered as video on github readme, unfortunately not in docs -->

https://user-images.githubusercontent.com/6318811/177030658-66f0eb5d-e136-44d8-99c9-86ae298ead5b.mp4




# einops 
[![Run tests](https://github.com/arogozhnikov/einops/actions/workflows/run_tests.yml/badge.svg)](https://github.com/arogozhnikov/einops/actions/workflows/run_tests.yml)
[![PyPI version](https://badge.fury.io/py/einops.svg)](https://badge.fury.io/py/einops)
[![Documentation](https://img.shields.io/badge/documentation-link-blue.svg)](https://einops.rocks/)
![Supported python versions](https://raw.githubusercontent.com/arogoz"
pupy,"# Pupy

[![Build Status](https://api.travis-ci.org/n1nj4sec/pupy.svg?branch=unstable)](https://travis-ci.org/n1nj4sec/pupy)

## Installation

Installation instructions are on the wiki, in addition to all other documentation. For maximum compatibility, it is recommended to use Docker Compose.

[Refer to the wiki](https://github.com/n1nj4sec/pupy/wiki/Installation)

## Description

Pupy is a cross-platform, multi function RAT and post-exploitation tool mainly written in python. It features an all-in-memory execution guideline and leaves a very low footprint. Pupy can communicate using multiple transports, migrate into processes using reflective injection, and load remote python code, python packages and python C-extensions from memory.

## Features

- Windows payload can load the entire Python interpreter from memory using a reflective DLL.
	- Pupy does not touch the disk.

- Can be packed into a single .py file and run without any dependencies other than the python standard library on a"
optimate,"# OptiMate

**[Legacy]**

This repository is now in a legacy phase and is no longer actively maintained. Although the source code is still available in the Git history, there will be no additional updates or official support.

**[About Nebuly]**

Our team is fully committed on creating the best user-experience platform for LLMs so that companies can understand user behavior at scale when interacting with their LLM-based products. 
- To learn more on how to get started, visit our [official documentation](https://docs.nebuly.com/welcome/overview)
- If you need enterprise support, please contact us [here](https://www.nebuly.com/nebuly-book-a-demo)

**[About optimate]**

We have open-sourced a couple of internal projects to the community, but we are not currently maintaining them. Optimate is a collection of libraries designed to help you optimize your AI models. It is an open-source project developed by Nebuly AI but is **not actively maintained**.

The tools available to assist you in yo"
CrackMapExec,"# No Longer Maintained

This project is no longer mantained due to the existence of a hostile fork.

# CrackMapExec

<p align=""center"">
  <img src=""https://cloud.githubusercontent.com/assets/5151193/17577511/d312ceb4-5f3b-11e6-8de5-8822246289fd.jpg"" alt=""cme""/>
</p>

You are on the **latest up-to-date** repository of the project CrackMapExec ! ğŸ‰

- ğŸš§ If you want to report a problem, open un [Issue](https://github.com/mpgn/CrackMapExec/issues) 
- ğŸ”€ If you want to contribute, open a [Pull Request](https://github.com/mpgn/CrackMapExec/pulls)
- ğŸ’¬ If you want to discuss, open a [Discussion](https://github.com/mpgn/CrackMapExec/discussions)

# Acknowledgments
**(These are the people who did the hard stuff)**

This project was originally inspired by:
- [CredCrack](https://github.com/gojhonny/CredCrack)
- [smbexec](https://github.com/pentestgeek/smbexec)
- [smbmap](https://github.com/ShawnDEvans/smbmap)

Unintentional contributors:

- The [Empire](https://github.com/PowerShellEmpire/Empire) pr"
ansible-for-devops,"# Ansible for DevOps Examples

[![CI](https://github.com/geerlingguy/ansible-for-devops/workflows/CI/badge.svg?event=push)](https://github.com/geerlingguy/ansible-for-devops/actions?query=workflow%3ACI) [![Molecule CI](https://github.com/geerlingguy/ansible-for-devops/workflows/Molecule%20CI/badge.svg?event=push)](https://github.com/geerlingguy/ansible-for-devops/actions?query=workflow%3A%22Molecule+CI%22)

This repository contains Ansible examples developed to support different sections of [Ansible for DevOps](https://www.ansiblefordevops.com/), a book on [Ansible](http://www.ansible.com/) by [Jeff Geerling](https://www.jeffgeerling.com/).

Many examples use Vagrant, VirtualBox, and Ansible to boot and configure VMs on your local workstation.

Not all playbooks follow all of Ansible's best practices, as they illustrate particular Ansible features in an instructive manner.

## Manuscript

The book's manuscript is released under the CC BY-SA license, and is publicly available in a separ"
composio,"<p align=""center"">
  <a href=""https://x.com/GanatraSoham/?utm_campaign=github-readme"" target=""_blank"">
    <img src=""./python/docs/imgs/follow_x.png"" width=""100%"" alt=""Follow me"" />
  </a>
  <br /> <br />
</p>
<p align=""center"">
  <a href=""https://app.composio.dev/?utm_campaign=github-readme"" target=""_blank"">
    <img src=""./python/docs/imgs/try_hosted.png"" width=""100%"" alt=""Sign up"" />
  </a>

  <br /> <br />
</p>
<p>
  <a href=""https://github.com/composiohq/composio/blob/master/README.md"">EN</a> | <a href=""https://github.com/composiohq/composio/blob/master/README-CN.md"">CN</a> | <a href=""https://github.com/composiohq/composio/blob/master/README-JP.md"">JP</a>
</p>

<p align=""center"">
  <a href=""https://composio.dev//#gh-dark-mode-only"">
    <img src=""./python/docs/imgs/composio_white_font.svg"" width=""318px"" alt=""Composio logo"" />
  </a>
  <a href=""https://composio.dev//#gh-light-mode-only"">
    <img src=""./python/docs/imgs/composio_black_font.svg"" width=""318px"" alt=""Composio Logo"" />
"
speech_recognition,"SpeechRecognition
=================

.. image:: https://img.shields.io/pypi/v/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/status/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: Development Status

.. image:: https://img.shields.io/pypi/pyversions/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: Supported Python Versions

.. image:: https://img.shields.io/pypi/l/SpeechRecognition.svg
    :target: https://pypi.python.org/pypi/SpeechRecognition/
    :alt: License

.. image:: https://api.travis-ci.org/Uberi/speech_recognition.svg?branch=master
    :target: https://travis-ci.org/Uberi/speech_recognition
    :alt: Continuous Integration Test Results

Library for performing speech recognition, with support for several engines and APIs, online and offline.

**UPDATE 2022-02-09**:"
xonsh,"xonsh
=====

.. class:: center

    **xonsh** is a Python-powered shell. Full-featured and cross-platform. The language is a superset of Python 3.6+ with additional shell primitives.  Xonsh word was made from *conch* (ğŸš, *@*) and indicates belonging to the command shells world.


.. list-table::
   :widths: 1 1

   *  -  **Xonsh is the Shell**
      -  **Xonsh is Python**

   *  -  .. code-block:: shell

            cd $HOME

            id $(whoami)

            cat /etc/passwd | grep root > ~/root.txt

            $PROMPT = '@ '


      -  .. code-block:: python

            2 + 2

            var = ""hello"".upper()

            import json; json.loads('{""a"":1}')

            [i for i in range(0,10)]

   *  -  **Xonsh is the Shell in Python**
      -  **Xonsh is Python in the Shell**

   *  -  .. code-block:: python

            len($(curl -L https://xon.sh))

            $PATH.append('/tmp')

            p'/etc/passwd'.read_text().find('root')

            xontrib load dalias
       "
pywal,"<h3 align=""center""><img src=""https://i.imgur.com/5WgMACe.gif"" width=""200px""></h3>
<p align=""center"">Generate and change color-schemes on the fly.</p>

<p align=""center"">
<a href=""https://travis-ci.org/dylanaraps/pywal""><img src=""https://travis-ci.org/dylanaraps/pywal.svg?branch=master""></a>
<a href=""./LICENSE.md""><img src=""https://img.shields.io/badge/license-MIT-blue.svg""></a>
<a href=""https://pypi.python.org/pypi/pywal/""><img src=""https://img.shields.io/pypi/v/pywal.svg""></a>
<a href=""https://www.patreon.com/dyla""><img src=""https://img.shields.io/badge/donate-patreon-yellow.svg""></a>
<a href=""https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=V7QNJNKS3WYVS""><img src=""https://img.shields.io/badge/donate-paypal-green.svg""></a>
</p>

<img src=""https://i.imgur.com/HhK3LDv.jpg"" alt=""img"" align=""right"" width=""400px"">

Pywal is a tool that generates a color palette from the dominant colors in an image. It then applies the colors system-wide and on-the-fly in all of your fa"
apex,"# Introduction

This repository holds NVIDIA-maintained utilities to streamline mixed precision and distributed training in Pytorch.
Some of the code here will be included in upstream Pytorch eventually.
The intent of Apex is to make up-to-date utilities available to users as quickly as possible.

## Full API Documentation: [https://nvidia.github.io/apex](https://nvidia.github.io/apex)

## [GTC 2019](https://github.com/mcarilli/mixed_precision_references/tree/master/GTC_2019) and [Pytorch DevCon 2019](https://github.com/mcarilli/mixed_precision_references/tree/master/Pytorch_Devcon_2019) Slides

# Contents

## 1. Amp:  Automatic Mixed Precision

**Deprecated. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)**

`apex.amp` is a tool to enable mixed precision training by changing only 3 lines of your script.
Users can easily experiment with different pure and mixed precision training modes by supplying
different flags to `amp.initialize`.

[Webinar introducing Amp](https://info"
espnet,"<div align=""left""><img src=""doc/image/espnet_logo1.png"" width=""550""/></div>

# ESPnet: end-to-end speech processing toolkit

|system/pytorch ver.|1.13.1|2.0.1|2.1.2|2.2.2|2.3.1|2.4.0|
| :---- | :---: | :---: | :---: | :---: | :---: | :---: |
|ubuntu/python3.10/pip||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query"
ml-ferret,"<!-- # Project Name

This software project accompanies the research paper, [Paper title](https://arxiv.org).

Brief description of the project.

## Documentation

## Getting Started  -->

# <img src=""figs/ferret_icon.png"" alt=""Alt text for the image"" width=""40"" height=""45""> Ferret: Refer and Ground Anything Anywhere at Any Granularity

*An End-to-End MLLM that Accept Any-Form Referring and Ground Anything in Response.* [[Paper](https://arxiv.org/abs/2310.07704)]

[Haoxuan You*](https://hxyou.github.io/), [Haotian Zhang*](https://haotian-zhang.github.io/), [Zhe Gan](https://zhegan27.github.io/), [Xianzhi Du](https://scholar.google.com/citations?user=l1hP40AAAAAJ&hl=en), [Bowen Zhang](https://zbwglory.github.io/), [Zirui Wang](https://www.cs.cmu.edu/~ziruiw/), [Liangliang Cao](http://llcao.net/), [Shih-Fu Chang](https://www.ee.columbia.edu/~sfchang/), [Yinfei Yang](https://sites.google.com/site/yinfeiyang/) 
[*: equal contribution]


## Overview

<p align=""center"">
    <img src=""figs/fer"
weiboSpider,"[![Build Status](https://github.com/dataabc/weiboSpider/workflows/Python%20application/badge.svg)](https://badge.fury.io/py/weibo-spider)
[![Python](https://img.shields.io/pypi/pyversions/weibo-spider)](https://badge.fury.io/py/weibo-spider)
[![PyPI](https://badge.fury.io/py/weibo-spider.svg)](https://badge.fury.io/py/weibo-spider)

# Weibo Spider

æœ¬ç¨‹åºå¯ä»¥è¿ç»­çˆ¬å–**ä¸€ä¸ª**æˆ–**å¤šä¸ª**æ–°æµªå¾®åšç”¨æˆ·ï¼ˆå¦‚[èƒ¡æ­Œ](https://weibo.cn/u/1223178222)ã€[è¿ªä¸½çƒ­å·´](https://weibo.cn/u/1669879400)ã€[éƒ­ç¢§å©·](https://weibo.cn/u/1729370543)ï¼‰çš„æ•°æ®ï¼Œå¹¶å°†ç»“æœä¿¡æ¯å†™å…¥**æ–‡ä»¶**æˆ–**æ•°æ®åº“**ã€‚å†™å…¥ä¿¡æ¯å‡ ä¹åŒ…æ‹¬ç”¨æˆ·å¾®åšçš„æ‰€æœ‰æ•°æ®ï¼ŒåŒ…æ‹¬**ç”¨æˆ·ä¿¡æ¯**å’Œ**å¾®åšä¿¡æ¯**ä¸¤å¤§ç±»ã€‚å› ä¸ºå†…å®¹å¤ªå¤šï¼Œè¿™é‡Œä¸å†èµ˜è¿°ï¼Œè¯¦ç»†å†…å®¹è§[è·å–åˆ°çš„å­—æ®µ](#è·å–åˆ°çš„å­—æ®µ)ã€‚å¦‚æœåªéœ€è¦ç”¨æˆ·ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®å®ç°åªçˆ¬å–å¾®åšç”¨æˆ·ä¿¡æ¯çš„åŠŸèƒ½ã€‚æœ¬ç¨‹åºéœ€è®¾ç½®cookieæ¥è·å–å¾®åšè®¿é—®æƒé™ï¼Œåé¢ä¼šè®²è§£[å¦‚ä½•è·å–cookie](#å¦‚ä½•è·å–cookie)ã€‚å¦‚æœä¸æƒ³è®¾ç½®cookieï¼Œå¯ä»¥ä½¿ç”¨[å…cookieç‰ˆ](https://github.com/dataabc/weibo-crawler)ï¼ŒäºŒè€…åŠŸèƒ½ç±»ä¼¼ã€‚

çˆ¬å–ç»“æœå¯å†™å…¥æ–‡ä»¶å’Œæ•°æ®åº“ï¼Œå…·ä½“çš„å†™å…¥æ–‡ä»¶ç±»å‹å¦‚ä¸‹ï¼š

- **txtæ–‡ä»¶**ï¼ˆé»˜è®¤ï¼‰
- **csvæ–‡ä»¶**ï¼ˆé»˜è®¤ï¼‰
- **jsonæ–‡ä»¶**ï¼ˆå¯é€‰ï¼‰
- **MySQLæ•°æ®åº“**ï¼ˆå¯é€‰ï¼‰
- **MongoDBæ•°æ®åº“**ï¼ˆå¯é€‰ï¼‰
- **SQLiteæ•°æ®åº“**ï¼ˆå¯é€‰ï¼‰

åŒæ—¶æ”¯æŒä¸‹è½½å¾®åšä¸­çš„å›¾ç‰‡å’Œè§†é¢‘ï¼Œå…·ä½“çš„å¯ä¸‹è½½æ–‡ä»¶å¦‚ä¸‹ï¼š

- **åŸåˆ›**å¾®åšä¸­çš„åŸå§‹**å›¾ç‰‡**ï¼ˆå¯é€‰ï¼‰
- **è½¬å‘**å¾®åšä¸­çš„åŸå§‹**å›¾ç‰‡**ï¼ˆå¯é€‰ï¼‰
- **åŸåˆ›**å¾®åšä¸­çš„**è§†é¢‘**ï¼ˆå¯é€‰ï¼‰
"
outlines,"<div align=""center"" style=""margin-bottom: 1em;"">

<img src=""./docs/assets/images/logo.png"" alt=""Outlines Logo"" width=500></img>

[![.txt Twitter][dottxt-twitter-badge]][dottxt-twitter]

[![Documentation][documentation-badge]][documentation]
[![Contributors][contributors-badge]][contributors]
[![Downloads][downloads-badge]][pypistats]
[![Discord][discord-badge]][discord]


*Robust (structured) text generation.*

Made with â¤ğŸ‘·ï¸ by the team at [.txt](https://dottxt.co).

</div>


``` bash
pip install outlines
```

First time here? Go to our [setup guide](https://dottxt-ai.github.io/outlines/welcome)

## Features

- [x] ğŸ¤– [Multiple model integrations](https://dottxt-ai.github.io/outlines/installation): OpenAI, transformers, llama.cpp, exllama2, mamba
- [x] ğŸ–ï¸ Simple and powerful prompting primitives based on the [Jinja templating engine](https://jinja.palletsprojects.com/)
- [x] ğŸš„ [Multiple choices](#multiple-choices), [type constraints](#type-constraint) and dynamic stopping
- [x] âš¡ Fast ["
machine_learning_examples,"machine_learning_examples
=========================

A collection of machine learning examples and tutorials.

Find associated tutorials at https://lazyprogrammer.me

Find associated courses at https://deeplearningcourses.com

Please note that not all code from all courses will be found in this repository. Some newer code examples (e.g. most of Tensorflow 2.0) were done in Google Colab. Therefore, you should check the instructions given in the lectures for the course you are taking.


How to I find the code for a particular course?
===============================================

The code for each course is separated by folder. You can determine which folder corresponds with which course by watching the ""Where to get the code"" lecture inside the course (usually Lecture 2 or 3).

Remember: one folder = one course.


Why you should not fork this repo
=================================

I've noticed that many people have out-of-date forks. Thus, I recommend not forking this repository if y"
python-for-android,"# python-for-android

python-for-android (p4a) is a development tool that packages Python apps into
binaries that can run on Android devices.

It can generate: 

* [Android Package](https://en.wikipedia.org/wiki/Apk_(file_format)) (APK)
  files, ready to install locally on a device, especially for testing. This format
  is used by many [app stores](https://en.wikipedia.org/wiki/List_of_Android_app_stores)
  but not [Google Play Store](https://play.google.com/store/). 
* [Android App Bundle](https://developer.android.com/guide/app-bundle/faq) 
  (AAB) files which can be shared on [Google Play Store](https://play.google.com/store/).
* [Android Archive](https://developer.android.com/studio/projects/android-library)
  (AAR) files which can be used as a re-usable bundle of resources for other 
  projects.
 
It supports multiple CPU architectures.

It supports apps developed with [Kivy framework](http://kivy.org), but was
built to be flexible about the backend libraries (through ""bootstraps"""
Monocraft,"# Monocraft

[![Github all releases](https://img.shields.io/github/downloads/IdreesInc/Monocraft/total.svg)](https://GitHub.com/IdreesInc/Monocraft/releases/)
![](https://img.shields.io/github/license/IdreesInc/Monocraft)
[![](https://img.shields.io/github/v/release/IdreesInc/Monocraft)](https://GitHub.com/IdreesInc/Monocraft/releases/)

## [`Download it here!`](https://github.com/IdreesInc/Monocraft/releases)
<br/>

![](images/preview.png)


The monospaced font for developers who like Minecraft a bit _too_ much.

If you'd like to see a vectorized version of this font, try [Miracode](https://github.com/IdreesInc/Miracode)!

*Notice: This project is not affiliated with Minecraft or Mojang in any way and is exclusively a fan project. This font emulates the typeface of the font used in the Minecraft UI, but it does not include any assets or font files from the original game.*

## Features

- Minecraft!
  - The characters in this font were based around the [typeface](https://github.com/Idr"
vaex,"[![Supported Python Versions](https://img.shields.io/pypi/pyversions/vaex-core)](https://pypi.org/project/vaex-core/)
[![Documentation](https://readthedocs.org/projects/vaex/badge/?version=latest)](https://docs.vaex.io)
[![Slack](https://img.shields.io/badge/slack-chat-green.svg)](https://join.slack.com/t/vaexio/shared_invite/zt-shhxzf5i-Cf5n2LtkoYgUjOjbB3bGQQ)

# What is Vaex?

Vaex is a high performance Python library for lazy **Out-of-Core DataFrames**
(similar to Pandas), to visualize and explore big tabular datasets. It
calculates *statistics* such as mean, sum, count, standard deviation etc, on an
*N-dimensional grid* for more than **a billion** (`10^9`) samples/rows **per
second**. Visualization is done using **histograms**, **density plots** and **3d
volume rendering**, allowing interactive exploration of big data. Vaex uses
memory mapping, zero memory copy policy and lazy computations for best
performance (no memory wasted).

# Installing
With pip:
```
$ pip install vaex
```
O"
anomaly-detection-resources,"Anomaly Detection Learning Resources
====================================

.. image:: https://img.shields.io/github/stars/yzhao062/anomaly-detection-resources.svg
   :target: https://github.com/yzhao062/anomaly-detection-resources/stargazers
   :alt: GitHub stars


.. image:: https://img.shields.io/github/forks/yzhao062/anomaly-detection-resources.svg?color=blue
   :target: https://github.com/yzhao062/anomaly-detection-resources/network
   :alt: GitHub forks


.. image:: https://img.shields.io/github/license/yzhao062/anomaly-detection-resources.svg?color=blue
   :target: https://github.com/yzhao062/anomaly-detection-resources/blob/master/LICENSE
   :alt: License


.. image:: https://awesome.re/badge-flat2.svg
   :target: https://awesome.re/badge-flat2.svg
   :alt: Awesome


.. image:: https://img.shields.io/badge/ADBench-benchmark_results-pink
   :target: https://github.com/Minqi824/ADBench
   :alt: Benchmark


----

`Outlier Detection <https://en.wikipedia.org/wiki/Anomaly_detection>`"
gixy,"GIXY
====
[![Mozilla Public License 2.0](https://img.shields.io/github/license/yandex/gixy.svg?style=flat-square)](https://github.com/yandex/gixy/blob/master/LICENSE)
[![Build Status](https://img.shields.io/travis/yandex/gixy.svg?style=flat-square)](https://travis-ci.org/yandex/gixy)
[![Your feedback is greatly appreciated](https://img.shields.io/maintenance/yes/2019.svg?style=flat-square)](https://github.com/yandex/gixy/issues/new)
[![GitHub issues](https://img.shields.io/github/issues/yandex/gixy.svg?style=flat-square)](https://github.com/yandex/gixy/issues)
[![GitHub pull requests](https://img.shields.io/github/issues-pr/yandex/gixy.svg?style=flat-square)](https://github.com/yandex/gixy/pulls)

# Overview
<img align=""right"" width=""192"" height=""192"" src=""/docs/logo.png"">

Gixy is a tool to analyze Nginx configuration.
The main goal of Gixy is to prevent security misconfiguration and automate flaw detection.

Currently supported Python versions are 2.7, 3.5, 3.6 and 3.7.

Disclaimer: "
OctoPrint,"<p align=""center""><img src=""https://octoprint.org/assets/img/logo.png"" alt=""OctoPrint's logo"" /></p>

<h1 align=""center"">OctoPrint</h1>

<p align=""center"">
  <img src=""https://img.shields.io/github/v/release/OctoPrint/OctoPrint?logo=github&logoColor=white"" alt=""GitHub release""/>
  <img src=""https://img.shields.io/pypi/v/OctoPrint?logo=python&logoColor=white"" alt=""PyPI""/>
  <img src=""https://img.shields.io/github/actions/workflow/status/OctoPrint/OctoPrint/build.yml?branch=master"" alt=""Build status""/>
  <a href=""https://community.octoprint.org""><img src=""https://img.shields.io/discourse/users?label=forum&logo=discourse&logoColor=white&server=https%3A%2F%2Fcommunity.octoprint.org"" alt=""Community Forum""/></a>
  <a href=""https://discord.octoprint.org""><img src=""https://img.shields.io/discord/704958479194128507?label=discord&logo=discord&logoColor=white"" alt=""Discord""/></a>
  <a href=""https://octoprint.org/conduct/""><img src=""https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopte"
ImageBind,"# ImageBind: One Embedding Space To Bind Them All

**[FAIR, Meta AI](https://ai.facebook.com/research/)** 

Rohit Girdhar*,
Alaaeldin El-Nouby*,
Zhuang Liu,
Mannat Singh,
Kalyan Vasudev Alwala,
Armand Joulin,
Ishan Misra*

To appear at CVPR 2023 (*Highlighted paper*)

[[`Paper`](https://facebookresearch.github.io/ImageBind/paper)] [[`Blog`](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/)] [[`Demo`](https://imagebind.metademolab.com/)] [[`Supplementary Video`](https://dl.fbaipublicfiles.com/imagebind/imagebind_video.mp4)] [[`BibTex`](#citing-imagebind)]

PyTorch implementation and pretrained models for ImageBind. For details, see the paper: **[ImageBind: One Embedding Space To Bind Them All](https://facebookresearch.github.io/ImageBind/paper)**.

ImageBind learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. It enables novel emergent applications â€˜out-of-the-boxâ€™ including cross-modal retrieval, composing modali"
spyder,"![Spyder â€” The Scientific Python Development Environment](https://raw.githubusercontent.com/spyder-ide/spyder/master/branding/logo/spyder_readme_banner.png)

*Copyright Â© 2009â€“ [Spyder Project Contributors](https://github.com/spyder-ide/spyder/graphs/contributors)* and others (see AUTHORS.txt)

*Some source files and icons may be under other authorship/licenses; see
[NOTICE.txt](https://github.com/spyder-ide/spyder/blob/master/NOTICE.txt).*

## Project status

[![license](https://img.shields.io/pypi/l/spyder.svg)](./LICENSE.txt)
[![pypi version](https://img.shields.io/pypi/v/spyder.svg)](https://pypi.org/project/spyder/)
[![conda version](https://img.shields.io/conda/vn/conda-forge/spyder.svg)](https://anaconda.org/conda-forge/spyder)
[![download count](https://img.shields.io/conda/dn/conda-forge/spyder.svg)](https://anaconda.org/conda-forge/spyder)
[![OpenCollective Backers](https://opencollective.com/spyder/backers/badge.svg?color=blue)](#backers)
[![OpenCollective Sponsors](https://"
LMFlow,"<p align=""center"" width=""50%"">
<img src=""assets/logo.png"" alt=""LMFlow"" style=""width: 50%; min-width: 200px; display: block; margin: auto; background-color: transparent;"">
</p>

# LMFlow

<h4 align=""center"">
    <p>
        <b>English</b> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_zh-hans.md"">ç®€ä½“ä¸­æ–‡</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_es.md"">EspaÃ±ol</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_jp.md"">æ—¥æœ¬èª</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_ko.md"">í•œêµ­ì–´</a> |
        <a href=""https://github.com/OptimalScale/LMFlow/blob/main/readme/README_hindi.md"">à¤¹à¤¿à¤‚à¤¦à¥€</a>
    <p>
</h4>

[![Website](https://img.shields.io/badge/Website-Demo-20B2AA.svg)](https://lmflow.com)
[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/OptimalScale/LMFlow/blob/main/LICENSE)
[![Python 3.9+](ht"
cookiecutter-data-science,"# Cookiecutter Data Science

_A logical, reasonably standardized but flexible project structure for doing and sharing data science work._

**Cookiecutter Data Science (CCDS)** is a tool for setting up a data science project template that incorporates best practices. To learn more about CCDS's philosophy, visit the [project homepage](https://cookiecutter-data-science.drivendata.org/).

> â„¹ï¸ Cookiecutter Data Science v2 has changed from v1. It now requires installing the new cookiecutter-data-science Python package, which extends the functionality of the [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/README.html) templating utility. Use the provided `ccds` command-line program instead of `cookiecutter`.

## Installation

Cookiecutter Data Science v2 requires Python 3.8+. Since this is a cross-project utility application, we recommend installing it with [pipx](https://pypa.github.io/pipx/). Installation command options:

```bash
# With pipx from PyPI (recommended)
pipx insta"
gpt-neo,"# GPT Neo

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5297715.svg)](https://doi.org/10.5281/zenodo.5297715) [![arXiv](https://img.shields.io/badge/arXiv-2101.00027-f9f107.svg)](https://arxiv.org/abs/2101.00027)

**As of August, 2021 code is no longer maintained. It is preserved here in archival form for people who wish to continue to use it.*

ğŸ‰ 1T or bust my dudes ğŸ‰

An implementation of model & data parallel [GPT3](https://arxiv.org/abs/2005.14165)-like models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.

**If you're just here to play with our pre-trained models, we strongly recommend you try out the [HuggingFace Transformer integration](https://huggingface.co/EleutherAI).**

Training and inference is officially supported on TPU and should work on GPU as well. This repository will be (mostly) archived as we move focus to our GPU-specific repo, [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/).

In addition to the functionality offered by GPT-3, "
DAIN,"# DAIN (Depth-Aware Video Frame Interpolation)
[Project](https://sites.google.com/view/wenbobao/dain) **|** [Paper](http://arxiv.org/abs/1904.00830)

[Wenbo Bao](https://sites.google.com/view/wenbobao/home),
[Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), 
[Chao Ma](https://sites.google.com/site/chaoma99/),
Xiaoyun Zhang, 
Zhiyong Gao, 
and [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)

IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CVPR 2019

This work is developed based on our TPAMI work [MEMC-Net](https://github.com/baowenbo/MEMC-Net), where we propose the adaptive warping layer. Please also consider referring to it.

### Table of Contents
1. [Introduction](#introduction)
1. [Citation](#citation)
1. [Requirements and Dependencies](#requirements-and-dependencies)
1. [Installation](#installation)
1. [Testing Pre-trained Models](#testing-pre-trained-models)
1. [Downloading Results](#downloading-results)
1. [Slow-motion Generation](#slow-"
git-filter-repo,"git filter-repo is a versatile tool for rewriting history, which includes
[capabilities I have not found anywhere
else](#design-rationale-behind-filter-repo).  It roughly falls into the
same space of tool as [git
filter-branch](https://git-scm.com/docs/git-filter-branch) but without the
capitulation-inducing poor
[performance](https://public-inbox.org/git/CABPp-BGOz8nks0+Tdw5GyGqxeYR-3FF6FT5JcgVqZDYVRQ6qog@mail.gmail.com/),
with far more capabilities, and with a design that scales usability-wise
beyond trivial rewriting cases.  [git filter-repo is now recommended by the
git project](https://git-scm.com/docs/git-filter-branch#_warning) instead
of git filter-branch.

While most users will probably just use filter-repo as a simple command
line tool (and likely only use a few of its flags), at its core filter-repo
contains a library for creating history rewriting tools.  As such, users
with specialized needs can leverage it to quickly create [entirely new
history rewriting tools](contrib/f"
OneForAll,"# OneForAll

[![Build Status](https://travis-ci.org/shmilylty/OneForAll.svg?branch=master)](https://travis-ci.org/shmilylty/OneForAll)
[![codecov](https://codecov.io/gh/shmilylty/OneForAll/branch/master/graph/badge.svg)](https://codecov.io/gh/shmilylty/OneForAll)
[![Maintainability](https://api.codeclimate.com/v1/badges/1287668a6b4c72af683e/maintainability)](https://codeclimate.com/github/shmilylty/OneForAll/maintainability)
[![License](https://img.shields.io/github/license/shmilylty/OneForAll)](https://github.com/shmilylty/OneForAll/tree/master/LICENSE)
[![python](https://img.shields.io/badge/python-3.6+-blue)](https://github.com/shmilylty/OneForAll/tree/master/)
[![python](https://img.shields.io/badge/release-v0.4.5-brightgreen)](https://github.com/shmilylty/OneForAll/releases)

ğŸ‘Š**OneForAllæ˜¯ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„å­åŸŸæ”¶é›†å·¥å…·**  ğŸ“[English Document](https://github.com/shmilylty/OneForAll/tree/master/docs/en-us/README.md)

![Example](./docs/usage_example.svg)

## ğŸš€ä¸Šæ‰‹æŒ‡å—

ğŸ“¢ è¯·åŠ¡å¿…èŠ±ä¸€ç‚¹æ—¶é—´é˜…è¯»æ­¤æ–‡æ¡£ï¼Œæœ‰åŠ©äºä½ å¿«é€Ÿç†Ÿæ‚‰OneForAllï¼

"
stable-dreamfusion,"# Stable-Dreamfusion

A pytorch implementation of the text-to-3D model **Dreamfusion**, powered by the [Stable Diffusion](https://github.com/CompVis/stable-diffusion) text-to-2D model.

**ADVERTISEMENT: Please check out [threestudio](https://github.com/threestudio-project/threestudio) for recent improvements and better implementation in 3D content generation!**

**NEWS (2023.6.12)**:

* Support of [Perp-Neg](https://perp-neg.github.io/) to alleviate multi-head problem in Text-to-3D.
* Support of Perp-Neg for both [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and [DeepFloyd-IF](https://github.com/deep-floyd/IF).

https://user-images.githubusercontent.com/25863658/236712982-9f93bd32-83bf-423a-bb7c-f73df7ece2e3.mp4

https://user-images.githubusercontent.com/25863658/232403162-51b69000-a242-4b8c-9cd9-4242b09863fa.mp4

### [Update Logs](assets/update_logs.md)

### Colab notebooks:
* Instant-NGP backbone (`-O`): [![Instant-NGP Backbone](https://colab.research.google.com/ass"
demucs,"# Demucs Music Source Separation

[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)
![tests badge](https://github.com/facebookresearch/demucs/workflows/tests/badge.svg)
![linter badge](https://github.com/facebookresearch/demucs/workflows/linter/badge.svg)


**Important:** As I am no longer working at Meta, **this repository is not maintained anymore**.
I've created a fork at [github.com/adefossez/demucs](https://github.com/adefossez/demucs). Note that this project is not actively maintained anymore
and only important bug fixes will be processed on the new repo. Please do not open issues for feature request or if Demucs doesn't work perfectly for your use case :)

This is the 4th release of Demucs (v4), featuring Hybrid Transformer based source separation.
**For the classic Hybrid Demucs (v3):** [Go this commit][demucs_v3].
If you are experiencing issues and want the old Demucs back, please f"
Airtest,"# Airtest &middot; [![Build status](https://travis-ci.org/AirtestProject/Airtest.svg?branch=master)](https://travis-ci.org/AirtestProject/Airtest)

**Cross-Platform UI Automation Framework for Games and Apps**

**è·¨å¹³å°çš„UIè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€‚ç”¨äºæ¸¸æˆå’ŒApp** ï¼ˆ[ä¸­æ–‡ç‰ˆç‚¹è¿™é‡Œ](./README_zh.md)ï¼‰


![image](./demo.gif)


## Features

*   **Write Once, Run Anywhere:** Airtest provides cross-platform APIs, including app installation, simulated input, assertion and so forth. Airtest uses image recognition technology to locate UI elements so that you can automate games and apps without injecting any code. 

*   **Fully Scalable:** Airtest cases can be easily run on large device farms, using commandline or python API. HTML reports with detailed info and screen recording allow you to quickly locate failure points. NetEase builds [Airlab](https://airlab.163.com/) on top of the Airtest Project.

*   **AirtestIDE:** AirtestIDE is an out of the box GUI tool that helps to create and run cases in a user-friendly way. AirtestIDE su"
sigma,"# Sigma - Generic Signature Format for SIEM Systems

<a href=""https://sigmahq.io/"">
<p align=""center"">
<br />
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""./images/sigma_logo_dark.png"">
  <img width=""454"" alt=""Sigma Logo"" src=""./images/sigma_logo_light.png"">
</picture>
</p>
</a>
<br />

<p align=""center"">
<a href=""https://github.com/SigmaHQ/sigma/actions?query=branch%3Amaster""><img src=""https://github.com/SigmaHQ/sigma/actions/workflows/sigma-test.yml/badge.svg?branch=master"" alt=""Sigma Build Status""></a> <a href=""https://sigmahq.io/""><img src=""https://cdn.jsdelivr.net/gh/SigmaHQ/sigmahq.github.io@master/images/Sigma%20Official%20Badge.svg"" alt=""Sigma Official Badge""></a> <img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/SigmaHQ/sigma"">
<img alt=""GitHub all releases"" src=""https://img.shields.io/github/downloads/SigmaHq/Sigma/total"">
<br />
<a href=""https://opensourcesecurityindex.io/"" target=""_blank"" rel=""noopener"">
<img style=""width: 170px;"" src"
conan,"<picture>
  <!-- These are also used for https://github.com/conan-io/.github/blob/main/profile/README.md -->
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/conan-io/conan/develop2/.github/conan2-logo-for-dark.svg"">
  <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/conan-io/conan/develop2/.github/conan2-logo-for-light.svg"">
  <img alt=""JFrog | Conan 2.0 Logo"" src=""https://raw.githubusercontent.com/conan-io/conan/develop2/.github/conan2-logo-with-bg.svg"">
</picture>

# Conan

Decentralized, open-source (MIT), C/C++ package manager.

- Homepage: https://conan.io/
- Github: https://github.com/conan-io/conan
- Docs: https://docs.conan.io
- Slack: https://cpplang.slack.com (#conan channel. Please, click [here](https://join.slack.com/t/cpplang/shared_invite/zt-1snzdn6rp-rOUxF3166oz1_11Tr5H~xg) to get an invitation)
- Twitter: https://twitter.com/conan_io


Conan is a package manager for C and C++ developers:

- "
fiftyone,"<div align=""center"">
<p align=""center"">

<!-- prettier-ignore -->
<img src=""https://user-images.githubusercontent.com/25985824/106288517-2422e000-6216-11eb-871d-26ad2e7b1e59.png"" height=""55px""> &nbsp;
<img src=""https://user-images.githubusercontent.com/25985824/106288518-24bb7680-6216-11eb-8f10-60052c519586.png"" height=""50px"">

**The open-source tool for building high-quality datasets and computer vision
models**

---

<!-- prettier-ignore -->
<a href=""https://voxel51.com/fiftyone"">Website</a> â€¢
<a href=""https://voxel51.com/docs/fiftyone"">Docs</a> â€¢
<a href=""https://colab.research.google.com/github/voxel51/fiftyone-examples/blob/master/examples/quickstart.ipynb"">Try it Now</a> â€¢
<a href=""https://voxel51.com/docs/fiftyone/tutorials/index.html"">Tutorials</a> â€¢
<a href=""https://github.com/voxel51/fiftyone-examples"">Examples</a> â€¢
<a href=""https://voxel51.com/blog/"">Blog</a> â€¢
<a href=""https://slack.voxel51.com"">Community</a>

[![PyPI python](https://img.shields.io/pypi/pyversions/fiftyone"
CodeGeeX,"<img src=""resources/logo/codegeex_logo.png"">

<p align=""center"">
    ğŸ  <a href=""https://codegeex.cn"" target=""_blank"">Homepage</a> | ğŸ“– <a href=""https://models.aminer.cn/codegeex/blog/"" target=""_blank"">Blog</a> | ğŸª§ <a href=""https://models.aminer.cn/codegeex/playground"" target=""_blank"">DEMO</a> | ğŸ¤– <a href=""https://codegeex.cn/download/request"" target=""_blank"">Download Model</a> | ğŸ“„ <a href=""https://arxiv.org/abs/2303.17568"" target=""_blank"">Paper</a> | ğŸŒ <a href=""README_zh.md"" target=""_blank"">ä¸­æ–‡</a>
</p>
<p align=""center"">
    ğŸ›  <a href=""https://marketplace.visualstudio.com/items?itemName=aminer.codegeex"" target=""_blank"">VS Code</a>, <a href=""https://plugins.jetbrains.com/plugin/20587-codegeex"" target=""_blank"">Jetbrains</a>, <a href=""https://plugins.jetbrains.com/plugin/20587-codegeex"" target=""_blank"">Cloud Studio</a> supported | ğŸ‘‹ Join our <a href=""https://discord.gg/8gjHdkmAN6"" target=""_blank"">Discord</a>, <a href=""https://join.slack.com/t/codegeexworkspace/shared_invite/zt-1s118ffrp-mp"
healthchecks,"# Healthchecks

[![Tests](https://github.com/healthchecks/healthchecks/actions/workflows/tests.yml/badge.svg)](https://github.com/healthchecks/healthchecks/actions/workflows/tests.yml)
[![Coverage Status](https://coveralls.io/repos/healthchecks/healthchecks/badge.svg?branch=master&service=github)](https://coveralls.io/github/healthchecks/healthchecks?branch=master)

Healthchecks is a cron job monitoring service. It listens for HTTP requests
and email messages (""pings"") from your cron jobs and scheduled tasks (""checks"").
When a ping does not arrive on time, Healthchecks sends out alerts.

Healthchecks comes with a web dashboard, API, 25+ integrations for
delivering notifications, monthly email reports, WebAuthn 2FA support,
team management features: projects, team members, read-only access.

The building blocks are:

* Python 3.10+
* Django 5.1
* PostgreSQL or MySQL

Healthchecks is licensed under the BSD 3-clause license.

Healthchecks is available as a hosted service
at [https://healt"
FastUI,"# FastUI

Find the documentation [here](https://docs.pydantic.dev/fastui/).
Join the discussion in the #fastui slack channel [here](https://pydanticlogfire.slack.com/archives/C0720M7D31S)

[![CI](https://github.com/pydantic/FastUI/actions/workflows/ci.yml/badge.svg)](https://github.com/pydantic/FastUI/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![pypi](https://img.shields.io/pypi/v/fastui.svg)](https://pypi.python.org/pypi/fastui)
[![versions](https://img.shields.io/pypi/pyversions/fastui.svg)](https://github.com/pydantic/FastUI)
[![license](https://img.shields.io/github/license/pydantic/FastUI.svg)](https://github.com/pydantic/FastUI/blob/main/LICENSE)

**Please note:** FastUI is still an active work in progress, do not expect it to be complete.

## The Principle (short version)

You can see a simple demo of an application built with FastUI [here](https://fastui-demo.onrender.com).

FastUI is a new way to build web application user interfaces defined by declarative Python"
QUANTAXIS,"# QUANTAXIS 2.0.0

[![Github workers](https://img.shields.io/github/watchers/quantaxis/quantaxis.svg?style=social&label=Watchers&)](https://github.com/quantaxis/quantaxis/watchers)
[![GitHub stars](https://img.shields.io/github/stars/quantaxis/quantaxis.svg?style=social&label=Star&)](https://github.com/quantaxis/quantaxis/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/quantaxis/quantaxis.svg?style=social&label=Fork&)](https://github.com/quantaxis/quantaxis/fork)

[ç‚¹å‡»å³ä¸Šè§’Starå’ŒWatchæ¥è·Ÿè¸ªé¡¹ç›®è¿›å±•! ç‚¹å‡»Forkæ¥åˆ›å»ºå±äºä½ çš„QUANTAXIS!]

![QUANTAXIS_LOGO_LAST_small.jpg](http://picx.gulizhu.com/Fn0TPEcwu_uhraf58_93Ul5yfvAz)

![gvp](http://picx.gulizhu.com/gvp.jpg)


æ›´å¤šæ–‡æ¡£åœ¨[QABook Release](https://github.com/QUANTAXIS/QUANTAXIS/releases/download/latest/quantaxis.pdf)

Quantitative Financial FrameWork

æœ¬é¡¹ç›®åˆ†ä¸ºå‡ ä¸ªå¤§å—:


1. QASU/ QAFetch æ”¯æŒå¤šå¸‚åœºæ•°æ®å­˜å‚¨/ è‡ªåŠ¨è¿ç»´/ æ•°æ®è·å–(mongodb/ clickhouse)

2. QAUtil æ”¯æŒäº¤æ˜“æ—¶é—´, äº¤æ˜“æ—¥å†, æ—¶é—´å‘å‰å‘åæ¨ç®—, å¸‚åœºè¯†åˆ«, dataframe æ•°æ®è½¬æ¢ç­‰

3. QIFI/ QAMarket ä¸€å¥—ç»Ÿä¸€çš„å¤šå¸‚åœº å¤šè¯­è¨€è´¦æˆ·ä½“ç³»
    - qifiaccount qifi çš„æ ‡å‡†è´¦æˆ·ä½“ç³»,"
py-faster-rcnn,"# py-faster-rcnn has been deprecated. Please see [Detectron](https://github.com/facebookresearch/Detectron), which includes an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870).

### Disclaimer

The official Faster R-CNN code (written in MATLAB) is available [here](https://github.com/ShaoqingRen/faster_rcnn).
If your goal is to reproduce the results in our NIPS 2015 paper, please use the [official code](https://github.com/ShaoqingRen/faster_rcnn).

This repository contains a Python *reimplementation* of the MATLAB code.
This Python implementation is built on a fork of [Fast R-CNN](https://github.com/rbgirshick/fast-rcnn).
There are slight differences between the two implementations.
In particular, this Python port
 - is ~10% slower at test-time, because some operations execute on the CPU in Python layers (e.g., 220ms / image vs. 200ms / image for VGG16)
 - gives similar, but not exactly the same, mAP as the MATLAB version
 - is *not compatible* with models trained using "
ctf-wiki,"# CTF Wiki

[![Discord](https://dcbadge.vercel.app/api/server/ekv7WDa9pq)](https://discord.gg/ekv7WDa9pq)

[ä¸­æ–‡](./README-zh_CN.md)  [English](./README.md)

Welcome to **CTF Wiki**ï¼

**CTF** (Capture The Flag) started from **DEFCON CTF**, a competitive game among computer security enthusiasts, originally hosted in 1996.

**CTF** covers a wide range of fields. Along with the evolving security technology, the difficulty of **CTF** challenges is getting harder and harder. As a result, the learning curve for beginners is getting steeper. Most online information is scattered and trivial. Beginners often don't know how to systematically learn **CTF**, which requires a lot of work and effort.

In order to let those people who are interested in **CTF**s start easily, in October 2016, **CTF Wiki** was established on Github. Along with gradually improved content over time, **CTF Wiki** has received lots of appreciation from security enthusiasts, many of those are guys that we think we would never"
server,"<!--
# Copyright 2018-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# P"
homelab,"# Khue's Homelab

**[Features](#features) â€¢ [Get Started](#get-started) â€¢ [Documentation](https://homelab.khuedoan.com)**

[![tag](https://img.shields.io/github/v/tag/khuedoan/homelab?style=flat-square&logo=semver&logoColor=white)](https://github.com/khuedoan/homelab/tags)
[![document](https://img.shields.io/website?label=document&logo=gitbook&logoColor=white&style=flat-square&url=https%3A%2F%2Fhomelab.khuedoan.com)](https://homelab.khuedoan.com)
[![license](https://img.shields.io/github/license/khuedoan/homelab?style=flat-square&logo=gnu&logoColor=white)](https://www.gnu.org/licenses/gpl-3.0.html)
[![stars](https://img.shields.io/github/stars/khuedoan/homelab?logo=github&logoColor=white&color=gold&style=flat-square)](https://github.com/khuedoan/homelab)

This project utilizes [Infrastructure as Code](https://en.wikipedia.org/wiki/Infrastructure_as_code) and [GitOps](https://www.weave.works/technologies/gitops) to automate provisioning, operating, and updating self-hosted services in m"
pypdf,"[![PyPI version](https://badge.fury.io/py/pypdf.svg)](https://badge.fury.io/py/pypdf)
[![Python Support](https://img.shields.io/pypi/pyversions/pypdf.svg)](https://pypi.org/project/pypdf/)
[![](https://img.shields.io/badge/-documentation-green)](https://pypdf.readthedocs.io/en/stable/)
[![GitHub last commit](https://img.shields.io/github/last-commit/py-pdf/pypdf)](https://github.com/py-pdf/pypdf)
[![codecov](https://codecov.io/gh/py-pdf/pypdf/branch/main/graph/badge.svg?token=id42cGNZ5Z)](https://codecov.io/gh/py-pdf/pypdf)

# pypdf

pypdf is a free and open-source pure-python PDF library capable of splitting,
[merging](https://pypdf.readthedocs.io/en/stable/user/merging-pdfs.html),
[cropping, and transforming](https://pypdf.readthedocs.io/en/stable/user/cropping-and-transforming.html)
the pages of PDF files. It can also add
custom data, viewing options, and
[passwords](https://pypdf.readthedocs.io/en/stable/user/encryption-decryption.html)
to PDF files. pypdf can
[retrieve text](https"
iOS-DeviceSupport,"# iOS-DeviceSupport

This repository holds the device support files for the iOS, and I will update it regularly.

## Usage

See docs: [https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/](https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/)

Below command will try to unzip all new device support files to `/Applications/Xcode.app`.

```sh
sudo ./deploy.py
```

You can use `-t` if your Xcode is not in `/Applications/` or has different name.

```sh
sudo ./deploy.py -t /Applications/Xcode\ 9.app
```

```sh
./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this help message and exit
  -t TARGET   The path for Xcode
```

## Supported versions

1. iOS8
   * 8.0 `2017/04/07`
   * 8.1 `2017/04/07`
   * 8.2 `2017/04/07`
   * 8.3 `2017/04/07`
   * 8.4 `2017/04/07`
2. iOS9
   * 9.0 `2017/04/07`
   * 9.1 `2017/04/07`
   * 9.2 `2017/04/07`
   * 9.3 `2017/04/07`
3. iOS10
   * 10.0 (14A345) `2017/04/07`
   * 10.0 `2"
yewtube,"
![](https://img.shields.io/pypi/v/yewtube.svg)  ![](https://img.shields.io/pypi/wheel/yewtube.svg)

## STOP GENOCIDE OF INNOCENT PEOPLE

![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/1280px-Flag_of_Palestine.svg.png)

<pre>
                      _         _          
                     | |       | |         
  _   _  _____      _| |_ _   _| |__   ___ 
 | | | |/ _ \ \ /\ / / __| | | | '_ \ / _ \
 | |_| |  __/\ V  V /| |_| |_| | |_) |  __/
  \__, |\___| \_/\_/  \__|\__,_|_.__/ \___|
   __/ |                                   
  |___/


</pre>

yewtube, forked from mps-youtube , is a Terminal based YouTube player and downloader. No Youtube API key required. <br>
Visit [this](./COLLABORATORS.md) page if you want to support maintainers of this project.

Installation
-----------
# Stable Version

### Using pip
1. Install using `pip install yewtube`
2. Run using, `yt`. Enjoy! 

### Using pipx (Recommended)
1.  Install **_pipx_** using `pip install pipx"
EdgeGPT,"> # This project has been archived. Due to personal circumstances, I lack the time to maintain this repository.

<div align=""center"">
  <img src=""https://socialify.git.ci/acheong08/EdgeGPT/image?font=Inter&language=1&logo=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2F9%2F9c%2FBing_Fluent_Logo.svg&owner=1&pattern=Floating%20Cogs&theme=Auto"" alt=""EdgeGPT"" width=""640"" height=""320"" />

# Edge GPT

_The reverse engineering the chat feature of the new version of Bing_

<a>English</a> -
<a href=""./README_zh-cn.md"">ç®€ä½“ä¸­æ–‡</a> -
<a href=""./README_zh-tw.md"">ç¹é«”ä¸­æ–‡</a> -
<a href=""./README_es.md"">EspaÃ±ol</a> -
<a href=""./README_ja.md"">æ—¥æœ¬èª</a>

</div>

<p align=""center"">
  <a href=""https://github.com/acheong08/EdgeGPT"">
    <img alt=""PyPI version"" src=""https://img.shields.io/pypi/v/EdgeGPT"">
  </a>
  <img alt=""Python version"" src=""https://img.shields.io/badge/python-3.8+-blue.svg"">
  <img alt=""Total downloads"" src=""https://static.pepy.tech/badge/edgegpt"">

</p>

<details open>

<summary>

"
metaflow,"![Metaflow_Logo_Horizontal_FullColor_Ribbon_Dark_RGB](https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png)

# Metaflow

Metaflow is a human-friendly library that helps scientists and engineers build and manage real-life data science projects. Metaflow was [originally developed at Netflix](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) to boost productivity of data scientists who work on a wide variety of projects from classical statistics to state-of-the-art deep learning.

For more information, see [Metaflow's website](https://metaflow.org) and [documentation](https://docs.metaflow.org).

## From prototype to production (and back)

Metaflow provides a simple, friendly API that covers foundational needs of ML, AI, and data science projects:
<img src=""./docs/prototype-to-prod.png"" width=""800px"">

1. [Rapid local prototyping](https://docs.metaflow.org/metaflow/basics), [support for "
graphene,"# ![Graphene Logo](http://graphene-python.org/favicon.png) [Graphene](http://graphene-python.org)  [![PyPI version](https://badge.fury.io/py/graphene.svg)](https://badge.fury.io/py/graphene) [![Coverage Status](https://coveralls.io/repos/graphql-python/graphene/badge.svg?branch=master&service=github)](https://coveralls.io/github/graphql-python/graphene?branch=master) [![](https://dcbadge.vercel.app/api/server/T6Gp6NFYHe?style=flat)](https://discord.gg/T6Gp6NFYHe)

[ğŸ’¬ Join the community on Discord](https://discord.gg/T6Gp6NFYHe)

**We are looking for contributors**! Please check the current issues to see how you can help â¤ï¸

## Introduction

[Graphene](http://graphene-python.org) is an opinionated Python library for building GraphQL schemas/types fast and easily.

- **Easy to use:** Graphene helps you use GraphQL in Python without effort.
- **Relay:** Graphene has builtin support for Relay.
- **Data agnostic:** Graphene supports any kind of data source: SQL (Django, SQLAlchemy), Mongo, "
deeplake,"<img src=""https://static.scarf.sh/a.png?x-pxid=bc3c57b0-9a65-49fe-b8ea-f711c4d35b82"" /><p align=""center"">
     <img src=""https://i.postimg.cc/rsjcWc3S/deeplake-logo.png"" width=""400""/>
</h1>

</br>

<h1 align=""center"">Deep Lake: Database for AI</h1>

<p align=""center"">
    <a href=""https://github.com/activeloopai/deeplake/actions/workflows/test-pr-on-label.yml""><img src=""https://github.com/activeloopai/deeplake/actions/workflows/test-push.yml/badge.svg"" alt=""PyPI version"" height=""18""></a>
    <a href=""https://pypi.org/project/deeplake/""><img src=""https://badge.fury.io/py/deeplake.svg"" alt=""PyPI version"" height=""18""></a>
    <a href='https://docs.deeplake.ai/en/latest/?badge=latest'>
     <img src='https://readthedocs.org/projects/deep-lake/badge/?version=latest' alt='Documentation Status' />
     </a>
    <a href=""https://pepy.tech/project/deeplake""><img src=""https://static.pepy.tech/badge/deeplake"" alt=""PyPI version"" height=""18""></a>
     <a href=""https://github.com/activeloopai/deepla"
trape,"trape (stable) v2.0
========

People tracker on the Internet: Learn to track the world, to avoid being traced.

---
Trape is an **OSINT** analysis and research tool, which allows people to track and execute intelligent **social engineering** attacks in real time. It was created with the aim of teaching the world how large Internet companies could obtain **confidential information** such as the status of sessions of their websites or services and control their users through their browser, without their knowledge, but It evolves with the aim of helping **government** organizations, companies and **researchers** to track the cybercriminals.

![--trape header](https://i.imgur.com/2ycpXEj.png)


At the beginning of the year 2018 was presented at **BlackHat Arsenal in Singapore**: https://www.blackhat.com/asia-18/arsenal.html#jose-pino and in multiple security events worldwide.

Some benefits
-----------
* **LOCATOR OPTIMIZATION:** Trace the path between you and the target you're tracking. E"
django-debug-toolbar,"=====================================
Django Debug Toolbar |latest-version|
=====================================

|jazzband| |build-status| |coverage| |docs| |python-support| |django-support|

.. |latest-version| image:: https://img.shields.io/pypi/v/django-debug-toolbar.svg
   :target: https://pypi.org/project/django-debug-toolbar/
   :alt: Latest version on PyPI

.. |jazzband| image:: https://jazzband.co/static/img/badge.svg
   :target: https://jazzband.co/
   :alt: Jazzband

.. |build-status| image:: https://github.com/jazzband/django-debug-toolbar/workflows/Test/badge.svg
   :target: https://github.com/jazzband/django-debug-toolbar/actions
   :alt: Build Status

.. |coverage| image:: https://img.shields.io/badge/Coverage-94%25-green
   :target: https://github.com/jazzband/django-debug-toolbar/actions/workflows/test.yml?query=branch%3Amain
   :alt: Test coverage status

.. |docs| image:: https://img.shields.io/readthedocs/django-debug-toolbar/latest.svg
   :target: https://readthed"
trax,"# Trax &mdash; Deep Learning with Clear Code and Speed

![train tracks](https://images.pexels.com/photos/461772/pexels-photo-461772.jpeg?dl&fit=crop&crop=entropy&w=32&h=21)
[![PyPI
version](https://badge.fury.io/py/trax.svg)](https://badge.fury.io/py/trax)
[![GitHub
Issues](https://img.shields.io/github/issues/google/trax.svg)](https://github.com/google/trax/issues)
![GitHub Build](https://github.com/google/trax/actions/workflows/build.yaml/badge.svg)
[![Contributions
welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)
[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/trax-ml/community)

[Trax](https://trax-ml.readthedocs.io/en/latest/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https://resear"
XAgent,"<div align= ""center"">
    <h1> <img src=""assets/readme/xagent_logo.png"" height=40 align=""texttop"">XAgent</h1>
</div>

<div align=""center"">

[![Twitter](https://img.shields.io/twitter/follow/XAgent?style=social)](https://twitter.com/XAgentTeam) [![Discord](https://img.shields.io/badge/XAgent-Discord-purple?style=flat)](https://discord.gg/zncs5aQkWZ) [![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://opensource.org/license/apache-2-0/) ![Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)

</div>

<p align=""center"">
    <a>English</a> â€¢
    <a href=""README_ZH.md"">ä¸­æ–‡</a> â€¢
    <a href=""README_JA.md"">æ—¥æœ¬èª</a>
</p>

<p align=""center"">
  <a href=""#quickstart"">Tutorial</a> â€¢
  <a href=""https://www.youtube.com/watch?v=QGkpd-tsFPA"">Demo</a> â€¢
  <a href=""https://blog.x-agent.net/blog/xagent/"">Blog</a> â€¢
  <a href=""https://xagent-doc.readthedocs.io/en/latest/"">Documentation</a> â€¢
  <a href=""#Citation"">Citation</a>
</p>


"
gitfiti,"[![Build Status](https://travis-ci.org/gelstudios/gitfiti.svg?branch=master)](https://travis-ci.org/gelstudios/gitfiti)

**gitfiti** _noun_ : Carefully crafted graffiti in a github commit history calendar.  

An example of gitfiti in the wild:  
![screenshot of gitfiti](https://raw.github.com/gelstudios/gitfiti/master/gitfiti-screenshot.png ""screenshot"")

`gitfiti.py` is a tool to decorate your github account's commit history calendar by (blatantly) abusing git's ability to accept commits _in the past_.

How? `gitfiti.py` generates a script (powershell or bash) that makes commits with the GIT_AUTHOR_DATE and GIT_COMMITTER_DATE environment variables set for each targeted pixel.

Since this is likely to clobber repo's history, it is highly recommend that you create a _new_ github repo when using gitfiti. Also, the generated script assumes you are using public-key authentication with git.

### Pixel Art

![pixel art examples](https://raw.github.com/gelstudios/gitfiti/master/pixels-large.p"
pyTelegramBotAPI,"
[![PyPi Package Version](https://img.shields.io/pypi/v/pyTelegramBotAPI.svg)](https://pypi.python.org/pypi/pyTelegramBotAPI)
[![Supported Python versions](https://img.shields.io/pypi/pyversions/pyTelegramBotAPI.svg)](https://pypi.python.org/pypi/pyTelegramBotAPI)
[![Documentation Status](https://readthedocs.org/projects/pytba/badge/?version=latest)](https://pytba.readthedocs.io/en/latest/?badge=latest)
[![PyPi downloads](https://img.shields.io/pypi/dm/pyTelegramBotAPI.svg)](https://pypi.org/project/pyTelegramBotAPI/)
[![PyPi status](https://img.shields.io/pypi/status/pytelegrambotapi.svg?style=flat-square)](https://pypi.python.org/pypi/pytelegrambotapi)

# <p align=""center"">pyTelegramBotAPI

<p align=""center"">A simple, but extensible Python implementation for the <a href=""https://core.telegram.org/bots/api"">Telegram Bot API</a>.</p>
<p align=""center"">Both synchronous and asynchronous.</p>

## <p align=""center"">Supported Bot API version: <a href=""https://core.telegram.org/bots/api#sept"
mopidy,"******
Mopidy
******

`Mopidy`_ is an extensible music server written in Python.

Mopidy plays music from local disk, Spotify, SoundCloud, Google Play Music, and
more. You edit the playlist from any phone, tablet, or computer using a variety
of MPD and web clients.

**Stream music from the cloud**

Vanilla Mopidy only plays music from files and radio streams.  Through
`extensions`_, Mopidy can play music from cloud services like Spotify,
SoundCloud, and Google Play Music.
With Mopidy's extension support, backends for new music sources can be easily
added.

**Mopidy is just a server**

Mopidy is a Python application that runs in a terminal or in the background on
Linux computers or Macs that have network connectivity and audio output.
Out of the box, Mopidy is an HTTP server. If you install the `Mopidy-MPD`_
extension, it becomes an MPD server too. Many additional frontends for
controlling Mopidy are available as extensions.

**Pick your favorite client**

You and the people around you "
catboost,"<img src=http://storage.mds.yandex.net/get-devtools-opensource/250854/catboost-logo.png width=300/>

[Website](https://catboost.ai) |
[Documentation](https://catboost.ai/docs/) |
[Tutorials](https://catboost.ai/docs/concepts/tutorials.html) |
[Installation](https://catboost.ai/docs/concepts/installation.html) |
[Release Notes](https://github.com/catboost/catboost/releases)

[![GitHub license](https://img.shields.io/github/license/catboost/catboost.svg)](https://github.com/catboost/catboost/blob/master/LICENSE)
[![PyPI version](https://badge.fury.io/py/catboost.svg)](https://badge.fury.io/py/catboost)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/catboost.svg)](https://anaconda.org/conda-forge/catboost)
[![GitHub issues](https://img.shields.io/github/issues/catboost/catboost.svg)](https://github.com/catboost/catboost/issues)
[![Telegram](https://img.shields.io/badge/chat-on%20Telegram-2ba2d9.svg)](https://t.me/catboost_en)
[![Twitter](https://img.shields.io/badge/@CatBoo"
imagen-pytorch,"<img src=""./imagen.png"" width=""450px""></img>

## Imagen - Pytorch

Implementation of <a href=""https://gweb-research-imagen.appspot.com/"">Imagen</a>, Google's Text-to-Image Neural Network that beats DALL-E2, in Pytorch. It is the new SOTA for text-to-image synthesis.

Architecturally, it is actually much simpler than DALL-E2. It consists of a cascading DDPM conditioned on text embeddings from a large pretrained T5 model (attention network). It also contains dynamic clipping for improved classifier free guidance, noise level conditioning, and a memory efficient unet design.

It appears neither CLIP nor prior network is needed after all. And so research continues.

<a href=""https://www.youtube.com/watch?v=xqDeAz0U-R4"">AI Coffee Break with Letitia</a> | <a href=""https://www.assemblyai.com/blog/how-imagen-actually-works/"">Assembly AI</a> | <a href=""https://www.youtube.com/watch?v=af6WPqvzjjk"">Yannic Kilcher</a>

Please join <a href=""https://discord.gg/xBPBXfcFHd""><img alt=""Join us on Discor"
mmsegmentation,"<div align=""center"">
  <img src=""resources/mmseg-logo.png"" width=""600""/>
  <div>&nbsp;</div>
  <div align=""center"">
    <b><font size=""5"">OpenMMLab website</font></b>
    <sup>
      <a href=""https://openmmlab.com"">
        <i><font size=""4"">HOT</font></i>
      </a>
    </sup>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <b><font size=""5"">OpenMMLab platform</font></b>
    <sup>
      <a href=""https://platform.openmmlab.com"">
        <i><font size=""4"">TRY IT OUT</font></i>
      </a>
    </sup>
  </div>
  <div>&nbsp;</div>

[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mmsegmentation)](https://pypi.org/project/mmsegmentation/)
[![PyPI](https://img.shields.io/pypi/v/mmsegmentation)](https://pypi.org/project/mmsegmentation)
[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmsegmentation.readthedocs.io/en/latest/)
[![badge](https://github.com/open-mmlab/mmsegmentation/workflows/build/badge.svg)](https://github.com/open-mmlab/mmsegmentation/actions)
[![codecov](https"
pysc2,"<div align=""center"">
  <a href=""https://www.youtube.com/watch?v=-fKUyT14G-8""
     target=""_blank"">
    <img src=""http://img.youtube.com/vi/-fKUyT14G-8/0.jpg""
         alt=""DeepMind open source PySC2 toolset for Starcraft II""
         width=""240"" height=""180"" border=""10"" />
  </a>
  <a href=""https://www.youtube.com/watch?v=6L448yg0Sm0""
     target=""_blank"">
    <img src=""http://img.youtube.com/vi/6L448yg0Sm0/0.jpg""
         alt=""StarCraft II 'mini games' for AI research""
         width=""240"" height=""180"" border=""10"" />
  </a>
  <a href=""https://www.youtube.com/watch?v=WEOzide5XFc""
     target=""_blank"">
    <img src=""http://img.youtube.com/vi/WEOzide5XFc/0.jpg""
         alt=""Trained and untrained agents play StarCraft II 'mini-game'""
         width=""240"" height=""180"" border=""10"" />
  </a>
</div>

# PySC2 - StarCraft II Learning Environment

[PySC2](https://github.com/deepmind/pysc2) is [DeepMind](http://deepmind.com)'s
Python component of the StarCraft II Learning Environment (SC2LE). It"
denoising-diffusion-pytorch,"<img src=""./images/denoising-diffusion.png"" width=""500px""></img>

## Denoising Diffusion Probabilistic Model, in Pytorch

Implementation of <a href=""https://arxiv.org/abs/2006.11239"">Denoising Diffusion Probabilistic Model</a> in Pytorch. It is a new approach to generative modeling that may <a href=""https://ajolicoeur.wordpress.com/the-new-contender-to-gans-score-matching-with-langevin-sampling/"">have the potential</a> to rival GANs. It uses denoising score matching to estimate the gradient of the data distribution, followed by Langevin sampling to sample from the true distribution.

This implementation was inspired by the official Tensorflow version <a href=""https://github.com/hojonathanho/diffusion"">here</a>

Youtube AI Educators - <a href=""https://www.youtube.com/watch?v=W-O7AZNzbzQ"">Yannic Kilcher</a> | <a href=""https://www.youtube.com/watch?v=344w5h24-h8"">AI Coffeebreak with Letitia</a> | <a href=""https://www.youtube.com/watch?v=HoKDTa5jHvg"">Outlier</a>

<a href=""https://github.co"
readthedocs.org,"Welcome to Read the Docs
========================

|build-status| |docs| |coverage|

Purpose
-------

`Read the Docs`_ hosts documentation for the open source community.
It supports many documentation tools
(e.g. Sphinx_ docs written with reStructuredText_, MkDocs_ docs written with markdown_, among others),
and can pull Git_ repositories.
Then we build documentation and host it for you.
Think of it as *Continuous Documentation*, or Docs as Code.

.. _Read the docs: https://readthedocs.org/
.. _Sphinx: http://www.sphinx-doc.org/
.. _reStructuredText: http://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html
.. _Git: http://git-scm.com/
.. _MkDocs: https://www.mkdocs.org/
.. _markdown: https://daringfireball.net/projects/markdown/

Documentation for Read the Docs
-------------------------------

You will find complete documentation for setting up your project at `the Read the Docs site`_.

.. _the Read the Docs site: https://docs.readthedocs.io/

Get in touch
------------

"
elastalert,"**ElastAlert is no longer maintained. Please use [ElastAlert2](https://github.com/jertel/elastalert2) instead.**


[![Build Status](https://travis-ci.org/Yelp/elastalert.svg)](https://travis-ci.org/Yelp/elastalert)
[![Join the chat at https://gitter.im/Yelp/elastalert](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Yelp/elastalert?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

## ElastAlert - [Read the Docs](http://elastalert.readthedocs.org).
### Easy & Flexible Alerting With Elasticsearch

ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.

ElastAlert works with all versions of Elasticsearch.

At Yelp, we use Elasticsearch, Logstash and Kibana for managing our ever increasing amount of data and logs.
Kibana is great for visualizing and querying data, but we quickly realized that it needed a companion tool for alerting
on inconsistencies in our data. Out of this need, "
hummingbot,"![Hummingbot](https://i.ibb.co/X5zNkKw/blacklogo-with-text.png)

----
[![License](https://img.shields.io/badge/License-Apache%202.0-informational.svg)](https://github.com/hummingbot/hummingbot/blob/master/LICENSE)
[![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/_hummingbot?style=social&label=_hummingbot)](https://twitter.com/_hummingbot)
[![Youtube](https://img.shields.io/youtube/channel/subscribers/UCxzzdEnDRbylLMWmaMjywOA)](https://www.youtube.com/@hummingbot)
[![Discord](https://img.shields.io/discord/530578568154054663?logo=discord&logoColor=white&style=flat-square)](https://discord.gg/hummingbot)

Hummingbot is an open source  framework that helps you build automated trading strategies, or **bots** that run on cryptocurrency exchanges.

This code is free and publicly available under the Apache 2.0 open source license!

## Why Hummingbot?

* **Both CEX and DEX connectors**: Hummingbot supports connectors to centralized exchanges like Binance and KuCoin, as we"
pysheeet,"
.. raw:: html

    <h1 align=""center"">
    <br>
      <a href=""https://www.pythonsheets.com""><img src=""docs/_static/logo.svg"" alt=""pysheeet"" width=200""></a>
    </h1>
    <p align=""center"">
      <a href=""https://github.com/crazyguitar/pysheeet/actions"">
        <img src=""https://github.com/crazyguitar/pysheeet/actions/workflows/pythonpackage.yml/badge.svg"" alt=""Build Status"">
      </a>
      <a href=""https://coveralls.io/github/crazyguitar/pysheeet?branch=master"">
        <img src=""https://coveralls.io/repos/github/crazyguitar/pysheeet/badge.svg?branch=master"" alt=""Coverage"">
      </a>
      <a href=""https://raw.githubusercontent.com/crazyguitar/pysheeet/master/LICENSE"">
        <img src=""https://img.shields.io/badge/License-MIT-blue.svg"" alt=""License MIT"">
      </a>
    </p>

Introduction
=============

Pysheeet was created with intention of collecting python code snippets for
reducing coding hours and making life easier and faster. Any contributions are welcome.
Please feel free"
python-small-examples,"
<div align=""center"">
<img src=""https://img.shields.io/badge/-Python-brightgreen"">
<img src=""https://img.shields.io/badge/-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-yellowgreen"">
<img src=""https://img.shields.io/badge/-%E7%AE%97%E6%B3%95-yellow"">
<img src=""https://img.shields.io/badge/-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-lightgrey"">
<a href=""https://static01.imgkr.com/temp/c6e10a16c4764dcdb32587760f6769ec.png"" width=""28%""><img src=""https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-Python%E5%B0%8F%E4%BE%8B%E5%AD%90-orange""></a>
</div>
<br>

<!-- <div align=""center"">
<img src=""https://static01.imgkr.com/temp/f379139a2c5d463799c35c1aa68911d7.png"" width=""18%""/>
</div> -->
</div>

## ä»‹ç»

å‘Šåˆ«æ¯ç‡¥ï¼Œå‘Šåˆ«æ¯ç‡¥ï¼Œè‡´åŠ›äºæ‰“é€  Python ç»å…¸å°ä¾‹å­ã€å°æ¡ˆä¾‹ã€‚ 

## License

å…è®¸æŒ‰ç…§è¦æ±‚è½¬è½½ï¼Œä½†ç¦æ­¢ç”¨äºä»»ä½•å•†ç”¨ç›®çš„ã€‚å¦‚æœè½¬è½½æœ¬åº“å°ä¾‹å­ã€å°æ¡ˆä¾‹ï¼Œè¯·å¤‡æ³¨ä¸‹æ–¹é“¾æ¥ï¼š

[Pythonå°ä¾‹å­æ‰€æœ‰æ±‡æ€»](https://ai-jupyter.com/python-small-examples/)

### æ›´å¤šæ•™ç¨‹

[AIæ¶ˆæ¯](https://ai-jupyter.com/)

[AIæ–°é—»æŠ¥é“](https://ai-jupyter.com/ai-news-all/)

[AIå¤§æ¨¡å‹](https://ai-jupyter.com/ai-llm/)

[AIå·¥å…·é›†](https://a"
AlphaPose,"
<div align=""center"">
    <img src=""docs/logo.jpg"", width=""400"">
</div>


## News!
- Nov 2022: [**AlphaPose paper**](http://arxiv.org/abs/2211.03375) is released! Checkout the paper for more details about this project.
- Sep 2022: [**Jittor** version](https://github.com/tycoer/AlphaPose_jittor) of AlphaPose is released! It achieves 1.45x speed up with resnet50 backbone on the training stage.
- July 2022: [**v0.6.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! [HybrIK](https://github.com/Jeff-sjtu/HybrIK) for 3D pose and shape estimation is supported!
- Jan 2022: [**v0.5.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Stronger whole body(face,hand,foot) keypoints! More models are availabel. Checkout [docs/MODEL_ZOO.md](docs/MODEL_ZOO.md)
- Aug 2020: [**v0.4.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Stronger tracking! Include whole body(face,hand,foot) keypoints! [Colab](https://colab.resea"
darts,"# Time Series Made Easy in Python

![darts](https://github.com/unit8co/darts/raw/master/static/images/darts-logo-trim.png ""darts"")

---
[![PyPI version](https://badge.fury.io/py/u8darts.svg)](https://badge.fury.io/py/darts)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/u8darts-all.svg)](https://anaconda.org/conda-forge/u8darts-all)
![Supported versions](https://img.shields.io/badge/python-3.8+-blue.svg)
[![Docker Image Version (latest by date)](https://img.shields.io/docker/v/unit8/darts?label=docker&sort=date)](https://hub.docker.com/r/unit8/darts)
![GitHub Release Date](https://img.shields.io/github/release-date/unit8co/darts)
![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/unit8co/darts/release.yml?branch=master)
[![Downloads](https://pepy.tech/badge/darts)](https://pepy.tech/project/darts)
[![Downloads](https://pepy.tech/badge/u8darts)](https://pepy.tech/project/u8darts)
[![codecov](https://codecov.io/gh/unit8co/darts/branch/master/gr"
docopt,"``docopt`` creates *beautiful* command-line interfaces
======================================================================

.. image:: https://travis-ci.org/docopt/docopt.svg?branch=master
    :target: https://travis-ci.org/docopt/docopt

.. image:: https://img.shields.io/pypi/v/docopt.svg
    :target: https://pypi.python.org/pypi/docopt

Video introduction to **docopt**: `PyCon UK 2012: Create *beautiful*
command-line interfaces with Python <http://youtu.be/pXhcPJK5cMc>`_

    New in version 0.6.1:

    - Fix issue `#85 <https://github.com/docopt/docopt/issues/85>`_
      which caused improper handling of ``[options]`` shortcut
      if it was present several times.

    New in version 0.6.0:

    - New argument ``options_first``, disallows interspersing options
      and arguments.  If you supply ``options_first=True`` to
      ``docopt``, it will interpret all arguments as positional
      arguments after first positional argument.

    - If option with argument could be repeated"
docker-stacks,"# Jupyter Docker Stacks

[![GitHub actions badge](https://github.com/jupyter/docker-stacks/actions/workflows/docker.yml/badge.svg)
](https://github.com/jupyter/docker-stacks/actions/workflows/docker.yml?query=branch%3Amain ""Docker images build status"")
[![Read the Docs badge](https://img.shields.io/readthedocs/jupyter-docker-stacks.svg)](https://jupyter-docker-stacks.readthedocs.io/en/latest/ ""Documentation build status"")
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/jupyter/docker-stacks/main.svg)](https://results.pre-commit.ci/latest/github/jupyter/docker-stacks/main ""pre-commit.ci build status"")
[![Discourse badge](https://img.shields.io/discourse/users.svg?color=%23f37626&server=https%3A%2F%2Fdiscourse.jupyter.org)](https://discourse.jupyter.org/ ""Jupyter Discourse Forum"")
[![Binder badge](https://static.mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyter/docker-stacks/main?urlpath=lab/tree/README.ipynb ""Launch a quay.io/jupyter/base-notebook cont"
imaginAIry,"# ImaginAIry ğŸ¤–ğŸ§ 

[![Downloads](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rOvQNs0Cmn_yU1bKWjCOHzGVDgZkaTtO?usp=sharing)
[![Downloads](https://pepy.tech/badge/imaginairy)](https://pepy.tech/project/imaginairy)
[![image](https://img.shields.io/pypi/v/imaginairy.svg)](https://pypi.org/project/imaginairy/)
[![image](https://img.shields.io/badge/license-MIT-green)](https://github.com/brycedrennan/imaginAIry/blob/master/LICENSE/)
[![Discord](https://flat.badgen.net/discord/members/FdD7ut3YjW)](https://discord.gg/FdD7ut3YjW)

AI imagined images. Pythonic generation of stable diffusion images **and videos** *!.

""just works"" on Linux and macOS(M1) (and sometimes windows).


```bash
# on macOS, make sure rust is installed first
# be sure to use Python 3.10, Python 3.11 is not supported at the moment
>> pip install imaginairy
>> imagine ""a scenic landscape"" ""a photo of a dog"" ""photo of a fruit bowl"" ""portrait photo of a freckled woman"" ""a "
binance-trade-bot,"# binance-trade-bot
> Automated cryptocurrency trading bot

![github](https://img.shields.io/github/workflow/status/edeng23/binance-trade-bot/binance-trade-bot)
![docker](https://img.shields.io/docker/pulls/edeng23/binance-trade-bot)
[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy?template=https://github.com/edeng23/binance-trade-bot)

[![Deploy to DO](https://mp-assets1.sfo2.digitaloceanspaces.com/deploy-to-do/do-btn-blue.svg)](https://cloud.digitalocean.com/apps/new?repo=https://github.com/coinbookbrasil/binance-trade-bot/tree/master&refcode=a076ff7a9a6a)


## Follow me on Twitter :)

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%400xedeng)](https://twitter.com/0xedeng)

## Why?

This project was inspired by the observation that all cryptocurrencies pretty much behave in the same way. When one spikes, they all spike, and when one takes a dive, they all do. _Pretty much_. Moreover, all co"
EfficientNet-PyTorch,"# EfficientNet PyTorch

### Quickstart

Install with `pip install efficientnet_pytorch` and load a pretrained EfficientNet with:
```python
from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0')
```

### Updates

#### Update (April 2, 2021)

The [EfficientNetV2 paper](https://arxiv.org/abs/2104.00298) has been released! I am working on implementing it as you read this :) 

About EfficientNetV2:
> EfficientNetV2 is a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. 

Here is a comparison: 
> <img src=""https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnetv2-image.png"" width=""100%"
sygil-webui,"# <center>Web-based UI for Stable Diffusion</center>

## Created by [Sygil.Dev](https://github.com/sygil-dev)

## Join us at Sygil.Dev's Discord Server [![Generic badge](https://flat.badgen.net/discord/members/ttM8Tm6wge?icon=discord)](https://discord.gg/ttM8Tm6wge)

## Installation instructions for:

- **[Windows](https://sygil-dev.github.io/sygil-webui/docs/Installation/windows-installation)**
- **[Linux](https://sygil-dev.github.io/sygil-webui/docs/Installation/linux-installation)**

### Want to ask a question or request a feature?

Come to our [Discord Server](https://discord.gg/gyXNe4NySY) or use [Discussions](https://github.com/sygil-dev/sygil-webui/discussions).

## Documentation

[Documentation is located here](https://sygil-dev.github.io/sygil-webui/)

## Want to contribute?

Check the [Contribution Guide](CONTRIBUTING.md)

[Sygil-Dev](https://github.com/Sygil-Dev) main devs:

* ![ZeroCool940711's avatar](https://avatars.githubusercontent.com/u/5977640"
pulse,"# PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
Code accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/pdf/2003.03808.pdf

## NOTE

We have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - **PULSE makes imaginary faces of people who do not exist, which should not be confused for real people.** It will **not** help identify or reconstruct the original image.

We also want to address concerns of bias in PULSE. **We have now included a new section in the [paper](https://arxiv.org/pdf/2003.03808.pdf) and an accompanying model card directly addressing this bias.**

---

![Transformation Preview](./readme_resources/014.jpeg)
![Transformation Preview](./readme_resources/034.jpeg)
![Transformation Preview](./readme_resources/094.jpeg)

Table of Contents
=================
- [PULSE: Self-Supervised Photo Upsampling via Lat"
cog,"# Cog: Containers for machine learning

Cog is an open-source tool that lets you package machine learning models in a standard, production-ready container.

You can deploy your packaged model to your own infrastructure, or to [Replicate](https://replicate.com/).

## Highlights

- ğŸ“¦ **Docker containers without the pain.** Writing your own `Dockerfile` can be a bewildering process. With Cog, you define your environment with a [simple configuration file](#how-it-works) and it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.

- ğŸ¤¬ï¸ **No more CUDA hell.** Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.

- âœ… **Define the inputs and outputs for your model with standard Python.** Then, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.

- ğŸ **Automa"
PaddleGAN,"
English | [ç®€ä½“ä¸­æ–‡](./README_cn.md)

# PaddleGAN

PaddleGAN provides developers with high-performance implementation of classic and SOTA Generative Adversarial Networks, and supports developers to quickly build, train and deploy GANs for academic, entertainment and industrial usage.

GAN-Generative Adversarial Network, was praised by ""the Father of Convolutional Networks""  **Yann LeCun (Yang Likun)**  as **[One of the most interesting ideas in the field of computer science in the past decade]**. It's the one research area in deep learning that AI researchers are most concerned about.

<div align='center'>
  <img src='./docs/imgs/ppgan.jpg'>
</div>

[![License](https://img.shields.io/badge/license-Apache%202-red.svg)](LICENSE)![python version](https://img.shields.io/badge/python-3.6+-orange.svg)

## ğŸª Hot Activities

- 2021.4.15~4.22

  GAN 7 Days Course Camp: Baidu Senior Research Developers help you learn the basic and advanced GAN knowledge in 7 days!

  **Courses videos and related ma"
monoid,"<img alt=""Monoid Banner"" src=""https://github.com/andreaslarsen/monoid/raw/master/Utilities/Images/MonoidReadme.png"" />
<p align=""center"">
<a href=""#font_log""><img alt=""version"" src=""https://img.shields.io/github/tag/larsenwork/monoid.svg"" height=""20px""></a>  <a href=""#license""><img alt=""license"" src=""https://img.shields.io/badge/license-MIT%20%2B%20OFL-lightgrey.svg"" height=""20px""></a>  <a href=""http://twitter.com/larsenwork""><img alt=""twitter"" src=""https://img.shields.io/badge/updates-%40larsenwork-blue.svg"" height=""20px""/></a>
</p>
<p align=""center""><a href=""#guide"">Guide</a>Â Â Â Â Â Â Â <a href=""#liga"">Ligature Support</a>Â Â Â Â Â Â Â <a href=""#links"">Links</a>Â Â Â Â Â Â Â <a href=""#font_log"">Log</a>Â Â Â Â Â Â Â <a href=""#license"">License</a>
</p>

<a name=""guide""></a>

&nbsp;

# Guide

### Live Preview + Download

[larsenwork.com/monoid](http://larsenwork.com/monoid)

&nbsp;

### Install

Quit your editor/program. Unzip and open the folder.

**Mac + Linux (with font-viewer)**  
Select the .ttf files and d"
tensorboardX,"# tensorboardX

[![PyPI version](https://badge.fury.io/py/tensorboardX.svg)](https://badge.fury.io/py/tensorboardX)
[![Documentation Status](https://readthedocs.org/projects/tensorboardx/badge/?version=latest)](https://tensorboardx.readthedocs.io/en/latest/?badge=latest)
[![Coverage Status](https://codecov.io/gh/lanpa/tensorboardX/branch/master/graph/badge.svg)](https://codecov.io/gh/lanpa/tensorboardX/)

Write TensorBoard events with simple function call.

The current release (v2.5) is tested on anaconda3, with PyTorch 1.11.0 / torchvision 0.12 / tensorboard 2.9.0.

* Support `scalar`, `image`, `figure`, `histogram`, `audio`, `text`, `graph`, `onnx_graph`, `embedding`, `pr_curve`, `mesh`, `hyper-parameters`
  and `video` summaries.

* [FAQ](https://github.com/lanpa/tensorboardX/wiki)


## Install

`pip install tensorboardX`

or build from source:

`pip install 'git+https://github.com/lanpa/tensorboardX'`

You can optionally install [`crc32c`](https://github.com/ICRAR/crc32c) to speed "
PathPlanning,"Overview
------
This repository implements some common path planning algorithms used in robotics, including Search-based algorithms and Sampling-based algorithms. We designed animation for each algorithm to display the running process. The related papers are listed in [Papers](https://github.com/zhm-real/PathPlanning#papers).

Directory Structure
------
    .
    â””â”€â”€ Search-based Planning
        â”œâ”€â”€ Breadth-First Searching (BFS)
        â”œâ”€â”€ Depth-First Searching (DFS)
        â”œâ”€â”€ Best-First Searching
        â”œâ”€â”€ Dijkstra's
        â”œâ”€â”€ A*
        â”œâ”€â”€ Bidirectional A*
        â”œâ”€â”€ Anytime Repairing A*
        â”œâ”€â”€ Learning Real-time A* (LRTA*)
        â”œâ”€â”€ Real-time Adaptive A* (RTAA*)
        â”œâ”€â”€ Lifelong Planning A* (LPA*)
        â”œâ”€â”€ Dynamic A* (D*)
        â”œâ”€â”€ D* Lite
        â””â”€â”€ Anytime D*
    â””â”€â”€ Sampling-based Planning
        â”œâ”€â”€ RRT
        â”œâ”€â”€ RRT-Connect
        â”œâ”€â”€ Extended-RRT
        â”œâ”€â”€ Dynamic-RRT
        â”œâ”€â”€ RRT*
        â”œâ”€â”€ Informed RRT*
        â”œâ”€â”€ RRT* Smart
        â”œâ”€â”€"
reactpy,"# <img src=""https://raw.githubusercontent.com/reactive-python/reactpy/main/branding/svg/reactpy-logo-square.svg"" align=""left"" height=""45""/> ReactPy

<p>
    <a href=""https://github.com/reactive-python/reactpy/actions"">
        <img src=""https://github.com/reactive-python/reactpy/workflows/test/badge.svg?event=push"">
    </a>
    <a href=""https://pypi.org/project/reactpy/"">
        <img src=""https://img.shields.io/pypi/v/reactpy.svg?label=PyPI"">
    </a>
    <a href=""https://github.com/reactive-python/reactpy/blob/main/LICENSE"">
        <img src=""https://img.shields.io/badge/License-MIT-purple.svg"">
    </a>
    <a href=""https://reactpy.dev/"">
        <img src=""https://img.shields.io/website?down_message=offline&label=Docs&logo=read-the-docs&logoColor=white&up_message=online&url=https%3A%2F%2Freactpy.dev%2Fdocs%2Findex.html"">
    </a>
    <a href=""https://discord.gg/uNb5P4hA9X"">
        <img src=""https://img.shields.io/discord/1111078259854168116?label=Discord&logo=discord"">
    </a>
</"
paperless,"[ en | [de](README-de.md) | [el](README-el.md) ]

![Paperless](https://raw.githubusercontent.com/the-paperless-project/paperless/master/src/paperless/static/paperless/img/logo-dark.png)

> ## Important news about the future of this project
> 
> It's been more than 5 years since I started this project on a whim as an effort to try to get a handle on the massive amount of paper I was dealing with in relation to various visa applications (expat life is complicated!)  Since then, the project has *exploded* in popularity, so much so that it overwhelmed me and working on it stopped being ""fun"" and started becoming a serious source of stress.
> 
> In an effort to fix this, I created the Paperless GitHub [organisation](https://github.com/the-paperless-project), and brought on a few people to manage the issue and pull request load.  Unfortunately, that model has proven to be unworkable too.  With 23 pull requests waiting and 157 issues slowly filling up with confused/annoyed people wanting to g"
platformio-core,"PlatformIO Core
===============

.. image:: https://github.com/platformio/platformio-core/workflows/Core/badge.svg
    :target: https://docs.platformio.org/en/latest/core/index.html
    :alt:  CI Build for PlatformIO Core
.. image:: https://github.com/platformio/platformio-core/workflows/Docs/badge.svg
    :target: https://docs.platformio.org?utm_source=github&utm_medium=core
    :alt:  CI Build for Docs
.. image:: https://github.com/platformio/platformio-core/workflows/Examples/badge.svg
    :target: https://github.com/platformio/platformio-examples
    :alt:  CI Build for dev-platform examples
.. image:: https://github.com/platformio/platformio-core/workflows/Projects/badge.svg
    :target: https://docs.platformio.org/en/latest/tutorials/index.html#projects
    :alt:  CI Build for the Community Projects
.. image:: https://img.shields.io/pypi/v/platformio.svg
    :target: https://pypi.python.org/pypi/platformio/
    :alt: Latest Version
.. image:: https://img.shields.io/badge/Platform"
text_classification,"Text Classification
-------------------------------------------------------------------------
The purpose of this repository is to explore text classification methods in NLP with deep learning.

#### Update: 

Customize an NLP API in three minutes, for free: <a href='https://www.cluebenchmarks.com/clueai.html'>NLP API Demo</a>

Language Understanding Evaluation benchmark for Chinese(<a href='https://www.CLUEbenchmarks.com'>CLUE benchmark<a/>): run 10 tasks & 9 baselines with one line of code, performance comparision with details.

Releasing Pre-trained Model of <a href=""https://github.com/brightmart/albert_zh"">ALBERT_Chinese</a> Training with 30G+ Raw Chinese Corpus, xxlarge, xlarge and more, Target to match State of the Art performance in Chinese, 2019-Oct-7, During the National Day of China!
 
<a href='https://github.com/brightmart/nlp_chinese_corpus'>Large Amount of Chinese Corpus for NLP Available!</a>

Google's BERT achieved new state of art result on more than 10 tasks in NLP usi"
DrissionPage,"How to use: [Documents](https://DrissionPage.cn)

This project is mainly updated in gitee, and will be submitted to GitHub after producing a stable version.
Check out the latest developments at [gitee](https://gitee.com/g1879/DrissionPage).

# âœ¨ï¸ Overview

DrissionPage is a python-based web page automation tool.
It can control the browser, send and receive data packets, and combine the two into one.
It can take into account the convenience of browser automation and the high efficiency of requests.
It is powerful and has countless built-in user-friendly designs and convenient functions.
Its syntax is concise and elegant, the amount of code is small, and it is friendly to novices.

Your star is the greatest support for me.ğŸ’–

---

# â˜• Buy me coffee

If this project is helpful to you, why not buy the author a cup of coffee :)

![](https://drissionpage.cn/code2.jpg)

---

<a href=""https://hellogithub.com/repository/dad1ecb7fbd34898a3380f5f0948ceb6"" target=""_blank""><"
easytrader,"# easytrader

[![Package](https://img.shields.io/pypi/v/easytrader.svg)](https://pypi.python.org/pypi/easytrader)
[![Travis](https://img.shields.io/travis/shidenggui/easytrader.svg)](https://travis-ci.org/shidenggui/easytrader)
[![License](https://img.shields.io/github/license/shidenggui/easytrader.svg)](https://github.com/shidenggui/easytrader/blob/master/LICENSE)

* è¿›è¡Œè‡ªåŠ¨çš„ç¨‹åºåŒ–è‚¡ç¥¨äº¤æ˜“
* æ”¯æŒè·Ÿè¸ª `joinquant`, `ricequant` çš„æ¨¡æ‹Ÿäº¤æ˜“
* æ”¯æŒè·Ÿè¸ª é›ªçƒç»„åˆ è°ƒä»“
* æ”¯æŒé€šç”¨çš„åŒèŠ±é¡ºå®¢æˆ·ç«¯æ¨¡æ‹Ÿæ“ä½œ
* å®ç°è‡ªåŠ¨ç™»å½•
* æ”¯æŒé€šè¿‡ webserver è¿œç¨‹æ“ä½œå®¢æˆ·ç«¯
* æ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨ï¼Œæ–¹ä¾¿å…¶ä»–è¯­è¨€é€‚é…
* åŸºäº Python3.6, Winã€‚æ³¨: Linux ä»…æ”¯æŒé›ªçƒ


### å¾®ä¿¡ç¾¤ä»¥åŠå…¬ä¼—å·

æ¬¢è¿å¤§å®¶æ‰«ç å…³æ³¨å…¬ä¼—å·ã€Œé£Ÿç¯é¬¼ã€ï¼Œä¸€èµ·äº¤æµã€‚è¿›ç¾¤å¯é€šè¿‡èœå•åŠ æˆ‘å¥½å‹ï¼Œå¤‡æ³¨é‡åŒ–ã€‚

![å…¬ä¼—å·äºŒç»´ç ](https://gitee.com/shidenggui/assets/raw/master/uPic/mp-qr.png)

è‹¥äºŒç»´ç å›  Github ç½‘ç»œæ— æ³•æ‰“å¼€ï¼Œè¯·ç‚¹å‡»[å…¬ä¼—å·äºŒç»´ç ](https://gitee.com/shidenggui/assets/raw/master/uPic/mp-qr.png)ç›´æ¥æ‰“å¼€å›¾ç‰‡ã€‚

### Author

**easytrader** Â© [shidenggui](https://github.com/shidenggui), Released under the [MIT](./LICENSE) License.<br>

> Blog [@shidenggui](https://shidenggui.com) Â· Weibo [@é£Ÿç¯é¬¼](https://www.weibo.com/u/1651274491) Â· T"
pdm,"<div align=""center"">

# PDM

A modern Python package and dependency manager supporting the latest PEP standards.
[ä¸­æ–‡ç‰ˆæœ¬è¯´æ˜](README_zh.md)

![PDM logo](https://raw.githubusercontent.com/pdm-project/pdm/main/docs/assets/logo_big.png)

[![Docs](https://img.shields.io/badge/Docs-mkdocs-blue?style=for-the-badge)](https://pdm-project.org)
[![Twitter Follow](https://img.shields.io/twitter/follow/pdm_project?label=get%20updates&logo=twitter&style=for-the-badge)](https://twitter.com/pdm_project)
[![Discord](https://img.shields.io/discord/824472774965329931?label=discord&logo=discord&style=for-the-badge)](https://discord.gg/Phn8smztpv)

![Github Actions](https://github.com/pdm-project/pdm/workflows/Tests/badge.svg)
[![PyPI](https://img.shields.io/pypi/v/pdm?logo=python&logoColor=%23cccccc)](https://pypi.org/project/pdm)
[![codecov](https://codecov.io/gh/pdm-project/pdm/branch/main/graph/badge.svg?token=erZTquL5n0)](https://codecov.io/gh/pdm-project/pdm)
[![Packaging status](https://repology.org/ba"
Bert-VITS2,"<div align=""center"">

<img alt=""LOGO"" src=""https://avatars.githubusercontent.com/u/122017386"" width=""256"" height=""256"" />

# Bert-VITS2

VITS2 Backbone with multilingual bert

For quick guide, please refer to `webui_preprocess.py`.

ç®€æ˜“æ•™ç¨‹è¯·å‚è§ `webui_preprocess.py`ã€‚

## ã€é¡¹ç›®æ¨ä»‹ã€‘
# FishAudioä¸‹çš„å…¨æ–°è‡ªå›å½’TTS [Fish-Speech](https://github.com/fishaudio/fish-speech)ç°å·²å¯ç”¨ï¼Œæ•ˆæœä¸ºç›®å‰å¼€æºSOTAæ°´å‡†ï¼Œä¸”åœ¨æŒç»­ç»´æŠ¤ï¼Œæ¨èä½¿ç”¨è¯¥é¡¹ç›®ä½œä¸ºBV2/GSVçš„æ›¿ä»£ã€‚æœ¬é¡¹ç›®çŸ­æœŸå†…ä¸å†è¿›è¡Œç»´æŠ¤ã€‚
## Demo Video: https://www.bilibili.com/video/BV18E421371Q
## Tech slides Video: https://www.bilibili.com/video/BV1zJ4m1K7cj
## è¯·æ³¨æ„ï¼Œæœ¬é¡¹ç›®æ ¸å¿ƒæ€è·¯æ¥æºäº[anyvoiceai/MassTTS](https://github.com/anyvoiceai/MassTTS) ä¸€ä¸ªéå¸¸å¥½çš„ttsé¡¹ç›®
## MassTTSçš„æ¼”ç¤ºdemoä¸º[aiç‰ˆå³°å“¥é”è¯„å³°å“¥æœ¬äºº,å¹¶æ‰¾å›äº†åœ¨é‡‘ä¸‰è§’å¤±è½çš„è…°å­](https://www.bilibili.com/video/BV1w24y1c7z9)

[//]: # (## æœ¬é¡¹ç›®ä¸[PlayVoice/vits_chinese]&#40;https://github.com/PlayVoice/vits_chinese&#41; æ²¡æœ‰ä»»ä½•å…³ç³»)

[//]: # ()
[//]: # (æœ¬ä»“åº“æ¥æºäºä¹‹å‰æœ‹å‹åˆ†äº«äº†aiå³°å“¥çš„è§†é¢‘ï¼Œæœ¬äººè¢«å…¶ä¸­çš„æ•ˆæœæƒŠè‰³ï¼Œåœ¨è‡ªå·±å°è¯•MassTTSä»¥åå‘ç°fsåœ¨éŸ³è´¨æ–¹é¢ä¸vitsæœ‰ä¸€å®šå·®è·ï¼Œå¹¶ä¸”trainingçš„pipelineæ¯”vitsæ›´å¤æ‚ï¼Œå› æ­¤æŒ‰ç…§å…¶æ€è·¯å°†bert)

## æˆç†Ÿçš„æ—…è¡Œè€…/å¼€æ‹“è€…/èˆ°é•¿/åšå£«/sensei/çŒé­”äºº/å–µå–µéœ²/Våº”å½“å‚é˜…ä»£ç è‡ªå·±å­¦ä¹ å¦‚ä½•è®­ç»ƒã€‚
"
BayesianOptimization,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/bayesian-optimization/BayesianOptimization/master/docsrc/static/func.png""><br><br>
</div>

# Bayesian Optimization

![tests](https://github.com/bayesian-optimization/BayesianOptimization/actions/workflows/run_tests.yml/badge.svg)
[![docs - stable](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fraw.githubusercontent.com%2Fbayesian-optimization%2FBayesianOptimization%2Fgh-pages%2Fversions.json&query=%24%5B%3F(%40.aliases%20%26%26%20%40.aliases.indexOf('stable')%20%3E%20-1)%5D.version&prefix=stable%20(v&suffix=)&label=docs)](https://bayesian-optimization.github.io/BayesianOptimization/)
[![Codecov](https://codecov.io/github/bayesian-optimization/BayesianOptimization/badge.svg?branch=master&service=github)](https://codecov.io/github/bayesian-optimization/BayesianOptimization?branch=master)
[![Pypi](https://img.shields.io/pypi/v/bayesian-optimization.svg)](https://pypi.python.org/pypi/bayesian-optimization)
![Py"
pytorch-cnn-visualizations,"# Convolutional Neural Network Visualizations 

This repository contains a number of convolutional neural network visualization techniques implemented in PyTorch.

**Note**: I removed cv2 dependencies and moved the repository towards PIL. A few things might be broken (although I tested all methods), I would appreciate if you could create an issue if something does not work.

**Note**: The code in this repository was tested with torch version 0.4.1 and some of the functions may not work as intended in later versions. Although it shouldn't be too much of an effort to make it work, I have no plans at the moment to make the code in this repository compatible with the latest version because I'm still using 0.4.1.

## Implemented Techniques

* [Gradient visualization with vanilla backpropagation](#gradient-visualization)
* [Gradient visualization with guided backpropagation](#gradient-visualization) [1]
* [Gradient visualization with saliency maps](#gradient-visualization) [4]
* [Gradient-we"
tianshou,"<div align=""center"">
  <a href=""http://tianshou.readthedocs.io""><img width=""300px"" height=""auto"" src=""https://github.com/thu-ml/tianshou/raw/master/docs/_static/images/tianshou-logo.png""></a>
</div>

---

[![PyPI](https://img.shields.io/pypi/v/tianshou)](https://pypi.org/project/tianshou/) [![Conda](https://img.shields.io/conda/vn/conda-forge/tianshou)](https://github.com/conda-forge/tianshou-feedstock) [![Read the Docs](https://readthedocs.org/projects/tianshou/badge/?version=master)](https://tianshou.org/en/master/) [![Pytest](https://github.com/thu-ml/tianshou/actions/workflows/pytest.yml/badge.svg)](https://github.com/thu-ml/tianshou/actions) [![codecov](https://img.shields.io/codecov/c/gh/thu-ml/tianshou)](https://codecov.io/gh/thu-ml/tianshou) [![GitHub issues](https://img.shields.io/github/issues/thu-ml/tianshou)](https://github.com/thu-ml/tianshou/issues) [![GitHub stars](https://img.shields.io/github/stars/thu-ml/tianshou)](https://github.com/thu-ml/tianshou/stargazers) [![Git"
bypy,"bypy - Python client for Baidu Yun (Personal Cloud Storage) ç™¾åº¦äº‘/ç™¾åº¦ç½‘ç›˜Pythonå®¢æˆ·ç«¯
====================================================================================

[![alt text](https://img.shields.io/pypi/v/bypy.svg ""PyPi Version"")](https://pypi.python.org/pypi/bypy)
[![alt text](https://img.shields.io/pypi/dm/bypy.svg ""PyPi Downloads"")](https://pypi.python.org/pypi/bypy)
[![alt text](https://travis-ci.org/houtianze/bypy.svg ""Build status"")](https://travis-ci.org/houtianze/bypy)
[![Coverage Status](https://coveralls.io/repos/houtianze/bypy/badge.svg?branch=master&service=github)](https://coveralls.io/github/houtianze/bypy?branch=master)
[![Code Climate](https://codeclimate.com/github/houtianze/bypy/badges/gpa.svg)](https://codeclimate.com/github/houtianze/bypy)
[![Join the chat at https://gitter.im/houtianze/bypy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/houtianze/bypy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

æç®€è¯´æ˜
-------

- å®‰è£…: `p"
visidata,"# VisiData v3.0

[![Tests](https://github.com/saulpw/visidata/workflows/visidata-ci-build/badge.svg)](https://github.com/saulpw/visidata/actions/workflows/main.yml)
[![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/saulpw/visidata)

[![discord](https://img.shields.io/discord/880915750007750737?label=discord)](https://visidata.org/chat)
[![mastodon @visidata@fosstodon.org][2.1]][2]
[![twitter @VisiData][1.1]][1]

A terminal interface for exploring and arranging tabular data.

![Frequency table](http://visidata.org/freq-move-row.gif)

VisiData supports tsv, csv, sqlite, json, xlsx (Excel), hdf5, and [many other formats](https://visidata.org/formats).

## Platform requirements

- Linux, OS/X, or Windows (with WSL)
- Python 3.8+
- additional Python modules are required for certain formats and sources

## Install

To install the latest release from PyPi:

    pip3 install visidata

To install the cutting edg"
jc,"[![Tests](https://github.com/kellyjonbrazil/jc/workflows/Tests/badge.svg?branch=master)](https://github.com/kellyjonbrazil/jc/actions)
[![Pypi](https://img.shields.io/pypi/v/jc.svg)](https://pypi.org/project/jc/)

> Check out the `jc` Python [package documentation](https://github.com/kellyjonbrazil/jc/tree/master/docs) for developers

> Try the `jc` [web demo](https://jc-web.onrender.com/) and [REST API](https://github.com/kellyjonbrazil/jc-restapi)

> `jc` is available as an
[Ansible filter plugin](https://docs.ansible.com/ansible/latest/collections/community/general/jc_filter.html#ansible-collections-community-general-jc-filter)
in the `community.general` collection. See this
[blog post](https://blog.kellybrazil.com/2020/08/30/parsing-command-output-in-ansible-with-jc/)
for an example.

# JC
JSON Convert

`jc` JSONifies the output of many CLI tools, file-types, and common strings
for easier parsing in scripts. See the [**Parsers**](#parsers) section for
supported commands, file-types"
InfoSpider,"<p align=""center"">
    <img src=""https://i.loli.net/2020/10/20/SKOdFZpVYo4LvgT.png"" alt=""InfoSpider logo""/>
</p>

***

<p align=""center"">
    <a>
        <img alt=""GitHub stars"" src=""https://img.shields.io/github/stars/kangvcar/infospider?style=social"">
    </a>
    <a>
        <img src=""https://img.shields.io/badge/python-v3-blue?style=flat-square"" alt=""UW2eVx.png"" />
    </a>
    <a>
        <img src=""https://img.shields.io/badge/platform-Windows-blue?style=flat-square"" alt=""UW2eVx.png"" />
    </a>
    <a>
        <img src=""https://img.shields.io/website?up_message=%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3&url=https%3A%2F%2Finfospider.vercel.app%2F"" alt=""UW2eVx.png"" />
    </a>
    <a>
    <img alt=""GitHub repo size"" src=""https://img.shields.io/github/repo-size/kangvcar/infospider?style=flat-square"">
    </a>
    <a>
    <img alt=""GitHub repo size"" src=""https://img.shields.io/badge/license-GPL-blue?style=flat-square"">
    </a>
</p>
<p align=""center"">ä¸€ä¸ªç¥å¥‡çš„å·¥å…·ç®±ï¼Œæ‹¿å›ä½ çš„ä¸ªäººä¿¡æ¯ã€‚</p>
<p align=""center"
llama-cpp-python,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/abetlen/llama-cpp-python/main/docs/icon.svg"" style=""height: 5rem; width: 5rem"">
</p>

#  Python Bindings for [`llama.cpp`](https://github.com/ggerganov/llama.cpp)

[![Documentation Status](https://readthedocs.org/projects/llama-cpp-python/badge/?version=latest)](https://llama-cpp-python.readthedocs.io/en/latest/?badge=latest)
[![Tests](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml)
[![PyPI](https://img.shields.io/pypi/v/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)
[![PyPI - License](https://img.shields.io/pypi/l/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-cpp-python)](https://pypi.or"
dream-textures,"![Dream Textures, subtitle: Stable Diffusion built-in to Blender](docs/assets/banner.png)

[![Latest Release](https://flat.badgen.net/github/release/carson-katri/dream-textures)](https://github.com/carson-katri/dream-textures/releases/latest)
[![Join the Discord](https://flat.badgen.net/badge/icon/discord?icon=discord&label)](https://discord.gg/EmDJ8CaWZ7)
[![Total Downloads](https://img.shields.io/github/downloads/carson-katri/dream-textures/total?style=flat-square)](https://github.com/carson-katri/dream-textures/releases/latest)
[![Buy on Blender Market](https://flat.badgen.net/badge/buy/blender%20market/orange)](https://www.blendermarket.com/products/dream-textures)

* Create textures, concept art, background assets, and more with a simple text prompt
* Use the 'Seamless' option to create textures that tile perfectly with no visible seam
* Texture entire scenes with 'Project Dream Texture' and depth to image
* Re-style animations with the Cycles render pass
* Run the models on your "
jukebox,"**Status:** Archive (code is provided as-is, no updates expected)

# Jukebox
Code for ""Jukebox: A Generative Model for Music""

[Paper](https://arxiv.org/abs/2005.00341) 
[Blog](https://openai.com/blog/jukebox) 
[Explorer](http://jukebox.openai.com/) 
[Colab](https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb) 

# Install
Install the conda package manager from https://docs.conda.io/en/latest/miniconda.html    
    
``` 
# Required: Sampling
conda create --name jukebox python=3.7.5
conda activate jukebox
conda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3
conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch
git clone https://github.com/openai/jukebox.git
cd jukebox
pip install -r requirements.txt
pip install -e .

# Required: Training
conda install av=7.0.01 -c conda-forge 
pip install ./tensorboardX
 
# Optional: Apex for faster training with fused_adam
conda install pytorch=1.1 torchvision=0"
sktime,"<a href=""https://www.sktime.net""><img src=""https://github.com/sktime/sktime/blob/main/docs/source/images/sktime-logo.svg"" width=""175"" align=""right"" /></a>

# Welcome to sktime

> A unified interface for machine learning with time series

:rocket: **Version 0.33.1 out now!** [Check out the release notes here](https://www.sktime.net/en/latest/changelog.html).

sktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes time series classification, regression, clustering, annotation, and forecasting. It comes with [time series algorithms](https://www.sktime.net/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune and validate time series models.

[scikit-learn]: https://scikit-learn.org/stable/

|  | **[Documentation](https://www.sktime.net/en/stable/users.html)** Â· **[Tutorials](https://www.sktime.net/en/stable/examples.html)** Â· **[Release Notes](https://www.skti"
ASRT_SpeechRecognition,"![](assets/asrt_title_header.png)

[![GPL-3.0 Licensed](https://img.shields.io/badge/License-GPL3.0-blue.svg?style=flat)](https://opensource.org/licenses/GPL-3.0) 
[![Stars](https://img.shields.io/github/stars/nl8590687/ASRT_SpeechRecognition)](https://github.com/nl8590687/ASRT_SpeechRecognition) 
[![TensorFlow Version](https://img.shields.io/badge/Tensorflow-2.5+-blue.svg)](https://www.tensorflow.org/) 
[![Python Version](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/) 
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5808434.svg)](https://doi.org/10.5281/zenodo.5808434)

ASRTæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œå¦‚æœæ‚¨è§‰å¾—å–œæ¬¢ï¼Œè¯·ç‚¹ä¸€ä¸ª **""Star""** å§~

**ReadMe Language** | ä¸­æ–‡ç‰ˆ | [English](https://github.com/nl8590687/ASRT_SpeechRecognition/blob/master/README_EN.md) |

[**ASRTé¡¹ç›®ä¸»é¡µ**](https://asrt.ailemon.net/) | 
[**å‘å¸ƒç‰ˆä¸‹è½½**](https://wiki.ailemon.net/docs/asrt-doc/download) | 
[**æŸ¥çœ‹æœ¬é¡¹ç›®çš„Wikiæ–‡æ¡£**](https://wiki.ailemon.net/docs/asrt-doc) | 
[**å®ç”¨æ•ˆæœä½“éªŒDemo**](https://asrt.ai"
WSABuilds,"## ""Microsoft is ending support for the Windows Subsystem for Androidâ„¢ï¸ (WSA). As a result, the Amazon Appstore on Windows and all applications and games dependent on WSA will no longer be supported beginning March 5, 2025."" 
###### (Source: [GitHub](https://github.com/microsoft/WSA/discussions/536) and [Microsoft Learn](https://learn.microsoft.com/en-us/windows/android/wsa/))
---
### WSABuilds has entered LTS (Long Term Support) for WSA version 2311.40000.5.0, where the Magisk version, KernelSU version and GApps version will be kept up to date via new releases.
### This repo will not be archived and support will still be given to any users installing WSA Builds from this repo. Thank you all for using this repository and supporting my work, its been a pleasure serving this community. 

---

## Next LTS Release Date:
### WSABuilds LTS 5 (v2407.40000.0.0): 
``TBD``
### WSABuilds LTS 4 (v2407.40000.0.0): 
~~``Monday 15th July 2024``~~  **Available Now (via the Pre-release buttons in [Down"
waydroid,"# Waydroid

Waydroid uses a container-based approach to boot a full Android system on a
regular GNU/Linux system like Ubuntu.

## Overview

Waydroid uses Linux namespaces (user, pid, uts, net, mount, ipc) to run a
full Android system in a container and provide Android applications on
any GNU/Linux-based platform.

The Android system inside the container has direct access to any needed hardware.

The Android runtime environment ships with a minimal customized Android system
image based on [LineageOS](https://lineageos.org/). The image is currently based
on Android 11.

## Documentation

Our documentation site can be found at [docs.waydro.id](https://docs.waydro.id)

## Reporting bugs

If you have found an issue with Waydroid, please [file a bug](https://github.com/Waydroid/waydroid/issues/new).

## Get in Touch

If you want to get in contact with the developers please feel free to join the
*Waydroid* groups in [Matrix](https://matrix.to/#/#waydroid:matrix.org) or [Telegram](https://t.me"
mage-ai,"<h1 align=""center"">
  <a
    target=""_blank""
    href=""https://mage.ai""
  >
    <img
      align=""center""
      alt=""Mage""
      src=""https://github.com/mage-ai/assets/blob/main/mascots/mascots-shorter.jpeg?raw=true""
      style=""width:100%;""
    />
  </a>
</h1>
<p align=""center"">
  ğŸ§™ A modern replacement for Airflow.
</p>

<p align=""center"">
  <a
    href=""https://docs.mage.ai""
    target=""_blank""
  ><b>Documentation</b></a>&nbsp;&nbsp;&nbsp;ğŸŒªï¸&nbsp;&nbsp;&nbsp;
  <a
    href=""https://youtu.be/GswOdShLGmg""
    target=""_blank""
  ><b>Get a 5 min overview</b></a>&nbsp;&nbsp;&nbsp;ğŸŒŠ&nbsp;&nbsp;&nbsp;
  <a
    href=""https://demo.mage.ai""
    target=""_blank""
  ><b>Play with live tool</b></a>&nbsp;&nbsp;&nbsp;ğŸ”¥&nbsp;&nbsp;&nbsp;
  <a
    href=""https://www.mage.ai/chat""
    target=""_blank""
  >
    <b>Get instant help</b>
  </a>
</p>
<div align=""center"">
  <a
    href=""https://pypi.org/project/mage-ai/""
    target=""_blank""
  >
    <img alt=""PyPi"" src=""https://img.shields.io/pypi/v/mage-ai?colo"
accelerate,"<!---
Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<p align=""center"">
    <br>
    <img src=""https://raw.githubusercontent.com/huggingface/accelerate/main/docs/source/imgs/accelerate_logo.png"" width=""400""/>
    <br>
<p>

<p align=""center"">
    <!-- Uncomment when CircleCI is set up
    <a href=""https://circleci.com/gh/huggingface/accelerate""><img alt=""Build"" src=""https://img.shields.io/circleci/build/github/huggingface/transformers/master""></a>
    -->
"
lutris,"******
Lutris
******

|LiberaPayBadge|_ |PatreonBadge|_

Lutris helps you install and play video games from all eras and from most
gaming systems. By leveraging and combining existing emulators, engine
re-implementations and compatibility layers, it gives you a central interface
to launch all your games.

The client can connect with existing services like Humble Bundle, GOG and Steam
to make your game libraries easily available. Game downloads and installations
are automated and can be modified through user made scripts.

Running Lutris
==============

If you have not installed Lutris through your package manager and are using the
source package, it is recommended that you install lutris at least once, even an
older version to have all dependencies available.
Once all dependencies are satisfied, you can run lutris directly from the source
directory with `./bin/lutris`

If you need to run lutris through gdb to troubleshoot segmentation faults, you
can use the following command:

`gdb -e"
jupyterhub,"**[Technical Overview](#technical-overview)** |
**[Installation](#installation)** |
**[Configuration](#configuration)** |
**[Docker](#docker)** |
**[Contributing](#contributing)** |
**[License](#license)** |
**[Help and Resources](#help-and-resources)**

---

# [JupyterHub](https://github.com/jupyterhub/jupyterhub)

[![Latest PyPI version](https://img.shields.io/pypi/v/jupyterhub?logo=pypi)](https://pypi.python.org/pypi/jupyterhub)
[![Latest conda-forge version](https://img.shields.io/conda/vn/conda-forge/jupyterhub?logo=conda-forge)](https://anaconda.org/conda-forge/jupyterhub)
[![Documentation build status](https://img.shields.io/readthedocs/jupyterhub?logo=read-the-docs)](https://jupyterhub.readthedocs.org/en/latest/)
[![GitHub Workflow Status - Test](https://img.shields.io/github/workflow/status/jupyterhub/jupyterhub/Test?logo=github&label=tests)](https://github.com/jupyterhub/jupyterhub/actions)
[![Test coverage of code](https://codecov.io/gh/jupyterhub/jupyterhub/branch/main/grap"
autogluon,"

<div align=""center"">
<img src=""https://user-images.githubusercontent.com/16392542/77208906-224aa500-6aba-11ea-96bd-e81806074030.png"" width=""350"">

## Fast and Accurate ML in 3 Lines of Code

[![Latest Release](https://img.shields.io/github/v/release/autogluon/autogluon)](https://github.com/autogluon/autogluon/releases)
[![Conda Forge](https://img.shields.io/conda/vn/conda-forge/autogluon.svg)](https://anaconda.org/conda-forge/autogluon)
[![Python Versions](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue)](https://pypi.org/project/autogluon/)
[![Downloads](https://pepy.tech/badge/autogluon/month)](https://pepy.tech/project/autogluon)
[![GitHub license](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](./LICENSE)
[![Discord](https://img.shields.io/discord/1043248669505368144?logo=discord&style=flat)](https://discord.gg/wjUmjqAc2N)
[![Twitter](https://img.shields.io/twitter/follow/autogluon?style=social)](https://twitter.com/autogluon)
[![Cont"
sqlfluff,"![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)

# The SQL Linter for Humans

[![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
[![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)

[![Coveralls](https://img.shields.io/coverallsCoverage/github/sqlfluff/sqlfluff?logo=coveralls&style=flat-square)](https://coveralls.io/github/sqlfluff/sqlfluff?branch=main)
[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/sqlfluff/"
CogVideo,"# CogVideo & CogVideoX

[ä¸­æ–‡é˜…è¯»](./README_zh.md)

[æ—¥æœ¬èªã§èª­ã‚€](./README_ja.md)

<div align=""center"">
<img src=resources/logo.svg width=""50%""/>
</div>
<p align=""center"">
Experience the CogVideoX-5B model online at <a href=""https://huggingface.co/spaces/THUDM/CogVideoX-5B"" target=""_blank""> ğŸ¤— Huggingface Space</a> or <a href=""https://modelscope.cn/studios/ZhipuAI/CogVideoX-5b-demo"" target=""_blank""> ğŸ¤– ModelScope Space</a>
</p>
<p align=""center"">
ğŸ“š View the <a href=""https://arxiv.org/abs/2408.06072"" target=""_blank"">paper</a> and <a href=""https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh"" target=""_blank"">user guide</a>
</p>
<p align=""center"">
    ğŸ‘‹ Join our <a href=""resources/WECHAT.md"" target=""_blank"">WeChat</a> and <a href=""https://discord.gg/dCGfUsagrD"" target=""_blank"">Discord</a> 
</p>
<p align=""center"">
ğŸ“ Visit <a href=""https://chatglm.cn/video?lang=en?fr=osm_cogvideo"">QingYing</a> and <a href=""https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9"">API Platform</a>"
pip-tools,"[![jazzband-image]][jazzband]
[![pypi][pypi-image]][pypi]
[![pyversions][pyversions-image]][pyversions]
[![pre-commit][pre-commit-image]][pre-commit]
[![buildstatus-gha][buildstatus-gha-image]][buildstatus-gha]
[![codecov][codecov-image]][codecov]
[![Matrix Room Badge]][Matrix Room]
[![Matrix Space Badge]][Matrix Space]
[![discord-chat-image]][discord-chat]

# pip-tools = pip-compile + pip-sync

A set of command line tools to help you keep your `pip`-based packages fresh,
even when you've pinned them. You do pin them, right? (In building your Python application and its dependencies for production, you want to make sure that your builds are predictable and deterministic.)

[![pip-tools overview for phase II][pip-tools-overview]][pip-tools-overview]

## Installation

Similar to `pip`, `pip-tools` must be installed in each of your project's
[virtual environments](https://packaging.python.org/tutorials/installing-packages/#creating-virtual-environments):

```console
$ source /path/to/venv/"
google-api-python-client,"# Google API Client

[![PyPI version](https://badge.fury.io/py/google-api-python-client.svg)](https://badge.fury.io/py/google-api-python-client)

This is the [Google API Python client library](https://cloud.google.com/apis/docs/client-libraries-explained#google_api_client_libraries)
for Google's discovery based APIs. To get started, please see the
[docs folder](https://github.com/googleapis/google-api-python-client/blob/main/docs/README.md).

This library is considered complete and is in maintenance mode. This means
that we will address critical bugs and security issues but will not add any
new features.

This library is officially supported by Google.  However, the maintainers of
this repository recommend using [Cloud Client Libraries for Python](https://github.com/googleapis/google-cloud-python),
where possible, for new code development. For more information, please visit
[Client Libraries Explained](https://cloud.google.com/apis/docs/client-libraries-explained).

## Version 2.0 Rele"
TinyLlama,"<div align=""center"">

# TinyLlama-1.1B
English | [ä¸­æ–‡](README_zh-CN.md)

[Chat Demo](https://huggingface.co/spaces/TinyLlama/tinyllama-chat) | [Discord](https://discord.gg/74Wcx4j5Nb)
</div>

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of ""just"" 90 days using 16 A100-40G GPUs ğŸš€ğŸš€. The training has started on 2023-09-01. 

<div align=""center"">
  <img src="".github/TinyLlama_logo.png"" width=""300""/>
</div>

We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### News
- 2023-12-18ï¼š Add two notes [1](https://whimsical-aphid-86d.notion.site/Release-of-TinyLlama-1-5T-Checkpoints-Postponed-01b266998c1c4"
PaLM-rlhf-pytorch,"<img src=""./chatgpt.png"" width=""450px""></img>

*<a href=""https://openai.com/blog/chatgpt/"">official chatgpt blogpost</a>*

## PaLM + RLHF - Pytorch (wip)

Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I'll add retrieval functionality too, Ã  la <a href=""https://github.com/lucidrains/RETRO-pytorch"">RETRO</a>

If you are interested in replicating something like ChatGPT out in the open, please consider joining <a href=""https://discord.gg/xBPBXfcFHd"">Laion <img alt=""Join us on Discord"" src=""https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white""></a>

Potential successor: <a href=""https://arxiv.org/abs/2305.18290"">Direct Preference Optimization</a> - all the code in this repo becomes ~ binary cross entropy loss, < 5 loc. So much for Reward models and PPO

## FAQ

- Does this contain a model for inference?

There is no trained model. This is just the ship and overall map. We still need millions "
faster-rcnn.pytorch,"# A *Faster* Pytorch Implementation of Faster R-CNN

## Write at the beginning

[05/29/2020] This repo was initaited about two years ago, developed as the first open-sourced object detection code which supports multi-gpu training. It has been integrating tremendous efforts from many people. However, we have seen many high-quality repos emerged in the last years, such as:

* [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)
* [detectron2](https://github.com/facebookresearch/detectron2)
* [mmdetection](https://github.com/open-mmlab/mmdetection)

**At this point, I think this repo is out-of-data in terms of the pipeline and coding style, and will not maintain actively. Though you can still use this repo as a playground, I highly recommend you move to the above repos to delve into west world of object detection!**

## Introduction

### :boom: Good news! This repo supports pytorch-1.0 now!!! We borrowed some code and techniques from [maskrcnn-benchmark](https://gi"
SpaceshipGenerator,"# Spaceship Generator

A Blender script to procedurally generate 3D spaceships from a random seed.

![Spaceship screenshots](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/spaceships_grid.jpg)

Usage
-----
* Install Blender 2.80 or greater: http://blender.org/download/
* Download newest `add_mesh_SpaceshipGenerator.zip` from the [Releases](https://github.com/a1studmuffin/SpaceshipGenerator/releases) section
* Under Edit > Preferences... > Add-ons > Install... open the downloaded ZIP file
* Under Edit > Preferences... > Add-ons enable the ""Add Mesh: Spaceship Generator"" script (search for ""spaceship"")
* Add a spaceship in the 3D View under Add > Mesh > Spaceship
* Expand the Spaceship tab that appears in the bottom left of the viewport to adjust procedural generation settings

How it works
------------

![Step-by-step animation](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/step-by-step-animation.gif)

Watch on"
PyMySQL,"[![Documentation Status](https://readthedocs.org/projects/pymysql/badge/?version=latest)](https://pymysql.readthedocs.io/)
[![codecov](https://codecov.io/gh/PyMySQL/PyMySQL/branch/main/graph/badge.svg?token=ppEuaNXBW4)](https://codecov.io/gh/PyMySQL/PyMySQL)

# PyMySQL

This package contains a pure-Python MySQL and MariaDB client library, based on [PEP
249](https://www.python.org/dev/peps/pep-0249/).

## Requirements

- Python -- one of the following:
  - [CPython](https://www.python.org/) : 3.7 and newer
  - [PyPy](https://pypy.org/) : Latest 3.x version
- MySQL Server -- one of the following:
  - [MySQL](https://www.mysql.com/) \>= 5.7
  - [MariaDB](https://mariadb.org/) \>= 10.4

## Installation

Package is uploaded on [PyPI](https://pypi.org/project/PyMySQL).

You can install it with pip:

    $ python3 -m pip install PyMySQL

To use ""sha256_password"" or ""caching_sha2_password"" for authenticate,
you need to install additional dependency:

    $ python3 -m pip install PyMySQL[rsa]

"
GLM-130B,"<img src=""resources/7D6433A42D189E2E6FBC62BE066BCE91.png"">

<p align=""center"">
   ğŸŒ <a href=""http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/"" target=""_blank"">Blog</a> â€¢ â¬ <a href=""https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform"" target=""_blank"">Download Model</a> â€¢ ğŸª§ <a href=""https://huggingface.co/spaces/THUDM/GLM-130B"" target=""_blank"">Demo</a> â€¢ âœ‰ï¸ <a href=""mailto:glm-130b@googlegroups.com"">Email</a> â€¢ ğŸ“ƒ <a href=""https://arxiv.org/abs/2210.02414"" target=""_blank"">Paper [ICLR 2023]</a><br>
</p>

<p align=""center"">
   ğŸ’¬ <a href=""https://groups.google.com/g/glm-130b-forum"" target=""_blank"">Google Group</a> (Updates) or <a href=""https://github.com/THUDM/GLM-130B/blob/main/resources/WECHAT.md"" target=""_blank"">Wechat Group</a> or <a href=""https://join.slack.com/t/glm-130b/shared_invite/zt-1f2ih11xy-EAuDComTAr~XVB3MywE9Cg"" target=""_blank"">Slack channel</a> (Discussions)
</p>

# GLM-130B: An Open Bilingual Pre-Trained Model
"
IF,"[![License](https://img.shields.io/badge/Code_License-Modified_MIT-blue.svg)](LICENSE)
[![License](https://img.shields.io/badge/Weights_License-DeepFloyd_IF-orange.svg)](LICENSE-MODEL)
[![Downloads](https://pepy.tech/badge/deepfloyd_if)](https://pepy.tech/project/deepfloyd_if)
[![Discord](https://img.shields.io/badge/Discord-%237289DA.svg?logo=discord&logoColor=white)](https://discord.gg/umz62Mgr)
[![Twitter](https://img.shields.io/badge/Twitter-%231DA1F2.svg?logo=twitter&logoColor=white)](https://twitter.com/deepfloydai)
[![Linktree](https://img.shields.io/badge/Linktree-%2339E09B.svg?logo=linktree&logoColor=white)](http://linktr.ee/deepfloyd)

# IF by [DeepFloyd Lab](https://deepfloyd.ai) at [StabilityAI](https://stability.ai/)

<p align=""center"">
  <img src=""./pics/nabla.jpg"" width=""100%"">
</p>

We introduce DeepFloyd IF, a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. DeepFloyd IF is a modular composed of a fro"
BlenderGIS,"Blender GIS
==========
Blender minimum version required : v2.83

Note : Since 2022, the OpenTopography web service requires an API key. Please register to opentopography.org and request a key. This service is still free.


[Wiki](https://github.com/domlysz/BlenderGIS/wiki/Home) - [FAQ](https://github.com/domlysz/BlenderGIS/wiki/FAQ) - [Quick start guide](https://github.com/domlysz/BlenderGIS/wiki/Quick-start) - [Flowchart](https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/flowchart.jpg)
--------------------

## Functionalities overview

**GIS datafile import :** Import in Blender most commons GIS data format : Shapefile vector, raster image, geotiff DEM, OpenStreetMap xml.

There are a lot of possibilities to create a 3D terrain from geographic data with BlenderGIS, check the [Flowchart](https://raw.githubusercontent.com/wiki/domlysz/blenderGIS/flowchart.jpg) to have an overview.

Exemple : import vector contour lines, create faces by triangulation and put a topographic raster "
CodeGeeX2,"![](resources/codegeex_logo.png)

<p align=""center"">
    ğŸ  <a href=""https://codegeex.cn"" target=""_blank"">ä¸»é¡µ</a>ï½œğŸ›  æ’ä»¶ <a href=""https://marketplace.visualstudio.com/items?itemName=aminer.codegeex"" target=""_blank"">VS Code</a>, <a href=""https://plugins.jetbrains.com/plugin/20587-codegeex"" target=""_blank"">Jetbrains</a>ï½œğŸ¤— <a href=""https://huggingface.co/THUDM/codegeex2-6b"" target=""_blank"">æ¨¡å‹ä¸‹è½½</a>ï½œğŸ“„ <a href=""https://arxiv.org/abs/2303.17568"" target=""_blank"">è®ºæ–‡</a>ï½œğŸ‘‹ åŠ å…¥<a href=""resources/wechat.md""target=""_blank"">å¾®ä¿¡å¼€å‘è€…äº¤æµç¾¤</a>
</p>

Read this in [English](README_EN.md)<br>
[æ—¥æœ¬èª](README_JA.md)ã§èª­ã‚€<br>
Lire en [FranÃ§ais](README_FR.md)

â­ï¸ æœ€æ–°ä¸€ä»£ [CodeGeeX4](https://github.com/THUDM/CodeGeeX4) æ¨¡å‹å·²ç»æ­£å¼å¼€æºã€‚
The newest [CodeGeeX4](https://github.com/THUDM/CodeGeeX4) has been released.

# CodeGeeX2: æ›´å¼ºå¤§çš„å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹

CodeGeeX2 æ˜¯å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹ [CodeGeeX](https://github.com/THUDM/CodeGeeX) ([KDDâ€™23](https://arxiv.org/abs/2303.17568)) çš„ç¬¬äºŒä»£æ¨¡å‹ã€‚ä¸åŒäºä¸€ä»£ CodeGeeXï¼ˆå®Œå…¨åœ¨å›½äº§åä¸ºæ˜‡è…¾èŠ¯ç‰‡å¹³å°è®­ç»ƒï¼‰ ï¼ŒCodeGeeX2 æ˜¯åŸºäº [ChatGLM2](https://github.co"
stable-diffusion-webui-forge,"# Stable Diffusion WebUI Forge

Stable Diffusion WebUI Forge is a platform on top of [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) (based on [Gradio](https://www.gradio.app/) <a href='https://github.com/gradio-app/gradio'><img src='https://img.shields.io/github/stars/gradio-app/gradio'></a>) to make development easier, optimize resource management, speed up inference, and study experimental features.

The name ""Forge"" is inspired from ""Minecraft Forge"". This project is aimed at becoming SD WebUI's Forge.

Forge is currently based on SD-WebUI 1.10.1 at [this commit](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/82a973c04367123ae98bd9abdf80d9eda9b910e2). (Because original SD-WebUI is almost static now, Forge will sync with original WebUI every 90 days, or when important fixes.)

# News

2024 Sep 7: New sampler `Flux Realistic` is available now! Recommended scheduler is ""simple"".

# Quick List

[Gradio 4 UI Must Read (TLDR: You need to "
instructor,"# Instructor: Structured LLM Outputs

Instructor is a Python library that makes it a breeze to work with structured outputs from large language models (LLMs). Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses. Get ready to supercharge your LLM workflows!

[![Twitter Follow](https://img.shields.io/twitter/follow/jxnlco?style=social)](https://twitter.com/jxnlco)
[![Discord](https://img.shields.io/discord/1192334452110659664?label=discord)](https://discord.gg/bD9YE9JArw)
[![Downloads](https://img.shields.io/pypi/dm/instructor.svg)](https://pypi.python.org/pypi/instructor)

## Want your logo on our website?

If your company use instructor a lot, we'd love to have your logo on our website! Please fill out [this form](https://q7gjsgfstrp.typeform.com/to/wluQlVVQ)

## Key Features

- **Response Models**: Specify Pydantic models to define the structure of your LLM outputs
- **Retry Management**: Easily conf"
UFO,"<h1 align=""center"">
    <b>UFO</b> <img src=""./assets/ufo_blue.png"" alt=""UFO Image"" width=""40"">: A <b>U</b>I-<b>Fo</b>cused Agent for Windows OS Interaction
</h1>


<div align=""center"">

[![arxiv](https://img.shields.io/badge/Paper-arXiv:202402.07939-b31b1b.svg)](https://arxiv.org/abs/2402.07939)&ensp;
![Python Version](https://img.shields.io/badge/Python-3776AB?&logo=python&logoColor=white-blue&label=3.10%20%7C%203.11)&ensp;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&ensp;
[![Documentation](https://img.shields.io/badge/Documentation-%230ABAB5?style=flat&logo=readthedocs&logoColor=black)](https://microsoft.github.io/UFO/)&ensp;
[![YouTube](https://img.shields.io/badge/YouTube-white?logo=youtube&logoColor=%23FF0000)](https://www.youtube.com/watch?v=QT_OhygMVXU)&ensp;
<!-- [![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/UFO_Agent)](https://twitter.com/intent/follow?screen_name=UFO_Agent) -->
<!-- ![Wel"
axolotl,"# Axolotl

![tests](https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests.yml/badge.svg)
![tests-nightly](https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests-nightly.yml/badge.svg)
![multigpu-semi-weekly tests](https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/multi-gpu-e2e.yml/badge.svg)

Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.

Features:
- Train various Huggingface models such as llama, pythia, falcon, mpt
- Supports fullfinetune, lora, qlora, relora, and gptq
- Customize configurations using a simple yaml file or CLI overwrite
- Load different dataset formats, use custom formats, or bring your own tokenized datasets
- Integrated with xformer, flash attention, [liger kernel](https://github.com/linkedin/Liger-Kernel), rope scaling, and multipacking
- Works with single GPU or multiple GPUs via FSDP or Deepspeed
- Easily run with Docker locally"
moto,"# Moto - Mock AWS Services

[![Join the chat at https://gitter.im/awsmoto/Lobby](https://badges.gitter.im/awsmoto/Lobby.svg)](https://gitter.im/awsmoto/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

[![Build Status](https://github.com/getmoto/moto/workflows/TestNDeploy/badge.svg)](https://github.com/getmoto/moto/actions)
[![Coverage Status](https://codecov.io/gh/getmoto/moto/branch/master/graph/badge.svg)](https://codecov.io/gh/getmoto/moto)
[![Docs](https://readthedocs.org/projects/pip/badge/?version=stable)](http://docs.getmoto.org)
[![PyPI](https://img.shields.io/pypi/v/moto.svg)](https://pypi.org/project/moto/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/moto.svg)](#)
[![PyPI - Downloads](https://img.shields.io/pypi/dw/moto.svg)](https://pypistats.org/packages/moto)
[![Code style: Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/"
SPADE,"[![License CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC4.0-blue.svg)](https://raw.githubusercontent.com/nvlabs/SPADE/master/LICENSE.md)
![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)

# Semantic Image Synthesis with SPADE
![GauGAN demo](https://nvlabs.github.io/SPADE//images/ocean.gif)

# New implementation available at imaginaire repository

We have a reimplementation of the SPADE method that is more performant. It is avaiable at [Imaginaire](https://github.com/NVlabs/imaginaire)

### [Project page](https://nvlabs.github.io/SPADE/) |   [Paper](https://arxiv.org/abs/1903.07291) | [Online Interactive Demo of GauGAN](https://www.nvidia.com/en-us/research/ai-playground/) | [GTC 2019 demo](https://youtu.be/p5U4NgVGAwg) | [Youtube Demo of GauGAN](https://youtu.be/MXWm6w4E5q0)

Semantic Image Synthesis with Spatially-Adaptive Normalization.<br>
[Taesung Park](http://taesung.me/),  [Ming-Yu Liu](http://mingyuliu.net/), [Ting-Chun Wang](https://tcwang0509.github.i"
pwnagotchi,"<p align=""center"">
  <small>Join the project community on our server!</small>
  <br/><br/>
  <a href=""https://discord.gg/https://discord.gg/btZpkp45gQ"" target=""_blank"" title=""Join our community!"">
    <img src=""https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ""/>
  </a>
</p>
<hr/>

<p align=""center"">
    <a href=""https://github.com/evilsocket/pwnagotchi/releases/latest""><img alt=""Release"" src=""https://img.shields.io/github/release/evilsocket/pwnagotchi.svg?style=flat-square""></a>
    <a href=""https://github.com/evilsocket/pwnagotchi/blob/master/LICENSE.md""><img alt=""Software License"" src=""https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square""></a>
    <a href=""https://github.com/evilsocket/pwnagotchi/graphs/contributors""><img alt=""Contributors"" src=""https://img.shields.io/github/contributors/evilsocket/pwnagotchi""/></a>
    <a href=""https://twitter.com/intent/follow?screen_name=pwnagotchi""><img src=""https://img.shields.io/twitter/follow/pwnagotch"
VALL-E-X,"# VALL-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning ğŸ”Š
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/qCBRmAnTxg)
<br>
English | [ä¸­æ–‡](README-ZH.md)
<br>
An open source implementation of Microsoft's [VALL-E X](https://arxiv.org/pdf/2303.03926) zero-shot TTS model.<br>
**We release our trained model to the public for research or application usage.**

![vallex-framework](/images/vallex_framework.jpg ""VALL-E X framework"")

VALL-E X is an amazing multilingual text-to-speech (TTS) model proposed by Microsoft. While Microsoft initially publish in their research paper, they did not release any code or pretrained models. Recognizing the potential and value of this technology, our team took on the challenge to reproduce the results and train our own model. We are glad to share our trained VALL-E X model with the community, allowing everyone to experience the power next-generation TTS! ğŸ§
<br>
<br>"
gitsome,"<p align=""center"">
  <img src=""http://i.imgur.com/0SXZ90y.gif"">
</p>
<p align=""center"">
  An <a href=""https://github.com/works-with/category/desktop-tools"">Official Integration</a> for GitHub and <a href=""#for-github-enterprise-users"">GitHub Enterprise</a>.
</p>

gitsome
=======

[![Build Status](https://travis-ci.org/donnemartin/gitsome.svg?branch=master)](https://travis-ci.org/donnemartin/gitsome) [![PyPI version](https://badge.fury.io/py/gitsome.svg)](http://badge.fury.io/py/gitsome) [![PyPI](https://img.shields.io/pypi/pyversions/gitsome.svg)](https://pypi.python.org/pypi/gitsome/) [![License](https://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)

## Why `gitsome`?

### The Git Command Line

Although the standard Git command line is a great tool to manage your Git-powered repos, it can be **tough to remember the usage** of:

* 150+ porcelain and plumbing commands
* Countless command-specific options
* Resources such as tags and branches
"
auto-sklearn,"# auto-sklearn

**auto-sklearn** is an automated machine learning toolkit and a drop-in replacement for a [scikit-learn](https://scikit-learn.org) estimator.

Find the documentation **[here](https://automl.github.io/auto-sklearn/)**. Quick links:
  * [Installation Guide](https://automl.github.io/auto-sklearn/master/installation.html)
  * [Releases](https://automl.github.io/auto-sklearn/master/releases.html)
  * [Manual](https://automl.github.io/auto-sklearn/master/manual.html)
  * [Examples](https://automl.github.io/auto-sklearn/master/examples/index.html)
  * [API](https://automl.github.io/auto-sklearn/master/api.html)

## auto-sklearn in one image

![image](doc/images/askl_pipeline.png)

## auto-sklearn in four lines of code

```python
import autosklearn.classification
cls = autosklearn.classification.AutoSklearnClassifier()
cls.fit(X_train, y_train)
predictions = cls.predict(X_test)
```

## Relevant publications

If you use auto-sklearn in scientific publications, we would appreciat"
TikTokDownloader,"<div align=""center"">
<img src=""https://github.com/JoeanAmier/TikTokDownloader/blob/master/static/images/TikTokDownloader.png"" alt=""TikTokDownloader"" height=""256"" width=""256""><br>
<h1>TikTokDownloader</h1>
<img alt=""GitHub"" src=""https://img.shields.io/github/license/JoeanAmier/TikTokDownloader?style=for-the-badge&color=ff6348"">
<img alt=""GitHub forks"" src=""https://img.shields.io/github/forks/JoeanAmier/TikTokDownloader?style=for-the-badge&color=ffa502"">
<img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/JoeanAmier/TikTokDownloader?style=for-the-badge&color=ffee6f"">
<img alt=""GitHub code size in bytes"" src=""https://img.shields.io/github/languages/code-size/JoeanAmier/TikTokDownloader?style=for-the-badge&color=13c2c2"">
<br>
<img alt=""Static Badge"" src=""https://img.shields.io/badge/Python-3.12-3498db?style=for-the-badge&logo=python&labelColor=fffa65"">
<img alt=""GitHub release (with filter)"" src=""https://img.shields.io/github/v/release/JoeanAmier/TikTokDownloader?style=fo"
TrumpScript,"# Final Update
It's been a while since we made any updates to TrumpScript, and we just wanted to make it official that our development on this project has stopped and that we will no longer be accepting issues or pull requests on this repo.

Frankly, this joke isn't funny anymore. Rather than spend your time beating the ""Trump is ridiculous"" meme to death, please actually do something instead and donate to:
* [American Civil Liberties Union](https://www.aclu.org)
* [National Resources Defense Council](https://www.nrdc.org)
* [Planned Parenthood](https://www.plannedparenthood.org)

# TrumpScript <img src=""https://raw.github.com/samshadwell/TrumpScript/master/TrumpScript.jpg"" width=""50px"" height=""50px"" />
Make Python great again

## Mission
TrumpScript is a language based upon the illustrious Donald Trump. As the undeniably best US President, we found that the current field of programming languages does not include any that Trump's glorious golden combover would approve of.

TrumpScript "
DeepCTR,"# DeepCTR

[![Python Versions](https://img.shields.io/pypi/pyversions/deepctr.svg)](https://pypi.org/project/deepctr)
[![TensorFlow Versions](https://img.shields.io/badge/TensorFlow-1.4+/2.0+-blue.svg)](https://pypi.org/project/deepctr)
[![Downloads](https://pepy.tech/badge/deepctr)](https://pepy.tech/project/deepctr)
[![PyPI Version](https://img.shields.io/pypi/v/deepctr.svg)](https://pypi.org/project/deepctr)
[![GitHub Issues](https://img.shields.io/github/issues/shenweichen/deepctr.svg
)](https://github.com/shenweichen/deepctr/issues)
<!-- [![Activity](https://img.shields.io/github/last-commit/shenweichen/deepctr.svg)](https://github.com/shenweichen/DeepCTR/commits/master) -->


[![Documentation Status](https://readthedocs.org/projects/deepctr-doc/badge/?version=latest)](https://deepctr-doc.readthedocs.io/)
![CI status](https://github.com/shenweichen/deepctr/workflows/CI/badge.svg)
[![codecov](https://codecov.io/gh/shenweichen/DeepCTR/branch/master/graph/badge.svg)](https://codecov."
python-dotenv,"# python-dotenv

[![Build Status][build_status_badge]][build_status_link]
[![PyPI version][pypi_badge]][pypi_link]

Python-dotenv reads key-value pairs from a `.env` file and can set them as environment
variables. It helps in the development of applications following the
[12-factor](https://12factor.net/) principles.

- [Getting Started](#getting-started)
- [Other Use Cases](#other-use-cases)
  * [Load configuration without altering the environment](#load-configuration-without-altering-the-environment)
  * [Parse configuration as a stream](#parse-configuration-as-a-stream)
  * [Load .env files in IPython](#load-env-files-in-ipython)
- [Command-line Interface](#command-line-interface)
- [File format](#file-format)
  * [Multiline values](#multiline-values)
  * [Variable expansion](#variable-expansion)
- [Related Projects](#related-projects)
- [Acknowledgements](#acknowledgements)

## Getting Started

```shell
pip install python-dotenv
```

If your application takes its configuration from"
hypothesis,"==========
Hypothesis
==========

Hypothesis is a family of testing libraries which let you write tests parametrized
by a source of examples. A Hypothesis implementation then generates simple and
comprehensible examples that make your tests fail.
This simplifies writing your tests and makes them more powerful at the same time,
by letting software automate the boring bits and do them to a higher standard than a human would,
freeing you to focus on the higher level test logic.

This sort of testing is often called ""property-based testing"",
and the most widely known implementation of the concept is the Haskell
library `QuickCheck <https://hackage.haskell.org/package/QuickCheck>`_,
but Hypothesis differs significantly from QuickCheck and is designed to fit
idiomatically and easily into existing styles of testing that you are used to,
with absolutely no familiarity with Haskell or functional programming needed.

`Hypothesis for Python <hypothesis-python>`_ is the original implementation,
an"
ultisnips,"![Build Status](https://github.com/SirVer/ultisnips/actions/workflows/main.yml/badge.svg)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/SirVer/ultisnips?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

UltiSnips
=========

UltiSnips is the ultimate solution for snippets in Vim. It has many features,
speed being one of them.

![GIF Demo](https://raw.github.com/SirVer/ultisnips/master/doc/demo.gif)

In this demo I am editing a python file. I first expand the `#!` snippet, then
the `class` snippet. The completion menu comes from
[YouCompleteMe](https://github.com/Valloric/YouCompleteMe), UltiSnips also
integrates with [deoplete](https://github.com/Shougo/deoplete.nvim),
[vim-easycomplete](https://github.com/jayli/vim-easycomplete) and more. I can
jump through placeholders and add text while the snippet inserts text in other
places automatically: when I add `Animal` as a base class, `__init__` gets
updated to call the base class constructor. When I add ar"
ajenti,"[![Logo](docs/img/Logo.png)](https://ajenti.org/)

Ajenti is a Linux & BSD modular server admin panel. Ajenti 2 provides a new interface and a better architecture, developed with [Python3](https://www.python.org/) and [AngularJS](https://angularjs.org/).

<p align=""center"">
    <a href=""https://crowdin.net/project/ajenti"">
        <img src=""https://badges.crowdin.net/ajenti/localized.svg"" alt=""Badge Crowdin"" />
    </a>
    <a href=""https://github.com/ajenti/ajenti/graphs/contributors"">
        <img src=""https://img.shields.io/github/contributors/ajenti/ajenti?label=Contributors"" alt=""Badge Contributors"" />
    </a>
    <a href=""https://raw.githubusercontent.com/ajenti/ajenti/master/LICENSE""> 
        <img src=""https://img.shields.io/github/license/ajenti/ajenti?label=License"" alt=""Badge License"" />
    </a>
</p>

----

# Feature highlights

* **Easy installation** : Ajenti 2 can be easy installed [with pip and the provided script](https://docs.ajenti.org/en/latest/man/install.html#ins"
angr,"# angr

[![Latest Release](https://img.shields.io/pypi/v/angr.svg)](https://pypi.python.org/pypi/angr/)
[![Python Version](https://img.shields.io/pypi/pyversions/angr)](https://pypi.python.org/pypi/angr/)
[![PyPI Statistics](https://img.shields.io/pypi/dm/angr.svg)](https://pypistats.org/packages/angr)
[![License](https://img.shields.io/github/license/angr/angr.svg)](https://github.com/angr/angr/blob/master/LICENSE)

angr is a platform-agnostic binary analysis framework.
It is brought to you by [the Computer Security Lab at UC Santa Barbara](https://seclab.cs.ucsb.edu), [SEFCOM at Arizona State University](https://sefcom.asu.edu), their associated CTF team, [Shellphish](https://shellphish.net), the open source community, and **[@rhelmot](https://github.com/rhelmot)**.

## Project Links
Homepage: https://angr.io

Project repository: https://github.com/angr/angr

Documentation: https://docs.angr.io

API Documentation: https://api.angr.io/en/latest/

## What is angr?

angr is a suite of P"
fast-stable-diffusion,"# Shoutout to <a href=""https://www.scenario.com"" target=""_blank""><img src='https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/.github/Scenario.png' width=""170"" height=""40"" style=""vertical-align: middle; margin-bottom: 8px; background-color: black""></a> and <a href=""https://www.paperspace.com"" target=""_blank""><img src='https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/.github/Paperspace.png' width=""170"" height=""40"" style=""vertical-align: middle; margin-bottom: 8px; background-color: black""></a> for sponsoring the project
 
# fast-stable-diffusion Notebooks, A1111 + ComfyUI + DreamBooth
Paperspace adaptations AUTOMATIC1111 Webui, ComfyUI and Dreambooth.
 
<center><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n"
Machine-Learning-Collection,"<p align=""center""><img width=""100%"" src=""ML/others/logo/torch_and_tf.svg"" /></p>

--------------------------------------------------------------------------------


[![Build Status](https://travis-ci.com/aladdinpersson/Machine-Learning-Collection.svg?branch=master)](https://travis-ci.com/aladdinpersson/Machine-Learning-Collection) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[logo]: https://github.com/AladdinPerzon/Machine-Learning-Collection/blob/master/ML/others/logo/youtube_logo.png

# Machine Learning Collection
In this repository you will find tutorials and projects related to Machine Learning. I try to make the code as clear as possible, and the goal is be to used as a learning resource and a way to lookup problems to solve specific problems. For most I have also done video explanations on YouTube if you want a walkthrough for the code. If you got any questions or suggestions for future videos I prefer if you ask it "
universe,"**This repository has been deprecated in favor of the Retro (https://github.com/openai/retro) library. See our Retro Contest (https://blog.openai.com/retro-contest) blog post for detalis.**

universe
***************

`Universe <https://openai.com/blog/universe/>`_ is a software
platform for measuring and training an AI's general intelligence
across the world's supply of games, websites and other
applications. This is the ``universe`` open-source library, which
provides a simple `Gym <https://github.com/openai/gym>`__
interface to each Universe environment.

Universe allows anyone to train and evaluate AI agents on an extremely
wide range of real-time, complex environments.

Universe makes it possible for any existing program to become an
OpenAI Gym environment, without needing special access to the
program's internals, source code, or APIs. It does this by packaging
the program into a Docker container, and presenting the AI with the
same interface a human uses: sending keyboard and mou"
PyTorch_Tutorial,"ï»¿# Pytorchæ¨¡å‹è®­ç»ƒå®ç”¨æ•™ç¨‹
<img src=""./Data/cover.png"" alt=""Image text"" style=""zoom:33%;"" />

---

ğŸ“¢ï¼šã€ŠPyTorchå®ç”¨æ•™ç¨‹ã€‹ï¼ˆç¬¬äºŒç‰ˆï¼‰å·²å¼€æºï¼Œæ¬¢è¿é˜…è¯»ï¼šhttps://tingsongyu.github.io/PyTorch-Tutorial-2nd/

ğŸ“¢ï¼šã€ŠPyTorchå®ç”¨æ•™ç¨‹ã€‹ï¼ˆç¬¬äºŒç‰ˆï¼‰å·²å¼€æºï¼Œæ¬¢è¿é˜…è¯»ï¼šhttps://tingsongyu.github.io/PyTorch-Tutorial-2nd/

ğŸ“¢ï¼šã€ŠPyTorchå®ç”¨æ•™ç¨‹ã€‹ï¼ˆç¬¬äºŒç‰ˆï¼‰å·²å¼€æºï¼Œæ¬¢è¿é˜…è¯»ï¼šhttps://tingsongyu.github.io/PyTorch-Tutorial-2nd/

ç¬¬äºŒç‰ˆæ–°å¢ä¸°å¯Œçš„**æ·±åº¦å­¦ä¹ åº”ç”¨æ¡ˆä¾‹**å’Œ**æ¨ç†éƒ¨ç½²æ¡†æ¶**ï¼ŒåŒ…æ‹¬CVã€NLPå’ŒLLMçš„åå¤šä¸ªå®æˆ˜é¡¹ç›®ï¼Œä»¥åŠONNXå’ŒTensorRTçš„æ•™ç¨‹ã€‚

# 1.ç®€ä»‹

æœ¬ä»£ç ä¸ºæ•™ç¨‹â€”â€”ã€ŠPytorchæ¨¡å‹è®­ç»ƒå®ç”¨æ•™ç¨‹ã€‹ä¸­é…å¥—ä»£ç ï¼›<br/>
ã€ŠPytorchæ¨¡å‹è®­ç»ƒå®ç”¨æ•™ç¨‹ã€‹å¯é€šè¿‡å¦‚ä¸‹æ–¹å¼è·å–ï¼š<br/>

1. https://github.com/tensor-yu/PyTorch_Tutorial/tree/master/Data<br/>
2. QQç¾¤ï¼š å››ç¾¤ï¼š854620826  <br/>


# 2.ç¯å¢ƒé…ç½®
ä»£ç åœ¨ä»¥ä¸‹ä¸¤ç§ç¯å¢ƒæµ‹è¯•è¿‡ï¼š<br/>
1. win10 64ä½ + python3.5 + pytorch==0.4.0 <br/>
2. mac + python3.6 + pytorch==0.4.1/ pytorch==1.0.0 <br/>

**ç¬¬ä¸€æ­¥ å®‰è£…å„ä¾èµ–åŒ…ï¼š**<br/>
pip install -r requirements.txt

**ç¬¬äºŒæ­¥ æ‰‹åŠ¨å®‰è£…pytorchåŠtorchvisionï¼š**<br/>
å‡é€‰æ‹©æ— gpuç‰ˆæœ¬è¿›è¡Œå®‰è£…ï¼Œè¿›å…¥å®˜ç½‘é€‰æ‹©ç›¸åº”çš„æŒ‡ä»¤è¿›è¡Œå®‰è£…
https://pytorch.org/get-started/locally/


# 3.é—®é¢˜åé¦ˆ
è‹¥å‘ç°ä»»ä½•é—®é¢˜å’Œæ”¹è¿›æ„è§ï¼Œè¯·æ‚¨éšæ—¶è”ç³»æˆ‘ã€‚<br/>
è”ç³»æ–¹å¼ï¼šyts3221@126.com<br/>
è¯»è€…qqç¾¤ï¼š

â€‹	ä¸€ç¾¤ï¼š671103375 (å·²æ»¡)  <br/>

â€‹	äºŒç¾¤ï¼š773031536"
GPT2-Chinese,"# GPT2-Chinese

## Description

- Chinese version of GPT2 training code, using BERT tokenizer or BPE tokenizer. It is based on the extremely awesome repository from HuggingFace team [Transformers](https://github.com/huggingface/transformers). Can write poems, news, novels, or train general language models. Support char level, word level and BPE level. Support large training corpus.
- ä¸­æ–‡çš„GPT2è®­ç»ƒä»£ç ï¼Œä½¿ç”¨BERTçš„Tokenizeræˆ–Sentencepieceçš„BPE modelï¼ˆæ„Ÿè°¢[kangzhonghua](https://github.com/kangzhonghua)çš„è´¡çŒ®ï¼Œå®ç°BPEæ¨¡å¼éœ€è¦ç•¥å¾®ä¿®æ”¹train.pyçš„ä»£ç ï¼‰ã€‚å¯ä»¥å†™è¯—ï¼Œæ–°é—»ï¼Œå°è¯´ï¼Œæˆ–æ˜¯è®­ç»ƒé€šç”¨è¯­è¨€æ¨¡å‹ã€‚æ”¯æŒå­—ä¸ºå•ä½æˆ–æ˜¯åˆ†è¯æ¨¡å¼æˆ–æ˜¯BPEæ¨¡å¼ï¼ˆéœ€è¦ç•¥å¾®ä¿®æ”¹train.pyçš„ä»£ç ï¼‰ã€‚æ”¯æŒå¤§è¯­æ–™è®­ç»ƒã€‚

## UPDATE 04.11.2024

- éå¸¸æ„Ÿè°¢å„ä½å¯¹æœ¬é¡¹ç›®çš„å…³æ³¨ã€‚ChatGPTå‘å¸ƒä»¥æ¥æœ¬é¡¹ç›®ä¹Ÿé‡æ–°å¼•èµ·äº†ä¸€äº›æ³¨æ„ã€‚é¡¹ç›®æœ¬èº«æ˜¯æˆ‘è‡ªå­¦Pytorchçš„ç»ƒæ‰‹é¡¹ç›®ï¼Œæˆ‘ä¹Ÿæ— æ„åšé•¿æœŸçš„ç»´æŠ¤æ›´æ–°ã€‚å¦‚æœå¤§å®¶å¯¹å¤§æ¨¡å‹LLMæ„Ÿå…´è¶£çš„è¯ï¼Œå¯ä»¥é‚®ä»¶æˆ‘(ned1991@gmail.com)åŠ ç¾¤æ²Ÿé€šï¼Œæˆ–æ˜¯åœ¨Issueä¸­è¿›è¡Œè®¨è®ºã€‚

## UPDATE 02.06.2021

- æœ¬é¡¹ç›®æ–°å¢äº†[é€šç”¨ä¸­æ–‡GPT-2é¢„è®­ç»ƒæ¨¡å‹](https://github.com/Morizeyao/GPT2-Chinese#%E6%A8%A1%E5%9E%8B%E5%88%86%E4%BA%AB)ã€[é€šç”¨ä¸­æ–‡GPT-2é¢„è®­ç»ƒå°æ¨¡å‹](https://github.com/Morizeyao/GPT2-Chinese#%E6%A8%A1%E5%9E%8B%E5%88%86%E4%BA%AB)ã€[ä¸­æ–‡æ­Œè¯GPT-2é¢„è®­ç»ƒæ¨¡å‹](https://g"
fuzzDicts,"# fuzzDicts
Web Pentesting Fuzz å­—å…¸,ä¸€ä¸ªå°±å¤Ÿäº†ã€‚

## log 

ä¸å®šæœŸæ›´æ–°ï¼Œä½¿ç”¨å‰å»ºè®®git pullä¸€ä¸‹ï¼ŒåŒæ­¥æ›´æ–°ã€‚


  **åˆ†äº«å­—å…¸å»ºè®®ç›´æ¥æäº¤PR** 

20210608:

* åœ¨rcePayloadså­—å…¸ä¸‹æ·»åŠ äº†ä¸€ä¸ª[Remote Code Execution ( Unix and Windows )](https://ansar0047.medium.com/remote-code-execution-unix-and-windows-4ed3367158b3)ä¸­æåˆ°çš„æ‰€æœ‰Payloadã€‚

20201202:

* åœ¨ç›®å½•å­—å…¸ä¸‹æ›´æ–°äº†ä¸€ä¸ª[Se7en](https://github.com/r00tSe7en)å¸ˆå‚…ç»™çš„adminç›®å½•å˜ç§ã€‚

20200510:

* ç”¨æˆ·åå­—å…¸ä¸‹æ–°å¢äº†ä¸€ä¸ªç™¾å®¶å§“top3000çš„æ‹¼éŸ³ï¼Œå»é‡å188æ¡ï¼ŒAttack!!!.


20200420:

* åˆå¹¶ä¸€ä¸ªç”±[lanyi1998](https://github.com/lanyi1998)æäº¤çš„prï¼Œæµ‹è¯•å¸¸ç”¨æ‰‹æœºå·ç top300+ï¼Œæ”¾åœ¨ç”¨æˆ·åå­—å…¸é‡Œé¢ï¼Œç“¶é¢ˆæµ‹è¯•æ—¶å¯ä»¥è¯•è¯•ï¼›æ·»åŠ ä¸€ä»½å›¢é˜ŸChildå¸ˆå‚…æä¾›çš„æŸé›†å›¢çš„å¼±å£ä»¤å­—å…¸ã€‚

20200410:

* æ–°å¢centOSå’ŒAIXä¸»æœºçš„/etc/ç›®å½•çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ”¾åœ¨ssrfDictç›®å½•ï¼Œå®æˆ˜ä¸­é‡åˆ°çš„ï¼Œaixå’Œå…¶ä»–ç³»ç»ŸåŒºåˆ«è¿˜æ˜¯è›®å¤§çš„ï¼Œä½œç”¨è‡ªå·±ç¢ç£¨ã€‚

20200406:

* åˆå¹¶ä¸€ä¸ªç”±[lewiswu1209](https://github.com/lewiswu1209)æäº¤çš„prï¼Œå¯†ç top19576ã€‚


20200221:

* æ›´æ–°ç”±[makoto56](https://github.com/makoto56)å¸ˆå‚…åŠ å¼ºåçš„webshellå¯†ç å­—å…¸,ç¦»èŒå­¦ä¹ ä¸­ï¼Œæ¯•ä¸šå‰ä¸ä¼šæœ‰å¤ªå¤šçš„webæµ‹è¯•ä»»åŠ¡ï¼ˆä¹Ÿä¸æƒ³å†ç»§ç»­æ‰“webäº†ï¼‰ï¼Œå­—å…¸æ›´æ–°é¢‘ç‡ä¼šé™ä½å¾ˆå¤šï¼Œå¦‚æœæœ‰å°ä¼™ä¼´æƒ³ä¸€èµ·ç»´æŠ¤å¯ä»¥è”ç³»æˆ‘å•Šã€‚

20200211:

* æ–°å¢ä¸€ä¸ªlotå­—å…¸ï¼Œæ•°æ®æ¥æºäºtgç¾¤é‡Œåˆ«äººå‘çš„50wäº’è”ç½‘lotè®¾å¤‡å¼±å£ä»¤ï¼Œç”±[sunu11](https://github.com/sunu11)å¸ˆå‚…æå–ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ äº†å›½å†…çš„æ•°æ®ã€‚é‡åˆ°ä¸çŸ¥åçš„è®¾å¤‡æ—¶ä¸€é˜µçˆ†æ€¼å’¯ï¼Œæ“…ç”¨å­—å…¸ï¼Œäº‹åŠåŠŸå€ã€‚

20200115:

* "
diff-match-patch,"The Diff Match and Patch libraries offer robust algorithms to perform the
operations required for synchronizing plain text.

1. Diff:
   * Compare two blocks of plain text and efficiently return a list of differences.
   * [Diff Demo](https://neil.fraser.name/software/diff_match_patch/demos/diff.html)
2. Match:
   * Given a search string, find its best fuzzy match in a block of plain text. Weighted for both accuracy and location.
   * [Match Demo](https://neil.fraser.name/software/diff_match_patch/demos/match.html)
3. Patch:
   * Apply a list of patches onto plain text. Use best-effort to apply patch even when the underlying text doesn't match.
   * [Patch Demo](https://neil.fraser.name/software/diff_match_patch/demos/patch.html)

Originally built in 2006 to power Google Docs, this library is now available in C++, C#, Dart, Java, JavaScript, Lua, Objective C, and Python.

### Reference

* [API](https://github.com/google/diff-match-patch/wiki/API) - Common API across all languages.
* [L"
pwndbg,"![repository-open-graph](https://github.com/pwndbg/pwndbg/assets/150354584/77b2e438-898f-416f-a989-4bef30759627)
# pwndbg

[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://choosealicense.com/licenses/mit/)
[![Unit tests](https://github.com/pwndbg/pwndbg/actions/workflows/tests.yml/badge.svg?branch=dev&event=push)](https://github.com/pwndbg/pwndbg/actions/workflows/tests.yml)
[![codecov.io](https://codecov.io/github/pwndbg/pwndbg/graph/badge.svg?token=i1cBPFVCav)](https://codecov.io/github/pwndbg/pwndbg?branch=dev)
[![Discord](https://img.shields.io/discord/843809097920413717?label=Discord&style=plastic)](https://discord.gg/x47DssnGwm)

`pwndbg` (/paÊŠnËˆdiËŒbÊŒÉ¡/) is a GDB plug-in that makes debugging with GDB suck less, with a focus on features needed by low-level software developers, hardware hackers, reverse-engineers and exploit developers.

It has a boatload of features, see [FEATURES.md](FEATURES.md) and [CHEATSHEET](https://drive.googl"
FlareSolverr,"# FlareSolverr

[![Latest release](https://img.shields.io/github/v/release/FlareSolverr/FlareSolverr)](https://github.com/FlareSolverr/FlareSolverr/releases)
[![Docker Pulls](https://img.shields.io/docker/pulls/flaresolverr/flaresolverr)](https://hub.docker.com/r/flaresolverr/flaresolverr/)
[![GitHub issues](https://img.shields.io/github/issues/FlareSolverr/FlareSolverr)](https://github.com/FlareSolverr/FlareSolverr/issues)
[![GitHub pull requests](https://img.shields.io/github/issues-pr/FlareSolverr/FlareSolverr)](https://github.com/FlareSolverr/FlareSolverr/pulls)
[![Donate PayPal](https://img.shields.io/badge/Donate-PayPal-yellow.svg)](https://www.paypal.com/paypalme/diegoheras0xff)
[![Donate Bitcoin](https://img.shields.io/badge/Donate-Bitcoin-f7931a.svg)](https://www.blockchain.com/btc/address/13Hcv77AdnFWEUZ9qUpoPBttQsUT7q9TTh)
[![Donate Ethereum](https://img.shields.io/badge/Donate-Ethereum-8c8c8c.svg)](https://www.blockchain.com/eth/address/0x0D1549BbB00926BF3D92c1A8A58"
objection,"# ğŸ“±objection - Runtime Mobile Exploration

`objection` is a runtime mobile exploration toolkit, powered by [Frida](https://www.frida.re/), built to help you assess the security posture of your mobile applications, without needing a jailbreak.

[![Twitter](https://img.shields.io/badge/twitter-%40leonjza-blue.svg)](https://twitter.com/leonjza)
[![PyPi](https://badge.fury.io/py/objection.svg)](https://pypi.python.org/pypi/objection)
[![Black Hat Arsenal](https://raw.githubusercontent.com/toolswatch/badges/master/arsenal/europe/2017.svg?sanitize=true)](https://www.blackhat.com/eu-17/arsenal-overview.html)
[![Black Hat Arsenal](https://raw.githubusercontent.com/toolswatch/badges/master/arsenal/usa/2019.svg?sanitize=true)](https://www.blackhat.com/us-19/arsenal-overview.html)

<img align=""right"" src=""./images/objection.png"" height=""220"" alt=""objection"">

- Supports both iOS and Android.
- Inspect and interact with container file systems.
- Bypass SSL pinning.
- Dump keychains.
- Perform memo"
umap,".. -*- mode: rst -*-

.. image:: doc/logo_large.png
  :width: 600
  :alt: UMAP logo
  :align: center

|pypi_version|_ |pypi_downloads|_

|conda_version|_ |conda_downloads|_

|License|_ |build_status|_ |Coverage|_

|Docs|_ |joss_paper|_

.. |pypi_version| image:: https://img.shields.io/pypi/v/umap-learn.svg
.. _pypi_version: https://pypi.python.org/pypi/umap-learn/

.. |pypi_downloads| image:: https://pepy.tech/badge/umap-learn/month
.. _pypi_downloads: https://pepy.tech/project/umap-learn

.. |conda_version| image:: https://anaconda.org/conda-forge/umap-learn/badges/version.svg
.. _conda_version: https://anaconda.org/conda-forge/umap-learn

.. |conda_downloads| image:: https://anaconda.org/conda-forge/umap-learn/badges/downloads.svg
.. _conda_downloads: https://anaconda.org/conda-forge/umap-learn

.. |License| image:: https://img.shields.io/pypi/l/umap-learn.svg
.. _License: https://github.com/lmcinnes/umap/blob/master/LICENSE.txt

.. |build_status| image:: https://dev.azure.com/TutteI"
FastSAM,"![](assets/logo.png)

# Fast Segment Anything

[[`ğŸ“•Paper`](https://arxiv.org/pdf/2306.12156.pdf)] [[`ğŸ¤—HuggingFace Demo`](https://huggingface.co/spaces/An-619/FastSAM)] [[`Colab demo`](https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9?usp=sharing)] [[`Replicate demo & API`](https://replicate.com/casia-iva-lab/fastsam)] [~~[`OpenXLab Demo`](https://openxlab.org.cn/apps/detail/zxair/FastSAM)~~] [[`Model Zoo`](#model-checkpoints)] [[`BibTeX`](#citing-fastsam)] [[`Video Demo`](https://youtu.be/yHNPyqazYYU)]

![FastSAM Speed](assets/head_fig.png)

The **Fast Segment Anything Model(FastSAM)** is a CNN Segment Anything Model trained using only 2% of the SA-1B dataset published by SAM authors. FastSAM achieves comparable performance with
the SAM method at **50Ã— higher run-time speed**.

![FastSAM design](assets/Overview.png)

**ğŸ‡ Updates**
- **`2024/6/25`** The edge jaggies issue has been slightly improved [#231](https://github.com/CASIA-IVA-Lab/FastSAM/pul"
electrum,"# Electrum - Lightweight Bitcoin client

```
Licence: MIT Licence
Author: Thomas Voegtlin
Language: Python (>= 3.8)
Homepage: https://electrum.org/
```

[![Build Status](https://api.cirrus-ci.com/github/spesmilo/electrum.svg?branch=master)](https://cirrus-ci.com/github/spesmilo/electrum)
[![Test coverage statistics](https://coveralls.io/repos/github/spesmilo/electrum/badge.svg?branch=master)](https://coveralls.io/github/spesmilo/electrum?branch=master)
[![Help translate Electrum online](https://d322cqt584bo4o.cloudfront.net/electrum/localized.svg)](https://crowdin.com/project/electrum)


## Getting started

_(If you've come here looking to simply run Electrum,
[you may download it here](https://electrum.org/#download).)_

Electrum itself is pure Python, and so are most of the required dependencies,
but not everything. The following sections describe how to run from source, but here
is a TL;DR:

```
$ sudo apt-get install libsecp256k1-dev
$ python3 -m pip install --user "".[gui,crypto]""
"
holehe,"# **Holehe OSINT - Email to Registered Accounts**
ğŸ‘‹ Hi there! For any professional inquiries or collaborations, please reach out to me at:
megadose@protonmail.com

ğŸ“§ Preferably, use your professional email for correspondence. Let's keep it short and sweet, and all in English!

![](https://files.catbox.moe/5we2ya.png)
![PyPI](https://img.shields.io/pypi/v/holehe) ![PyPI - Week](https://img.shields.io/pypi/dw/holehe) ![PyPI - Downloads](https://static.pepy.tech/badge/holehe) ![PyPI - License](https://img.shields.io/pypi/l/holehe)

# [Holehe Online Version](https://osint.industries/)

## **Summary**

*Efficiently finding registered accounts from emails.*

Holehe checks if an email is attached to an account on sites like twitter, instagram, imgur and more than 120 others.

+ Retrieves information using the forgotten password function.
+ **[Does not alert the target email.](https://github.com/megadose/holehe/issues/12)**
+ Runs on [Python 3](https://www.python.org/downloads/release/python-3"
edx-platform,"Open edX Platform
#################
| |License: AGPL v3| |Status| |Python CI|

.. |License: AGPL v3| image:: https://img.shields.io/badge/License-AGPL_v3-blue.svg
  :target: https://www.gnu.org/licenses/agpl-3.0

.. |Python CI| image:: https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml/badge.svg
  :target: https://github.com/openedx/edx-platform/actions/workflows/unit-tests.yml

.. |Status| image:: https://img.shields.io/badge/status-maintained-31c653

Purpose
*******
The `Open edX Platform <https://openedx.org>`_ is a service-oriented platform for authoring and
delivering online learning at any scale.  The platform is written in
Python and JavaScript and makes extensive use of the Django
framework. At the highest level, the platform is composed of a
monolith, some independently deployable applications (IDAs), and
micro-frontends (MFEs) based on the ReactJS.

This repository hosts the monolith at the center of the Open edX
platform.  Functionally, the edx-platform"
cheatsheets,"# Cheatsheets for Matplotlib users

## Cheatsheets
Cheatsheet [(download pdf)](https://matplotlib.org/cheatsheets/cheatsheets.pdf) | |
:------------------------------------------------------------------------------:|:----------------------------------------------------------:
![](https://matplotlib.org/cheatsheets/cheatsheets-1.png)                       | ![](https://matplotlib.org/cheatsheets/cheatsheets-2.png)

## Handouts

Beginner handout [(download pdf)](https://matplotlib.org/cheatsheets/handout-beginner.pdf) | Intermediate handout [(download pdf)](https://matplotlib.org/cheatsheets/handout-intermediate.pdf) | Tips handout [(download pdf)](https://matplotlib.org/cheatsheets/handout-tips.pdf)
:-----------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------:
![](https://ma"
real-url,"# Real-Url

## è¯´æ˜

æ²¡æƒ³åˆ°è¿˜æœ‰è¿™ä¹ˆå¤šæœ‹å‹å‘ issue å’Œé‚®ä»¶å’¨è¯¢é—®é¢˜ï¼Œæ„Ÿè°¢å¤§å®¶çš„æ”¯æŒğŸˆï¼å› ä¸ºæœ‰æ—¶å¾ˆå¿™ï¼Œå›å¤å’Œæäº¤ä»£ç çš„å‘¨æœŸä¼šæœ‰ç‚¹é•¿ï¼ŒæŠ±æ­‰å“¦ğŸ˜‹

è¿™ä¸ªä»“åº“å­˜æ”¾çš„æ˜¯ï¼šè·å–ä¸€äº›ç›´æ’­å¹³å°çœŸå®æµåª’ä½“åœ°å€ï¼ˆç›´æ’­æºï¼‰å’Œå¼¹å¹•çš„ Python ä»£ç å®ç°ã€‚è·å–çš„åœ°å€ç»æµ‹è¯•ï¼Œå‡å¯åœ¨ PotPlayerã€VLCã€DPlayer(flv.js + hls.js)ç­‰æ’­æ”¾å™¨ä¸­æ’­æ”¾ã€‚

>  ğŸ¤˜ğŸ‘ŒğŸ¤™ğŸ™ğŸ‰ğŸ‘‰ ï¼šå¦‚æœè¯¥é¡¹ç›®èƒ½å¸®åŠ©åˆ°æ‚¨ï¼Œæ¬¢è¿ star å’Œ prï¼›æˆ–åœ¨æ‚¨çš„é¡¹ç›®ä¸­æ ‡æ³¨ Real-Url ä¸ºå‚è€ƒæ¥æºã€‚

ç›®å‰å·²å®ç°ï¼š

 **59** ä¸ªç›´æ’­å¹³å°çš„ç›´æ’­æºè·å–ï¼šæ–—é±¼ç›´æ’­ã€è™ç‰™ç›´æ’­ã€å“”å“©å“”å“©ç›´æ’­ã€æˆ˜æ——ç›´æ’­ã€ç½‘æ˜“ CC ç›´æ’­ã€ç«çŒ«ç›´æ’­ã€ä¼é¹…ç”µç«ã€YY ç›´æ’­ã€ä¸€ç›´æ’­ã€å¿«æ‰‹ç›´æ’­ã€èŠ±æ¤’ç›´æ’­ã€æ˜ å®¢ç›´æ’­ã€è¥¿ç“œç›´æ’­ã€è§¦æ‰‹ç›´æ’­ï¼ˆå·²å€’é—­ï¼‰ã€NOW ç›´æ’­ã€æŠ–éŸ³ç›´æ’­ï¼Œçˆ±å¥‡è‰ºç›´æ’­ã€é…·ç‹—ç›´æ’­ã€é¾™ç ç›´æ’­ã€PPS å¥‡ç§€ç›´æ’­ã€å…­é—´æˆ¿ã€17 ç›´æ’­ã€æ¥ç–¯ç›´æ’­ã€ä¼˜é…·è½®æ’­å°ã€ç½‘æ˜“ LOOK ç›´æ’­ã€åƒå¸†ç›´æ’­ã€é™Œé™Œç›´æ’­ã€å°ç±³ç›´æ’­ã€è¿…é›·ç›´æ’­ã€äº¬ä¸œç›´æ’­ã€ä¼é¹…ä½“è‚²ã€äººäººç›´æ’­ã€æ£‰èŠ±ç³–ç›´æ’­ã€ä¹ç§€ç›´æ’­ã€ç¾šèŒç›´æ’­ã€95ç§€ã€æ–°æµªç–¯æ’­ã€çº¢äººç›´æ’­ã€è‰¾ç±³ç›´æ’­ã€KKç›´æ’­ã€é…·æˆ‘èšæ˜Ÿã€ä¹å—¨ç›´æ’­ã€ç§€è‰²ç›´æ’­ã€æ˜Ÿå…‰ç›´æ’­ã€æˆ‘ç§€ç›´æ’­ã€çƒ­çŒ«ç›´æ’­ã€è‰ºæ°”å±±ç›´æ’­ã€AcFun ç›´æ’­ã€çŒ«è€³FMã€ç•…ç§€é˜ã€Twitchã€TikTokã€å¤®è§†é¢‘ã€PPä½“è‚²ã€zhibotvã€è…¾è®¯ä½“è‚²ç›´æ’­ã€çˆ±å¥‡è‰ºä½“è‚²ç›´æ’­ã€liveUã€bigoliveã€å’ªå’•è§†é¢‘ä½“è‚²ã€‚

 **18** ä¸ªç›´æ’­å¹³å°çš„å¼¹å¹•è·å–ï¼šæ–—é±¼ç›´æ’­ã€è™ç‰™ç›´æ’­ã€å“”å“©å“”å“©ç›´æ’­ã€å¿«æ‰‹ç›´æ’­ã€ç«çŒ«ç›´æ’­ã€ä¼é¹…ç”µç«ã€èŠ±æ¤’ç›´æ’­ã€æ˜ å®¢ç›´æ’­ã€ç½‘æ˜“ CC ç›´æ’­ã€é…·ç‹—ç›´æ’­ã€é¾™ç ç›´æ’­ã€PPS å¥‡ç§€ã€æœç‹åƒå¸†ã€æˆ˜æ——ç›´æ’­ã€æ¥ç–¯ç›´æ’­ã€ç½‘æ˜“ LOOK ç›´æ’­ã€AcFun ç›´æ’­ã€è‰ºæ°”å±±ç›´æ’­ã€‚

## è¿è¡Œ

1. é¡¹ç›®ä½¿ç”¨äº†å¾ˆç®€å•çš„ Python ä»£ç ï¼Œä»…åœ¨ Python 3 ç¯å¢ƒè¿è¡Œæµ‹è¯•ã€‚
2. å…·ä½“æ‰€éœ€æ¨¡å—è¯·æŸ¥çœ‹ requirements.txt
3. è·å–æ–—é±¼å’Œçˆ±å¥‡è‰ºçš„ç›´æ’­æºï¼Œéœ€ JavaScript ç¯å¢ƒï¼Œå¯ä½¿ç”¨ node.jsã€‚çˆ±å¥‡è‰ºç›´æ’­é‡Œæœ‰ä¸ªå‚æ•°æ˜¯åŠ ç›çš„ MD5ï¼Œç”±ä»“åº“ä¸­çš„ iqiyi.js ç”Ÿæˆã€‚
4. æ¯ä¸ªå¹³å°çš„ç›´æ’­æºå’Œå¼¹å¹•è·å–åŠŸèƒ½ç›¸äº’ç‹¬ç«‹ï¼Œä»¥åå†æ•´åˆã€‚å¼¹å¹•é£Ÿç”¨ï¼špython main.py

## åé¦ˆ

æœ‰ç›´æ’­å¹³å°å¤±æ•ˆæˆ–æ–°å¢å…¶ä»–å¹³å°è§£æçš„ï¼Œå¯å‘ [i"
TensorLayer,"<a href=""https://tensorlayer.readthedocs.io/"">
    <div align=""center"">
        <img src=""img/tl_transparent_logo.png"" width=""50%"" height=""30%""/>
    </div>
</a>

<!--- [![PyPI Version](https://badge.fury.io/py/tensorlayer.svg)](https://badge.fury.io/py/tensorlayer) --->
<!--- ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tensorlayer.svg)) --->

![GitHub last commit (branch)](https://img.shields.io/github/last-commit/tensorlayer/tensorlayer/master.svg)
[![Supported TF Version](https://img.shields.io/badge/TensorFlow-2.0.0%2B-brightgreen.svg)](https://github.com/tensorflow/tensorflow/releases)
[![Documentation Status](https://readthedocs.org/projects/tensorlayer/badge/)](https://tensorlayer.readthedocs.io/)
[![Build Status](https://travis-ci.org/tensorlayer/tensorlayer.svg?branch=master)](https://travis-ci.org/tensorlayer/tensorlayer)
[![Downloads](http://pepy.tech/badge/tensorlayer)](http://pepy.tech/project/tensorlayer)
[![Downloads](https://pepy.tech/badge/tensorlay"
bilingual_book_maker,"**[ä¸­æ–‡](./README-CN.md) | English**
[![litellm](https://img.shields.io/badge/%20%F0%9F%9A%85%20liteLLM-OpenAI%7CAzure%7CAnthropic%7CPalm%7CCohere%7CReplicate%7CHugging%20Face-blue?color=green)](https://github.com/BerriAI/litellm)

# bilingual_book_maker
The bilingual_book_maker is an AI translation tool that uses ChatGPT to assist users in creating multi-language versions of epub/txt/srt files and books. This tool is exclusively designed for translating epub books that have entered the public domain and is not intended for copyrighted works. Before using this tool, please review the project's **[disclaimer](./disclaimer.md)**.

![image](https://user-images.githubusercontent.com/15976103/222317531-a05317c5-4eee-49de-95cd-04063d9539d9.png)

## Supported Models
gpt-4, gpt-3.5-turbo, claude-2, palm, llama-2, azure-openai, command-nightly, gemini
For using Non-OpenAI models, use class `liteLLM()` - liteLLM supports all models above.
Find more info here for using liteLLM: https://github.com/B"
PyTorch-YOLOv3,"# PyTorch YOLO
A minimal PyTorch implementation of YOLOv3, with support for training, inference and evaluation.

YOLOv4 and YOLOv7 weights are also compatible with this implementation.

[![CI](https://github.com/eriklindernoren/PyTorch-YOLOv3/actions/workflows/main.yml/badge.svg)](https://github.com/eriklindernoren/PyTorch-YOLOv3/actions/workflows/main.yml) [![PyPI pyversions](https://img.shields.io/pypi/pyversions/pytorchyolo.svg)](https://pypi.python.org/pypi/pytorchyolo/) [![PyPI license](https://img.shields.io/pypi/l/pytorchyolo.svg)](LICENSE)

## Installation
### Installing from source

For normal training and evaluation we recommend installing the package from source using a poetry virtual environment.

```bash
git clone https://github.com/eriklindernoren/PyTorch-YOLOv3
cd PyTorch-YOLOv3/
pip3 install poetry --user
poetry install
```

You need to join the virtual environment by running `poetry shell` in this directory before running any of the following commands without the `poet"
generative-models,"# Generative Models
Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.
Also present here are RBM and Helmholtz Machine.

## Note:
Generated samples will be stored in `GAN/{gan_model}/out` (or `VAE/{vae_model}/out`, etc) directory during training.

## What's in it?

#### Generative Adversarial Nets (GAN)
  1. [Vanilla GAN](https://arxiv.org/abs/1406.2661)
  2. [Conditional GAN](https://arxiv.org/abs/1411.1784)
  3. [InfoGAN](https://arxiv.org/abs/1606.03657)
  4. [Wasserstein GAN](https://arxiv.org/abs/1701.07875)
  5. [Mode Regularized GAN](https://arxiv.org/abs/1612.02136)
  6. [Coupled GAN](https://arxiv.org/abs/1606.07536)
  7. [Auxiliary Classifier GAN](https://arxiv.org/abs/1610.09585)
  8. [Least Squares GAN](https://arxiv.org/abs/1611.04076v2)
  9. [Boundary Seeking GAN](https://arxiv.org/abs/1702.08431)
  10. [Energy Based GAN](https://arxiv.org/abs/1609.03126)
  11. [f-GAN](https://arxiv.org/abs/1606.00709)
  12. [Generative Adversarial Parallelization]("
deep-learning-models,"# Trained image classification models for Keras

**THIS REPOSITORY IS DEPRECATED. USE THE MODULE `keras.applications` INSTEAD.**

Pull requests will not be reviewed nor merged. Direct any PRs to `keras.applications`. Issues are not monitored either.

----

This repository contains code for the following Keras models:

- VGG16
- VGG19
- ResNet50
- Inception v3
- CRNN for music tagging

All architectures are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image dimension ordering set in your Keras configuration file at `~/.keras/keras.json`. For instance, if you have set `image_dim_ordering=tf`, then any model loaded from this repository will get built according to the TensorFlow dimension ordering convention, ""Width-Height-Depth"".

Pre-trained weights can be automatically loaded upon instantiation (`weights='imagenet'` argument in model constructor for all image models, `weights='msd'` for the music tagging model). Weights are"
Anti-Anti-Spider,"## åŸºäºCNNçš„éªŒè¯ç å›¾ç‰‡è¯†åˆ«
### ç®€ä»‹
	æœ¬é¡¹ç›®é‡‡ç”¨alexnetæ¨¡å‹å’Œletnetæ¨¡å‹ï¼Œå¯æ ¹æ®å®é™…éœ€è¦é€‰æ‹©(åœ¨train_model.pyä¸­çš„trainå‡½æ•°ä¿®æ”¹å³å¯)95.5%
### ä½œè€…æœ‰è¯è¯´
	ä¸çŸ¥ä¸è§‰è¿™ä¸ªgitåº“ä¼´éšæˆ‘ä»16åˆ°åˆ°20å¹´ï¼Œå¸¦ç»™æˆ‘è‡ªå·±æœ€æ£’çš„ä¸€æ®µäººç”Ÿæ—…ç¨‹ï¼Œ
	æ•´ç†äº†è¿™ä»½æ–‡æ¡£ï¼Œå¸Œæœ›ä»»ä½•æƒ³å­¦ä¹ å›¾ç‰‡è¯†åˆ«ï¼Œç©ç©å·ç§¯ç¥ç»ç½‘ç»œçš„åŒå­¦å¯ä»¥æœ€ä¾¿æ·çš„ä¸Šæ‰‹ä½“éªŒã€‚
	è¯·è°¨æ…ä½¿ç”¨æŠ€æœ¯ï¼Œä»…æ”¯æŒå­¦ä¹ ï¼Œä¸æ”¯æŒä»»ä½•é»‘ç°äº§ç›¸å…³
	å¯å‚çœ‹ï¼šhttps://www.urlteam.cn/?p=1893 https://www.urlteam.cn/?p=1406
	åŸå…ˆçš„Anti-Anti-Spider å…¨éƒ¨å†…å®¹ç§»åŠ¨åˆ° åŸAnti-Anti-Spider ç›®å½•ä¸‹
	æœ‰ä½•ç–‘é—®å¯é‚®ä»¶ 543429245@qq.com å’¨è¯¢
	æ¨¡å‹æ–‡ä»¶ä¸‹è½½ å¦‚æœå‡ºç°æ— æ³•è§£å‹ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
	https://www.urlteam.cn/%E5%8F%AF%E7%94%A8%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%8E%E8%AE%AD%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B.zip

#### Alexnet æ¨¡å‹ç»“æ„

![](src/READMEIMG2.PNG)

æ ¹æ®éªŒè¯ç çš„å¤æ‚åº¦ä¸åŒï¼Œè®­ç»ƒçš„æ—¶é—´ä¹Ÿä¼šæœ‰è¾ƒå¤§çš„ä¸åŒ
![](src/READMEIMG1.PNG)

###  ä½¿ç”¨æ–¹æ³•
	1.å¼€å§‹è®­ç»ƒæ ·æœ¬å‰ï¼Œä¿®æ”¹conf/config.json
	2.å°†é¢„å¤„ç†è¿‡çš„æ•°æ®é›†åˆ†æˆéªŒè¯é›†å’Œè®­ç»ƒé›†ï¼Œæ”¾åˆ°sampleç›®å½•ä¸‹
	3.è¿è¡Œtrain_model.pyå¼€å§‹è®­ç»ƒï¼Œè®­ç»ƒå®Œæˆçš„æ¨¡å‹ä¿å­˜è‡³model_resultä¸­
	4.å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ”¾ç½®model_resultï¼Œè¿è¡Œcnn_models/recognition.pyï¼Œé€‰å®šéªŒè¯ç ï¼Œå³å¯çœ‹åˆ°æ¨¡å‹æ•ˆæœ
### ç¯å¢ƒé…ç½®
TensorFlow CPUç‰ˆæœ¬å®‰è£…ï¼š`pip install tensorflow==1.9.0`
TensorFlow GPUç‰ˆæœ¬å®‰è£…ï¼š`pip install tensorflow-gpu==1.9.0`
GUPç‰ˆæœ¬çš„å®‰è£…æ¯”è¾ƒéº»çƒ¦ï¼Œéœ€è¦å®‰è£…CUDAå’ŒcuDNNæ‰èƒ½ä½¿tensorflowè°ƒåŠ¨GPU
ä¸‹å›¾ä¸ºTensorFlowï¼ŒPythonï¼ŒCUDAä¸cuDNNä¹‹é—´çš„ç‰ˆæœ¬å¯¹åº”å…³ç³»ï¼š
"
clone-voice,"[English README](./README_EN.md)  / [æåŠ©é¡¹ç›®](https://github.com/jianchang512/pyvideotrans/issues/80) / [Discord](https://discord.gg/7ZWbwKGMcx)

# CVå£°éŸ³å…‹éš†å·¥å…·

> æœ¬é¡¹ç›®æ‰€ç”¨æ¨¡å‹ä¸º[coqui.ai](https://coqui.ai/)å‡ºå“çš„xtts_v2ï¼Œæ¨¡å‹å¼€æºåè®®ä¸º[Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt),ä½¿ç”¨æœ¬é¡¹ç›®è¯·éµå¾ªè¯¥åè®®ï¼Œåè®®å…¨æ–‡è§ https://coqui.ai/cpml.txt


 è¿™æ˜¯ä¸€ä¸ªå£°éŸ³å…‹éš†å·¥å…·ï¼Œå¯ä½¿ç”¨ä»»ä½•äººç±»éŸ³è‰²ï¼Œå°†ä¸€æ®µæ–‡å­—åˆæˆä¸ºä½¿ç”¨è¯¥éŸ³è‰²è¯´è¯çš„å£°éŸ³ï¼Œæˆ–è€…å°†ä¸€ä¸ªå£°éŸ³ä½¿ç”¨è¯¥éŸ³è‰²è½¬æ¢ä¸ºå¦ä¸€ä¸ªå£°éŸ³ã€‚
 
 ä½¿ç”¨éå¸¸ç®€å•ï¼Œæ²¡æœ‰Nå¡GPUä¹Ÿå¯ä»¥ä½¿ç”¨ï¼Œä¸‹è½½é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ŒåŒå‡» app.exe æ‰“å¼€ä¸€ä¸ªwebç•Œé¢ï¼Œé¼ æ ‡ç‚¹ç‚¹å°±èƒ½ç”¨ã€‚
 
 æ”¯æŒ **ä¸­ã€è‹±ã€æ—¥ã€éŸ©ã€æ³•ã€å¾·ã€æ„ç­‰16ç§è¯­è¨€**ï¼Œå¯åœ¨çº¿ä»éº¦å…‹é£å½•åˆ¶å£°éŸ³ã€‚
 
 ä¸ºä¿è¯åˆæˆæ•ˆæœï¼Œå»ºè®®å½•åˆ¶æ—¶é•¿5ç§’åˆ°20ç§’ï¼Œå‘éŸ³æ¸…æ™°å‡†ç¡®ï¼Œä¸è¦å­˜åœ¨èƒŒæ™¯å™ªå£°ã€‚
 
 è‹±æ–‡æ•ˆæœå¾ˆæ£’ï¼Œä¸­æ–‡æ•ˆæœè¿˜å‡‘åˆã€‚


> **[èµåŠ©å•†]**
> 
> [![](https://github.com/user-attachments/assets/e3e2e6f9-e2e4-44e4-860b-9d1ce5b53d4f)](https://302.ai/)
>  [302.AI](https://302.ai)æ˜¯ä¸€ä¸ªæ±‡é›†å…¨çƒé¡¶çº§å“ç‰Œçš„AIè¶…å¸‚ï¼ŒæŒ‰éœ€ä»˜è´¹ï¼Œé›¶æœˆè´¹ï¼Œé›¶é—¨æ§›ä½¿ç”¨å„ç§ç±»å‹AIã€‚
> 
> åŠŸèƒ½å…¨é¢: å°†æœ€å¥½ç”¨çš„AIé›†æˆåˆ°åœ¨å¹³å°ä¹‹ä¸Šï¼ŒåŒ…æ‹¬ä¸é™äºAIèŠå¤©ï¼Œå›¾ç‰‡ç”Ÿæˆï¼Œå›¾ç‰‡å¤„ç†ï¼Œè§†é¢‘ç”Ÿæˆï¼Œå…¨æ–¹ä½è¦†ç›–ã€‚
> 
> ç®€å•æ˜“ç”¨: æä¾›æœºå™¨äººï¼Œå·¥å…·å’ŒAPIå¤šç§ä½¿ç”¨æ–¹æ³•ï¼Œå¯ä»¥æ»¡è¶³ä»å°ç™½åˆ°å¼€å‘è€…å¤šç§è§’è‰²çš„éœ€æ±‚ã€‚
> 
> æŒ‰éœ€ä»˜è´¹é›¶é—¨æ§›: ä¸æä¾›æœˆä»˜å¥—é¤ï¼Œå¯¹äº§å“ä¸è®¾ä»»ä½•é—¨æ§›ï¼ŒæŒ‰éœ€ä»˜è´¹ï¼Œå…¨éƒ¨å¼€æ”¾ã€‚å……å€¼ä½™é¢æ°¸ä¹…æœ‰æ•ˆã€‚
> 
> ç®¡ç†è€…å’Œä½¿ç”¨è€…åˆ†ç¦»ï¼š ç®¡ç†è€…ä¸€é”®åˆ†äº«ï¼Œä½¿ç”¨è€…æ— éœ€ç™»å½•ã€‚


# è§†é¢‘æ¼”ç¤º


https://github.com/jianchang512/clone-voice/assets/3378335/"
starcoder,"# ğŸ’« StarCoder

[Paper](https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view) | [Model](https://huggingface.co/bigcode/starcoder) | [Playground](https://huggingface.co/spaces/bigcode/bigcode-playground) | [VSCode](https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode) | [Chat](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)

# What is this about?
ğŸ’« StarCoder is a language model (LM) trained on source code and natural language text. Its training data incorporates more that 80 different programming languages as well as text extracted from GitHub issues and commits and from notebooks. This repository showcases how we get an overview of this LM's capabilities.

# News

* **May 9, 2023:** We've fine-tuned StarCoder to act as a helpful coding assistant ğŸ’¬! Check out the `chat/` directory for the training code and play with the model [here](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground).

# Disclaimer

Before you "
EmotiVoice,"<div align=""center"">
<a href=""https://trendshift.io/repositories/4833"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/4833"" alt=""netease-youdao%2FEmotiVoice | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>

<font size=4> README: EN | <a href=""./README.zh.md"">ä¸­æ–‡</a>  </font>
    <h1>EmotiVoice ğŸ˜Š: a Multi-Voice and Prompt-Controlled TTS Engine</h1>
</div>

<div align=""center"">
    <a href=""./README.zh.md""><img src=""https://img.shields.io/badge/README-ä¸­æ–‡ç‰ˆæœ¬-red""></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href=""./LICENSE""><img src=""https://img.shields.io/badge/license-Apache--2.0-yellow""></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href=""https://twitter.com/YDopensource""><img src=""https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}""></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
</div>
<br>

**EmotiVoice** is a powerful and modern open-source text-to-speech engine that is available to you at no cost. EmotiVoice speaks bot"
CenterNet,"# Objects as Points
Object detection, 3D detection, and pose estimation using center point detection:
![](readme/fig2.png)
> [**Objects as Points**](http://arxiv.org/abs/1904.07850),            
> Xingyi Zhou, Dequan Wang, Philipp Kr&auml;henb&uuml;hl,        
> *arXiv technical report ([arXiv 1904.07850](http://arxiv.org/abs/1904.07850))*         


Contact: [zhouxy2017@gmail.com](mailto:zhouxy2017@gmail.com). Any questions or discussions are welcomed! 

## Updates

 - (June, 2020) We released a state-of-the-art Lidar-based 3D detection and tracking framework [CenterPoint](https://github.com/tianweiy/CenterPoint).
 - (April, 2020) We released a state-of-the-art (multi-category-/ pose-/ 3d-) tracking extension [CenterTrack](https://github.com/xingyizhou/CenterTrack).

## Abstract 

Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, ineffici"
programming-talks,"# Programming Talks

I watch a lot of talks that I love to share with my friends, fellows and coworkers.
As I consider all GitHubbers my friends (oh yeah!), I decided it's time to share the
list.

There are talks on programming language specifics as well as a more general section I call ""theory"".
But don't expect to always get theoretical computer science for every talk there;
most of them are on the architecture and design of software.

I welcome every contribution to the list; for guidelines look [below](#contributing).

**Disclaimer:** I did not give any of the talks on the list and am responsible neither
for their content nor for their presentation. All links below will direct you to
external sites (mostly YouTube, really), be aware of that. If you are one of the people
responsible for the talks or the platform presenting it and want it removed,
tell me and I'll sort it out with you.

**[A]** after a talk name denotes a talk that *someone* thought could be listened to as audio, wit"
stanza,"<div align=""center""><img src=""https://github.com/stanfordnlp/stanza/raw/dev/images/stanza-logo.png"" height=""100px""/></div>

<h2 align=""center"">Stanza: A Python NLP Library for Many Human Languages</h2>

<div align=""center"">
    <a href=""https://github.com/stanfordnlp/stanza/actions"">
       <img alt=""Run Tests"" src=""https://github.com/stanfordnlp/stanza/actions/workflows/stanza-tests.yaml/badge.svg"">
    </a>
    <a href=""https://pypi.org/project/stanza/"">
        <img alt=""PyPI Version"" src=""https://img.shields.io/pypi/v/stanza?color=blue"">
    </a>
    <a href=""https://anaconda.org/stanfordnlp/stanza"">
        <img alt=""Conda Versions"" src=""https://img.shields.io/conda/vn/stanfordnlp/stanza?color=blue&label=conda"">
    </a>
    <a href=""https://pypi.org/project/stanza/"">
        <img alt=""Python Versions"" src=""https://img.shields.io/pypi/pyversions/stanza?colorB=blue"">
    </a>
</div>

The Stanford NLP Group's official Python NLP library. It contains support for running various accur"
Omost,"# Omost

Omost is a project to convert LLM's coding capability to image generation (or more accurately, image composing) capability. 

The name `Omost` (pronunciation: almost) has two meanings: 1) everytime after you use Omost, your image is almost there; 2) the `O` mean ""omni"" (multi-modal) and `most` means we want to get the most out of it.

Omost provides LLMs models that will write codes to compose image visual contents with Omost's virtual `Canvas` agent. This `Canvas` can be rendered by specific implementations of image generators to actually generate images.

Currently, we provide 3 pretrained LLM models based on variations of Llama3 and Phi3 (see also the model notes at the end of this page).

All models are trained with mixed data of (1) ground-truth annotations of several datasets including Open-Images, (2) extracted data by automatically annotating images, (3) reinforcement from DPO (Direct Preference Optimization, ""whether the codes can be compiled by python 3.10 or not"" as"
featuretools,"<p align=""center"">
<img width=50% src=""https://www.featuretools.com/wp-content/uploads/2017/12/FeatureLabs-Logo-Tangerine-800.png"" alt=""Featuretools"" />
</p>
<p align=""center"">
<i>""One of the holy grails of machine learning is to automate more and more of the feature engineering process.""</i> â€• Pedro Domingos, <a href=""https://bit.ly/things_to_know_ml"">A Few Useful Things to Know about Machine Learning</a>
</p>

<p align=""center"">
    <a href=""https://github.com/alteryx/featuretools/actions/workflows/tests_with_latest_deps.yaml"" alt=""Tests"" target=""_blank"">
        <img src=""https://github.com/alteryx/featuretools/actions/workflows/tests_with_latest_deps.yaml/badge.svg?branch=main"" alt=""Tests"" />
    </a>
    <a href=""https://codecov.io/gh/alteryx/featuretools"">
        <img src=""https://codecov.io/gh/alteryx/featuretools/branch/main/graph/badge.svg""/>
    </a>
    <a href='https://featuretools.alteryx.com/en/stable/?badge=stable'>
        <img src='https://readthedocs.com/projects/fea"
WeixinBot,"# WeixinBot [![star this repo](http://github-svg-buttons.herokuapp.com/star.svg?user=Urinx&repo=WeixinBot&style=flat&background=1081C1)](http://github.com/Urinx/WeixinBot) [![fork this repo](http://github-svg-buttons.herokuapp.com/fork.svg?user=Urinx&repo=WeixinBot&style=flat&background=1081C1)](http://github.com/Urinx/WeixinBot/fork) ![python](https://img.shields.io/badge/python-2.7%20&%203.6-ff69b4.svg)

ç½‘é¡µç‰ˆå¾®ä¿¡APIï¼ŒåŒ…å«ç»ˆç«¯ç‰ˆå¾®ä¿¡åŠå¾®ä¿¡æœºå™¨äºº

## Contents
* [Demo](#Demo)
* [Web Weixin Pipeline](#Web-Weixin-Pipeline)
* [Web Weixin API](#Web-Weixin-API)
* [Discussion Group](#Discussion-Group)
* [Recent Update](#Recent-Update)

## <a name=""Demo"">Demo</a>
ä¸ºäº†ç¡®ä¿èƒ½æ­£å¸¸è¿è¡Œç¤ºä¾‹è„šæœ¬ï¼Œè¯·å®‰è£…æ‰€éœ€çš„ç¬¬ä¸‰æ–¹åŒ…ã€‚

```
pip install -r requirements.txt
```

æ³¨ï¼šä¸‹é¢æ¼”ç¤ºçš„å›¾ç‰‡ä¸åŠŸèƒ½å¯èƒ½ä¸æ˜¯æœ€æ–°çš„ï¼Œå…·ä½“è¯·çœ‹æºç ã€‚

<div align=center>
<img src=""imgs/1.png"" width=""500"" height=""550""/>
</div>

æŒ‰ç…§æ“ä½œæŒ‡ç¤ºåœ¨æ‰‹æœºå¾®ä¿¡ä¸Šæ‰«æäºŒç»´ç ç„¶åç™»å½•ï¼Œä½ å¯ä»¥é€‰æ‹©æ˜¯å¦å¼€å¯è‡ªåŠ¨å›å¤æ¨¡å¼ã€‚

![2](imgs/2.png)

å¼€å¯è‡ªåŠ¨å›å¤æ¨¡å¼åï¼Œå¦‚æœæ¥æ”¶åˆ°çš„æ˜¯æ–‡å­—æ¶ˆæ¯å°±ä¼šè‡ªåŠ¨å›å¤ï¼ŒåŒ…æ‹¬ç¾¤æ¶ˆæ¯ã€‚

![3](imgs/3.png)

åç‰‡ï¼Œé“¾æ¥ï¼ŒåŠ¨ç”»è¡¨æƒ…å’Œåœ°å€ä½ç½®æ¶ˆæ¯ã€‚

![4](imgs/4.png)

![5](imgs/5.png)

ç½‘é¡µç‰ˆä¸Šæœ‰çš„åŠŸèƒ½ç›®å‰åŸºæœ¬ä¸Š"
hyperopt,"
# Hyperopt: Distributed Hyperparameter Optimization

<p align=""center"">
<img src=""https://i.postimg.cc/TPmffWrp/hyperopt-new.png"" />
</p>

[![build](https://github.com/hyperopt/hyperopt/actions/workflows/build.yml/badge.svg)](https://github.com/hyperopt/hyperopt/actions/workflows/build.yml)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/hyperopt/hyperopt/master.svg)](https://results.pre-commit.ci/latest/github/hyperopt/hyperopt/master)
[![PyPI version](https://badge.fury.io/py/hyperopt.svg)](https://badge.fury.io/py/hyperopt)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/hyperopt/badges/version.svg)](https://anaconda.org/conda-forge/hyperopt)

[Hyperopt](https://github.com/hyperopt/hyperopt) is a Python library for serial and parallel optimization over awkward
search spaces, which may include real-valued, discrete, and conditional
dimensions.

## Getting started

Install hyperopt from PyPI

```bash
pip install hyperopt
```

to run your first example

"
volatility,"============================================================================
Volatility Framework - Volatile memory extraction utility framework
============================================================================

The Volatility Framework is a completely open collection of tools,
implemented in Python under the GNU General Public License, for the
extraction of digital artifacts from volatile memory (RAM) samples.
The extraction techniques are performed completely independent of the
system being investigated but offer visibilty into the runtime state
of the system. The framework is intended to introduce people to the
techniques and complexities associated with extracting digital artifacts
from volatile memory samples and provide a platform for further work into
this exciting area of research.

The Volatility distribution is available from: 
http://www.volatilityfoundation.org/#!releases/component_71401

Volatility should run on any platform that supports 
Python (http://www.pyt"
lbry-sdk,"# <img src=""https://raw.githubusercontent.com/lbryio/lbry-sdk/master/lbry.png"" alt=""LBRY"" width=""48"" height=""36"" /> LBRY SDK [![build](https://github.com/lbryio/lbry-sdk/actions/workflows/main.yml/badge.svg)](https://github.com/lbryio/lbry-sdk/actions/workflows/main.yml) [![coverage](https://coveralls.io/repos/github/lbryio/lbry-sdk/badge.svg)](https://coveralls.io/github/lbryio/lbry-sdk)

LBRY is a decentralized peer-to-peer protocol for publishing and accessing digital content. It utilizes the [LBRY blockchain](https://github.com/lbryio/lbrycrd) as a global namespace and database of digital content. Blockchain entries contain searchable content metadata, identities, rights and access rules. LBRY also provides a data network that consists of peers (seeders) uploading and downloading data from other peers, possibly in exchange for payments, as well as a distributed hash table used by peers to discover other peers.

LBRY SDK for Python is currently the most fully featured implementation"
aws-shell,"aws-shell - The interactive productivity booster for the AWS CLI
================================================================

.. image:: https://aws-developer-blog-media.s3-us-west-2.amazonaws.com/cli/Super-Charge-Your-AWS-Command-Line-Experience-with-aws-shell/aws-shell-final.gif


Installation
============

The aws-shell requires python and `pip`_ to install.
You can install the aws-shell using `pip`_::

    $ pip install aws-shell

If you are not installing into a virtualenv you can run::

    $ sudo pip install aws-shell

**Mac OS X (10.11 El Capitan) users**: There is a known issue with Apple and
its included python package dependencies (more info at
https://github.com/pypa/pip/issues/3165).
We are investigating ways to fix this issue but in the meantime,
to install the aws-shell, you can run:
``sudo pip install aws-shell --upgrade --ignore-installed six``

Once you've installed the aws-shell, you can now run::

    $ aws-shell

To exit the shell, press ``Ctrl-D``.

Upgrading"
mae,"## Masked Autoencoders: A PyTorch Implementation

<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png"" width=""480"">
</p>


This is a PyTorch/GPU re-implementation of the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377):
```
@Article{MaskedAutoencoders2021,
  author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll{\'a}r and Ross Girshick},
  journal = {arXiv:2111.06377},
  title   = {Masked Autoencoders Are Scalable Vision Learners},
  year    = {2021},
}
```

* The original implementation was in TensorFlow+TPU. This re-implementation is in PyTorch+GPU.

* This repo is a modification on the [DeiT repo](https://github.com/facebookresearch/deit). Installation and preparation follow that repo.

* This repo is based on [`timm==0.3.2`](https://github.com/rwightman/pytorch-image-models), for which a [fix](https://github.com/rwightman/pytorch-im"
awesome-cheatsheet,"<img src=""https://cdn.rawgit.com/detailyang/awesome-cheatsheet/master/awesome.svg"" alt=""awesome"" width=""120"" align=""right"" >

# Awesome Cheatsheet 

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) ![Branch master](https://img.shields.io/badge/branch-master-brightgreen.svg?style=flat-square) [![Build Status](https://api.travis-ci.org/detailyang/awesome-cheatsheet.svg)](https://travis-ci.org/detailyang/awesome-cheatsheet)    [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/detailyang/awesome-cheatsheet/master/LICENSE)
> List of useful cheatsheets

Inspired by [@sindresorhus](https://github.com/sindresorhus) [awesome](https://github.com/sindresorhus/awesome) and improved by these **[amazing contributors](https://github.com/detailyang/awesome-cheatsheet/graphs/contributors)**.

#### *If you see a link here is not fit, you can fi"
latexify_py,"# latexify

[![Python](https://img.shields.io/pypi/pyversions/latexify-py.svg)](https://pypi.org/project/latexify-py/)
[![PyPI Latest Release](https://img.shields.io/pypi/v/latexify-py.svg)](https://pypi.org/project/latexify-py/)
[![License](https://img.shields.io/pypi/l/latexify-py.svg)](https://github.com/google/latexify_py/blob/main/LICENSE)
[![Downloads](https://pepy.tech/badge/latexify-py/month)](https://pepy.tech/project/latexify-py)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)

`latexify` is a Python package to compile a fragment of Python source code to a
corresponding $\LaTeX$ expression:

![Example of latexify usage](https://raw.githubusercontent.com/google/latexify_py/main/example.jpg)

`latexify` provides the following functionalities:

* Libraries to compile Python sourc"
records,"# Records: SQL for Humansâ„¢

[![image](https://img.shields.io/pypi/v/records.svg)](https://pypi.python.org/pypi/records)

**Records is a very simple, but powerful, library for making raw SQL
queries to most relational databases.**

![image](https://farm1.staticflickr.com/569/33085227621_7e8da49b90_k_d.jpg)

Just write SQL. No bells, no whistles. This common task can be
surprisingly difficult with the standard tools available. This library
strives to make this workflow as simple as possible, while providing an
elegant interface to work with your query results.

*Database support includes RedShift, Postgres, MySQL, SQLite, Oracle,
and MS-SQL (drivers not included).*

## â˜¤ The Basics

We know how to write SQL, so let's send some to our database:

``` python
import records

db = records.Database('postgres://...')
rows = db.query('select * from active_users')    # or db.query_file('sqls/active-users.sql')
```

Grab one row at a time:

``` python
>>> rows[0]
<Record {""username"": ""model-t"", ""a"
Ultra-Light-Fast-Generic-Face-Detector-1MB,"[English](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB ) | [ä¸­æ–‡ç®€ä½“](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/README_CN.md)
# Ultra-Light-Fast-Generic-Face-Detector-1MB 
# Ultra-lightweight face detection model
![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/27.jpg)
This model is a lightweight facedetection model designed for edge computing devices.

- In terms of model size, the default FP32 precision (.pth) file size is **1.04~1.1MB**, and the inference framework int8 quantization size is about **300KB**.
- In terms of the calculation amount of the model, the input resolution of 320x240 is about **90~109 MFlops**.
- There are two versions of the model, version-slim (network backbone simplification,slightly faster) and version-RFB (with the modified RFB module, higher precision).
- Widerface training pre-training model with different input resolutions of 320x240 and 640"
keras-yolo3,"# keras-yolo3

[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](LICENSE)

## Introduction

A Keras implementation of YOLOv3 (Tensorflow backend) inspired by [allanzelener/YAD2K](https://github.com/allanzelener/YAD2K).


---

## Quick Start

1. Download YOLOv3 weights from [YOLO website](http://pjreddie.com/darknet/yolo/).
2. Convert the Darknet YOLO model to a Keras model.
3. Run YOLO detection.

```
wget https://pjreddie.com/media/files/yolov3.weights
python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5
python yolo_video.py [OPTIONS...] --image, for image detection mode, OR
python yolo_video.py [video_path] [output_path (optional)]
```

For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with `--model model_file` and `--anchors anchor_file`.

### Usage
Use --help to see usage of yolo_video.py:
```
usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]
                     [--classes CLASSES] [--gpu_num GPU_NUM] ["
MachineLearning_Python,"æœºå™¨å­¦ä¹ ç®—æ³•Pythonå®ç°
=========

[![MIT license](https://img.shields.io/dub/l/vibe-d.svg)](https://github.com/lawlite19/MachineLearning_Python/blob/master/LICENSE)

## ç›®å½•
* [æœºå™¨å­¦ä¹ ç®—æ³•Pythonå®ç°](#æœºå™¨å­¦ä¹ ç®—æ³•pythonå®ç°)
	* [ä¸€ã€çº¿æ€§å›å½’](#ä¸€çº¿æ€§å›å½’)
		* [1ã€ä»£ä»·å‡½æ•°](#1ä»£ä»·å‡½æ•°)
		* [2ã€æ¢¯åº¦ä¸‹é™ç®—æ³•](#2æ¢¯åº¦ä¸‹é™ç®—æ³•)
		* [3ã€å‡å€¼å½’ä¸€åŒ–](#3å‡å€¼å½’ä¸€åŒ–)
		* [4ã€æœ€ç»ˆè¿è¡Œç»“æœ](#4æœ€ç»ˆè¿è¡Œç»“æœ)
		* [5ã€ä½¿ç”¨scikit-learnåº“ä¸­çš„çº¿æ€§æ¨¡å‹å®ç°](#5ä½¿ç”¨scikit-learnåº“ä¸­çš„çº¿æ€§æ¨¡å‹å®ç°)
	* [äºŒã€é€»è¾‘å›å½’](#äºŒé€»è¾‘å›å½’)
		* [1ã€ä»£ä»·å‡½æ•°](#1ä»£ä»·å‡½æ•°)
		* [2ã€æ¢¯åº¦](#2æ¢¯åº¦)
		* [3ã€æ­£åˆ™åŒ–](#3æ­£åˆ™åŒ–)
		* [4ã€Så‹å‡½æ•°ï¼ˆå³ï¼‰](#4så‹å‡½æ•°å³)
		* [5ã€æ˜ å°„ä¸ºå¤šé¡¹å¼](#5æ˜ å°„ä¸ºå¤šé¡¹å¼)
		* [6ã€ä½¿ç”¨çš„ä¼˜åŒ–æ–¹æ³•](#6ä½¿ç”¨scipyçš„ä¼˜åŒ–æ–¹æ³•)
		* [7ã€è¿è¡Œç»“æœ](#7è¿è¡Œç»“æœ)
		* [8ã€ä½¿ç”¨scikit-learnåº“ä¸­çš„é€»è¾‘å›å½’æ¨¡å‹å®ç°](#8ä½¿ç”¨scikit-learnåº“ä¸­çš„é€»è¾‘å›å½’æ¨¡å‹å®ç°)
	* [é€»è¾‘å›å½’_æ‰‹å†™æ•°å­—è¯†åˆ«_OneVsAll](#é€»è¾‘å›å½’_æ‰‹å†™æ•°å­—è¯†åˆ«_onevsall)
		* [1ã€éšæœºæ˜¾ç¤º100ä¸ªæ•°å­—](#1éšæœºæ˜¾ç¤º100ä¸ªæ•°å­—)
		* [2ã€OneVsAll](#2onevsall)
		* [3ã€æ‰‹å†™æ•°å­—è¯†åˆ«](#3æ‰‹å†™æ•°å­—è¯†åˆ«)
		* [4ã€é¢„æµ‹](#4é¢„æµ‹)
		* [5ã€è¿è¡Œç»“æœ](#5è¿è¡Œç»“æœ)
		* [6ã€ä½¿ç”¨scikit-learnåº“ä¸­çš„é€»è¾‘å›å½’æ¨¡å‹å®ç°](#6ä½¿ç”¨scikit-learnåº“ä¸­çš„é€»è¾‘å›å½’æ¨¡å‹å®ç°)
	* [ä¸‰ã€BPç¥ç»ç½‘ç»œ](#ä¸‰bpç¥ç»ç½‘ç»œ)
		* [1ã€ç¥ç»ç½‘ç»œmodel](#1ç¥ç»ç½‘ç»œmodel)
		* [2ã€ä»£ä»·å‡½æ•°](#2ä»£ä»·å‡½æ•°)
		* [3ã€æ­£åˆ™åŒ–](#3æ­£åˆ™åŒ–)
		* [4ã€åå‘ä¼ æ’­BP](#4åå‘ä¼ æ’­bp)
		* [5ã€BPå¯ä»¥æ±‚æ¢¯åº¦çš„åŸå› ](#5bpå¯ä»¥æ±‚æ¢¯"
bup,"bup: It backs things up
=======================

bup is a program that backs things up.  It's short for ""backup."" Can you
believe that nobody else has named an open source program ""bup"" after all
this time?  Me neither.

Despite its unassuming name, bup is pretty cool.  To give you an idea of
just how cool it is, I wrote you this poem:

                             Bup is teh awesome
                          What rhymes with awesome?
                            I guess maybe possum
                           But that's irrelevant.
			
Hmm.  Did that help?  Maybe prose is more useful after all.


Reasons bup is awesome
----------------------

bup has a few advantages over other backup software:

 - It uses a rolling checksum algorithm (similar to rsync) to split large
   files into chunks.  The most useful result of this is you can backup huge
   virtual machine (VM) disk images, databases, and XML files incrementally,
   even though they're typically all in one huge file, and not use "
GPTCache,"# GPTCache : A Library for Creating Semantic Cache for LLM Queries
Slash Your LLM API Costs by 10x ğŸ’°, Boost Speed by 100x âš¡ 

[![Release](https://img.shields.io/pypi/v/gptcache?label=Release&color&logo=Python)](https://pypi.org/project/gptcache/)
[![pip download](https://img.shields.io/pypi/dm/gptcache.svg?color=bright-green&logo=Pypi)](https://pypi.org/project/gptcache/)
[![Codecov](https://img.shields.io/codecov/c/github/zilliztech/GPTCache/dev?label=Codecov&logo=codecov&token=E30WxqBeJJ)](https://codecov.io/gh/zilliztech/GPTCache)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/license/mit/)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/zilliz_universe.svg?style=social&label=Follow%20%40Zilliz)](https://twitter.com/zilliz_universe)
[![Discord](https://img.shields.io/discord/1092648432495251507?label=Discord&logo=discord)](https://discord.gg/Q8C6WEjSWV)

ğŸ‰ GPTCache has been fully integrated with ğŸ¦œï¸ğŸ”—[LangChain](https://github.c"
linux-insides-zh,"# Linux å†…æ ¸æ­ç§˜

ä¸€ç³»åˆ—å…³äº Linux å†…æ ¸å’Œå…¶å†…åœ¨æœºç†çš„å¸–å­ã€‚

**ç›®çš„å¾ˆç®€å•** - åˆ†äº«æˆ‘å¯¹ Linux å†…æ ¸æœºç†çš„ä¸€äº›æµ…è§ï¼Œå¸®åŠ©è¯»è€…ç†è§£ Linux å†…æ ¸æœºç†å’Œå…¶ä»–åº•å±‚å†…å®¹ã€‚

**é—®é¢˜/å»ºè®®**: å¦‚æœ‰ç›¸å…³é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ issueã€‚ä¸€æ–¹é¢ï¼Œå¯¹äºè‹±æ–‡åŸæ–‡é—®é¢˜ï¼Œè¯·åœ¨ä¸Šæ¸¸ä»“åº“ - [linux-insides](https://github.com/0xAX/linux-insides) ä¸­æäº¤ issueï¼›å¦ä¸€æ–¹é¢ï¼Œå¯¹äºä¸­æ–‡ç¿»è¯‘é—®é¢˜ï¼Œè¯·åœ¨ä¸‹æ¸¸ä»“åº“ - [linux-insides-zh](https://github.com/hust-open-atom-club/linux-insides-zh) ä¸­æäº¤ issueã€‚

## è´¡çŒ®

å¦‚æœ‰ç›¸å…³é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ä¸åæŒ‡æ•™ï¼Œæäº¤ issues æˆ–è€… PRsã€‚å¯¹äº `linux-insides-zh` ç¿»è¯‘é¡¹ç›®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹æ³•è¿›è¡Œè´¡çŒ®ï¼š

- è‹±æ–‡ç¿»è¯‘ï¼Œç›®å‰åªæä¾›ç®€ä½“ä¸­æ–‡çš„è¯‘æ–‡ï¼›
- åŒæ­¥æœªè¢«ç¿»è¯‘çš„è‹±æ–‡åŸæœ¬ï¼Œå…¶å®å°±æ˜¯å°†ä¸Šæ¸¸è‹±æ–‡åŒæ­¥åˆ°æœ¬é¡¹ç›®ä¸­ï¼›
- æ›´æ–°å·²ç»ç¿»è¯‘çš„ä¸­æ–‡è¯‘æ–‡ï¼Œå…¶å®å°±æ˜¯æŸ¥çœ‹ä¸Šæ¸¸è‹±æ–‡çš„æ›´æ–°ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¯¹ä¸­æ–‡è¯‘æ–‡è¿›è¡Œæ›´æ–°ï¼›
- æ ¡å¯¹å½“å‰å·²ç»ç¿»è¯‘è¿‡çš„ä¸­æ–‡è¯‘æ–‡ï¼ŒåŒ…æ‹¬ä¿®æ”¹é”™åˆ«å­—ï¼Œæ¶¦è‰²ç­‰å·¥ä½œï¼›

ç›®å‰æœ¬é¡¹ç›®çš„**ç¿»è¯‘è¿›åº¦**ä¸**ç¿»è¯‘è®¤é¢†è§„åˆ™**ï¼Œè¯·æŸ¥çœ‹ [TRANSLATION_STATUS.md](TRANSLATION_STATUS.md)ã€‚

åœ¨å¼€å§‹ç¿»è¯‘ä¹‹å‰ï¼Œè¯·é˜…è¯» [CONTRIBUTING.md](CONTRIBUTING.md) ä¸ [TRANSLATION_NOTES.md](TRANSLATION_NOTES.md)ã€‚å…³äºç¿»è¯‘çº¦å®šçš„ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼ŒåŒæ ·è¯·æäº¤ issue è®¨è®ºã€‚

## é‚®ä»¶åˆ—è¡¨

æˆ‘ä»¬å¼€æºä¿±ä¹éƒ¨å†…éƒ¨æœ‰ä¸€ä¸ª[ Google Group é‚®ä»¶åˆ—è¡¨](https://groups.google.com/g/hust-os-kernel-patches)æ¥å­¦ä¹ å’Œè´¡çŒ® Linux å†…æ ¸æºç ã€‚

**åŠ å…¥é‚®ä»¶åˆ—è¡¨** å‘é€ä»»æ„ä¸»é¢˜/å†…å®¹çš„é‚®ä»¶åˆ° hust-os-kernel-patches+subscribe@googlegroups.comã€‚éšåï¼Œä½ å°†è·å¾—ä¸€å°ç¡®è®¤é‚®ä»¶ï¼Œå¹¶åŠ å…¥é‚®ä»¶åˆ—è¡¨ã€‚å¦‚æœä½ æœ‰è°·æ­Œè´¦å·ï¼Œä½ å¯ä»¥é€šè¿‡ä¸Šè¿°ç½‘å€ç›´æ¥åŠ å…¥æˆ‘ä»¬é‚®ä»¶åˆ—è¡¨ã€‚

## ä¸­æ–‡ç»´æŠ¤è€…

[@m"
Movie_Data_Capture,"<h1 align=""center"">Movie Data Capture</h1>

## ç½‘ç«™
[www.mvdc.top](https://www.mvdc.top)

## æ ¸å¿ƒä»£ç å¼€æº
"
WeasyPrint,"**The Awesome Document Factory**

WeasyPrint is a smart solution helping web developers to create PDF
documents. It turns simple HTML pages into gorgeous statistical reports,
invoices, ticketsâ€¦

From a technical point of view, WeasyPrint is a visual rendering engine for
HTML and CSS that can export to PDF. It aims to support web standards for
printing. WeasyPrint is free software made available under a BSD license.

It is based on various libraries but *not* on a full rendering engine like
WebKit or Gecko. The CSS layout engine is written in Python, designed for
pagination, and meant to be easy to hack on.

* Free software: BSD license
* For Python 3.9+, tested on CPython and PyPy
* Documentation: https://doc.courtbouillon.org/weasyprint
* Examples: https://weasyprint.org/#samples
* Changelog: https://github.com/Kozea/WeasyPrint/releases
* Code, issues, tests: https://github.com/Kozea/WeasyPrint
* Code of conduct: https://www.courtbouillon.org/code-of-conduct
* Professional support: ht"
LWM,"# Large World Model (LWM)

[[Project]](https://largeworldmodel.github.io/)
[[Paper]](https://arxiv.org/abs/2402.08268)
[[Models]](https://huggingface.co/LargeWorldModel)

**Large World Model (LWM)** is a general-purpose large-context multimodal autoregressive model. It is trained on a large dataset of diverse long videos and books using RingAttention, and can perform language, image, and video understanding and generation.


## Approach

<div align=""center"">
  <img src=""./imgs/data.png""/>
</div>

Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video"
gcn,"# Graph Convolutional Networks

This is a TensorFlow implementation of Graph Convolutional Networks for the task of (semi-supervised) classification of nodes in a graph, as described in our paper:
 
Thomas N. Kipf, Max Welling, [Semi-Supervised Classification with Graph Convolutional Networks](http://arxiv.org/abs/1609.02907) (ICLR 2017)

For a high-level explanation, have a look at our blog post:

Thomas Kipf, [Graph Convolutional Networks](http://tkipf.github.io/graph-convolutional-networks/) (2016)

## Installation

```bash
python setup.py install
```

## Requirements
* tensorflow (>0.12)
* networkx

## Run the demo

```bash
cd gcn
python train.py
```

## Data

In order to use your own data, you have to provide 
* an N by N adjacency matrix (N is the number of nodes), 
* an N by D feature matrix (D is the number of features per node), and
* an N by E binary label matrix (E is the number of classes).

Have a look at the `load_data()` function in `utils.py` for an example.

In this ex"
exo,"<div align=""center"">

<picture>
  <source media=""(prefers-color-scheme: light)"" srcset=""/docs/exo-logo-black-bg.jpg"">
  <img alt=""exo logo"" src=""/docs/exo-logo-transparent.png"" width=""50%"" height=""50%"">
</picture>

exo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).


<h3>

[Discord](https://discord.gg/EUnjGpsmWw) | [Telegram](https://t.me/+Kh-KqHTzFYg3MGNk) | [X](https://x.com/exolabs)

</h3>

[![GitHub Repo stars](https://img.shields.io/github/stars/exo-explore/exo)](https://github.com/exo-explore/exo/stargazers)
[![Tests](https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)

</div>

---

Forget expensive NVIDIA GPUs, unify your existing de"
ipwndfu,"![](repo/ipwndfu.png)
# Open-source jailbreaking tool for many iOS devices


**Read [disclaimer](#disclaimer) before using this software.*


## checkm8

* permanent unpatchable bootrom exploit for hundreds of millions of iOS devices

* meant for researchers, this is not a jailbreak with Cydia yet

* allows dumping SecureROM, decrypting keybags for iOS firmware, and demoting device for JTAG

* current SoC support: s5l8947x, s5l8950x, s5l8955x, s5l8960x, t8002, t8004, t8010, t8011, t8015

* future SoC support: s5l8940x, s5l8942x, s5l8945x, s5l8747x, t7000, t7001, s7002, s8000, s8001, s8003, t8012

* full jailbreak with Cydia on latest iOS version is possible, but requires additional work


## Quick start guide for checkm8

1. Use a cable to connect device to your Mac. Hold buttons as needed to enter DFU Mode.

2. First run ```./ipwndfu -p``` to exploit the device. Repeat the process if it fails, it is not reliable.

3. Run ```./ipwndfu --dump-rom``` to get a dump of SecureROM.

4. Run ``"
Chinese-LLaMA-Alpaca-2,"# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)é¡¹ç›®å¯åŠ¨ï¼

[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki) | [**â“æé—®/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/issues) | [**ğŸ’¬è®¨è®º/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/discussions) | [**âš”ï¸ç«æŠ€åœº/Arena**](http://llm-arena.ymcui.com/)

<p align=""center"">
    <br>
    <img src=""./pics/banner.png"" width=""800""/>
    <br>
</p>
<p align=""center"">
    <img alt=""GitHub"" src=""https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca-2.svg?color=blue&style=flat-square"">
    <img alt=""GitHub release (latest by date)"" src=""https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca-2"">
    <img alt=""GitHub top language"" src=""https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca-2"">
    <a href=""https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca-2/dashboard?utm_source=gh&utm_medium=re"
yowsup,"# yowsup [![Build Status](https://travis-ci.org/tgalal/yowsup.svg?branch=master)](https://travis-ci.org/tgalal/yowsup)

<a href=""https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=Z9KKEUVYEY6BN"" target=""_blank""><img src=""https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif"" /></a>

**For private consultancy feel free to directly schedule it over [codementor](https://www.codementor.io/@tgalal).**

---

yowsup is a python library that enables building applications that can communicate with WhatsApp users.
The project started as the protocol engine behind [Wazapp for Meego](https://wiki.maemo.org/Wazapp) and
[OpenWA for BB10](https://www.lowyat.net/2013/5896/try-this-openwhatsapp-for-blackberry-10/). Now as a standalone
library it can be used to power any custom WhatsApp client.

```
updated: 2021-12-14
yowsup version: 3.3.0
yowsup-cli version: 3.2.1
requires:
- python>=2.7,<=3.7
- consonance==0.1.5
- python-axolotl==0.2.2
- protobuf>=3.6.0
- six==1.10
uses:
 - ar"
gspread,"# Google Spreadsheets Python API v4

![main workflow](https://img.shields.io/github/actions/workflow/status/burnash/gspread/main.yaml?logo=github)
![GitHub licence](https://img.shields.io/pypi/l/gspread?logo=github)
![GitHub downloads](https://img.shields.io/github/downloads-pre/burnash/gspread/latest/total?logo=github)
![documentation](https://img.shields.io/readthedocs/gspread?logo=readthedocs)
![PyPi download](https://img.shields.io/pypi/dm/gspread?logo=pypi)
![PyPi version](https://img.shields.io/pypi/v/gspread?logo=pypi)
![python version](https://img.shields.io/pypi/pyversions/gspread?style=pypi)

Simple interface for working with Google Sheets.

Features:

- Open a spreadsheet by **title**, **key** or **URL**.
- Read, write, and format cell ranges.
- Sharing and access control.
- Batching updates.

## Installation

```sh
pip install gspread
```

Requirements: Python 3.8+.

## Basic Usage

1. [Create credentials in Google API Console](http://gspread.readthedocs.org/en/latest/oauth"
librosa,"[![librosa logo](docs/img/librosa_logo_text.svg)](https://librosa.org/)

# librosa


A python package for music and audio analysis.  

[![PyPI](https://img.shields.io/pypi/v/librosa.svg)](https://pypi.python.org/pypi/librosa)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/librosa/badges/version.svg)](https://anaconda.org/conda-forge/librosa)
[![License](https://img.shields.io/pypi/l/librosa.svg)](https://github.com/librosa/librosa/blob/main/LICENSE.md)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.591533.svg)](https://doi.org/10.5281/zenodo.591533)

[![CI](https://github.com/librosa/librosa/actions/workflows/ci.yml/badge.svg)](https://github.com/librosa/librosa/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/librosa/librosa/branch/main/graph/badge.svg?token=ULWnUHaIJC)](https://codecov.io/gh/librosa/librosa)
[![Docs](https://github.com/librosa/librosa/actions/workflows/docs.yml/badge.svg)](https://librosa.org/doc/latest/index.html)

#  Table of Contents

-"
PentestGPT,"<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->
<a name=""readme-top""></a>

<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown ""reference style"" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]



<!-- PROJECT LOGO -->
<br />
<div align=""center"">
  <a href=""https://github.com/GreyDGL/PentestGPT"">
  </a>

<h3 align=""center"">PentestGPT</h3>

  <p align=""center"">
    A "
frappe,"<div align=""center"">
	<picture>
		<source media=""(prefers-color-scheme: dark)"" srcset="".github/frappe-framework-logo-dark.svg"">
		<img src="".github/frappe-framework-logo.svg"" height=""50"">
	</picture>
	<h3>
		a web framework with <a href=""https://www.youtube.com/watch?v=LOjk3m0wTwg"">""batteries included""</a>
	</h3>
	<h5>
		it's pronounced - <em>fra-pay</em>
	</h5>
</div>

<div align=""center"">
	<a target=""_blank"" href=""#LICENSE"" title=""License: MIT"">
		<img src=""https://img.shields.io/badge/License-MIT-success.svg"">
	</a>
	<a target=""_blank"" href=""https://www.python.org/downloads/"" title=""Python version"">
		<img src=""https://img.shields.io/badge/python-%3E=_3.10-success.svg"">
	</a>
	<a href=""https://frappeframework.com/docs"">
		<img src=""https://img.shields.io/badge/docs-%F0%9F%93%96-success.svg""/>
	</a>
	<a href=""https://github.com/frappe/frappe/actions/workflows/server-tests.yml"">
		<img src=""https://github.com/frappe/frappe/actions/workflows/server-tests.yml/badge.svg"">
	</a>
	<a href="
dowhy,"|BuildStatus|_ |PyPiVersion|_ |PythonSupport|_ |Downloads|_ |discord|_

.. |PyPiVersion| image:: https://img.shields.io/pypi/v/dowhy.svg
.. _PyPiVersion: https://pypi.org/project/dowhy/

.. |PythonSupport| image:: https://img.shields.io/pypi/pyversions/dowhy.svg
.. _PythonSupport: https://pypi.org/project/dowhy/

.. |BuildStatus| image:: https://github.com/py-why/dowhy/actions/workflows/ci.yml/badge.svg
.. _BuildStatus: https://github.com/py-why/dowhy/actions

.. |Downloads| image:: https://pepy.tech/badge/dowhy
.. _Downloads: https://pepy.tech/project/dowhy

.. |discord| image:: https://img.shields.io/discord/818456847551168542
.. _discord: https://discord.gg/cSBGb3vsZb

.. image:: dowhy-logo-large.png
  :width: 50%
  :align: center


`Checkout the documentation <https://py-why.github.io/dowhy/>`_
===============================================================

- The documentation, user guide, sample notebooks and other information are available at
    `https://py-why.github.io/dowhy "
face-alignment,"# Face Recognition

Detect facial landmarks from Python using the world's most accurate face alignment network, capable of detecting points in both 2D and 3D coordinates.

Build using [FAN](https://www.adrianbulat.com)'s state-of-the-art deep learning based face alignment method. 

<p align=""center""><img src=""docs/images/face-alignment-adrian.gif"" /></p>

**Note:** The lua version is available [here](https://github.com/1adrianb/2D-and-3D-face-alignment).

For numerical evaluations it is highly recommended to use the lua version which uses indentical models with the ones evaluated in the paper. More models will be added soon.

[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)  [![Test Face alignmnet](https://github.com/1adrianb/face-alignment/workflows/Test%20Face%20alignmnet/badge.svg)](https://github.com/1adrianb/face-alignment/actions?query=workflow%3A%22Test+Face+alignmnet%22) [![Anaconda-Server Badge](https://a"
dotbot,"# Dotbot [![Build Status](https://github.com/anishathalye/dotbot/workflows/CI/badge.svg)](https://github.com/anishathalye/dotbot/actions?query=workflow%3ACI) [![Coverage](https://codecov.io/gh/anishathalye/dotbot/branch/master/graph/badge.svg)](https://app.codecov.io/gh/anishathalye/dotbot) [![PyPI](https://img.shields.io/pypi/v/dotbot.svg)](https://pypi.org/pypi/dotbot/) [![Python 3.6+](https://img.shields.io/badge/python-3.6%2B-blue)](https://pypi.org/pypi/dotbot/)

Dotbot makes installing your dotfiles as easy as `git clone $url && cd dotfiles
&& ./install`, even on a freshly installed system!

- [Rationale](#rationale)
- [Getting Started](#getting-started)
- [Configuration](#configuration)
- [Directives](#directives) ([Link](#link), [Create](#create), [Shell](#shell), [Clean](#clean), [Defaults](#defaults))
- [Plugins](#plugins)
- [Command-line Arguments](#command-line-arguments)
- [Wiki][wiki]

---

## Rationale

Dotbot is a tool that bootstraps your dotfiles (it's a [Dot]files
[b"
SciencePlots,"Science Plots
=============

<p align=""left"">
    <table>
        <tr>
            <td style=""text-align: center;"">PyPI version</td>
            <td style=""text-align: center;"">
                <a href=""https://badge.fury.io/py/SciencePlots"">
                    <img src=""https://badge.fury.io/py/SciencePlots.svg"" alt=""PyPI version"" height=""18""/>
                </a>
            </td>
        </tr>
        <tr>
            <td style=""text-align: center;"">conda-forge version</td>
            <td style=""text-align: center;"">
                <a href=""https://anaconda.org/conda-forge/pvlib"">
                    <img src=""https://anaconda.org/conda-forge/scienceplots/badges/version.svg"" alt=""conda-forge version"" height=""18""/>
                </a>
            </td>
        </tr>
        <tr>
            <td style=""text-align: center;"">DOI</td>
            <td style=""text-align: center;"">
                <a href=""https://zenodo.org/badge/latestdoi/144605189"">
                    <img src=""htt"
checkov,"[![checkov](https://raw.githubusercontent.com/bridgecrewio/checkov/main/docs/web/images/checkov_blue_logo.png)](#)
       
[![Maintained by Prisma Cloud](https://img.shields.io/badge/maintained_by-Prisma_Cloud-blue)](https://prismacloud.io/?utm_source=github&utm_medium=organic_oss&utm_campaign=checkov)
[![build status](https://github.com/bridgecrewio/checkov/workflows/build/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Abuild)
[![security status](https://github.com/bridgecrewio/checkov/workflows/security/badge.svg)](https://github.com/bridgecrewio/checkov/actions?query=event%3Apush+branch%3Amaster+workflow%3Asecurity)
[![code_coverage](https://raw.githubusercontent.com/bridgecrewio/checkov/main/coverage.svg?sanitize=true)](https://github.com/bridgecrewio/checkov/actions?query=workflow%3Acoverage)
[![docs](https://img.shields.io/badge/docs-passing-brightgreen)](https://www.checkov.io/1.Welcome/What%20is%20Checkov.html?utm_source=github&utm_medium=organic_os"
machine-learning-course,"

###################################################
A Machine Learning Course with Python
###################################################

.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat
    :target: https://github.com/pyairesearch/machine-learning-for-everybody/pulls
.. image:: https://badges.frapsoft.com/os/v2/open-source.png?v=103
    :target: https://github.com/ellerbrock/open-source-badge/
.. image:: https://img.shields.io/badge/Made%20with-Python-1f425f.svg
      :target: https://www.python.org/
.. image:: https://img.shields.io/github/contributors/machinelearningmindset/machine-learning-course.svg
      :target: https://github.com/machinelearningmindset/machine-learning-course/graphs/contributors
.. image:: https://img.shields.io/badge/book-pdf-blue.svg
   :target: https://machinelearningmindset.com/wp-content/uploads/2019/06/machine-learning-course.pdf
.. image:: https://img.shields.io/badge/official-documentation-green.svg
   :tar"
django-ninja,"<a href=""https://github.com/vitalik/django-ninja/issues/383""><img width=""814"" alt=""SCR-20230123-m1t"" src=""https://user-images.githubusercontent.com/95222/214056666-585c0479-c122-4cb3-add4-b8844088ccdd.png""></a>



<a href=""https://github.com/vitalik/django-ninja/issues/383"">^ Please read ^</a>




<p align=""center"">
  <a href=""https://django-ninja.dev/""><img src=""https://django-ninja.dev/img/logo-big.png""></a>
</p>
<p align=""center"">
    <em>Fast to learn, fast to code, fast to run</em>
</p>


![Test](https://github.com/vitalik/django-ninja/actions/workflows/test_full.yml/badge.svg)
![Coverage](https://img.shields.io/codecov/c/github/vitalik/django-ninja)
[![PyPI version](https://badge.fury.io/py/django-ninja.svg)](https://badge.fury.io/py/django-ninja)
[![Downloads](https://static.pepy.tech/personalized-badge/django-ninja?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=downloads/month)](https://pepy.tech/project/django-ninja)

# Django Ninja "
marshmallow,"********************************************
marshmallow: simplified object serialization
********************************************

|pypi| |build-status| |pre-commit| |docs|

.. |pypi| image:: https://badgen.net/pypi/v/marshmallow
    :target: https://pypi.org/project/marshmallow/
    :alt: Latest version

.. |build-status| image:: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml/badge.svg
    :target: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml
    :alt: Build status

.. |pre-commit| image:: https://results.pre-commit.ci/badge/github/marshmallow-code/marshmallow/dev.svg
   :target: https://results.pre-commit.ci/latest/github/marshmallow-code/marshmallow/dev
   :alt: pre-commit.ci status

.. |docs| image:: https://readthedocs.org/projects/marshmallow/badge/
   :target: https://marshmallow.readthedocs.io/
   :alt: Documentation

**marshmallow** is an ORM/ODM/framework-agnostic library for converting complex d"
BentoML,"<picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/bentoml/BentoML/assets/489344/d3e6c95d-d224-49a5-9cff-0789f094e127"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://github.com/bentoml/BentoML/assets/489344/de4da660-6aeb-4e5a-bf76-b7177435444d"">
    <img alt=""BentoML: Unified Model Serving Framework"" src=""https://github.com/bentoml/BentoML/assets/489344/de4da660-6aeb-4e5a-bf76-b7177435444d"" width=""370"" style=""max-width: 100%;"">
</picture>

## Unified Model Serving Framework

ğŸ± Build model inference APIs and multi-model serving systems with any open-source or custom AI models. ğŸ‘‰ [Join our Slack community!](https://l.bentoml.com/join-slack)

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202-green.svg)](https://github.com/bentoml/BentoML?tab=Apache-2.0-1-ov-file)
[![Releases](https://img.shields.io/github/v/release/bentoml/bentoml.svg)](https://github.com/bentoml/bentoml/releases)
[![CI](https://github.com/bentom"
fantasque-sans,"Fantasque Sans Mono
===================

A programming font, designed with functionality in mind, and with some
wibbly-wobbly handwriting-like fuzziness that makes it unassumingly cool.
[Download](https://github.com/belluzj/fantasque-sans/releases/latest) or 
see [installation instructions](#installation).


![](Specimen/urxvt13.png)

Previously known as *Cosmic Sans Neue Mono*. It
appeared that [similar names were already in use for other
fonts](https://github.com/belluzj/cosmic-sans-neue/issues/16), and that
people tended to extend their instinctive hatred of Comic Sans to this very
font of mine (which of course can only be *loved*). Why the previous name?
Here is my original explanation:

> The name comes from my realization that at some point it looked like the
> mutant child of Comic Sans and Helvetica Neue. Hopefully it is not the
> case any more.

Inspirational sources include Inconsolata and Monaco. I have also been using
Consolas a lot in my programming life, so it may have so"
multi-v2ray,"# multi-v2ray
V2ray/Xrayå¤šç”¨æˆ·ç®¡ç†è„šæœ¬ï¼Œå‘å¯¼å¼ç®¡ç†[æ–°å¢|åˆ é™¤|ä¿®æ”¹]ä¼ è¾“åè®®  
![](https://img.shields.io/pypi/v/v2ray-util.svg) 
[![Downloads](https://pepy.tech/badge/v2ray-util)](https://pepy.tech/project/v2ray-util)
[![Downloads](https://pepy.tech/badge/v2ray-util/month)](https://pepy.tech/project/v2ray-util)
![](https://img.shields.io/docker/pulls/jrohy/v2ray.svg)
![](https://img.shields.io/github/license/Jrohy/multi-v2ray.svg)

## [ä¸­æ–‡](README.md)  [English](README_EN.md)

## ç‰¹è‰²
- [x] æ”¯æŒXrayç®¡ç†, v2rayå’Œxrayç›¸äº’ç‹¬ç«‹, ä¸åŒå‘½ä»¤(v2ray/xray)è¿›å…¥ä¸åŒçš„coreç®¡ç†
- [x] è°ƒç”¨v2rayå®˜æ–¹apiè¿›è¡Œæµé‡ç»Ÿè®¡
- [x] **å¤šç”¨æˆ·, å¤šç«¯å£ç®¡ç†**, æ··åˆä¼ è¾“åè®®ç®¡ç†ä¸å†æ˜¯æ¢¦
- [x] é¦–æ¬¡å®‰è£…æ—¶äº§ç”Ÿéšæœºç«¯å£ï¼Œé»˜è®¤é…ç½®mkcp + éšæœºä¸€ç§ (srtp | wechat-video | utp | dtls | wireguard) headerä¼ªè£…;  
 Â å®‰è£…å®Œæˆæ˜¾ç¤ºé…ç½®ä¿¡æ¯;
- [x] æŸ¥çœ‹é…ç½®ä¿¡æ¯æ˜¾ç¤ºvmess/vlesså­—ç¬¦ä¸²(v2rayNçš„åˆ†äº«é“¾æ¥æ ¼å¼)
- [x] ç”Ÿæˆ**Telegram**çš„socks5/MTProtoåˆ†äº«é“¾æ¥, æ”¯æŒsocks5 + tlsç»„åˆ
- [x] æ”¯æŒhttp/2, éšæœºç”Ÿæˆä¼ªè£…h2 path
- [x] å¼€å¯å…³é—­tcpFastOpen
- [x] ç›´æ¥å¼€å¯[CDN](https://github.com/Jrohy/multi-v2ray/wiki/CloudFlare-cdn%E4%BB%A3%E7%90%86v2ray%E6%B5%81%E9%87%8F)
- [x] å¼€å¯å…³é—­åŠ¨æ€ç«¯å£
- [x] å®šæ—¶æ›´æ–°v2ray(éœ€æ‰‹åŠ¨å¼€å¯)
- ["
KeymouseGo,"<div align=""center"">

# KeymouseGo

<br>
<img src=""Preview.png"" width=""50%"" height=""50%"" />

<div>
    <img alt=""platform"" src=""https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-blueviolet"">
</div>
<div>
    <img alt=""license"" src=""https://img.shields.io/github/license/taojy123/KeymouseGo"">
    <img alt=""language"" src=""https://img.shields.io/badge/python-%3E%3D%203.7-green"">
    <img alt=""stars"" src=""https://img.shields.io/github/stars/taojy123/KeymouseGo?style=social"">
</div>

<br>

[ç®€ä½“ä¸­æ–‡](README.md) | [English](README_en-US.md)

</div>

åŠŸèƒ½ï¼šè®°å½•ç”¨æˆ·çš„é¼ æ ‡é”®ç›˜æ“ä½œï¼Œé€šè¿‡è§¦å‘æŒ‰é’®è‡ªåŠ¨æ‰§è¡Œä¹‹å‰è®°å½•çš„æ“ä½œï¼Œå¯è®¾å®šæ‰§è¡Œçš„æ¬¡æ•°ï¼Œå¯ä»¥ç†è§£ä¸º `ç²¾ç®€ç»¿è‰²ç‰ˆ` çš„ `æŒ‰é”®ç²¾çµ`ã€‚

ç”¨é€”ï¼šåœ¨è¿›è¡ŒæŸäº›æ“ä½œç®€å•ã€å•è°ƒé‡å¤çš„å·¥ä½œæ—¶ï¼Œä½¿ç”¨æœ¬è½¯ä»¶å°±å¯ä»¥å¾ˆçœåŠ›äº†ã€‚è‡ªå·±åªè¦åšä¸€éï¼Œç„¶åæ¥ä¸‹æ¥å°±è®©ç”µè„‘æ¥åšã€‚


# ç›®å½•

+ [å®‰è£…](#å®‰è£…)
+ [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
  + [åŸºæœ¬æ“ä½œ](#åŸºæœ¬æ“ä½œ)
  + [æç¤º](#æç¤º)
  + [è„šæœ¬è¯­æ³•è¯´æ˜](#è„šæœ¬è¯­æ³•è¯´æ˜)
  + [è‡ªå®šä¹‰æ‰©å±•](#è‡ªå®šä¹‰æ‰©å±•)
+ [å…³äºä½œè€…](#å…³äºä½œè€…)
+ [å¼€æºè´¡çŒ®è€…](#å¼€æºè´¡çŒ®è€…)
+ [æ›´æ–°è¯´æ˜](#æ›´æ–°è¯´æ˜)

# å®‰è£…

è¯¥è½¯ä»¶é€šè¿‡ `Python` è¯­è¨€ç¼–å†™ï¼Œå·²æ‰“åŒ…ä¸ºå¯æ‰§è¡Œæ–‡ä»¶ï¼Œæœªå®‰è£… `Python` çš„ç”¨æˆ·å¯ç›´æ¥ä¸‹è½½ [release](https://github.com/taojy123/KeymouseGo/releases) ç‰ˆæœ¬ ï¼Œç›´æ¥ç‚¹å‡» `KeymouseGo` è¿è¡Œ

### "
financial-machine-learning,"[![Repo-Updater](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_status.yml/badge.svg)](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_status.yml)
[![Wiki-Generator](https://github.com/firmai/financial-machine-learning/actions/workflows/wiki_gen.yml/badge.svg)](https://github.com/firmai/financial-machine-learning/actions/workflows/wiki_gen.yml)
[![Repo-Search](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_search.yml/badge.svg)](https://github.com/firmai/financial-machine-learning/actions/workflows/repo_search.yml)
[![Gitter](https://badges.gitter.im/financial-machine-learning/community.svg)](https://gitter.im/financial-machine-learning/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

# Future of the Community
## Two things:
1. We have decided to start a [Slack](https://join.slack.com/t/mlquant/shared_invite/zt-ztvxxxtz-8_LCEwi1Wvy4cvkVm~IcyQ) group ([invite](https://join.slack.com/"
sh,".. image:: https://raw.githubusercontent.com/amoffat/sh/master/images/logo-230.png
    :target: https://amoffat.github.com/sh
    :alt: Logo

**If you are migrating from 1.* to 2.*, please see MIGRATION.md**

|

.. image:: https://img.shields.io/pypi/v/sh.svg?style=flat-square
    :target: https://pypi.python.org/pypi/sh
    :alt: Version
.. image:: https://img.shields.io/pypi/dm/sh.svg?style=flat-square
    :target: https://pypi.python.org/pypi/sh
    :alt: Downloads Status
.. image:: https://img.shields.io/pypi/pyversions/sh.svg?style=flat-square
    :target: https://pypi.python.org/pypi/sh
    :alt: Python Versions
.. image:: https://img.shields.io/coveralls/amoffat/sh.svg?style=flat-square
    :target: https://coveralls.io/r/amoffat/sh?branch=master
    :alt: Coverage Status

|

sh is a full-fledged subprocess replacement for Python 3.8 - 3.11, and PyPy
that allows you to call *any* program as if it were a function:

.. code:: python

    from sh import ifconfig
    print(ifconfig("
FlagEmbedding,"<h1 align=""center"">FlagEmbedding</h1>
<p align=""center"">
    <a href=""https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d"">
        <img alt=""Build"" src=""https://img.shields.io/badge/BGE_series-ğŸ¤—-yellow"">
    </a>
    <a href=""https://github.com/FlagOpen/FlagEmbedding"">
            <img alt=""Build"" src=""https://img.shields.io/badge/Contribution-Welcome-blue"">
    </a>
    <a href=""https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/badge/LICENSE-MIT-green"">
    </a>
    <a href=""https://huggingface.co/C-MTEB"">
        <img alt=""Build"" src=""https://img.shields.io/badge/C_MTEB-ğŸ¤—-yellow"">
    </a>
    <a href=""https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding"">
        <img alt=""Build"" src=""https://img.shields.io/badge/FlagEmbedding-1.1-red"">
    </a>
</p>

<h4 align=""center"">
    <p>
        <a href=#news>News</a> |
        <a href=#installation>Installation</a> |
"
Dango-Translator,"# å›¢å­ç¿»è¯‘å™¨ - åŸºäºOCRçš„ç”Ÿè‚‰ç¿»è¯‘è½¯ä»¶


[![æœ€æ–°ç‰ˆæœ¬](https://img.shields.io/badge/%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC-Ver5.2.2-ff69b4)](https://github.com/PantsuDango/Dango-Translator)
[![æ›´æ–°æ—¶é—´](https://img.shields.io/badge/%E6%9B%B4%E6%96%B0%E6%97%B6%E9%97%B4-2024--06--01-ff69b4)]()
[![æ“ä½œç³»ç»Ÿ](https://img.shields.io/badge/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-win7--11-ff69b4)]()
[![GitHubStars](https://img.shields.io/github/stars/PantsuDango/Dango-Translator)]()
[![GitHubForks](https://img.shields.io/github/forks/PantsuDango/Dango-Translator)]()
[![ä½œè€…](https://img.shields.io/badge/QQ-%E8%83%96%E6%AC%A1%E5%9B%A2%E5%AD%90-ff69b4)](https://github.com/PantsuDango/ImageHub/blob/master/DangoTranslate/public/%E4%BD%9C%E8%80%85.png)
[![ç¾¤å·](https://img.shields.io/badge/%E6%9C%80%E6%96%B0%E4%BA%A4%E6%B5%81%E7%BE%A4-11%E7%BE%A4835628840-ff69b4)](https://github.com/PantsuDango/ImageHub/blob/master/DangoTranslate/public/qrcode_1717254152512.jpg)

  
## ç®€ä»‹

å›¢å­ç¿»è¯‘å™¨æ˜¯ä¸€æ¬¾ç”Ÿè‚‰ç¿»è¯‘è½¯ä»¶ï¼Œé€šè¿‡OCRè¯†åˆ«å±å¹•ç‰¹å®šèŒƒå›´å†…çš„æ–‡å­—ï¼Œç„¶åå°†è¯†åˆ«åˆ°çš„æ–‡å­—è°ƒå–å„ç§ç¿»è¯‘æºï¼Œå¹¶å®æ—¶è¾“å‡ºç¿»è¯‘ç»“æœã€‚

+"
autograd,"# Autograd  [![Checks status][checks-badge]][checks-url] [![Tests status][tests-badge]][tests-url] [![Publish status][publish-badge]][publish-url] [![asv][asv-badge]](#)

[publish-badge]: https://github.com/HIPS/autograd/actions/workflows/publish.yml/badge.svg
[checks-badge]: https://github.com/HIPS/autograd/actions/workflows/check.yml/badge.svg
[tests-badge]: https://github.com/HIPS/autograd/actions/workflows/test.yml/badge.svg
[asv-badge]: http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat
[publish-url]: https://github.com/HIPS/autograd/actions/workflows/publish.yml
[checks-url]: https://github.com/HIPS/autograd/actions/workflows/check.yml
[tests-url]: https://github.com/HIPS/autograd/actions/workflows/test.yml

Autograd can automatically differentiate native Python and Numpy code. It can
handle a large subset of Python's features, including loops, ifs, recursion and
closures, and it can even take derivatives of derivatives of derivatives. It
supports reverse-mode "
gh-proxy,"# gh-proxy

## ç®€ä»‹

github releaseã€archiveä»¥åŠé¡¹ç›®æ–‡ä»¶çš„åŠ é€Ÿé¡¹ç›®ï¼Œæ”¯æŒcloneï¼Œæœ‰Cloudflare Workersæ— æœåŠ¡å™¨ç‰ˆæœ¬ä»¥åŠPythonç‰ˆæœ¬

## æ¼”ç¤º

[https://gh.api.99988866.xyz/](https://gh.api.99988866.xyz/)

æ¼”ç¤ºç«™ä¸ºå…¬å…±æœåŠ¡ï¼Œå¦‚æœ‰å¤§è§„æ¨¡ä½¿ç”¨éœ€æ±‚è¯·è‡ªè¡Œéƒ¨ç½²ï¼Œæ¼”ç¤ºç«™æœ‰ç‚¹ä¸å ªé‡è´Ÿ

![imagea272c95887343279.png](https://img.maocdn.cn/img/2021/04/24/imagea272c95887343279.png)

å½“ç„¶ä¹Ÿæ¬¢è¿[æèµ ](#æèµ )ä»¥æ”¯æŒä½œè€…

## pythonç‰ˆæœ¬å’Œcf workerç‰ˆæœ¬å·®å¼‚

- pythonç‰ˆæœ¬æ”¯æŒè¿›è¡Œæ–‡ä»¶å¤§å°é™åˆ¶ï¼Œè¶…è¿‡è®¾å®šè¿”å›åŸåœ°å€ [issue #8](https://github.com/hunshcn/gh-proxy/issues/8)

- pythonç‰ˆæœ¬æ”¯æŒç‰¹å®šuser/repo å°ç¦/ç™½åå• ä»¥åŠpassby [issue #41](https://github.com/hunshcn/gh-proxy/issues/41)

## ä½¿ç”¨

ç›´æ¥åœ¨copyå‡ºæ¥çš„urlå‰åŠ `https://gh.api.99988866.xyz/`å³å¯

ä¹Ÿå¯ä»¥ç›´æ¥è®¿é—®ï¼Œåœ¨inputè¾“å…¥

***å¤§é‡ä½¿ç”¨è¯·è‡ªè¡Œéƒ¨ç½²ï¼Œä»¥ä¸ŠåŸŸåä»…ä¸ºæ¼”ç¤ºä½¿ç”¨ã€‚***

è®¿é—®ç§æœ‰ä»“åº“å¯ä»¥é€šè¿‡

`git clone https://user:TOKEN@ghproxy.com/https://github.com/xxxx/xxxx` [#71](https://github.com/hunshcn/gh-proxy/issues/71)

ä»¥ä¸‹éƒ½æ˜¯åˆæ³•è¾“å…¥ï¼ˆä»…ç¤ºä¾‹ï¼Œæ–‡ä»¶ä¸å­˜åœ¨ï¼‰ï¼š

- åˆ†æ”¯æºç ï¼šhttps://github.com/hunshcn/project/archive/master.zip

- releaseæºç ï¼šhttps://github.com/hunshcn/project/archive/v0.1.0.tar.gz

- releaseæ–‡ä»¶ï¼šhttps://github.com/hunshcn/project/releases/download/v0.1.0/"
corenet,"# CoreNet: A library for training deep neural networks

CoreNet is a deep neural network toolkit that allows researchers and engineers to train standard and novel small and large-scale models for variety of tasks, including foundation models (e.g., CLIP and LLM), object classification, object detection, and semantic segmentation.

## Table of contents

   * [What's new?](#whats-new)
   * [Research efforts at Apple using CoreNet](#research-efforts-at-apple-using-corenet)
   * [Installation](#installation)
   * [Directory Structure](#directory-structure)
   * [Maintainers](#maintainers)
   * [Contributing to CoreNet](#contributing-to-corenet)
   * [License](#license)
   * [Relationship with CVNets](#relationship-with-cvnets)
   * [Citation](#citation)

## What's new?

   * ***April 2024***: Version 0.1.0 of the CoreNet library includes
      * OpenELM
      * CatLIP
      * MLX examples

## Research efforts at Apple using CoreNet

Below is the list of publications from Apple that uses Co"
pdfGPT,"# pdfGPT
## Demo
1. **Demo URL**: https://bhaskartripathi-pdfgpt-turbo.hf.space
2. **Demo Video**:
   
   [![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/LzPgmmqpBk8/0.jpg)](https://www.youtube.com/watch?v=LzPgmmqpBk8)
#### Version Updates (27 July, 2023):
1. Improved error handling
2. PDF GPT now supports Turbo models and GPT4 including 16K and 32K token model.
3. Pre-defined questions for auto-filling the input.
4. Implemented Chat History feature.
![image](https://github.com/bhaskatripathi/pdfGPT/assets/35177508/11549b24-9ed4-4dcb-a877-bad9c2266bf9)


### Note on model performance
```If you find the response for a specific question in the PDF is not good using Turbo models, then you need to understand that Turbo models such as gpt-3.5-turbo are chat completion models and will not give a good response in some cases where the embedding similarity is low. Despite the claim by OpenAI, the turbo model is not the best model for Q&A. In those specific cases, either use the good old text"
PyGithub,"# PyGitHub

[![PyPI](https://img.shields.io/pypi/v/PyGithub.svg)](https://pypi.python.org/pypi/PyGithub)
![CI](https://github.com/PyGithub/PyGithub/workflows/CI/badge.svg)
[![readthedocs](https://img.shields.io/badge/docs-stable-brightgreen.svg?style=flat)](https://pygithub.readthedocs.io/en/stable/?badge=stable)
[![License](https://img.shields.io/badge/license-LGPL-blue.svg)](https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License)
[![Slack](https://img.shields.io/badge/Slack%20channel-%20%20-blue.svg)](https://join.slack.com/t/pygithub-project/shared_invite/zt-duj89xtx-uKFZtgAg209o6Vweqm8xeQ)
[![Open Source Helpers](https://www.codetriage.com/pygithub/pygithub/badges/users.svg)](https://www.codetriage.com/pygithub/pygithub)
[![codecov](https://codecov.io/gh/PyGithub/PyGithub/branch/master/graph/badge.svg)](https://codecov.io/gh/PyGithub/PyGithub)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

PyGitHub is a Pyt"
3d-photo-inpainting,"# [CVPR 2020] 3D Photography using Context-aware Layered Depth Inpainting

[![Open 3DPhotoInpainting in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)

### [[Paper](https://arxiv.org/abs/2004.04727)] [[Project Website](https://shihmengli.github.io/3D-Photo-Inpainting/)] [[Google Colab](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)]

<p align='center'>
<img src='https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_teaser.jpg' width='900'/>
</p>

We propose a method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that iteratively synthesizes new local color-and"
nginx-book,".. nginx_book documentation master file, created by
   sphinx-quickstart on Wed Feb 29 17:58:19 2012.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Nginxå¼€å‘ä»å…¥é—¨åˆ°ç²¾é€š
=============================


ç¼˜èµ·
++++++

nginxç”±äºå‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ä¸–ç•ŒèŒƒå›´å†…å—åˆ°äº†è¶Šæ¥è¶Šå¤šäººçš„å…³æ³¨ï¼Œåœ¨æ·˜å®å†…éƒ¨å®ƒæ›´æ˜¯è¢«å¹¿æ³›çš„ä½¿ç”¨ï¼Œä¼—å¤šçš„å¼€å‘ä»¥åŠè¿ç»´åŒå­¦éƒ½è¿«åˆ‡çš„æƒ³è¦äº†è§£nginxæ¨¡å—çš„å¼€å‘å’Œå®ƒçš„å†…éƒ¨åŸç†ï¼Œä½†æ˜¯å›½å†…å´æ²¡æœ‰ä¸€æœ¬å…³äºè¿™æ–¹é¢çš„ä¹¦ï¼Œæºäºæ­¤æˆ‘ä»¬å†³å®šè‡ªå·±æ¥å†™ä¸€æœ¬ã€‚æœ¬ä¹¦çš„ä½œè€…ä¸ºæ·˜å®æ ¸å¿ƒç³»ç»ŸæœåŠ¡å™¨å¹³å°ç»„çš„æˆå‘˜ï¼Œæœ¬ä¹¦å†™ä½œçš„æ€è·¯æ˜¯ä»æ¨¡å—å¼€å‘é€æ¸è¿‡æ¸¡åˆ°nginxåŸç†å‰–æã€‚ä¹¦ç±çš„å†…å®¹ä¼šå®šæœŸåœ¨è¿™é‡Œæ›´æ–°ï¼Œæ¬¢è¿å¤§å®¶æå‡ºå®è´µæ„è§ï¼Œä¸ç®¡æ˜¯æœ¬ä¹¦çš„å†…å®¹é—®é¢˜ï¼Œè¿˜æ˜¯å­—è¯é”™è¯¯ï¼Œéƒ½æ¬¢è¿å¤§å®¶æäº¤issue(ç« èŠ‚æ ‡é¢˜çš„å·¦ä¾§æœ‰è¯„æ³¨æŒ‰é’®)ï¼Œæˆ‘ä»¬ä¼šåŠæ—¶çš„è·Ÿè¿›ã€‚

.. topic:: æ›´æ–°å†å²

    .. csv-table:: 
       :header: æ—¥æœŸ, æè¿°
       :widths: 20, 160
       :quote: $
       :delim: |

       2012/03/01|åˆ›å»ºç›®å½•å¤§çº²
       2012/03/28|å¢åŠ äº†æ ·ç« 
       2012/05/25|æ›´æ–°æ ·ç« 
       2012/06/08|å¢åŠ ç¬¬5ç« 
       2012/06/11|å¢åŠ ç¬¬4ç« 
       2012/06/26|å¢åŠ ç¬¬6ç« (event module)
       2012/06/27|æ›´æ–°ç¬¬5ç« éƒ¨åˆ†å†…å®¹
       2012/07/04|æ›´æ–°ç¬¬6ç« event moduleéƒ¨åˆ†å†…å®¹
       2012/07/12|å¢åŠ ç¬¬12ç« ï¼ˆè¯·æ±‚å¤´è¯»å–ï¼Œsubrequestè§£æï¼‰
       2012/08/14|å¢åŠ ç¬¬2ç« (nginxåŸºç¡€æ¶æ„åŠåŸºç¡€æ¦‚å¿µ)
       2012/08/14|å¢åŠ ç¬¬2ç« (ngx_str_tæ•°æ®ç»“æ„ä»‹ç»)
      "
howmanypeoplearearound,"
# howmanypeoplearearound 

Count the number of people around you :family_man_man_boy: by monitoring wifi signals :satellite:.

*howmanypeoplearearound* calculates the number of people in the vicinity
using the approximate number of smartphones as a proxy (since [~70% of people have smartphones nowadays](https://twitter.com/conradhackett/status/701798230619590656)). 
A cellphone is determined to be in proximity to the computer based on sniffing WiFi probe 
requests. Possible uses of *howmanypeoplearearound* include: monitoring foot traffic in your house
with Raspberry Pis, seeing if your roommates are home, etc.

Tested on Linux (Raspbian and Ubuntu) and Mac OS X.

### **It may be illegal** to monitor networks for MAC addresses, especially on networks that *you do not own*. Please check your country's laws (for US [Section 18 U.S. Code Â§ 2511](https://www.law.cornell.edu/uscode/text/18/2511)) - [discussion](https://github.com/schollz/howmanypeoplearearound/issues/4).

Getting started
="
models,"# æ¬¢è¿ä½¿ç”¨é£æ¡¨äº§ä¸šçº§å¼€æºæ¨¡å‹åº“

## ç®€ä»‹

é£æ¡¨çš„äº§ä¸šçº§æ¨¡å‹åº“ï¼ŒåŒ…å«å¤§é‡ç»è¿‡äº§ä¸šå®è·µé•¿æœŸæ‰“ç£¨çš„ä¸»æµæ¨¡å‹ä»¥åŠåœ¨å›½é™…ç«èµ›ä¸­çš„å¤ºå† æ¨¡å‹ï¼›æä¾›é¢å‘è¯­ä¹‰ç†è§£ã€å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€æ–‡å­—è¯†åˆ«ã€è¯­éŸ³åˆæˆç­‰åœºæ™¯çš„å¤šä¸ªç«¯åˆ°ç«¯å¼€å‘å¥—ä»¶ï¼Œæ»¡è¶³ä¼ä¸šä½æˆæœ¬å¼€å‘å’Œå¿«é€Ÿé›†æˆçš„éœ€æ±‚ã€‚é£æ¡¨çš„æ¨¡å‹åº“æ˜¯å›´ç»•å›½å†…ä¼ä¸šå®é™…ç ”å‘æµç¨‹é‡èº«å®šåˆ¶æ‰“é€ çš„äº§ä¸šçº§æ¨¡å‹åº“ï¼ŒæœåŠ¡ä¼ä¸šéå¸ƒèƒ½æºã€é‡‘èã€å·¥ä¸šã€å†œä¸šç­‰å¤šä¸ªé¢†åŸŸã€‚

## è¿‘æœŸæ›´æ–°

**`2022-11-29`**: æ›´æ–°`release/2.4`åˆ†æ”¯ï¼Œé£æ¡¨å®˜æ–¹æ¨¡å‹è¶…è¿‡600ä¸ªï¼Œç”Ÿæ€æ¨¡å‹è¶…è¿‡260ä¸ªï¼ˆæ•°é‡æŒç»­æ›´æ–°ä¸­ï¼‰.

**`2022-5-17`**: æ›´æ–°`release/2.3`åˆ†æ”¯ï¼Œé£æ¡¨å®˜æ–¹æ¨¡å‹è¶…è¿‡500ä¸ªï¼Œç”Ÿæ€æ¨¡å‹è¶…è¿‡170ä¸ª.

**`2021-11-30`**: æ›´æ–°`release/2.2`åˆ†æ”¯ï¼Œç³»ç»Ÿçš„æ¢³ç†äº†é£æ¡¨å®˜æ–¹æ¨¡å‹ã€å­¦æœ¯æ¨¡å‹å’Œç¤¾åŒºæ¨¡å‹çš„æ¸…å•ï¼Œå…¶ä¸­å®˜æ–¹æ¨¡å‹è¶…è¿‡400ä¸ªï¼Œç”Ÿæ€æ¨¡å‹è¶…è¿‡100ä¸ª

**`Note`**:`release/2.2`ä»¥ååˆ†æ”¯æ¨¡å‹å‡åŸºäºåŠ¨æ€å›¾å®ç°ï¼Œç›®å‰`dev-static`åˆ†æ”¯ä¸­ä»æœ‰ä¸€äº›é™æ€å›¾æ¨¡å‹ä»£ç ï¼Œæœ‰éœ€è¦çš„å¼€å‘è€…å¯ä»¥ç»§ç»­åˆ‡æ¢åˆ°`dev-static`åˆ†æ”¯ä½¿ç”¨.

## ä¸»è¦å†…å®¹
|  ç›®å½• |   è¯´æ˜ |
| --- | --- |
| [å®˜æ–¹æ¨¡å‹(official)](docs/official/README.md) |â€¢ é¢å‘äº§ä¸šå®è·µï¼Œæ•°é‡è¶…è¿‡600ä¸ª<br />â€¢ [é£æ¡¨PPç³»åˆ—æ¨¡å‹](docs/official/PP-Models.md)ï¼Œæ•ˆæœä¸ç²¾åº¦æœ€ä½³å¹³è¡¡<br />â€¢ æ”¯æŒä½¿ç”¨åŠ¨æ€å›¾å¼€å‘è§†è§‰ã€è‡ªç„¶è¯­è¨€ã€è¯­éŸ³å’Œæ¨èç­‰é¢†åŸŸæ¨¡å‹<br />â€¢ é£æ¡¨å®˜æ–¹å®ç°å¹¶æä¾›æŒç»­æŠ€æœ¯æ”¯æŒåŠç­”ç–‘<br />â€¢ ä¸é£æ¡¨æ ¸å¿ƒæ¡†æ¶ç‰ˆæœ¬å¯¹é½ï¼Œå·²ç»ç»è¿‡å……åˆ†çš„æµ‹è¯•ä¿è¯ |
|[å­¦æœ¯æ¨¡å‹(research)](docs/research/README.md) |â€¢ é¢å‘å­¦æœ¯å‰æ²¿ï¼Œä¾§é‡å¯¹äºé—®é¢˜çš„æŒç»­æ›´æ–°<br />â€¢ ä¸»è¦ç”±é£æ¡¨ç›¸å…³çš„å­¦æœ¯ç”Ÿæ€åˆä½œä¼™ä¼´è´¡çŒ®|
|[ç¤¾åŒºæ¨¡å‹(community)](docs/community/README.md) | â€¢ é¢å‘æ›´å¤šä¸°å¯Œåœºæ™¯ï¼Œä¾§é‡å¯¹äºå­¦æœ¯è®ºæ–‡çš„è¦†ç›–<br />â€¢ ä¸»è¦ç”±é£æ¡¨ç”Ÿæ€å¼€å‘è€…è´¡çŒ®ï¼ŒæŒç»­æ›´æ–°ä¸­|

## æ¬¢è¿åŠ å…¥é£æ¡¨æ¨¡å‹åº“æŠ€æœ¯äº¤æµç¾¤
- å¦‚æœä½ å¸Œæœ›äº†è§£é£æ¡¨æ¨¡å‹åº“æœ€æ–°è¿›å±•ï¼Œæˆ–è€…å¸Œ"
asyncpg,"asyncpg -- A fast PostgreSQL Database Client Library for Python/asyncio
=======================================================================

.. image:: https://github.com/MagicStack/asyncpg/workflows/Tests/badge.svg
   :target: https://github.com/MagicStack/asyncpg/actions?query=workflow%3ATests+branch%3Amaster
   :alt: GitHub Actions status
.. image:: https://img.shields.io/pypi/v/asyncpg.svg
   :target: https://pypi.python.org/pypi/asyncpg

**asyncpg** is a database interface library designed specifically for
PostgreSQL and Python/asyncio.  asyncpg is an efficient, clean implementation
of PostgreSQL server binary protocol for use with Python's ``asyncio``
framework.  You can read more about asyncpg in an introductory
`blog post <http://magic.io/blog/asyncpg-1m-rows-from-postgres-to-python/>`_.

asyncpg requires Python 3.8 or later and is supported for PostgreSQL
versions 9.5 to 16.  Older PostgreSQL versions or other databases implementing
the PostgreSQL protocol *may* work, but "
EZFN-Lobbybot,"# Welcome ğŸ¥³

EZFN.DEV is a free Fortnite Lobbybot which allows you to see all Fortnite cosmetics.

## Get Help
[Join my Discord](https://ezfn.dev/discord)

<sub>If you see this on github.com, visit [my website](https://ezfn.dev)...</sub>
"
PythonSpiderNotes,"# [Pythonå…¥é—¨ç½‘ç»œçˆ¬è™«ä¹‹ç²¾åç‰ˆ](https://github.com/lining0806/PythonSpiderNotes)

*** 

Pythonå­¦ä¹ ç½‘ç»œçˆ¬è™«ä¸»è¦åˆ†3ä¸ªå¤§çš„ç‰ˆå—ï¼š**æŠ“å–**ï¼Œ**åˆ†æ**ï¼Œ**å­˜å‚¨**  

å¦å¤–ï¼Œæ¯”è¾ƒå¸¸ç”¨çš„çˆ¬è™«æ¡†æ¶[Scrapy](http://scrapy.org/)ï¼Œè¿™é‡Œæœ€åä¹Ÿè¯¦ç»†ä»‹ç»ä¸€ä¸‹ã€‚    

é¦–å…ˆåˆ—ä¸¾ä¸€ä¸‹æœ¬äººæ€»ç»“çš„ç›¸å…³æ–‡ç« ï¼Œè¿™äº›è¦†ç›–äº†å…¥é—¨ç½‘ç»œçˆ¬è™«éœ€è¦çš„åŸºæœ¬æ¦‚å¿µå’ŒæŠ€å·§ï¼š[å®å“¥çš„å°ç«™-ç½‘ç»œçˆ¬è™«](http://www.lining0806.com/category/spider/)  
***

å½“æˆ‘ä»¬åœ¨æµè§ˆå™¨ä¸­è¾“å…¥ä¸€ä¸ªurlåå›è½¦ï¼Œåå°ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæ¯”å¦‚è¯´ä½ è¾“å…¥[http://www.lining0806.com/](http://www.lining0806.com/)ï¼Œä½ å°±ä¼šçœ‹åˆ°å®å“¥çš„å°ç«™é¦–é¡µã€‚

ç®€å•æ¥è¯´è¿™æ®µè¿‡ç¨‹å‘ç”Ÿäº†ä»¥ä¸‹å››ä¸ªæ­¥éª¤ï¼š

* æŸ¥æ‰¾åŸŸåå¯¹åº”çš„IPåœ°å€ã€‚
* å‘IPå¯¹åº”çš„æœåŠ¡å™¨å‘é€è¯·æ±‚ã€‚
* æœåŠ¡å™¨å“åº”è¯·æ±‚ï¼Œå‘å›ç½‘é¡µå†…å®¹ã€‚
* æµè§ˆå™¨è§£æç½‘é¡µå†…å®¹ã€‚

ç½‘ç»œçˆ¬è™«è¦åšçš„ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯å®ç°æµè§ˆå™¨çš„åŠŸèƒ½ã€‚é€šè¿‡æŒ‡å®šurlï¼Œç›´æ¥è¿”å›ç»™ç”¨æˆ·æ‰€éœ€è¦çš„æ•°æ®ï¼Œè€Œä¸éœ€è¦ä¸€æ­¥æ­¥äººå·¥å»æ“çºµæµè§ˆå™¨è·å–ã€‚

## æŠ“å–  
è¿™ä¸€æ­¥ï¼Œä½ è¦æ˜ç¡®è¦å¾—åˆ°çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯HTMLæºç ï¼Œè¿˜æ˜¯Jsonæ ¼å¼çš„å­—ç¬¦ä¸²ç­‰ã€‚  

#### 1. æœ€åŸºæœ¬çš„æŠ“å–  

æŠ“å–å¤§å¤šæ•°æƒ…å†µå±äºgetè¯·æ±‚ï¼Œå³ç›´æ¥ä»å¯¹æ–¹æœåŠ¡å™¨ä¸Šè·å–æ•°æ®ã€‚  

é¦–å…ˆï¼ŒPythonä¸­è‡ªå¸¦urllibåŠurllib2è¿™ä¸¤ä¸ªæ¨¡å—ï¼ŒåŸºæœ¬ä¸Šèƒ½æ»¡è¶³ä¸€èˆ¬çš„é¡µé¢æŠ“å–ã€‚å¦å¤–ï¼Œ[requests](https://github.com/kennethreitz/requests)ä¹Ÿæ˜¯éå¸¸æœ‰ç”¨çš„åŒ…ï¼Œä¸æ­¤ç±»ä¼¼çš„ï¼Œè¿˜æœ‰[httplib2](https://github.com/jcgregorio/httplib2)ç­‰ç­‰ã€‚    

```
Requestsï¼š
	import requests
	response = requests.get(url)
	content = requests.get(url).content
	print ""response headers:"", response.headers
	print ""content:"", content
Urllib2ï¼š
	"
Gymnasium,"[![Python](https://img.shields.io/pypi/pyversions/gymnasium.svg)](https://badge.fury.io/py/gymnasium)
[![PyPI](https://badge.fury.io/py/gymnasium.svg)](https://badge.fury.io/py/gymnasium)
[![arXiv](https://img.shields.io/badge/arXiv-2407.17032-b31b1b.svg)](https://arxiv.org/abs/2407.17032)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

<p align=""center"">
    <img src=""https://raw.githubusercontent.com/Farama-Foundation/Gymnasium/main/gymnasium-text.png"" width=""500px""/>
</p>

Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. This is a fork of OpenAI's [Gym](https://github.com/open"
folium,"|PyPI| |Test| |Gitter| |DOI| |binder|

.. |PyPI| image:: https://img.shields.io/pypi/v/folium.svg
    :target: https://pypi.org/project/folium
    :alt: PyPI Package

.. |Test| image:: https://github.com/python-visualization/folium/actions/workflows/test_code.yml/badge.svg
    :target: https://github.com/python-visualization/folium/actions/workflows/test_code.yml
    :alt: Code tests

.. |Gitter| image:: https://badges.gitter.im/python-visualization/folium.svg
    :target: https://gitter.im/python-visualization/folium
    :alt: Gitter

.. |DOI| image:: https://zenodo.org/badge/18669/python-visualization/folium.svg
   :target: https://zenodo.org/badge/latestdoi/18669/python-visualization/folium
   :alt: DOI

.. |binder| image:: https://mybinder.org/badge_logo.svg
 :target: https://mybinder.org/v2/gh/python-visualization/folium/main?filepath=examples

folium
======

.. image:: https://github.com/python-visualization/folium/blob/main/docs/_static/folium_logo.png
   :height: 100px


Python"
gef,"<p align=""center"">
  <img src=""https://i.imgur.com/o0L8lPN.png"" alt=""logo""/>
</p>

<p align=""center"">
    <a href=""https://discord.gg/hSbqxxBgRX""><img alt=""Discord"" src=""https://img.shields.io/badge/Discord-BlahCats-yellow""></a>
  <a href=""https://hugsy.github.io/gef""><img alt=""Docs"" src=""https://img.shields.io/badge/Docs-gh--pages-brightgreen""></a>
  <a title=""Use the IDs: gef/gef-demo"" href=""https://demo.gef.blah.cat""><img alt=""Try GEF"" src=""https://img.shields.io/badge/Demo-Try%20GEF%20Live-blue""></a>
</p>

`GEF` (pronounced Ê¤É›f - ""Jeff"") is a set of commands for x86/64, ARM, MIPS, PowerPC and SPARC to
assist exploit developers and reverse-engineers when using old school GDB. It provides additional
features to GDB using the Python API to assist during the process of dynamic analysis and exploit
development. Application developers will also benefit from it, as GEF lifts a great part of regular
GDB obscurity, avoiding repeating traditional commands, or bringing out the relevant inform"
librephotos,"[![Discord](https://img.shields.io/discord/784619049208250388?style=plastic)][discord] [![Website](https://img.shields.io/website?down_color=lightgrey&down_message=offline&style=plastic&up_color=blue&up_message=online&url=https%3A%2F%2Flibrephotos.com)](https://librephotos.com/)
[![Read the docs](https://img.shields.io/static/v1?label=Read&message=the%20docs&color=blue&style=plastic)](https://docs.librephotos.com/) [![GitHub contributors](https://img.shields.io/github/contributors/librephotos/librephotos?style=plastic)](https://github.com/LibrePhotos/librephotos/graphs/contributors)
<a href=""https://hosted.weblate.org/engage/librephotos/"">
<img src=""https://hosted.weblate.org/widgets/librephotos/-/librephotos-frontend/svg-badge.svg"" alt=""Translation status"" />
</a>

# LibrePhotos

![](https://github.com/LibrePhotos/librephotos/blob/dev/screenshots/mockups_main_fhd.png?raw=true)
<sub>Mockup designed by rawpixel.com / Freepik</sub>

- **Stable** demo is available here:https://demo1.libre"
hug,"[![HUG](https://raw.github.com/hugapi/hug/develop/artwork/logo.png)](http://hug.rest)
===================

[![PyPI version](https://badge.fury.io/py/hug.svg)](http://badge.fury.io/py/hug)
[![Build Status](https://travis-ci.org/hugapi/hug.svg?branch=develop)](https://travis-ci.org/hugapi/hug)
[![Windows Build Status](https://ci.appveyor.com/api/projects/status/0h7ynsqrbaxs7hfm/branch/master?svg=true)](https://ci.appveyor.com/project/TimothyCrosley/hug)
[![Coverage Status](https://coveralls.io/repos/hugapi/hug/badge.svg?branch=develop&service=github)](https://coveralls.io/github/hugapi/hug?branch=master)
[![License](https://img.shields.io/github/license/mashape/apistatus.svg)](https://pypi.python.org/pypi/hug/)
[![Join the chat at https://gitter.im/timothycrosley/hug](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/timothycrosley/hug?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

_________________

[Read Latest Documentation](https://hugapi.githu"
dumb-init,"dumb-init
========

[![PyPI version](https://badge.fury.io/py/dumb-init.svg)](https://pypi.python.org/pypi/dumb-init)


**dumb-init** is a simple process supervisor and init system designed to run as
PID 1 inside minimal container environments (such as [Docker][docker]). It is
deployed as a small, statically-linked binary written in C.

Lightweight containers have popularized the idea of running a single process or
service without normal init systems like [systemd][systemd] or
[sysvinit][sysvinit]. However, omitting an init system often leads to incorrect
handling of processes and signals, and can result in problems such as
containers which can't be gracefully stopped, or leaking containers which
should have been destroyed.

`dumb-init` enables you to simply prefix your command with `dumb-init`. It acts
as PID 1 and immediately spawns your command as a child process, taking care to
properly handle and forward signals as they are received.


## Why you need an init system

Normally, whe"
gpt-neox,"[![GitHub issues](https://img.shields.io/github/issues/EleutherAI/gpt-neox)](https://github.com/EleutherAI/gpt-neox/issues)
[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Weights & Biases monitoring"" height=20>](https://wandb.ai/eleutherai/neox)

# GPT-NeoX

This repository records [EleutherAI](https://www.eleuther.ai)'s library for training large-scale language models on GPUs. Our current framework is based on NVIDIA's [Megatron Language Model](https://github.com/NVIDIA/Megatron-LM) and has been augmented with techniques from [DeepSpeed](https://www.deepspeed.ai) as well as some novel optimizations. We aim to make this repo a centralized and accessible place to gather techniques for training large-scale autoregressive language models, and accelerate research into large-scale training. This library is in widespread use in [academic, industry, and government labs](https://github.com/EleutherAI/gpt-neox#adoption-and-publications), including"
modelscope,"
<p align=""center"">
    <br>
    <img src=""https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif"" width=""400""/>
    <br>
<p>

<div align=""center"">

[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%2"
chainlit,"# Welcome to Chainlit by Literal AI ğŸ‘‹

[![](https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat)](https://discord.gg/k73SQ3FyUh)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&label=Follow%20%40chainlit_io)](https://twitter.com/chainlit_io)
![PyPI - Downloads](https://img.shields.io/pypi/dm/chainlit)
[![GitHub Contributors](https://img.shields.io/github/contributors/chainlit/chainlit)](https://github.com/chainlit/chainlit/graphs/contributors)
[![CI](https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg)](https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml)

**Build production-ready Conversational AI applications in minutes, not weeks âš¡ï¸**

Chainlit is an open-source async Python framework which allows developers to build scalable Conversational AI or agentic applications.

- âœ… ChatGPT-like application
- âœ… Embedded Chatbot & Software Copilot
- âœ… Slack & Discord
- âœ… Custom frontend (build your own agenti"
gpt-migrate,"<div align=""center"">

# â— &nbsp; GPT-Migrate &nbsp; â—‘

**Easily migrate your codebase from one framework or language to another.**

<p>
<a href=""https://github.com/0xpayne/gpt-migrate/blob/main/LICENSE""><img alt=""Github License"" src=""https://img.shields.io/badge/License-MIT-green.svg"" /></a>
<a href=""https://github.com/0xpayne/gpt-migrate""><img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/0xpayne/gpt-migrate?style=social"" /></a>
</p>

<br />

</div>

If you've ever faced the pain of migrating a codebase to a new framework or language, this project is for you.

https://user-images.githubusercontent.com/25165841/250232917-bcc99ce8-99b7-4e3d-a653-f89e163ed825.mp4

Migration is a costly, tedious, and non-trivial problem. Do not trust the current version blindly and please use responsibly. Please also be aware that costs can add up quickly as GPT-Migrate is designed to write (and potentially re-write) the entirety of a codebase.

However, with the collective brilliance o"
flask-restful,"# Flask-RESTful

[![Build Status](https://travis-ci.org/flask-restful/flask-restful.svg?branch=master)](http://travis-ci.org/flask-restful/flask-restful)
[![Coverage Status](http://img.shields.io/coveralls/flask-restful/flask-restful/master.svg)](https://coveralls.io/r/flask-restful/flask-restful)
[![PyPI Version](http://img.shields.io/pypi/v/Flask-RESTful.svg)](https://pypi.python.org/pypi/Flask-RESTful)

Flask-RESTful provides the building blocks for creating a great REST API.

## User Guide

You'll find the user guide and all documentation [here](https://flask-restful.readthedocs.io/)

"
BackgroundMattingV2,"# Real-Time High-Resolution Background Matting

![Teaser](https://github.com/PeterL1n/Matting-PyTorch/blob/master/images/teaser.gif?raw=true)

Official repository for the paper [Real-Time High-Resolution Background Matting](https://arxiv.org/abs/2012.07810). Our model requires capturing an additional background image and produces state-of-the-art matting results at 4K 30fps and HD 60fps on an Nvidia RTX 2080 TI GPU.

* [Visit project site](https://grail.cs.washington.edu/projects/background-matting-v2/)
* [Watch project video](https://www.youtube.com/watch?v=oMfPTeYDF9g)

**Disclaimer**: The video conversion script in this repo is not meant be real-time. Our research's main contribution is the neural architecture for high resolution refinement and the new matting datasets. The `inference_speed_test.py` script allows you to measure the tensor throughput of our model, which should achieve real-time. The `inference_video.py` script allows you to test your video on our model, but the video"
Depth-Anything,"<div align=""center"">
<h2>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</h2>

[**Lihe Yang**](https://liheyoung.github.io/)<sup>1</sup> Â· [**Bingyi Kang**](https://scholar.google.com/citations?user=NmHgX-wAAAAJ)<sup>2&dagger;</sup> Â· [**Zilong Huang**](http://speedinghzl.github.io/)<sup>2</sup> Â· [**Xiaogang Xu**](https://xiaogang00.github.io/)<sup>3,4</sup> Â· [**Jiashi Feng**](https://sites.google.com/site/jshfeng/)<sup>2</sup> Â· [**Hengshuang Zhao**](https://hszhao.github.io/)<sup>1*</sup>

<sup>1</sup>HKU&emsp;&emsp;&emsp;&emsp;<sup>2</sup>TikTok&emsp;&emsp;&emsp;&emsp;<sup>3</sup>CUHK&emsp;&emsp;&emsp;&emsp;<sup>4</sup>ZJU

&dagger;project lead&emsp;*corresponding author

**CVPR 2024**

<a href=""https://arxiv.org/abs/2401.10891""><img src='https://img.shields.io/badge/arXiv-Depth Anything-red' alt='Paper PDF'></a>
<a href='https://depth-anything.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything-green' alt='Project Page'></a>
<a href='h"
bertviz,"<h1 align=""center"">
    BertViz
</h1>
<h3 align=""center"">
 Visualize Attention in NLP Models
</h3>
<h3 align=""center"">
    <a href=""#-quick-tour"">Quick Tour</a> &bull;
    <a href=""#%EF%B8%8F-getting-started"">Getting Started</a> &bull;
    <a href=""https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing"">Colab Tutorial</a> &bull;
    <a href=""#-paper"">Paper</a>
</h3>

BertViz is an interactive tool for visualizing attention in [Transformer](https://jalammar.github.io/illustrated-transformer/) language models such as BERT, GPT2, or T5. It can be run inside a Jupyter or Colab
 notebook through a simple Python API that supports most [Huggingface models](https://huggingface.co/models). BertViz extends the
   [Tensor2Tensor visualization tool](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization)
    by [Llion Jones](https://medium.com/@llionj), providing multiple views that each offer a unique lens into the attention mechanism.

"
imbalanced-learn,".. -*- mode: rst -*-

.. _scikit-learn: http://scikit-learn.org/stable/

.. _scikit-learn-contrib: https://github.com/scikit-learn-contrib

|Azure|_ |Codecov|_ |CircleCI|_ |PythonVersion|_ |Pypi|_ |Gitter|_ |Black|_

.. |Azure| image:: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_apis/build/status/scikit-learn-contrib.imbalanced-learn?branchName=master
.. _Azure: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_build

.. |Codecov| image:: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn/branch/master/graph/badge.svg
.. _Codecov: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn

.. |CircleCI| image:: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn.svg?style=shield
.. _CircleCI: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn/tree/master

.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg
.. _PythonVersion: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg

.. |Pypi| image:"
docker-py,"# Docker SDK for Python

[![Build Status](https://github.com/docker/docker-py/actions/workflows/ci.yml/badge.svg)](https://github.com/docker/docker-py/actions/workflows/ci.yml)

A Python library for the Docker Engine API. It lets you do anything the `docker` command does, but from within Python apps â€“ run containers, manage containers, manage Swarms, etc.

## Installation

The latest stable version [is available on PyPI](https://pypi.python.org/pypi/docker/). Install with pip:

    pip install docker

> Older versions (< 6.0) required installing `docker[tls]` for SSL/TLS support.
> This is no longer necessary and is a no-op, but is supported for backwards compatibility.

## Usage

Connect to Docker using the default socket or the configuration in your environment:

```python
import docker
client = docker.from_env()
```

You can run containers:

```python
>>> client.containers.run(""ubuntu:latest"", ""echo hello world"")
'hello world\n'
```

You can run containers in the background:

```pyt"
NLP_ability,"# èƒŒæ™¯ä»‹ç»

NLPæ—¥å¸¸å·¥ä½œç»éªŒå’Œè®ºæ–‡è§£æï¼ŒåŒ…å«ï¼šé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ–‡æœ¬è¡¨å¾ï¼Œæ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œæ–‡æœ¬åˆ†ç±»ï¼Œå¤šæ¨¡æ€ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œè¯å‘é‡ã€‚

æˆ‘è§‰å¾—NLPæ˜¯ä¸€ä¸ªå€¼å¾—æ·±è€•çš„é¢†åŸŸï¼Œæ‰€ä»¥å¸Œæœ›å¯ä»¥ä¸åœçš„æå‡è‡ªå·±æ ¸å¿ƒç«äº‰åŠ›å’Œè‡ªå·±çš„æ®µä½ï¼

å¾®ä¿¡å…¬ä¼—å·ï¼šDASOU

## æ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†

### Transformer

1. [å²ä¸Šæœ€å…¨Transformeré¢è¯•é¢˜](./æ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†/Transformer/å²ä¸Šæœ€å…¨Transformeré¢è¯•é¢˜.md)
2. [ç­”æ¡ˆè§£æ(1)-å²ä¸Šæœ€å…¨Transformeré¢è¯•é¢˜](./æ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†/Transformer/ç­”æ¡ˆè§£æ(1)â€”å²ä¸Šæœ€å…¨Transformeré¢è¯•é¢˜ï¼šçµé­‚20é—®å¸®ä½ å½»åº•æå®šTransformer.md) 
3. [Pytorchä»£ç åˆ†æ--å¦‚ä½•è®©Bertåœ¨finetuneå°æ•°æ®é›†æ—¶æ›´â€œç¨³â€ä¸€ç‚¹](./æ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†/Bert/Pytorchä»£ç åˆ†æ-å¦‚ä½•è®©Bertåœ¨finetuneå°æ•°æ®é›†æ—¶æ›´â€œç¨³â€ä¸€ç‚¹.md)
4. [è§£å†³è€å¤§éš¾é—®é¢˜-å¦‚ä½•ä¸€è¡Œä»£ç å¸¦ä½ éšå¿ƒæ‰€æ¬²é‡æ–°åˆå§‹åŒ–bertçš„æŸäº›å‚æ•°(é™„Pytorchä»£ç è¯¦ç»†è§£è¯»)](https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/%E8%A7%A3%E5%86%B3%E8%80%81%E5%A4%A7%E9%9A%BE%E9%97%AE%E9%A2%98-%E5%A6%82%E4%BD%95%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%B8%A6%E4%BD%A0%E9%9A%8F%E5%BF%83%E6%89%80%E6%AC%B2%E9%87%8D%E6%96%B0%E5%88%9D%E5%A7%8B%E5%8C%96bert%E7%9A%84%E6%9F%90%E4%BA%9B%E5%8F%82%E6%95%B0(%E9%99%84Pytorch%E4%BB%A3%E7%A0%81).md)
5. [3åˆ†é’Ÿä»é›¶è§£è¯»Transformerçš„Encoder](https:/"
AzurLaneAutoScript,"**| [English](README_en.md) | ç®€ä½“ä¸­æ–‡ | [æ—¥æœ¬èª](README_jp.md) |**

# AzurLaneAutoScript

#### Discord [![](https://img.shields.io/discord/720789890354249748?logo=discord&logoColor=ffffff&color=4e4c97)](https://discord.gg/AQN6GeJ) QQç¾¤  ![](https://img.shields.io/badge/QQ%20Group-1087735381-4e4c97)
Azur Lane bot with GUI (Supports CN, EN, JP, TW, able to support other servers), designed for 24/7 running scenes, can take over almost all Azur Lane gameplay. Azur Lane, as a mobile game, has entered the late stage of its life cycle. During the period from now to the server down, please reduce the time spent on the Azur Lane and leave everything to Alas.

Alas is a free open source software, link: https://github.com/LmeSzinc/AzurLaneAutoScript

Alasï¼Œä¸€ä¸ªå¸¦GUIçš„ç¢§è“èˆªçº¿è„šæœ¬ï¼ˆæ”¯æŒå›½æœ, å›½é™…æœ, æ—¥æœ, å°æœ, å¯ä»¥æ”¯æŒå…¶ä»–æœåŠ¡å™¨ï¼‰ï¼Œä¸º 7x24 è¿è¡Œçš„åœºæ™¯è€Œè®¾è®¡ï¼Œèƒ½æ¥ç®¡è¿‘ä¹å…¨éƒ¨çš„ç¢§è“èˆªçº¿ç©æ³•ã€‚ç¢§è“èˆªçº¿ï¼Œä½œä¸ºä¸€ä¸ªæ‰‹æ¸¸ï¼Œå·²ç»è¿›å…¥äº†ç”Ÿå‘½å‘¨æœŸçš„æ™šæœŸã€‚ä»ç°åœ¨åˆ°å…³æœçš„è¿™æ®µæ—¶é—´é‡Œï¼Œè¯·å‡å°‘èŠ±è´¹åœ¨ç¢§è“èˆªçº¿ä¸Šçš„æ—¶é—´ï¼ŒæŠŠä¸€åˆ‡éƒ½äº¤ç»™ Alasã€‚

Alas æ˜¯ä¸€æ¬¾å…è´¹å¼€æºè½¯ä»¶ï¼Œåœ°å€ï¼šhttps://github.com/LmeSzinc/AzurLaneAutoScript

EN support, thanks **[@whoamikyo](https://gi"
SerpentAI,"![](https://s3.ca-central-1.amazonaws.com/serpent-ai-assets/SerpentFBCover.png)

# Serpent.AI - Game Agent Framework (Python)

[![](https://img.shields.io/badge/project-website-brightgreen.svg?colorB=1bcc6f&longCache=true)](http://serpent.ai)
[![](https://img.shields.io/badge/project-blog-brightgreen.svg?colorB=1bcc6f&longCache=true)](http://blog.serpent.ai)
[![](https://img.shields.io/badge/project-wiki-brightgreen.svg?colorB=1bcc6f&longCache=true)](https://github.com/SerpentAI/SerpentAI/wiki)    
[![](https://img.shields.io/badge/pypi-v2018.1.2-brightgreen.svg?colorB=007ec6&longCache=true)]()
[![](https://img.shields.io/badge/python-3.6-brightgreen.svg?colorB=007ec6&longCache=true)]()
[![](https://img.shields.io/badge/license-MIT-brightgreen.svg?colorB=007ec6&longCache=true)]()  
[![](https://img.shields.io/badge/twitter-@Serpent__AI-brightgreen.svg?colorB=1da1f2&longCache=true)](https://twitter.com/Serpent_AI)

## Update: Revival (May 2020)

Development work has resumed on the frame"
tinydb,".. image:: https://raw.githubusercontent.com/msiemens/tinydb/master/artwork/logo.png
    :scale: 100%
    :height: 150px

|Build Status| |Coverage| |Version|

Quick Links
***********

- `Example Code`_
- `Supported Python Versions`_
- `Documentation <http://tinydb.readthedocs.org/>`_
- `Changelog <https://tinydb.readthedocs.io/en/latest/changelog.html>`_
- `Extensions <https://tinydb.readthedocs.io/en/latest/extensions.html>`_
- `Contributing`_

Introduction
************

TinyDB is a lightweight document oriented database optimized for your happiness :)
It's written in pure Python and has no external dependencies. The target are
small apps that would be blown away by a SQL-DB or an external database server.

TinyDB is:

- **tiny:** The current source code has 1800 lines of code (with about 40%
  documentation) and 1600 lines tests.

- **document oriented:** Like MongoDB_, you can store any document
  (represented as ``dict``) in TinyDB.

- **optimized for your happiness:** TinyDB is de"
mealie,"[![Latest Release][latest-release-shield]][latest-release-url]
[![Contributors][contributors-shield]][contributors-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![AGPL License][license-shield]][license-url]
[![Docker Pulls][docker-pull]][docker-url]
[![GHCR Pulls][ghcr-pulls]][ghcr-url]

<!-- PROJECT LOGO -->
<br />
<p align=""center"">
  <a href=""https://github.com/mealie-recipes/mealie"">
<svg style=""width:100px;height:100px"" viewBox=""0 0 24 24"">
    <path fill=""currentColor"" d=""M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z"" />
</svg>
  </a>

  <h3 align=""center"">Mealie</h3>

  <p align=""center"">
    A Place For All Your Recipes
    <br />
 "
get_subscribe,"# â° å…è´¹æœºåœº å…è´¹æ¢¯å­ ç¿»å¢™VPN

## âš ï¸ æ³¨æ„

- æ¬¢è¿æ— äº§é˜¶çº§é©å‘½æ–—å£«å…è´¹ä½¿ç”¨æœ¬è®¢é˜…
- é“¾æ¥æ¥è‡ªç½‘ç»œï¼Œä»…ä½œå­¦ä¹ ä½¿ç”¨
- ä½¿ç”¨é¡µé¢æ‰€æä¾›çš„ä»»æ„èµ„æºæ—¶ï¼Œè¯·åŠ¡å¿…éµå®ˆå½“åœ°æ³•å¾‹

## ğŸš€ æ¯12å°æ—¶æ›´æ–°ä¸€æ¬¡

- clashè®¢é˜…é“¾æ¥ï¼š`https://git.io/emzclash`

- v2rayè®¢é˜…é“¾æ¥ï¼š`https://git.io/emzv2ray`

æ‰‹æœºç”¨æˆ·æ— æ³•è®¿é—®ä¸Šæ–¹çŸ­é“¾æ¥æ—¶å¯ä»¥ç”¨ä¸‹é¢çš„é•¿é“¾æ¥

- clashè®¢é˜…é“¾æ¥ï¼š`https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/clash.yml`

- v2rayè®¢é˜…é“¾æ¥ï¼š`https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/v2ray.txt`


## ğŸ“˜ å®¢æˆ·ç«¯ä½¿ç”¨æ–¹æ³•

- ğŸ“± [Android](https://www.ermao.net/skill/clashforandroid/)
- ğŸ–¥ [Windows](https://www.ermao.net/uncategorized/clash-for-windows/)

## ğŸ’¸ ä»˜è´¹è®¢é˜…

æˆ‘æœç½—çš„ä¸€äº›æ¯”è¾ƒä¾¿å®œçš„æœºåœºï¼ˆæœˆæ¶ˆè´¹10å—ä»¥ä¸‹ï¼‰ï¼Œè§‰å¾—å…è´¹è®¢é˜…ä¸å¥½ä½¿çš„æœ‹å‹ä»¬å¯ä»¥åœ¨è¿™é‡Œé¢æ‰¾æ‰¾ã€‚

[https://www.ermao.net/posts/vpn](https://www.ermao.net/posts/vpn)

| é“¾æ¥ | ä»·ä½ | å¤‡æ³¨ |
|----|----|----|
|[m.ssone.io](https://hello-ssone.com/register?aff=aBHsE1pF)|10å…ƒ 100G/æœˆ|æ­£å¸¸è®¿é—®|
|[https://www.efcloud.bio](https://www.efcloud.bio/#/register?code=kbbSUTvm)|	9 å…ƒ 350G/æœˆ|æ­£å¸¸è®¿é—®|
|[https://www.fccloud.cc](https://www.fccloud.cc/#/register?code=AYsN4z5L)|	10 å…ƒ 150G/æœˆ|æ­£å¸¸è®¿é—®|
|[https://ss.vgsseven"
ffsubsync,"FFsubsync
=======

[![CI Status](https://github.com/smacke/ffsubsync/workflows/ffsubsync/badge.svg)](https://github.com/smacke/ffsubsync/actions)
[![Support Ukraine](https://badgen.net/badge/support/UKRAINE/?color=0057B8&labelColor=FFD700)](https://github.com/vshymanskyy/StandWithUkraine/blob/main/docs/README.md)
[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-maroon.svg)](https://opensource.org/licenses/MIT)
[![Python Versions](https://img.shields.io/pypi/pyversions/ffsubsync.svg)](https://pypi.org/project/ffsubsync)
[![Documentation Status](https://readthedocs.org/projects/ffsubsync/badge/?version=latest)](https://ffsubsync.readthedocs.io/en/latest/?badge=latest)
[![PyPI Version](https://img.shields.io/pypi/v/ffsubsync.svg)](https://pypi.org/project/ffsubsync)


Language"
faust,".. XXX Need to change this image to readthedocs before release

.. image:: https://raw.githubusercontent.com/robinhood/faust/8ee5e209322d9edf5bdb79b992ef986be2de4bb4/artwork/banner-alt1.png

===========================
 Deprecation Notice
===========================

This library has been deprecated and no longer managed or supported. The current active community project can be found at https://github.com/faust-streaming/faust

===========================
 Python Stream Processing
===========================

|build-status| |coverage| |license| |wheel| |pyversion| |pyimp|

:Version: 1.10.4
:Web: http://faust.readthedocs.io/
:Download: http://pypi.org/project/faust
:Source: http://github.com/robinhood/faust
:Keywords: distributed, stream, async, processing, data, queue, state management


.. sourcecode:: python

    # Python Streams
    # Forever scalable event processing & in-memory durable K/V store;
    # as a library w/ asyncio & static typing.
    import faust

**Faust** is a strea"
OpenNMT-py,"# Announcement: OpenNMT-py is no longer actively supported.

We started a new project [Eole](https://eole-nlp.github.io/eole/) available on [Github](https://github.com/eole-nlp/eole)

It is a spin-off of OpenNMT-py in terms of features but we revamped a lot of stuff.

Eole handles NMT, LLM, Encoders as well as a new concept of Estimator within a NMT Model See this [post](https://medium.com/p/05b00b271a47) and this [news](https://www.linkedin.com/posts/vincentnguyenngoc_embarrassingly-small-english-to-german-model-activity-7203400634727841792-FCre?utm_source=share&utm_medium=member_desktop)

If you are a developer, switch now. If you are a user only, then we will publish the first py-pi versions shortly.


# OpenNMT-py: Open-Source Neural Machine Translation and (Large) Language Models

[![Build Status](https://github.com/OpenNMT/OpenNMT-py/workflows/Lint%20&%20Tests/badge.svg)](https://github.com/OpenNMT/OpenNMT-py/actions)
[![Documentation](https://img.shields.io/badge/docs-latest-blu"
python,"# Kubernetes Python Client

[![Build Status](https://travis-ci.org/kubernetes-client/python.svg?branch=master)](https://travis-ci.org/kubernetes-client/python)
[![PyPI version](https://badge.fury.io/py/kubernetes.svg)](https://badge.fury.io/py/kubernetes)
[![codecov](https://codecov.io/gh/kubernetes-client/python/branch/master/graph/badge.svg)](https://codecov.io/gh/kubernetes-client/python ""Non-generated packages only"")
[![pypi supported versions](https://img.shields.io/pypi/pyversions/kubernetes.svg)](https://pypi.python.org/pypi/kubernetes)
[![Client Capabilities](https://img.shields.io/badge/Kubernetes%20client-Silver-blue.svg?style=flat&colorB=C0C0C0&colorA=306CE8)](http://bit.ly/kubernetes-client-capabilities-badge)
[![Client Support Level](https://img.shields.io/badge/kubernetes%20client-beta-green.svg?style=flat&colorA=306CE8)](http://bit.ly/kubernetes-client-support-badge)

Python client for the [kubernetes](http://kubernetes.io/) API.

## Installation

From source:

```
git c"
vits,"# VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech

### Jaehyeon Kim, Jungil Kong, and Juhee Son

In our recent [paper](https://arxiv.org/abs/2106.06103), we propose VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.

Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duratio"
Auto_Bangumi,"<p align=""center"">
    <img src=""docs/image/icons/light-icon.svg#gh-light-mode-only"" width=50%/ alt="""">
    <img src=""docs/image/icons/dark-icon.svg#gh-dark-mode-only"" width=50%/ alt="""">
</p>
<p align=""center"">
    <img title=""docker build version"" src=""https://img.shields.io/docker/v/estrellaxd/auto_bangumi"" alt="""">
    <img title=""release date"" src=""https://img.shields.io/github/release-date/estrellaxd/auto_bangumi"" alt="""">
    <img title=""docker pull"" src=""https://img.shields.io/docker/pulls/estrellaxd/auto_bangumi"" alt="""">
    <img title=""python version"" src=""https://img.shields.io/badge/python-3.11-blue"" alt="""">
</p>

<p align=""center"">
  <a href=""https://www.autobangumi.org"">å®˜æ–¹ç½‘ç«™</a> | <a href=""https://www.autobangumi.org/deploy/quick-start.html"">å¿«é€Ÿå¼€å§‹</a> | <a href=""https://www.autobangumi.org/changelog/3.0.html"">æ›´æ–°æ—¥å¿—</a> | <a href=""https://t.me/autobangumi_update"">æ›´æ–°æ¨é€</a> | <a href=""https://t.me/autobangumi"">TG ç¾¤ç»„</a>
</p>

# é¡¹ç›®è¯´æ˜

<p align=""center"">
    <img title=""AutoBangumi"
eve,"Eve
====
.. image:: https://img.shields.io/pypi/v/eve.svg?style=flat-square
    :target: https://pypi.org/project/eve

.. image:: https://github.com/pyeve/eve/workflows/CI/badge.svg
  :target: https://github.com/pyeve/eve/actions?query=workflow%3ACI

.. image:: https://img.shields.io/pypi/pyversions/eve.svg?style=flat-square
    :target: https://pypi.org/project/eve

.. image:: https://img.shields.io/badge/license-BSD-blue.svg?style=flat-square
    :target: https://en.wikipedia.org/wiki/BSD_License

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

Eve is an open source Python REST API framework designed for human beings. It
allows to effortlessly build and deploy highly customizable, fully featured
RESTful Web Services. Eve offers native support for MongoDB, and SQL backends
via community extensions.

Eve is Simple
-------------
.. code-block:: python

    from eve import Eve

    app = Eve()
    app.run()

The API is now"
BasicSR,"<p align=""center"">
  <img src=""assets/basicsr_xpixel_logo.png"" height=120>
</p>

## <div align=""center""><b><a href=""README.md"">English</a> | <a href=""README_CN.md"">ç®€ä½“ä¸­æ–‡</a></b></div>

<div align=""center"">

[![LICENSE](https://img.shields.io/github/license/xinntao/basicsr.svg)](https://github.com/xinntao/BasicSR/blob/master/LICENSE.txt)
[![PyPI](https://img.shields.io/pypi/v/basicsr)](https://pypi.org/project/basicsr/)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/xinntao/BasicSR.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/xinntao/BasicSR/context:python)
[![python lint](https://github.com/xinntao/BasicSR/actions/workflows/pylint.yml/badge.svg)](https://github.com/xinntao/BasicSR/blob/master/.github/workflows/pylint.yml)
[![Publish-pip](https://github.com/xinntao/BasicSR/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/xinntao/BasicSR/blob/master/.github/workflows/publish-pip.yml)
[![gitee mirror](https://github.com/xinntao/BasicSR/act"
ragas,"<h1 align=""center"">
  <img style=""vertical-align:middle"" height=""200""
  src=""./docs/_static/imgs/logo.png"">
</h1>
<p align=""center"">
  <i>Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines</i>
</p>

<p align=""center"">
    <a href=""https://github.com/explodinggradients/ragas/releases"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/explodinggradients/ragas.svg"">
    </a>
    <a href=""https://www.python.org/"">
            <img alt=""Build"" src=""https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple"">
    </a>
    <a href=""https://github.com/explodinggradients/ragas/blob/master/LICENSE"">
        <img alt=""License"" src=""https://img.shields.io/github/license/explodinggradients/ragas.svg?color=green"">
    </a>
    <a href=""https://colab.research.google.com/github/explodinggradients/ragas/blob/main/docs/quickstart.ipynb"">
        <img alt=""Open In Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg"">
    "
tensorflow_practice,"Tensroflowç»ƒä¹ 
======

ç›¸å…³æ•°æ®é›†ä¸‹è½½åœ°å€ï¼šé“¾æ¥:https://pan.baidu.com/s/1GMv7_3qruoVZBJMvN-afGA  å¯†ç :ako7
åŸºäºtf1.4

ç›®å½•

1ã€åŸºç¡€<br>
[åŸºæœ¬è¯­æ³•<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/basic.py)
[tensorBoardä½¿ç”¨<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/tensorBoard.py)
[dropout<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/dropout.py)
[æ¨¡å‹ä¿å­˜ä¸é‡è½½<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/save2file.py)
[åŸºæœ¬ç¥ç»ç½‘ç»œ<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/first_nerual_network.py)
[å·ç§¯ç¥ç»ç½‘ç»œ<br>](https://github.com/princewen/tensorflow_practice/blob/master/basic/CNN.py)
<br>
2ã€è‡ªç„¶è¯­è¨€ç›¸å…³<br>
[static_RNN<br>](https://github.com/princewen/tensorflow_practice/blob/master/nlp/RNN_static_cell.py)
[dynamic_RNN<br>](https://github.com/princewen/tensorflow_practice/blob/master/nlp/RNN_dynamic_cell.py)
[LSTM<br>](https://github.com/princewen/tensorflow_practice/blob/master/nlp/LSTM.py)
[LSTM_"
backgroundremover,"# BackgroundRemover
![Background Remover](https://raw.githubusercontent.com/nadermx/backgroundremover/main/examplefiles/backgroundremoverexample.png)
<img alt=""background remover video"" src=""https://raw.githubusercontent.com/nadermx/backgroundremover/main/examplefiles/backgroundremoverprocessed.gif"" height=""200"" /><br>
BackgroundRemover is a command line tool to remove background from [image](https://github.com/nadermx/backgroundremover#image) and [video](https://github.com/nadermx/backgroundremover#video) using AI, made by [nadermx](https://john.nader.mx) to power [https://BackgroundRemoverAI.com](https://backgroundremoverai.com). If you wonder why it was made read this [short blog post](https://johnathannader.com/my-first-open-source-project/).<br>


### Requirements

* python >= 3.6
* python3.6-dev #or what ever version of python you use
* torch and torchvision stable version (https://pytorch.org)
* ffmpeg 4.4+

* To clarify, you must install both python and whatever dev version of "
The-Grand-Complete-Data-Science-Materials,"# The Grand Complete Data Science Guide With Videos And Materials

## 1. Complete Python Playlist For Data Analytics And Data Science

- Python In English: https://www.youtube.com/watch?v=bPrmA1SEN2k&list=PLZoTAELRMXVNUL99R4bDlVYsncUNvwUBB
- Python In Hindi: https://www.youtube.com/watch?v=MJd9d9Mpxg0&list=PLTDARY42LDV4qqiJd1Z1tShm3mp9-rP4v

## 2. Complete Stats Playlist For Data Analytics And Data Science

- Stats In English One Shot: https://www.youtube.com/watch?v=LZzq1zSL1bs
- Stats In English Detailed Playlist: https://www.youtube.com/watch?v=zRUliXuwJCQ&list=PLZoTAELRMXVMhVyr3Ri9IQ-t5QPBtxzJO
- Stats In Hindi Detailed Playlist: https://www.youtube.com/watch?v=7y3XckjaVOw&list=PLTDARY42LDV6YHSRo669_uDDGmUEmQnDJ

## 3. Complete SQL For Data Analytics And Data Science

- Complete SQl Detailed Playlist English: https://www.youtube.com/watch?v=us1XyayQ6fU&list=PLZoTAELRMXVNMRWlVf0bDDSxNEn38u9Cl
- Complete SQL Detailed Playlist Hindi : **Coming Soon**
- Complete SQL One Shot : **Coming"
DeepPavlov,"# DeepPavlov 1.0

[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
![Python 3.6, 3.7, 3.8, 3.9, 3.10, 3.11](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-green.svg)
[![Downloads](https://pepy.tech/badge/deeppavlov)](https://pepy.tech/project/deeppavlov)
[![Static Badge](https://img.shields.io/badge/DeepPavlov%20Community-blue)](https://forum.deeppavlov.ai/)
[![Static Badge](https://img.shields.io/badge/DeepPavlov%20Demo-blue)](https://demo.deeppavlov.ai/)


DeepPavlov 1.0 is an open-source NLP framework built on [PyTorch](https://pytorch.org/) and [transformers](https://github.com/huggingface/transformers). DeepPavlov 1.0 is created for modular and configuration-driven development of state-of-the-art NLP models and supports a wide range of NLP model applications. DeepPavlov 1.0 is designed for practitioners with limited knowledge of NLP/ML.

## Quick Links

|name|Description|
|--|--|
| â­"
Github-Ranking,"[Github Ranking](./README.md)
==========

**A list of the most github stars and forks repositories.**

*Last Automatic Update Time: 2024-09-26T03:04:35Z*

## Table of Contents

* [Most Stars](#most-stars)
* [Most Forks](#most-forks)
* [ActionScript](#actionscript)
* [C](#c)
* [C\#](#c-1)
* [C\+\+](#c-2)
* [Clojure](#clojure)
* [CoffeeScript](#coffeescript)
* [CSS](#css)
* [Dart](#dart)
* [DM](#dm)
* [Elixir](#elixir)
* [Go](#go)
* [Groovy](#groovy)
* [Haskell](#haskell)
* [HTML](#html)
* [Java](#java)
* [JavaScript](#javascript)
* [Julia](#julia)
* [Kotlin](#kotlin)
* [Lua](#lua)
* [MATLAB](#matlab)
* [Objective\-C](#objective-c)
* [Perl](#perl)
* [PHP](#php)
* [PowerShell](#powershell)
* [Python](#python)
* [R](#r)
* [Ruby](#ruby)
* [Rust](#rust)
* [Scala](#scala)
* [Shell](#shell)
* [Swift](#swift)
* [TeX](#tex)
* [TypeScript](#typeScript)
* [Vim script](#vim-script)
## Most Stars

This is top 10, for more click **[Top 100 Stars](Top100/Top-100-stars.md)**

| Ranking | Project Name |"
patroni,"|Tests Status| |Coverage Status|

Patroni: A Template for PostgreSQL HA with ZooKeeper, etcd or Consul
--------------------------------------------------------------------

You can find a version of this documentation that is searchable and also easier to navigate at `patroni.readthedocs.io <https://patroni.readthedocs.io>`__.


There are many ways to run high availability with PostgreSQL; for a list, see the `PostgreSQL Documentation <https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling>`__.

Patroni is a template for high availability (HA) PostgreSQL solutions using Python. For maximum accessibility, Patroni supports a variety of distributed configuration stores like `ZooKeeper <https://zookeeper.apache.org/>`__, `etcd <https://github.com/coreos/etcd>`__, `Consul <https://github.com/hashicorp/consul>`__ or `Kubernetes <https://kubernetes.io>`__. Database engineers, DBAs, DevOps engineers, and SREs who are looking to quickly deploy HA PostgreSQL in datacent"
DeepLearningFlappyBird,"# Using Deep Q-Network to Learn How To Play Flappy Bird

<img src=""./images/flappy_bird_demp.gif"" width=""250"">

7 mins version: [DQN for flappy bird](https://www.youtube.com/watch?v=THhUXIhjkCM)

## Overview
This project follows the description of the Deep Q Learning algorithm described in Playing Atari with Deep Reinforcement Learning [2] and shows that this learning algorithm can be further generalized to the notorious Flappy Bird.

## Installation Dependencies:
* Python 2.7 or 3
* TensorFlow 0.7
* pygame
* OpenCV-Python

## How to Run?
```
git clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git
cd DeepLearningFlappyBird
python deep_q_network.py
```

## What is Deep Q-Network?
It is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.

For those who are interested in deep reinforcement learning, I highly recommend to read the following post:

[Demystifying Deep Re"
werkzeug,"# Werkzeug

*werkzeug* German noun: ""tool"". Etymology: *werk* (""work""), *zeug* (""stuff"")

Werkzeug is a comprehensive [WSGI][] web application library. It began as
a simple collection of various utilities for WSGI applications and has
become one of the most advanced WSGI utility libraries.

It includes:

-   An interactive debugger that allows inspecting stack traces and
    source code in the browser with an interactive interpreter for any
    frame in the stack.
-   A full-featured request object with objects to interact with
    headers, query args, form data, files, and cookies.
-   A response object that can wrap other WSGI applications and handle
    streaming data.
-   A routing system for matching URLs to endpoints and generating URLs
    for endpoints, with an extensible system for capturing variables
    from URLs.
-   HTTP utilities to handle entity tags, cache control, dates, user
    agents, cookies, files, and more.
-   A threaded WSGI server for use while developing appl"
monkey,"# Infection Monkey
[![GitHub release (latest by date)](https://img.shields.io/github/v/release/guardicore/monkey)](https://github.com/guardicore/monkey/releases)

[![Build Status](https://app.travis-ci.com/guardicore/monkey.svg?branch=develop)](https://app.travis-ci.com/guardicore/monkey)
[![codecov](https://codecov.io/gh/guardicore/monkey/branch/develop/graph/badge.svg)](https://codecov.io/gh/guardicore/monkey)

![GitHub stars](https://img.shields.io/github/stars/guardicore/monkey)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/guardicore/monkey)

Welcome to Infection Monkey!  We're glad you could swing by.ğŸ’ Here's all the
info you'll need to start monkeying around.

## What is Infection Monkey?
Infection Monkey is an open-source adversary emulation platform that helps you
improve your security posture using empirical data. The Monkey uses various
methods to self-propagate across a network and reports its activities to a
centralized command and control serve"
icloud_photos_downloader,"# iCloud Photos Downloader [![Quality Checks](https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Quality%20Checks/badge.svg)](https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/quality-checks.yml) [![Multi Platform Docker Build](https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Docker%20Build/badge.svg)](https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/docker-build.yml) [![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

- A command-line tool to download all your iCloud photos.
- Works on Linux, Windows, and macOS; laptop, desktop, and NAS
- Available as an executable for direct downloading and through package managers/ecosystems ([Docker](https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker), [PyPI](https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi), [AUR](http"
ScoutSuite,"<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/4206926/49877604-10457580-fe26-11e8-92d7-cd876c4f6454.png"" width=350/>
</p>

#

[![Workflow](https://github.com/nccgroup/ScoutSuite/workflows/CI%20Workflow/badge.svg)](https://github.com/nccgroup/ScoutSuite/actions)
[![CodeCov](https://codecov.io/gh/nccgroup/ScoutSuite/branch/master/graph/badge.svg)](https://codecov.io/gh/nccgroup/ScoutSuite)

[![PyPI version](https://badge.fury.io/py/ScoutSuite.svg)](https://badge.fury.io/py/ScoutSuite)
[![PyPI downloads](https://img.shields.io/pypi/dm/scoutsuite)](https://img.shields.io/pypi/dm/scoutsuite)
[![Docker Hub](https://img.shields.io/badge/Docker%20Hub-rossja%2Fncc--scoutsuite-blue)](https://hub.docker.com/r/rossja/ncc-scoutsuite/)
[![Docker Pulls](https://img.shields.io/docker/pulls/rossja/ncc-scoutsuite.svg?style=flat-square)](https://hub.docker.com/r/rossja/ncc-scoutsuite/)

## Description

Scout Suite is an open source multi-cloud security-auditing tool, which enab"
pix2pixHD,"<img src='imgs/teaser_720.gif' align=""right"" width=360>

<br><br><br><br>

# pix2pixHD
### [Project](https://tcwang0509.github.io/pix2pixHD/) | [Youtube](https://youtu.be/3AIpPlzM_qs) | [Paper](https://arxiv.org/pdf/1711.11585.pdf) <br>
Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic image-to-image translation. It can be used for turning semantic label maps into photo-realistic images or synthesizing portraits from face label maps. <br><br>
[High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://tcwang0509.github.io/pix2pixHD/)  
 [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyuliu.net/)<sup>1</sup>, [Jun-Yan Zhu](http://people.eecs.berkeley.edu/~junyanz/)<sup>2</sup>, Andrew Tao<sup>1</sup>, [Jan Kautz](http://jankautz.com/)<sup>1</sup>, [Bryan Catanzaro](http://catanzaro.name/)<sup>1</sup>  
 <sup>1</sup>NVIDIA Corporation, <sup>2</sup>UC Berkeley  
 In CVPR 20"
aws-devops-zero-to-hero,"# aws-devops-zero-to-hero

Complete YouTube playlist - https://www.youtube.com/playlist?list=PLdpzxOOAlwvLNOxX0RfndiYSt1Le9azze

AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples. Each day's class will provide real-time knowledge on AWS services, allowing you to apply what you've learned and gain practical skills in working with AWS in a DevOps context.

## Day 1: Introduction to AWS

You will learn what is private and public cloud. Why companies are moving to public cloud, what are the advantages of moving to cloud.

Also, you will be introduced to the basics of AWS, including the core services and their significance in DevOps practices. Finally learn how to set up an AWS account and navigate the AWS Management Console.

## Day 2: IAM (Identity and Access Management)

You will explore IAM, which is used for managing access to AWS resources. You'll learn how to create IAM users, gro"
boxmot,"# BoxMOT: pluggable SOTA tracking modules for segmentation, object detection and pose estimation models

<div align=""center"">
  <p>
  <img src=""assets/images/track_all_seg_1280_025conf.gif"" width=""400""/>
  </p>
  <br>
  <div>
  <a href=""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml""><img src=""https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml/badge.svg"" alt=""CI CPU testing""></a>
  <a href=""https://pepy.tech/project/boxmot""><img src=""https://static.pepy.tech/badge/boxmot""></a>
  <br>
  <a href=""https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a>
<a href=""https://doi.org/10.5281/zenodo.8132989""><img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.8132989.svg"" alt=""DOI""></a>
<a href=""https://hub.docker.com/r/boxmot/boxmot""><img src=""https://img.shields.io/docker/pulls/boxmot/boxmot?logo=docker"" alt=""Ultralytic"
jupytext,"![](https://github.com/mwouts/jupytext/blob/17aea37c612f33a4e27eeee4b81966f1506920fd/docs/images/logo_large.png?raw=true)

<!-- INDEX-START -->

[![CI](https://github.com/mwouts/jupytext/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/mwouts/jupytext/actions)
[![Documentation Status](https://readthedocs.org/projects/jupytext/badge/?version=latest)](https://jupytext.readthedocs.io/en/latest/?badge=latest)
[![codecov.io](https://codecov.io/github/mwouts/jupytext/coverage.svg?branch=main)](https://codecov.io/gh/mwouts/jupytext/branch/main)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![GitHub language count](https://img.shields.io/github/languages/count/mwouts/jupytext)](docs/languages.md)
[![Conda Version](https://img.shields.io/conda/vn/conda-forge/jupytext.svg)](https://anaconda.org/conda-forge/jupytext)
[![Pypi](https://img.shields.io/pypi/v/jupytext.svg)](https://pypi.python.org/pypi/jupytext)
[!"
autocut,"# AutoCut: é€šè¿‡å­—å¹•æ¥å‰ªåˆ‡è§†é¢‘

AutoCut å¯¹ä½ çš„è§†é¢‘è‡ªåŠ¨ç”Ÿæˆå­—å¹•ã€‚ç„¶åä½ é€‰æ‹©éœ€è¦ä¿ç•™çš„å¥å­ï¼ŒAutoCut å°†å¯¹ä½ è§†é¢‘ä¸­å¯¹åº”çš„ç‰‡æ®µè£åˆ‡å¹¶ä¿å­˜ã€‚ä½ æ— éœ€ä½¿ç”¨è§†é¢‘ç¼–è¾‘è½¯ä»¶ï¼Œåªéœ€è¦ç¼–è¾‘æ–‡æœ¬æ–‡ä»¶å³å¯å®Œæˆå‰ªåˆ‡ã€‚

**2024.03.10æ›´æ–°**ï¼šæ”¯æŒ pip å®‰è£…å’Œæä¾› import è½¬å½•ç›¸å…³çš„åŠŸèƒ½

```shell
# Install
pip install autocut-sub
```

```python
from autocut import Transcribe, load_audio
```


**2023.10.14æ›´æ–°**ï¼šæ”¯æŒ faster-whisper å’ŒæŒ‡å®šä¾èµ–ï¼ˆä½†ç”±äº Action é™åˆ¶æš‚æ—¶ç§»é™¤äº† faster-whisper çš„æµ‹è¯•è¿è¡Œï¼‰

```shell
# for whisper only
pip install .

# for whisper and faster-whisper
pip install '.[faster]'

# for whisper and openai-whisper
pip install '.[openai]'

# for all
pip install '.[all]'
```

```shell
# using faster-whisper
autocut -t xxx --whisper-mode=faster
```

```shell
# using openai api
export OPENAI_API_KEY=sk-xxx
autocut -t xxx --whisper-mode=openai --openai-rpm=3
```

**2023.8.13æ›´æ–°**ï¼šæ”¯æŒè°ƒç”¨ Openai Whisper API
```shell
export OPENAI_API_KEY=sk-xxx
autocut -t xxx --whisper-mode=openai --openai-rpm=3
```

## ä½¿ç”¨ä¾‹å­

å‡å¦‚ä½ å½•åˆ¶çš„è§†é¢‘æ”¾åœ¨ `2022-11-04/` è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œã€‚é‚£ä¹ˆè¿è¡Œ

```bash
autocut -d 2022-11-04
```

> æç¤ºï¼šå¦‚æœä½ ä½¿ç”¨ OBS å½•å±ï¼Œå¯ä»¥åœ¨ `è®¾ç½®->é«˜çº§->å½•åƒ->æ–‡ä»¶åæ ¼å¼` ä¸­å°†ç©ºæ ¼æ”¹æˆ `/`ï¼Œå³"
RedditVideoMakerBot,"# Reddit Video Maker Bot ğŸ¥

All done WITHOUT video editing or asset compiling. Just pure âœ¨programming magicâœ¨.

Created by Lewis Menelaws & [TMRRW](https://tmrrwinc.ca)

<a target=""_blank"" href=""https://tmrrwinc.ca"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://user-images.githubusercontent.com/6053155/170528535-e274dc0b-7972-4b27-af22-637f8c370133.png"">
  <source media=""(prefers-color-scheme: light)"" srcset=""https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png"">
  <img src=""https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png"" width=""350"">
</picture>

</a>

## Video Explainer

[![lewisthumbnail](https://user-images.githubusercontent.com/6053155/173631669-1d1b14ad-c478-4010-b57d-d79592a789f2.png)
](https://www.youtube.com/watch?v=3gjcY_00U1w)

## Motivation ğŸ¤”

These videos on TikTok, YouTube and Instagram get MILLIONS of views across all platforms and require very "
skypilot,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png"">
    <img alt=""SkyPilot"" src=""https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png"" width=55%>
  </picture>
</p>

<p align=""center"">
  <a href=""https://skypilot.readthedocs.io/en/latest/"">
    <img alt=""Documentation"" src=""https://readthedocs.org/projects/skypilot/badge/?version=latest"">
  </a>

  <a href=""https://github.com/skypilot-org/skypilot/releases"">
    <img alt=""GitHub Release"" src=""https://img.shields.io/github/release/skypilot-org/skypilot.svg"">
  </a>

  <a href=""http://slack.skypilot.co"">
    <img alt=""Join Slack"" src=""https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack"">
  </a>

</p>

<h3 align=""center"">
    Run AI on Any Infra â€” Unified, Faster, Cheaper
</h3>

----
:fire: *News* :fire:
- [Sep, 2024] Point, L"
PyQt,"# å„ç§å„æ ·çš„PyQtæµ‹è¯•å’Œä¾‹å­

[![Blog](https://img.shields.io/badge/blog-pyqt-green.svg)](https://pyqt.site)
[![codebeat badge](https://codebeat.co/badges/d23d0dc8-aef3-43d2-96aa-e3215b2c9861)](https://codebeat.co/projects/github-com-pyqt5-pyqt-master)
[![Badge](https://img.shields.io/badge/link-996.icu-%23FF4D5B.svg?style=flat-square)](https://996.icu/#/zh_CN)
[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square)](https://github.com/996icu/996.ICU/blob/master/LICENSE)

[https://pyqt.site](https://pyqt.site) è®ºå›æ˜¯ä¸“é—¨é’ˆå¯¹PyQt5å­¦ä¹ å’Œæå‡å¼€è®¾çš„ç½‘ç«™ï¼Œåˆ†äº«å¤§å®¶å¹³æ—¶å­¦ä¹ ä¸­è®°å½•çš„ç¬”è®°å’Œä¾‹å­ï¼Œä»¥åŠå¯¹é‡åˆ°çš„é—®é¢˜è¿›è¡Œæ”¶é›†æ•´ç†ã€‚

[![GitHub watchers](https://img.shields.io/github/watchers/PyQt5/PyQt.svg?style=social&label=Watch)](https://github.com/PyQt5/PyQt)
[![GitHub stars](https://img.shields.io/github/stars/PyQt5/PyQt.svg?style=social)](https://github.com/PyQt5/PyQt)
[![GitHub forks](https://img.shields.io/github/forks/PyQt5/PyQt.svg?style=social)](https://github.com/PyQt5/PyQt/fork)

å¦‚æœæ‚¨è§‰å¾—è¿™é‡Œçš„ä¸œè¥¿å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œåˆ«å¿˜äº†å¸®å¿™ç‚¹ä¸€é¢—:star:å°æ˜Ÿæ˜Ÿ:star:
"
DeepSeek-Coder,"<p align=""center"">
<img width=""1000px"" alt=""DeepSeek Coder"" src=""pictures/logo.png"">
</p>
<p align=""center""><a href=""https://www.deepseek.com/"">[<img src=""pictures/home.png"" width=""20px""> Homepage]</a> | <a href=""https://coder.deepseek.com/"">[ğŸ¤– Chat with DeepSeek Coder]</a> | <a href=""https://huggingface.co/deepseek-ai"">[ğŸ¤— Models Download]</a> | <a href=""https://discord.gg/Tc7c45Zzu5"">[Discord]</a> | <a href=""https://github.com/guoday/assert/blob/main/QR.png?raw=true"">[WeChat (å¾®ä¿¡)]</a></p>
<p align=""center"">
  <a href=""https://huggingface.co/papers/2401.14196""><b>Paper Link</b>ğŸ‘ï¸</a>
</p>
<hr>


### 1. Introduction of DeepSeek Coder

DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and"
stock,"### è¯´æ˜ï¼Œé¡¹ç›®è¿ç§»åˆ°äº†Gitee å•¦ï¼Œæœ€åä¸€æ¬¡ä¿®æ”¹ï¼Œ2023-06-02 æ‰§è¡Œå­˜æ¡£

é¡¹ç›®è¿ç§»åˆ°è¿™é‡Œäº†ï¼šæ­¤é¡¹ç›®åç»­æ›´æ–°è®¿é—®è¿™é‡Œï¼š

https://gitee.com/pythonstock/stock

githubé¡¹ç›®åç»­å°±Archiveså­˜æ¡£äº†ï¼Œä¸å†æ›´æ–°äº†ï¼

csdnçš„pythonstockä¸“æ åœ°å€ï¼Œç›¸å…³èµ„æ–™éƒ½åœ¨è¿™é‡Œæœ‰è¯´æ˜ï¼š

https://blog.csdn.net/freewebsys/category_9285317.html


### pythonstock V2 é¡¹ç›®ç®€ä»‹


**ç‰¹åˆ«è¯´æ˜ï¼šè‚¡å¸‚æœ‰é£é™©æŠ•èµ„éœ€è°¨æ…ï¼Œæœ¬é¡¹ç›®åªèƒ½ç”¨äºPythonä»£ç å­¦ä¹ ï¼Œè‚¡ç¥¨åˆ†æï¼ŒæŠ•èµ„å¤±è´¥äºé’±ä¸è´Ÿè´£ï¼Œä¸ç®—BUGã€‚**

```
é¡¹ç›®åœ°å€ï¼šhttps://github.com/pythonstock/stock
PythonStock V2 æ˜¯åŸºäºPythonçš„pandasï¼Œakshareï¼Œbokehï¼Œtornadoï¼Œstockstatsï¼Œta-libç­‰æ¡†æ¶å¼€å‘çš„å…¨æ ˆè‚¡ç¥¨ç³»ç»Ÿã€‚
é¡¹ç›®åˆ›å»ºäº2017å¹´7æœˆ17æ—¥ï¼Œæ¯æœˆä¸å®šæœŸæ›´æ–°ã€‚
1ï¼‰å¯ä»¥ç›´æ¥ä½¿ç”¨dockerç›´æ¥æœ¬åœ°éƒ¨ç½²è¿è¡Œï¼Œæ•´ä¸ªé¡¹ç›®åœ¨docker hubä¸Šå‹ç¼©å200MBï¼Œæœ¬åœ°å ç”¨500MBç£ç›˜ç©ºé—´ã€‚
2ï¼‰ä½¿ç”¨Dockerè§£å†³äº†Pythonåº“å®‰è£…é—®é¢˜ï¼Œä½¿ç”¨Mariadbï¼ˆMySQLï¼‰å­˜å‚¨æ•°æ®ã€‚å€ŸåŠ©akshareæŠ“å–æ•°æ®ã€‚
3ï¼‰ä½¿ç”¨cronåšå®šæ—¶ä»»åŠ¡ï¼Œæ¯å¤©è¿›è¡Œæ•°æ®æŠ“å–è®¡ç®—ï¼Œæ¯å¤©18ç‚¹å¼€å§‹è¿›è¡Œæ•°æ®è®¡ç®—ï¼Œè®¡ç®—å½“æ—¥æ•°æ®ï¼Œä½¿ç”¨300å¤©æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œå¤§çº¦éœ€è¦15åˆ†é’Ÿè®¡ç®—å®Œæ¯•ã€‚
4ï¼‰è‚¡ç¥¨æ•°æ®æ¥å£é˜²æ­¢è¢«å°ï¼ŒæŒ‰å¤©è¿›è¡Œæ•°æ®ç¼“å­˜ï¼Œå‚¨å­˜æœ€è¿‘3å¤©æ•°æ®ï¼Œæ¯å¤©å®šæ—¶æ¸…é™¤ï¼ŒåŒæ—¶ä½¿ç”¨read_pickle to_pickle çš„gzipå‹ç¼©æ¨¡å¼å­˜å‚¨ã€‚
5ï¼‰ä½¿ç”¨tornadoå¼€å‘webç³»ç»Ÿï¼Œæ”¯æŒæ¯æ—¥è‚¡ç¥¨æ•°æ®-ä¸œè´¢ï¼Œé¾™è™æ¦œ-ä¸ªè‚¡ä¸Šæ¦œ-æ–°æµªï¼Œæ•°æ®ä¸­å¿ƒ-å¤§å®—äº¤æ˜“è¡Œæƒ…ç­‰ã€‚
6ï¼‰æ•°æ®å±•ç¤ºç³»ç»Ÿï¼Œæ˜¯é€šç”¨æ•°æ®å±•ç¤ºç³»ç»Ÿï¼Œé…ç½®å­—å…¸æ¨¡æ¿ä¹‹åï¼Œé¡µé¢è‡ªåŠ¨åŠ è½½æ•°æ®ï¼Œå¹¶å®Œæˆæ•°æ®å±•ç¤ºï¼Œåç»­è‡ªå·±å¼€å‘çš„æŒ‡æ ‡æ•°æ®å¯ä»¥åŠ å…¥è¿›å»ã€‚
7ï¼‰å¢åŠ æ›²çº¿æ•°æ®åˆ†æï¼Œåœ¨æŸ¥çœ‹è‚¡ç¥¨ä¸­ï¼Œå¯ä»¥ç›´æ¥è·³è½¬åˆ°ä¸œæ–¹è´¢å¯Œé¡µé¢æŸ¥çœ‹ç›¸å…³ä¿¡æ¯ï¼Œç‚¹å‡»æŒ‡æ ‡ä¹‹åä½¿ç”¨Bokehå°†å¤šè¾¾ 17 ä¸ªæŒ‡æ ‡çš„æ•°æ®ç»˜å›¾ï¼Œè¿›è¡Œå›¾è¡¨å±•ç¤ºã€‚
8) 2.0 æœ€å¤§çš„æ›´æ–°åœ¨äºæ›¿æ¢tushareåº“ï¼ˆå› éƒ¨åˆ†åº“ä¸èƒ½ä½¿ç”¨ï¼‰ï¼Œä½¿ç”¨akshareè¿›è¡Œæ•°æ®æŠ“å–ã€‚

åŸºç¡€åº“ç‰ˆæœ¬
"
tenacity,"Tenacity
========
.. image:: https://img.shields.io/pypi/v/tenacity.svg
    :target: https://pypi.python.org/pypi/tenacity

.. image:: https://circleci.com/gh/jd/tenacity.svg?style=svg
    :target: https://circleci.com/gh/jd/tenacity

.. image:: https://img.shields.io/endpoint.svg?url=https://api.mergify.com/badges/jd/tenacity&style=flat
   :target: https://mergify.io
   :alt: Mergify Status

**Please refer to the** `tenacity documentation <https://tenacity.readthedocs.io/en/latest/>`_ **for a better experience.**

Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in
Python, to simplify the task of adding retry behavior to just about anything.
It originates from `a fork of retrying
<https://github.com/rholder/retrying/issues/65>`_ which is sadly no longer
`maintained <https://julien.danjou.info/python-tenacity/>`_. Tenacity isn't
api compatible with retrying but adds significant new functionality and
fixes a number of longstanding bugs.

The simplest use case"
cryptography,"pyca/cryptography
=================

.. image:: https://img.shields.io/pypi/v/cryptography.svg
    :target: https://pypi.org/project/cryptography/
    :alt: Latest Version

.. image:: https://readthedocs.org/projects/cryptography/badge/?version=latest
    :target: https://cryptography.io
    :alt: Latest Docs

.. image:: https://github.com/pyca/cryptography/workflows/CI/badge.svg?branch=main
    :target: https://github.com/pyca/cryptography/actions?query=workflow%3ACI+branch%3Amain


``cryptography`` is a package which provides cryptographic recipes and
primitives to Python developers. Our goal is for it to be your ""cryptographic
standard library"". It supports Python 3.7+ and PyPy3 7.3.11+.

``cryptography`` includes both high level recipes and low level interfaces to
common cryptographic algorithms such as symmetric ciphers, message digests, and
key derivation functions. For example, to encrypt something with
``cryptography``'s high level symmetric encryption recipe:

.. code-block:: "
streaming-llm,"# Efficient Streaming Language Models with Attention Sinks 
[[paper](http://arxiv.org/abs/2309.17453)] [[slides](assets/StreamingLLM.pdf)][[video](https://youtu.be/hvJsEzP34o8)]

![schemes](figures/schemes.png)

https://github.com/mit-han-lab/streaming-llm/assets/40906949/2bd1cda4-a0bd-47d1-a023-fbf7779b8358

## TL;DR
We deploy LLMs for infinite-length inputs without sacrificing efficiency and performance.

## News

- [2024/02] StreamingLLM is covered by [MIT News as a spotlight](https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213)!
- [2024/01] StreamingLLM is integrated by HPC-AI Tech [SwiftInfer](https://github.com/hpcaitech/SwiftInfer) to support infinite input length for LLM inference.
- [2024/01] StreamingLLM is integrated by NVIDIA [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm)!
- [2023/12] StreamingLLM is integrated by CMU, UW, and OctoAI, enabling endless and efficient LLM generati"
DjangoBlog,"# DjangoBlog

ğŸŒ
*[English](/docs/README-en.md) âˆ™ [ç®€ä½“ä¸­æ–‡](README.md)*

åŸºäº`python3.10`å’Œ`Django4.0`çš„åšå®¢ã€‚   

[![Django CI](https://github.com/liangliangyy/DjangoBlog/actions/workflows/django.yml/badge.svg)](https://github.com/liangliangyy/DjangoBlog/actions/workflows/django.yml) [![CodeQL](https://github.com/liangliangyy/DjangoBlog/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/liangliangyy/DjangoBlog/actions/workflows/codeql-analysis.yml) [![codecov](https://codecov.io/gh/liangliangyy/DjangoBlog/branch/master/graph/badge.svg)](https://codecov.io/gh/liangliangyy/DjangoBlog)  [![license](https://img.shields.io/github/license/liangliangyy/djangoblog.svg)]()  

## ä¸»è¦åŠŸèƒ½ï¼š
- æ–‡ç« ï¼Œé¡µé¢ï¼Œåˆ†ç±»ç›®å½•ï¼Œæ ‡ç­¾çš„æ·»åŠ ï¼Œåˆ é™¤ï¼Œç¼–è¾‘ç­‰ã€‚æ–‡ç« ã€è¯„è®ºåŠé¡µé¢æ”¯æŒ`Markdown`ï¼Œæ”¯æŒä»£ç é«˜äº®ã€‚
- æ”¯æŒæ–‡ç« å…¨æ–‡æœç´¢ã€‚
- å®Œæ•´çš„è¯„è®ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬å‘è¡¨å›å¤è¯„è®ºï¼Œä»¥åŠè¯„è®ºçš„é‚®ä»¶æé†’ï¼Œæ”¯æŒ`Markdown`ã€‚
- ä¾§è¾¹æ åŠŸèƒ½ï¼Œæœ€æ–°æ–‡ç« ï¼Œæœ€å¤šé˜…è¯»ï¼Œæ ‡ç­¾äº‘ç­‰ã€‚
- æ”¯æŒOauthç™»é™†ï¼Œç°å·²æœ‰Google,GitHub,facebook,å¾®åš,QQç™»å½•ã€‚
- æ”¯æŒ`Redis`ç¼“å­˜ï¼Œæ”¯æŒç¼“å­˜è‡ªåŠ¨åˆ·æ–°ã€‚
- ç®€å•çš„SEOåŠŸèƒ½ï¼Œæ–°å»ºæ–‡ç« ç­‰ä¼šè‡ªåŠ¨é€šçŸ¥Googleå’Œç™¾åº¦ã€‚
- é›†æˆäº†ç®€å•çš„å›¾åºŠåŠŸèƒ½ã€‚
- é›†æˆ`django-compressor`ï¼Œè‡ªåŠ¨å‹ç¼©`css`ï¼Œ`js`ã€‚
- ç½‘ç«™å¼‚å¸¸é‚®ä»¶æé†’ï¼Œè‹¥æœ‰æœªæ•æ‰"
django-extensions,"===================
 Django Extensions
===================

.. image:: https://img.shields.io/pypi/l/django-extensions.svg
   :target: https://raw.githubusercontent.com/django-extensions/django-extensions/master/LICENSE

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/compile_catalog.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/linters.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/precommit.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/pytest.yml/badge.svg
    :target: https://github.com/django-extensions/django-extensions/actions

.. image:: https://github.com/django-e"
pyWhat,"<p align='center'>
<img src='images/logo.png'>
<p align=""center"">â¡ï¸ <a href=""http://discord.skerritt.blog"">Discord</a> â¬…ï¸<br>
<i>The easiest way to identify anything</i><br>
<code>pip3 install pywhat && pywhat --help</code>
</p>

<p align=""center"">
  <a href=""http://discord.skerritt.blog""><img alt=""Discord"" src=""https://img.shields.io/discord/754001738184392704""></a> <a href=""https://pypi.org/project/pywhat/""><img alt=""PyPI - Downloads"" src=""https://pepy.tech/badge/pywhat/month""></a>  <a href=""https://twitter.com/bee_sec_san""><img alt=""Twitter Follow"" src=""https://img.shields.io/twitter/follow/bee_sec_san?style=social""></a> <a href=""https://pypi.org/project/pywhat/""><img alt=""PyPI - Python Version"" src=""https://img.shields.io/pypi/pyversions/pywhat""></a> <a href=""https://pypi.org/project/pywhat/""><img alt=""PyPI"" src=""https://img.shields.io/pypi/v/pywhat""></a>
</p>
<hr>

# ğŸ¤” `What` is this?

![](images/main_demo.gif)

Imagine this: You come across some mysterious text ğŸ§™â€â™‚ï¸ `0x5290840009"
ipex-llm,"> [!IMPORTANT]
> ***`bigdl-llm` has now become `ipex-llm` (see the migration guide [here](docs/mddocs/Quickstart/bigdl_llm_migration.md)); you may find the original `BigDL` project [here](https://github.com/intel-analytics/BigDL-2.x).***
 
---

#  ğŸ’« IntelÂ® LLM Library for PyTorch* 
<p>
  <b>< English</b> | <a href='./README.zh-CN.md'>ä¸­æ–‡</a> >
</p>

**`IPEX-LLM`** is a PyTorch library for running **LLM** on Intel CPU and GPU *(e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max)* with very low latency[^1]. 
> [!NOTE]
> - *It is built on top of the excellent work of **`llama.cpp`**, **`transformers`**, **`bitsandbytes`**, **`vLLM`**, **`qlora`**, **`AutoGPTQ`**, **`AutoAWQ`**, etc.*
> - *It provides seamless integration with [llama.cpp](docs/mddocs/Quickstart/llama_cpp_quickstart.md), [Ollama](docs/mddocs/Quickstart/ollama_quickstart.md), [Text-Generation-WebUI](docs/mddocs/Quickstart/webui_quickstart.md), [HuggingFace transformers](python/llm/example/GPU/HuggingFace), [Lang"
lm-evaluation-harness,"# Language Model Evaluation Harness

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10256836.svg)](https://doi.org/10.5281/zenodo.10256836)

---

*Latest News ğŸ“£*

- [2024/09] We are prototyping allowing users of LM Evaluation Harness to create and evaluate on text+image multimodal input, text output tasks, and have just added the `hf-multimodal` and `vllm-vlm` model types and `mmmu` task as a prototype feature. We welcome users to try out this in-progress feature and stress-test it for themselves, and suggest they check out [`lmms-eval`](https://github.com/EvolvingLMMs-Lab/lmms-eval), a wonderful project originally forking off of the lm-evaluation-harness, for a broader range of multimodal tasks, models, and features.
- [2024/07] [API model](docs/API_guide.md) support has been updated and refactored, introducing support for batched and async requests, and making it significantly easier to customize and use for your own purposes. **To run Llama 405B, we recommend using VLLM's OpenA"
SlowFast,"# PySlowFast

PySlowFast is an open source video understanding codebase from FAIR that provides state-of-the-art video classification models with efficient training. This repository includes implementations of the following methods:

- [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982)
- [Non-local Neural Networks](https://arxiv.org/abs/1711.07971)
- [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1912.00998)
- [X3D: Progressive Network Expansion for Efficient Video Recognition](https://arxiv.org/abs/2004.04730)
- [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)
- [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)
- [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526)
- [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)
- [Masked Autoencod"
r0capture,"# r0capture

å®‰å“åº”ç”¨å±‚æŠ“åŒ…é€šæ€è„šæœ¬

## ç®€ä»‹

- ä»…é™å®‰å“å¹³å°ï¼Œæµ‹è¯•å®‰å“7ã€8ã€9ã€10ã€11ã€12ã€13ã€14 å¯ç”¨ ï¼›
- æ— è§†æ‰€æœ‰è¯ä¹¦æ ¡éªŒæˆ–ç»‘å®šï¼Œä¸ç”¨è€ƒè™‘ä»»ä½•è¯ä¹¦çš„äº‹æƒ…ï¼›
- é€šæ€TCP/IPå››å±‚æ¨¡å‹ä¸­çš„åº”ç”¨å±‚ä¸­çš„å…¨éƒ¨åè®®ï¼›
- é€šæ€åè®®åŒ…æ‹¬ï¼šHttp,WebSocket,Ftp,Xmpp,Imap,Smtp,Protobufç­‰ç­‰ã€ä»¥åŠå®ƒä»¬çš„SSLç‰ˆæœ¬ï¼›
- é€šæ€æ‰€æœ‰åº”ç”¨å±‚æ¡†æ¶ï¼ŒåŒ…æ‹¬HttpUrlConnectionã€Okhttp1/3/4ã€Retrofit/Volleyç­‰ç­‰ï¼›
- æ— è§†åŠ å›ºï¼Œä¸ç®¡æ˜¯æ•´ä½“å£³è¿˜æ˜¯äºŒä»£å£³æˆ–VMPï¼Œä¸ç”¨è€ƒè™‘åŠ å›ºçš„äº‹æƒ…ï¼›
- å¦‚æœæœ‰æŠ“ä¸åˆ°çš„æƒ…å†µæ¬¢è¿æissueï¼Œæˆ–è€…ç›´æ¥åŠ vxï¼šr0ysueï¼Œè¿›è¡Œåé¦ˆ~

### June.18th 2023 updateï¼šæµ‹è¯•Pixel4/å®‰å“13/KernelSU/Frida16 åŠŸèƒ½å·¥ä½œæ­£å¸¸ æ­£å¸¸æŠ“åŒ… å¯¼å‡ºè¯ä¹¦

### January.14th 2021 updateï¼šå¢åŠ å‡ ä¸ªè¾…åŠ©åŠŸèƒ½

- å¢åŠ Appæ”¶å‘åŒ…å‡½æ•°å®šä½åŠŸèƒ½
- å¢åŠ Appå®¢æˆ·ç«¯è¯ä¹¦å¯¼å‡ºåŠŸèƒ½
- æ–°å¢hostè¿æ¥æ–¹å¼â€œ-Hâ€ï¼Œç”¨äºFrida-serverç›‘å¬åœ¨éæ ‡å‡†ç«¯å£æ—¶çš„è¿æ¥

## ç”¨æ³•

- æ¨èç¯å¢ƒï¼š[https://github.com/r0ysue/AndroidSecurityStudy/blob/master/FRIDA/A01/README.md](https://github.com/r0ysue/AndroidSecurityStudy/blob/master/FRIDA/A01/README.md)

åˆ‡è®°ä»…é™å®‰å“å¹³å°7ã€8ã€9ã€10ã€11 å¯ç”¨ ï¼Œç¦æ­¢ä½¿ç”¨æ¨¡æ‹Ÿå™¨ã€‚

- Spawn æ¨¡å¼ï¼š

`$ python3 r0capture.py -U -f com.coolapk.market -v`

- Attach æ¨¡å¼ï¼ŒæŠ“åŒ…å†…å®¹ä¿å­˜æˆpcapæ–‡ä»¶ä¾›åç»­åˆ†æï¼š 

`$ python3 r0capture.py -U é…·å®‰ -v -p iqiyi.pcap` 

å»ºè®®ä½¿ç”¨`Attach`æ¨¡å¼ï¼Œä»æ„Ÿå…´è¶£çš„åœ°æ–¹å¼€å§‹æŠ“åŒ…ï¼Œå¹¶ä¸”ä¿å­˜æˆ`pcap`æ–‡ä»¶ï¼Œä¾›åç»­ä½¿ç”¨Wiresharkè¿›è¡Œåˆ†æã€‚
> è€ç‰ˆæœ¬Fridaä½¿ç”¨åŒ…åï¼Œæ–°ç‰ˆæœ¬Fridaä½¿ç”¨APPåã€‚APPåå¿…é¡»æ˜¯ç‚¹å¼€appåï¼Œfrida-ps -"
pkuseg-python,"# pkusegï¼šä¸€ä¸ªå¤šé¢†åŸŸä¸­æ–‡åˆ†è¯å·¥å…·åŒ… [**(English Version)**](readme/readme_english.md)

pkuseg æ˜¯åŸºäºè®ºæ–‡[[Luo et. al, 2019](#è®ºæ–‡å¼•ç”¨)]çš„å·¥å…·åŒ…ã€‚å…¶ç®€å•æ˜“ç”¨ï¼Œæ”¯æŒç»†åˆ†é¢†åŸŸåˆ†è¯ï¼Œæœ‰æ•ˆæå‡äº†åˆ†è¯å‡†ç¡®åº¦ã€‚



## ç›®å½•

* [ä¸»è¦äº®ç‚¹](#ä¸»è¦äº®ç‚¹)
* [ç¼–è¯‘å’Œå®‰è£…](#ç¼–è¯‘å’Œå®‰è£…)
* [å„ç±»åˆ†è¯å·¥å…·åŒ…çš„æ€§èƒ½å¯¹æ¯”](#å„ç±»åˆ†è¯å·¥å…·åŒ…çš„æ€§èƒ½å¯¹æ¯”)
* [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
* [è®ºæ–‡å¼•ç”¨](#è®ºæ–‡å¼•ç”¨)
* [ä½œè€…](#ä½œè€…)
* [å¸¸è§é—®é¢˜åŠè§£ç­”](#å¸¸è§é—®é¢˜åŠè§£ç­”)



## ä¸»è¦äº®ç‚¹

pkusegå…·æœ‰å¦‚ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š

1. å¤šé¢†åŸŸåˆ†è¯ã€‚ä¸åŒäºä»¥å¾€çš„é€šç”¨ä¸­æ–‡åˆ†è¯å·¥å…·ï¼Œæ­¤å·¥å…·åŒ…åŒæ—¶è‡´åŠ›äºä¸ºä¸åŒé¢†åŸŸçš„æ•°æ®æä¾›ä¸ªæ€§åŒ–çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æ ¹æ®å¾…åˆ†è¯æ–‡æœ¬çš„é¢†åŸŸç‰¹ç‚¹ï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±åœ°é€‰æ‹©ä¸åŒçš„æ¨¡å‹ã€‚ æˆ‘ä»¬ç›®å‰æ”¯æŒäº†æ–°é—»é¢†åŸŸï¼Œç½‘ç»œé¢†åŸŸï¼ŒåŒ»è¯é¢†åŸŸï¼Œæ—…æ¸¸é¢†åŸŸï¼Œä»¥åŠæ··åˆé¢†åŸŸçš„åˆ†è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ä½¿ç”¨ä¸­ï¼Œå¦‚æœç”¨æˆ·æ˜ç¡®å¾…åˆ†è¯çš„é¢†åŸŸï¼Œå¯åŠ è½½å¯¹åº”çš„æ¨¡å‹è¿›è¡Œåˆ†è¯ã€‚å¦‚æœç”¨æˆ·æ— æ³•ç¡®å®šå…·ä½“é¢†åŸŸï¼Œæ¨èä½¿ç”¨åœ¨æ··åˆé¢†åŸŸä¸Šè®­ç»ƒçš„é€šç”¨æ¨¡å‹ã€‚å„é¢†åŸŸåˆ†è¯æ ·ä¾‹å¯å‚è€ƒ [**example.txt**](https://github.com/lancopku/pkuseg-python/blob/master/example.txt)ã€‚
2. æ›´é«˜çš„åˆ†è¯å‡†ç¡®ç‡ã€‚ç›¸æ¯”äºå…¶ä»–çš„åˆ†è¯å·¥å…·åŒ…ï¼Œå½“ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ï¼Œpkusegå¯ä»¥å–å¾—æ›´é«˜çš„åˆ†è¯å‡†ç¡®ç‡ã€‚
3. æ”¯æŒç”¨æˆ·è‡ªè®­ç»ƒæ¨¡å‹ã€‚æ”¯æŒç”¨æˆ·ä½¿ç”¨å…¨æ–°çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
4. æ”¯æŒè¯æ€§æ ‡æ³¨ã€‚


## ç¼–è¯‘å’Œå®‰è£…

- ç›®å‰**ä»…æ”¯æŒpython3**
- **ä¸ºäº†è·å¾—å¥½çš„æ•ˆæœå’Œé€Ÿåº¦ï¼Œå¼ºçƒˆå»ºè®®å¤§å®¶é€šè¿‡pip installæ›´æ–°åˆ°ç›®å‰çš„æœ€æ–°ç‰ˆæœ¬**

1. é€šè¿‡PyPIå®‰è£…(è‡ªå¸¦æ¨¡å‹æ–‡ä»¶)ï¼š
	```
	pip3 install pkuseg
	ä¹‹åé€šè¿‡import pkusegæ¥å¼•ç”¨
	```
   **å»ºè®®æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬**ä»¥è·å¾—æ›´å¥½çš„å¼€ç®±ä½“éªŒï¼š
   	```
	pip3 install -U pkuseg
	```
2. å¦‚æœPyPIå®˜æ–¹æºä¸‹è½½é€Ÿåº¦ä¸ç†æƒ³ï¼Œå»ºè®®ä½¿ç”¨é•œåƒæºï¼Œæ¯”å¦‚ï¼š   
  "
mycroft-core,"[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE.md) 
[![CLA](https://img.shields.io/badge/CLA%3F-Required-blue.svg)](https://mycroft.ai/cla) 
[![Team](https://img.shields.io/badge/Team-Mycroft_Core-violetblue.svg)](https://github.com/MycroftAI/contributors/blob/master/team/Mycroft%20Core.md) 
![Status](https://img.shields.io/badge/-Production_ready-green.svg)

![Unit Tests](https://github.com/mycroftai/mycroft-core/workflows/Unit%20Tests/badge.svg)
[![codecov](https://codecov.io/gh/MycroftAI/mycroft-core/branch/dev/graph/badge.svg?token=zQzRlkXxAr)](https://codecov.io/gh/MycroftAI/mycroft-core)

# This project is no longer actively maintained

Mycroft core is no longer maintaiend and probably likely not work on your computer anymore. [Open Voice OS](https://openvoiceos.org) and [Neon-core](https://github.com/NeonGeckoCom/NeonCore) are both spiritual successors to Mycroft. (And some of the old code may live on there.)

# Old Readme

Mycroft is a hackable"
lerobot,"<p align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""media/lerobot-logo-thumbnail.png"">
    <source media=""(prefers-color-scheme: light)"" srcset=""media/lerobot-logo-thumbnail.png"">
    <img alt=""LeRobot, Hugging Face Robotics Library"" src=""media/lerobot-logo-thumbnail.png"" style=""max-width: 100%;"">
  </picture>
  <br/>
  <br/>
</p>

<div align=""center"">

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shield"
boltons,"# Boltons

*boltons should be builtins.*

<a href=""https://boltons.readthedocs.io/en/latest/""><img src=""https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat""></a>
<a href=""https://pypi.python.org/pypi/boltons""><img src=""https://img.shields.io/pypi/v/boltons.svg""></a>
<a href=""https://anaconda.org/conda-forge/boltons""><img src=""https://img.shields.io/conda/vn/conda-forge/boltons.svg""></a>
<a href=""https://ports.macports.org/port/py-boltons/summary""><img src=""https://repology.org/badge/version-for-repo/macports/python:boltons.svg?header=MacPorts""></a>
<a href=""https://pypi.python.org/pypi/boltons""><img src=""https://img.shields.io/pypi/pyversions/boltons.svg""></a>
<a href=""http://calver.org""><img src=""https://img.shields.io/badge/calver-YY.MINOR.MICRO-22bfda.svg""></a>

**Boltons** is a set of over 230 BSD-licensed, pure-Python utilities
in the same spirit as â€” and yet conspicuously missing from â€”
[the standard library][stdlib], including:

  * [Atomic file saving][atomic], "
watchdog,"Watchdog
========

|Build Status|
|CirrusCI Status|

Python API and shell utilities to monitor file system events.

Works on 3.9+.

Example API Usage
-----------------

A simple program that uses watchdog to monitor directories specified
as command-line arguments and logs events generated:

.. code-block:: python

    import time

    from watchdog.events import FileSystemEvent, FileSystemEventHandler
    from watchdog.observers import Observer


    class MyEventHandler(FileSystemEventHandler):
        def on_any_event(self, event: FileSystemEvent) -> None:
            print(event)


    event_handler = MyEventHandler()
    observer = Observer()
    observer.schedule(event_handler, ""."", recursive=True)
    observer.start()
    try:
        while True:
            time.sleep(1)
    finally:
        observer.stop()
        observer.join()


Shell Utilities
---------------

Watchdog comes with an *optional* utility script called ``watchmedo``.
Please type ``watchmedo --help`` at the shel"
aws-sam-cli,"<p align=""center"">
</p>

# AWS SAM CLI

![Apache 2.0 License](https://img.shields.io/github/license/aws/aws-sam-cli)
![SAM CLI Version](https://img.shields.io/github/release/aws/aws-sam-cli.svg?label=CLI%20Version)
![Install](https://img.shields.io/badge/brew-aws/tap/aws--sam--cli-orange)
![pip](https://img.shields.io/badge/pip-aws--sam--cli-9cf)

[Installation](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html) | [Blogs](https://serverlessland.com/blog?tag=AWS%20SAM) | [Videos](https://serverlessland.com/video?tag=AWS%20SAM) | [AWS Docs](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html) | [Roadmap](https://github.com/aws/aws-sam-cli/wiki/SAM-CLI-Roadmap) | [Try It Out](https://s12d.com/jKo46elk) | [Slack Us](https://join.slack.com/t/awsdevelopers/shared_invite/zt-yryddays-C9fkWrmguDv0h2EEDzCqvw)

The AWS Serverless Application Model (SAM) CLI is an open-source CLI tool that help"
PyTorch-VAE,"<h1 align=""center"">
  <b>PyTorch VAE</b><br>
</h1>

<p align=""center"">
      <a href=""https://www.python.org/"">
        <img src=""https://img.shields.io/badge/Python-3.5-ff69b4.svg"" /></a>
       <a href= ""https://pytorch.org/"">
        <img src=""https://img.shields.io/badge/PyTorch-1.3-2BAF2B.svg"" /></a>
       <a href= ""https://github.com/AntixK/PyTorch-VAE/blob/master/LICENSE.md"">
        <img src=""https://img.shields.io/badge/license-Apache2.0-blue.svg"" /></a>
         <a href= ""https://twitter.com/intent/tweet?text=PyTorch-VAE:%20Collection%20of%20VAE%20models%20in%20PyTorch.&url=https://github.com/AntixK/PyTorch-VAE"">
        <img src=""https://img.shields.io/twitter/url/https/shields.io.svg?style=social"" /></a>

</p>

**Update 22/12/2021:** Added support for PyTorch Lightning 1.5.6 version and cleaned up the code.

A collection of Variational AutoEncoders (VAEs) implemented in pytorch with focus on reproducibility. The aim of this project is to provide
a quick and simple working "
TikTokDownload,"
![é¡¹ç›®å›¾](https://tvax2.sinaimg.cn/large/006908GAly1hgn9zod1yuj30zk0hstmf.jpg)

<h1 align=""center"">âœ¨ æŠ–éŸ³å»æ°´å°ä½œå“ä¸‹è½½ âœ¨</h1>
<div align=""center"">

[English](README-EN.md) | ç®€ä½“ä¸­æ–‡

[![License: MIT](https://img.shields.io/github/license/johnserf-seed/tiktokdownload?style=for-the-badge)](https://github.com/Johnserf-Seed/TikTokDownload/blob/main/LICENSE)
![Release Download](https://img.shields.io/github/downloads/Johnserf-Seed/TikTokDownload/total?style=for-the-badge)
![GitHub Repo size](https://img.shields.io/github/repo-size/Johnserf-Seed/TikTokDownload?style=for-the-badge&color=3cb371)
[![GitHub Repo Languages](https://img.shields.io/github/languages/top/Johnserf-Seed/TikTokDownload?style=for-the-badge)](https://github.com/BeyondDimension/SteamTools/search?l=c%23)
[![Python v3.11.1](https://img.shields.io/badge/python-v3.11.1-orange?style=for-the-badge)](https://github.com/Johnserf-Seed/TikTokDownload)
![Terminal: wt](https://img.shields.io/badge/Terminal-wt-blue?style=for-the-badge)

[![GitHub S"
boto,"####
Deprecation notice
####

**This package is no longer maintained and has been replaced by** `Boto3 <https://github.com/boto/boto3>`__.
**Issues and pull requests are not reviewed. If you are having an issue with the** `Boto3 <https://github.com/boto/boto3>`__ **package or the** `AWS CLI <https://github.com/aws/aws-cli>`__, **please open an issue on their respective repositories.**

####
boto
####
boto 2.49.0

Released: 11-July-2018

.. image:: https://pypip.in/d/boto/badge.svg
        :target: https://pypi.python.org/pypi/boto/


************
Introduction
************

Boto is a Python package that provides interfaces to Amazon Web Services.
Currently, all features work with Python 2.6 and 2.7. Work is under way to
support Python 3.3+ in the same codebase. Modules are being ported one at
a time with the help of the open source community, so please check below
for compatibility with Python 3.3+.

To port a module to Python 3.3+, please view our `Contributing Guidelines`_
and the `Po"
point-e,"# PointÂ·E

![Animation of four 3D point clouds rotating](point_e/examples/paper_banner.gif)

This is the official code and model release for [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751).

# Usage

Install with `pip install -e .`.

To get started with examples, see the following notebooks:

 * [image2pointcloud.ipynb](point_e/examples/image2pointcloud.ipynb) - sample a point cloud, conditioned on some example synthetic view images.
 * [text2pointcloud.ipynb](point_e/examples/text2pointcloud.ipynb) - use our small, worse quality pure text-to-3D model to produce 3D point clouds directly from text descriptions. This model's capabilities are limited, but it does understand some simple categories and colors.
 * [pointcloud2mesh.ipynb](point_e/examples/pointcloud2mesh.ipynb) - try our SDF regression model for producing meshes from point clouds.

For our P-FID and P-IS evaluation scripts, see:

 * [evaluate_pfid.py](point_e/evals/scr"
ngxtop,"================================================================
``ngxtop`` - **real-time** metrics for nginx server (and others)
================================================================

**ngxtop** parses your nginx access log and outputs useful, ``top``-like, metrics of your nginx server.
So you can tell what is happening with your server in real-time.

    ``ngxtop`` is designed to run in a short-period time just like the ``top`` command for troubleshooting and monitoring
    your Nginx server at the moment. If you need a long running monitoring process or storing your webserver stats in external
    monitoring / graphing system, you can try `Luameter <https://luameter.com>`_.

``ngxtop`` tries to determine the correct location and format of nginx access log file by default, so you can just run
``ngxtop`` and having a close look at all requests coming to your nginx server. But it does not limit you to nginx
and the default top view. ``ngxtop`` is flexible enough for you to c"
marimo,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg"">
</p>

<p align=""center"">
  <em>A reactive Python notebook that's reproducible, git-friendly, and deployable as scripts or apps.</em>

<p align=""center"">
  <a href=""https://docs.marimo.io"" target=""_blank""><strong>Docs</strong></a> Â·
  <a href=""https://discord.gg/JE7nhX6mD8"" target=""_blank""><strong>Discord</strong></a> Â·
  <a href=""https://github.com/marimo-team/marimo/tree/main/examples"" target=""_blank""><strong>Examples</strong></a>
</p>

<p align=""center"">
  <b>English | </b>
  <a href=""https://github.com/marimo-team/marimo/blob/main/README_Chinese.md"" target=""_blank""><b>ç®€ä½“ä¸­æ–‡</b></a>
</p>

<p align=""center"">
<a href=""https://pypi.org/project/marimo/""><img src=""https://img.shields.io/pypi/v/marimo?color=%2334D058&label=pypi"" /></a>
<a href=""https://anaconda.org/conda-forge/marimo""><img src=""https://img.shields.io/conda/vn/conda-forge/marimo.svg""/></a>
<a href"
grip,"Grip -- GitHub Readme Instant Preview
=====================================

[![Current version on PyPI](http://img.shields.io/pypi/v/grip.svg)][pypi]
[![Say Thanks!](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/joeyespo)

Render local readme files before sending off to GitHub.

Grip is a command-line server application written in Python that uses the
[GitHub markdown API][markdown] to render a local readme file. The styles
and rendering come directly from GitHub, so you'll know exactly how it will appear.
Changes you make to the Readme will be instantly reflected in the browser without
requiring a page refresh.


Motivation
----------

Sometimes you just want to see the exact readme
result before committing and pushing to GitHub.

Especially when doing [Readme-driven development][rdd].


Installation
------------

To install grip, simply:

```console
$ pip install grip
```

On OS X, you can also install with Homebrew:

```console
$ brew install grip"
buku,"<h1 align=""center"">buku</h1>

<p align=""center"">
<a href=""https://github.com/jarun/buku/releases/latest""><img src=""https://img.shields.io/github/release/jarun/buku.svg?maxAge=600"" alt=""Latest release"" /></a>
<a href=""https://repology.org/project/buku/versions""><img src=""https://repology.org/badge/tiny-repos/buku.svg?header=repos"" alt=""Availability""></a>
<a href=""https://pypi.org/project/buku/""><img src=""https://img.shields.io/pypi/v/buku.svg?maxAge=600"" alt=""PyPI"" /></a>
<a href=""https://circleci.com/gh/jarun/workflows/buku""><img src=""https://img.shields.io/circleci/project/github/jarun/buku.svg"" alt=""Build Status"" /></a>
<a href=""https://buku.readthedocs.io/en/latest/?badge=latest""><img src=""https://readthedocs.org/projects/buku/badge/?version=latest"" alt=""Docs Status"" /></a>
<a href=""https://en.wikipedia.org/wiki/Privacy-invasive_software""><img src=""https://img.shields.io/badge/privacy-âœ“-crimson"" alt=""Privacy Awareness"" /></a>
<a href=""https://github.com/jarun/buku/blob/master/LICENS"
pyinstrument,"pyinstrument
============

[![PyPI version](https://badge.fury.io/py/pyinstrument.svg)](https://badge.fury.io/py/pyinstrument)
[![.github/workflows/test.yml](https://github.com/joerick/pyinstrument/actions/workflows/test.yml/badge.svg)](https://github.com/joerick/pyinstrument/actions/workflows/test.yml)
[![Build wheels](https://github.com/joerick/pyinstrument/actions/workflows/wheels.yml/badge.svg)](https://github.com/joerick/pyinstrument/actions/workflows/wheels.yml)

[Documentation](https://pyinstrument.readthedocs.io/)

<!-- MARK intro start -->

[![Screenshot](https://github.com/joerick/pyinstrument/raw/main/docs/img/screenshot.jpg)](https://github.com/joerick/pyinstrument/raw/main/docs/img/screenshot.jpg)

Pyinstrument is a Python profiler. A profiler is a tool to help you optimize
your code - make it faster. To get the biggest speed increase you should
[focus on the slowest part of your program](https://en.wikipedia.org/wiki/Amdahl%27s_law).
Pyinstrument helps you find it!

> â˜•ï¸ "
sqlglot,"![SQLGlot logo](sqlglot.png)

SQLGlot is a no-dependency SQL parser, transpiler, optimizer, and engine. It can be used to format SQL or translate between [23 different dialects](https://github.com/tobymao/sqlglot/blob/main/sqlglot/dialects/__init__.py) like [DuckDB](https://duckdb.org/), [Presto](https://prestodb.io/) / [Trino](https://trino.io/), [Spark](https://spark.apache.org/) / [Databricks](https://www.databricks.com/), [Snowflake](https://www.snowflake.com/en/), and [BigQuery](https://cloud.google.com/bigquery/). It aims to read a wide variety of SQL inputs and output syntactically and semantically correct SQL in the targeted dialects.

It is a very comprehensive generic SQL parser with a robust [test suite](https://github.com/tobymao/sqlglot/blob/main/tests/). It is also quite [performant](#benchmarks), while being written purely in Python.

You can easily [customize](#custom-dialects) the parser, [analyze](#metadata) queries, traverse expression trees, and programmatically [bu"
metaseq,"

# Metaseq
A codebase for working with [Open Pre-trained Transformers](projects/OPT), originally forked from [fairseq](https://github.com/facebookresearch/fairseq).


## Community Integrations

### Using OPT with ğŸ¤— Transformers

The OPT 125M--66B models are now available in [Hugging Face Transformers](https://github.com/huggingface/transformers/releases/tag/v4.19.0). You can access them under the `facebook` organization on the [Hugging Face Hub](https://huggingface.co/facebook)

### Using OPT-175B with Alpa

The OPT 125M--175B models are now supported in the [Alpa project](https://alpa-projects.github.io/tutorials/opt_serving.html), which 
enables serving OPT-175B with more flexible parallelisms on older generations of GPUs, such as 40GB A100, V100, T4, M60, etc.

### Using OPT with Colossal-AI

The OPT models are now supported in the [Colossal-AI](https://github.com/hpcaitech/ColossalAI#OPT), which helps users to efficiently and quickly deploy OPT models training and inference, reduc"
isort,"[![isort - isort your imports, so you don't have to.](https://raw.githubusercontent.com/pycqa/isort/main/art/logo_large.png)](https://pycqa.github.io/isort/)

------------------------------------------------------------------------

[![PyPI version](https://badge.fury.io/py/isort.svg)](https://badge.fury.io/py/isort)
[![Test Status](https://github.com/pycqa/isort/workflows/Test/badge.svg?branch=develop)](https://github.com/pycqa/isort/actions?query=workflow%3ATest)
[![Lint Status](https://github.com/pycqa/isort/workflows/Lint/badge.svg?branch=develop)](https://github.com/pycqa/isort/actions?query=workflow%3ALint)
[![Code coverage Status](https://codecov.io/gh/pycqa/isort/branch/main/graph/badge.svg)](https://codecov.io/gh/pycqa/isort)
[![License](https://img.shields.io/github/license/mashape/apistatus.svg)](https://pypi.org/project/isort/)
[![Join the chat at https://gitter.im/timothycrosley/isort](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/timothycrosley/isort?utm_so"
jrnl,"<!--
Copyright Â© 2012-2023 jrnl contributors
License: https://www.gnu.org/licenses/gpl-3.0.html
-->

<p align=""center"">
<a href=""https://jrnl.sh"">
<img align=""center"" src=""https://raw.githubusercontent.com/jrnl-org/jrnl/develop/docs_theme/assets/readme-header.png""/>
</a>
</p>

jrnl
 [![Testing](https://github.com/jrnl-org/jrnl/workflows/Testing/badge.svg)](https://github.com/jrnl-org/jrnl/actions?query=workflow%3ATesting)
 [![Downloads](https://pepy.tech/badge/jrnl)](https://pepy.tech/project/jrnl)
 [![Version](http://img.shields.io/pypi/v/jrnl.svg?style=flat)](https://pypi.python.org/pypi/jrnl/)
 [![Homebrew](https://img.shields.io/homebrew/v/jrnl?style=flat-square)](https://formulae.brew.sh/formula/jrnl)
 [![Gitter](https://img.shields.io/gitter/room/jrnl-org/jrnl)](https://gitter.im/jrnl-org/jrnl)
 [![Changelog](https://img.shields.io/badge/changelog-on%20github-green)](https://github.com/jrnl-org/jrnl/blob/develop/CHANGELOG.md)
====

_To get help, [submit an issue](https://github.c"
uiautomator2,"# uiautomator2
[![PyPI](https://img.shields.io/pypi/v/uiautomator2.svg)](https://pypi.python.org/pypi/uiautomator2)
![PyPI](https://img.shields.io/pypi/pyversions/uiautomator2.svg)
[![codecov](https://codecov.io/gh/openatx/uiautomator2/graph/badge.svg?token=d0ZLkqorBu)](https://codecov.io/gh/openatx/uiautomator2)

QQäº¤æµç¾¤: **815453846**
Discord: <https://discord.gg/PbJhnZJKDd>

> æœ‰æ®µæ—¶é—´æ²¡æœ‰ç»´æŠ¤è¿™ä¸ªé¡¹ç›®äº†ï¼ˆå¯èƒ½æœ‰ä¸¤å¹´äº†ï¼‰ï¼Œä½†æ˜¯æœ€è¿‘å·¥ä½œéœ€è¦åˆé‡æ–°ç ”ç©¶ä¸€ä¸‹AndroidåŸç”Ÿè‡ªåŠ¨åŒ–ï¼Œå½“ç„¶åˆè°ƒç ”äº†Appiumï¼Œå¯¹æ¯”ä¸‹æ¥ä¸€çœ‹ï¼Œå‘ç°uiautomator2è¿™ä¸ªé¡¹ç›®çš„è¿è¡Œé€Ÿåº¦æ˜¯çœŸçš„å¥½å¿«ï¼Œä»æ£€æµ‹å…ƒç´ åˆ°ç‚¹å‡»ï¼Œéƒ½æ˜¯æ¯«ç§’çº§çš„ï¼Œä»£ç ä¹Ÿæ¯”è¾ƒå¥½ç†è§£ã€‚çœŸæ˜¯æ²¡æƒ³åˆ°ä»¥å‰ç«Ÿç„¶å†™å‡ºäº†è¿™ä¹ˆç¥å¥‡çš„é¡¹ç›®ï¼Œè¿™ä¹ˆå¥½çš„é¡¹ç›®æ€ä¹ˆèƒ½è®©å®ƒè½ç°å‘¢ï¼Œå¾—å¥½å¥½æ•´ä¸€æ•´ï¼Œä¸€äº›åƒåœ¾ä»£ç æ¸…ç†æ¸…ç†ã€‚æ‰€ä»¥é¡¹ç›®ç‰ˆæœ¬ä»2.x.xå‡çº§åˆ°äº†3.x.x

è¿˜åœ¨ç”¨2.x.xç‰ˆæœ¬çš„ç”¨æˆ·ï¼Œå¯ä»¥å…ˆçœ‹ä¸€ä¸‹[2to3](docs/2to3.md) å†å†³å®šæ˜¯å¦è¦å‡çº§3.x.x ï¼ˆæˆ‘ä¸ªäººè¿˜æ˜¯éå¸¸å»ºè®®å‡çº§çš„ï¼‰

2åˆ°3æ¯•ç«Ÿæ˜¯å¤§ç‰ˆæœ¬å‡çº§ï¼Œå¾ˆå¤šçš„å‡½æ•°åˆ æ‰äº†ã€‚é¦–å…ˆåˆ æ‰çš„å°±æ˜¯atx-agentï¼Œå…¶æ¬¡è¿˜æœ‰ä¸€å †atx-agentç›¸å…³çš„å‡½æ•°ã€‚åºŸå¼ƒçš„åŠŸèƒ½æ¯”å¦‚init.

å„ç§ä¾èµ–åº“çš„ç‰ˆæœ¬å·

- [![PyPI](https://img.shields.io/pypi/v/uiautomator2.svg?label=uiautomator2)](https://pypi.python.org/pypi/uiautomator2)
- [![PyPI](https://img.shields.io/pypi/v/adbutils.svg?label=adbutils)](https://github.com/openatx/adbutils)
- [![GitHub tag (latest SemVer)](h"
Time-Series-Library,"# Time Series Library (TSLib)
TSLib is an open-source library for deep learning researchers, especially for deep time series analysis.

We provide a neat code base to evaluate advanced deep time series models or develop your model, which covers five mainstream tasks: **long- and short-term forecasting, imputation, anomaly detection, and classification.**

:triangular_flag_on_post:**News** (2024.07) We wrote a comprehensive survey of [[Deep Time Series Models]](https://arxiv.org/abs/2407.13278) with a rigorous benchmark based on TSLib. In this paper, we summarized the design principles of current time series models supported by insightful experiments, hoping to be helpful to future research.

:triangular_flag_on_post:**News** (2024.04) Many thanks for the great work from [frecklebars](https://github.com/thuml/Time-Series-Library/pull/378). The famous sequential model [Mamba](https://arxiv.org/abs/2312.00752) has been included in our library. See [this file](https://github.com/thuml/Time"
krita-ai-diffusion,"<h1><img width=""64px"" src=""ai_diffusion/icons/logo-128.png""> Generative AI <i>for Krita</i></h1>

âœ¨[Features](#features) | â­³ [Download](https://github.com/Acly/krita-ai-diffusion/releases/latest) | ğŸ› ï¸[Installation](https://www.interstice.cloud/plugin) | ğŸï¸ [Video](https://youtu.be/Ly6USRwTHe0) | ğŸ–¼ï¸[Gallery](#gallery) | ğŸ“–[Wiki](https://github.com/Acly/krita-ai-diffusion/wiki) | ğŸ’¬[Discussion](https://github.com/Acly/krita-ai-diffusion/discussions) | ğŸ—£ï¸[Discord](https://discord.gg/pWyzHfHHhU)

This is a plugin to use generative AI in image painting and editing workflows from within Krita. For a more visual introduction, see [**www.interstice.cloud**](https://www.interstice.cloud)

The main goals of this project are:
* **Precision and Control.** Creating entire images from text can be unpredictable.
  To get the result you envision, you can restrict generation to selections,
  refine existing content with a variable degree of strength, focus text on image
  regions, and guide generation wi"
